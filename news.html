<!DOCTYPE html>
<html lang="en">

<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-156955408-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-156955408-1');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>ONNX Runtime | Community</title>
    <link rel="icon" href="./images/ONNXRuntime-Favicon.png" type="image/gif" sizes="16x16">
    <link rel="stylesheet" href="css/fonts.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="css/custom.css">
    <link rel="stylesheet" href="css/responsive.css">
</head>

<body>
    <a class="skip-main" href="#skipMain">Skip to main content</a>
    <div class="main-wrapper">
        <div class="top-banner-bg">
            <header class="fixed-top header-content">
                <nav class="navbar navbar-expand-md navbar-custom" aria-label="Main menu">
                    <a id="ONNXLogo" class="navbar-brand" href="./index.html">
                        <img src="images/svg/ONNX-Runtime-logo.svg" class="d-inline-block align-top onnx-logo" alt="ONNX Runtime Home" />
                    </a>
                    <button class="navbar-toggler p-0" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <div class="collapse navbar-collapse border-md-top mt-md-0 mt-2" id="navbarNav">
                        <div class="mr-auto"></div>
                        <div class="my-md-2 mb-0 mt-2 my-lg-0 pl-3 pl-md-0">
                            <ul class="navbar-nav navbar-nav mr-auto text-uppercase" id="navigation">
                                <li class="nav-item">
                                    <a class="nav-link pr-3" href="./index.html#getStartedTable">Get Started</a>
                                </li>
                                <li class="nav-item">
                                    <a class="nav-link pr-3" href="./docs">Docs</a>
                                </li>
                                <li class="nav-item">
                                    <a class="nav-link pr-3" target="_blank" href="https://cloudblogs.microsoft.com/opensource/tag/onnx">News</a>
                                </li>
                                <li class="nav-item">
                                    <a class="nav-link pr-3" href="./about.html">About</a>
                                </li>
								<li class="nav-item dropdown">
									<a class="nav-link dropdown-toggle pr-3" data-toggle="dropdown" href="#" role="button" aria-expanded="false">Community</a>
									<div class="dropdown-menu dropdown-menu-right">
									  <a class="dropdown-item" href="https://www.youtube.com/onnxruntime" target="_blank">YouTube</a>
									  <a class="dropdown-item" href="https://www.linkedin.com/company/77691267/admin/" target="_blank">Linkedin</a>
									  <a class="dropdown-item" href="https://www.tiktok.com/@onnxruntimeai" target="_blank">TikTok</a>
									  <a class="dropdown-item" href="https://twitter.com/onnxruntime" target="_blank">Twitter</a>
									  <a class="dropdown-item" href="./community.html" target="_blank">Testimonials</a>
									</div>
								</li>
                                <li class="nav-item">
                                    <a class="nav-link" target="_blank" href="http://github.com/microsoft/onnxruntime">GitHub</a>
                                </li>
                            </ul>
                        </div>
                    </div>
                </nav>
            </header>

            <div id="skipMain" role="main">
                <div class="outer-container mx-auto py-5">
                    
                    <section class="blue-title-columns py-md-5 pb-4 pt-4 mt-5">
                        <div class="container-fluid">
                            <h1 class="mb-3 blue-text">News & Announcements</h1>
                            </div>

                            <div class="news row blue-title-columns">
								<div class="col-12 col-md-8 mb-4 mb-md-0 blue-text">
									<h3 class="mr-xl-5 mb-3">Live demos of machine learning models with ONNX and Hugging Face Spaces</h3>
                                    <span class="article-date">June 6, 2022</span><br/><br/>
									<span class="article-blurb">
                                        Choosing which machine learning model to use, sharing a model with a colleague, and quickly trying out a model are all reasons why you may find yourself wanting to quickly run inference on a model. You can configure your environment and download Jupyter notebooks, but it would be nicer if there was a way to run a model with even less effort...
                                    </span><br/><br/><a href="https://cloudblogs.microsoft.com/opensource/2022/06/06/live-demos-of-machine-learning-models-with-onnx-and-hugging-face-spaces/" target="_blank" class="link"><span class="link-content">Read
										more</span><span class="link-arrow fa fa-angle-right"></span></a>
								</div>

								<div class="col-12 col-md-4 mb-4 mb-md-0 text-center">
									<div class="news-img pb-4 pb-md-5 mr-xl-5 text-center"><img src="./images/huggingface-spaces.png" alt="Code blurb from tutorial"></div>
								</div>
							</div>
                            
                            <div class="news row blue-title-columns">
								<div class="col-12 col-md-8 mb-4 mb-md-0 blue-text">
									<h3 class="mr-xl-5 mb-3">Optimizing and deploying transformer INT8 inference with ONNX Runtime-TensorRT on NVIDIA GPUs</h3>
                                    <span class="article-date">May 2, 2022</span><br/><br/>
									<span class="article-blurb">
                                        Transformer-based models have revolutionized the natural language processing (NLP) domain. Ever since its inception, transformer architecture has been integrated into models like Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-trained Transformer (GPT) for performing tasks such as text generation or summarization and question and answering to name a few...
                                    </span><br/><br/><a href="https://cloudblogs.microsoft.com/opensource/2022/05/02/optimizing-and-deploying-transformer-int8-inference-with-onnx-runtime-tensorrt-on-nvidia-gpus/" target="_blank" class="link"><span class="link-content">Read
										more</span><span class="link-arrow fa fa-angle-right"></span></a>
								</div>

								<div class="col-12 col-md-4 mb-4 mb-md-0 text-center">
									<div class="news-img pb-4 pb-md-5 mr-xl-5 text-center"><img src="./images/BERT-ORTTRT.png" alt="Graph comparing BERT model with ORT-TRT and PyTorch"></div>
								</div>
							</div>

                            <div class="news row blue-title-columns">
								<div class="col-12 col-md-8 mb-4 mb-md-0 blue-text">
									<h3 class="mr-xl-5 mb-3">Scaling-up PyTorch inference: Serving billions of daily NLP inferences with ONNX Runtime</h3>
                                    <span class="article-date">April 19, 2022</span><br/><br/>
									<span class="article-blurb">
                                        Scale, performance, and efficient deployment of state-of-the-art Deep Learning models are ubiquitous challenges as applied machine learning grows across the industry. We’re happy to see that the ONNX Runtime Machine Learning model inferencing solution we’ve built and use in high-volume Microsoft products and services also resonates with our open source community, enabling new capabilities that drive content relevance and productivity...
                                    </span><br/><br/><a href="https://cloudblogs.microsoft.com/opensource/2022/04/19/scaling-up-pytorch-inference-serving-billions-of-daily-nlp-inferences-with-onnx-runtime/" target="_blank" class="link"><span class="link-content">Read
										more</span><span class="link-arrow fa fa-angle-right"></span></a>
								</div>

								<div class="col-12 col-md-4 mb-4 mb-md-0 text-center">
									<div class="news-img pb-4 pb-md-5 mr-xl-5 text-center"><img src="./images/hypefactors-ort.png" alt="Hypefactors and ORT image"></div>
								</div>
							</div>
                            
                            <div class="news row blue-title-columns">
								<div class="col-12 col-md-8 mb-4 mb-md-0 blue-text">
									<h3 class="mr-xl-5 mb-3">Add AI to mobile applications with Xamarin and ONNX Runtime</h3>
                                    <span class="article-date">December 14, 2021</span><br/><br/>
									<span class="article-blurb">
                                        ONNX Runtime now supports building mobile applications in C# with Xamarin. Support for Android and iOS is included in the ONNX Runtime release 1.10 NuGet package. This enables C# developers to build AI applications for Android and iOS to execute ONNX models on mobile devices with ONNX Runtime...
                                    </span><br/><br/><a href="https://cloudblogs.microsoft.com/opensource/2021/12/14/add-ai-to-mobile-applications-with-xamarin-and-onnx-runtime/" target="_blank" class="link"><span class="link-content">Read
										more</span><span class="link-arrow fa fa-angle-right"></span></a>
								</div>

								<div class="col-12 col-md-4 mb-4 mb-md-0 text-center">
									<div class="news-img pb-4 pb-md-5 mr-xl-5 text-center"><img src="./images/xamarin-ort.png" alt="Xamarin and ORT image"></div>
								</div>
							</div>

                            <div class="news row blue-title-columns">
								<div class="col-12 col-md-8 mb-4 mb-md-0 blue-text">
									<h3 class="mr-xl-5 mb-3">ONNX Runtime Web—running your machine learning model in browser</h3>
                                    <span class="article-date">September 2, 2021</span><br/><br/>
									<span class="article-blurb">
										We are introducing ONNX Runtime Web (ORT Web), a new feature in ONNX Runtime to enable JavaScript developers to run and deploy machine learning models in browsers. It also helps enable new classes of on-device computation. ORT Web will be replacing the soon to be deprecated onnx.js...
									</span><br/><br/><a href="https://cloudblogs.microsoft.com/opensource/2021/09/02/onnx-runtime-web-running-your-machine-learning-model-in-browser/" target="_blank" class="link"><span class="link-content">Read
										more</span><span class="link-arrow fa fa-angle-right"></span></a>
								</div>

								<div class="col-12 col-md-4 mb-4 mb-md-0 text-center">
									<div class="news-img pb-4 pb-md-5 mr-xl-5 text-center"><img src="./images/ort-web.png" alt="ORT Web overview diagram"></div>
								</div>
							</div>

                            <div class="news row blue-title-columns">
								<div class="col-12 col-md-8 mb-4 mb-md-0 blue-text">
									<h3 class="mr-xl-5 mb-3">Accelerate PyTorch transformer model training with ONNX Runtime – a deep dive</h3>
                                    <span class="article-date">July 13, 2021</span><br/><br/>
									<span class="article-blurb">
										ONNX Runtime (ORT) for PyTorch accelerates training large scale models across multiple GPUs with up to 37% increase in training throughput over PyTorch and up to 86% speed up when combined with DeepSpeed...
									</span><br/><br/><a href="https://techcommunity.microsoft.com/t5/azure-ai/accelerate-pytorch-transformer-model-training-with-onnx-runtime/ba-p/2540471" target="_blank" class="link"><span class="link-content">Read
										more</span><span class="link-arrow fa fa-angle-right"></span></a>
								</div>

								<div class="col-12 col-md-4 mb-4 mb-md-0 text-center">
									<div class="news-img pb-4 pb-md-5 mr-xl-5 text-center"><img src="./images/ort-pt-training.png" alt="ORT Training performance"></div>
								</div>
							</div>

							<div class="news row blue-title-columns">
								<div class="col-12 col-md-8 mb-4 mb-md-0 blue-text">
									<h3 class="mr-xl-5 mb-3">Accelerate PyTorch training with torch-ort</h3>
                                    <span class="article-date">July 13, 2021</span><br/><br/>
									<span class="article-blurb">
										With a simple change to your PyTorch training script, you can now speed up training large language models with torch_ort.ORTModule, running on the target hardware of your choice.<br/><br/><p>Training deep learning models requires ever-increasing compute and memory resources. Today we release torch_ort.ORTModule, to accelerate distributed training of PyTorch models, reducing the time and resources needed for training...</p>
									</span><br/><br/><a href="https://cloudblogs.microsoft.com/opensource/2021/07/13/accelerate-pytorch-training-with-torch-ort/" target="_blank" class="link"><span class="link-content">Read
										more</span><span class="link-arrow fa fa-angle-right"></span></a>
								</div>
								<div class="col-12 col-md-4 mb-4 mb-md-0 text-center">
									<div class="news-img pb-4 pb-md-5 mr-xl-5 text-center"><img src="./images/ort-training-code.png" alt="Simple code for ORT Training"></div>
								</div>
							</div>

                            <div class="news row blue-title-columns ">
								<div class="col-12 col-md-8 mb-4 mb-md-0 blue-text">
									<h3 class="mr-xl-5 mb-3">ONNX Runtime release 1.8.1 previews support for accelerated training on AMD GPUs with the AMD ROCm™ Open Software Platform</h3>
                                    <span class="article-date">July 13, 2021</span><br/><br/>
                                    <span class="article-blurb">
										ONNX Runtime is an open-source project that is designed to accelerate machine learning across a wide range of frameworks, operating systems, and hardware platforms. Today, we are excited to announce a preview version of ONNX Runtime in release 1.8.1 featuring support for AMD Instinct™ GPUs facilitated by the AMD ROCm™ open software platform...</span>
										<br/><br/><a href="https://cloudblogs.microsoft.com/opensource/2021/07/13/onnx-runtime-release-1-8-1-previews-support-for-accelerated-training-on-amd-gpus-with-the-amd-rocm-open-software-platform/" target="_blank" class="link"><span class="link-content">Read
										more</span><span class="link-arrow fa fa-angle-right"></span></a>
								</div>
								<div class="col-12 col-md-4 mb-4 mb-md-0 text-center">
									<div class="news-img pb-4 pb-md-5 mr-xl-5 text-center"><img src="./images/ort-rocm.png" alt="ORT and ROCm"></div>
								</div>
							</div>

							<div class="news row blue-title-columns ">
								<div class="col-12 col-md-8 mb-4 mb-md-0 blue-text">
									<h3 class="mr-xl-5 mb-3">Journey to optimize large scale transformer model inference with ONNX Runtime</h3>
                                    <span class="article-date">June 30, 2021</span><br/><br/>
									<span class="article-blurb">
										Large-scale transformer models, such as GPT-2 and GPT-3, are among the most useful self-supervised transformer language models for natural language processing tasks such as language translation, question answering, passage summarization, text generation, and so on...</span><br/>
										<br/><a href="https://cloudblogs.microsoft.com/opensource/2021/06/30/journey-to-optimize-large-scale-transformer-model-inference-with-onnx-runtime/" target="_blank" class="link"><span class="link-content">Read
										more</span><span class="link-arrow fa fa-angle-right"></span></a>
								</div>
								<div class="col-12 col-md-4 mb-4 mb-md-0 text-center">
									<div class="news-img pb-4 pb-md-5 mr-xl-5 text-center"><img src="./images/gptc-vscode.png" alt="Deploying GTP-C in VS Code"></div>
								</div>
							</div>

							<div class="news row blue-title-columns ">
								<div class="col-12 col-md-8 mb-4 mb-md-0 blue-text">
									<h3 class="mr-xl-5 mb-3">SAS and Microsoft collaborate to democratize the use of Deep Learning Models</h3>
									<span class="article-date">March 30, 2021</span><br/><br/>
                                    <span class="article-blurb">
									Artificial Intelligence (AI) developers enjoy the flexibility of choosing a model training framework of their choice. This includes both open-source frameworks as well as vendor-specific ones. While this is great for innovation, it does introduce the challenge of operationalization across different hardware platforms...</span>
									<br/><br/><a href="https://communities.sas.com/t5/SAS-Communities-Library/SAS-and-Microsoft-collaborate-to-democratize-the-use-of-Deep/ta-p/730072" target="_blank" class="link"><span class="link-content">Read
										more</span><span class="link-arrow fa fa-angle-right"></span></a>
								</div>
								<div class="col-12 col-md-4 mb-4 mb-md-0 text-center">
									<div class="news-img pb-4 pb-md-5 mr-xl-5 text-center"><img src="./images/sas-ort.png" alt="ORT and SAS"></div>
								</div>
							
							</div>

							<div class="news row blue-title-columns">
								<div class="col-12 col-md-8 mb-4 mb-md-0 blue-text">
									<h3 class="mr-xl-5 mb-3">Optimizing BERT model for Intel CPU Cores using ONNX runtime default execution provider</h3>
									<span class="article-date">March 1, 2021</span><br/><br/>
                                    <span class="article-blurb">
									The performance improvements provided by ONNX Runtime powered by Intel® Deep Learning Boost: Vector Neural Network Instructions (Intel® DL Boost: VNNI) greatly improves performance of machine learning model execution for developers...
									</span><br/><br/><a href="https://cloudblogs.microsoft.com/opensource/2021/03/01/optimizing-bert-model-for-intel-cpu-cores-using-onnx-runtime-default-execution-provider/" target="_blank" class="link"><span class="link-content">Read
										more</span><span class="link-arrow fa fa-angle-right"></span></a>
								</div>

								<div class="col-12 col-md-4 mb-4 mb-md-0 text-center">
									<div class="news-img pb-4 pb-md-5 mr-xl-5 text-center"><img src="./images/ort_intel.png" alt="ORT and Intel AI"></div>
								</div>
							</div>

							
                        </div>
                    </section>
                </div>
            </div>
	    </div>	    
    </div>
    <!-- Partial footer.html Start-->
    <div w3-include-html="footer.html"></div>
    <!-- Partial footer.html End-->

    <a id="back-to-top" href="JavaScript:void(0);" class="btn btn-lg back-to-top" role="button" aria-label="Back to top"><span class="fa fa-angle-up"></span></a>

    <script src="https://www.w3schools.com/lib/w3.js"></script>
    <script>w3.includeHTML();</script>
    <script src="https://code.jquery.com/jquery-3.4.1.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>
    <script src="./js/custom.js"></script>

</body>

</html>
