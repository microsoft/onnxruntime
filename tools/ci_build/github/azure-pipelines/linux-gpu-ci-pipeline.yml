jobs:
- job: Linux_Build
  timeoutInMinutes: 120
  workspace:
    clean: all
  pool: Linux-CPU-2019
  steps:
  - checkout: self
    clean: true
    submodules: recursive

  - template: templates/get-docker-image-steps.yml
    parameters:
      Dockerfile: tools/ci_build/github/linux/docker/Dockerfile.manylinux2014_cuda11
      Context: tools/ci_build/github/linux/docker
      DockerBuildArgs: "--network=host --build-arg POLICY=manylinux2014 --build-arg PLATFORM=x86_64 --build-arg BASEIMAGE=nvcr.io/nvidia/cuda:11.1.1-cudnn8-devel-centos7 --build-arg DEVTOOLSET_ROOTPATH=/opt/rh/devtoolset-9/root --build-arg PREPEND_PATH=/opt/rh/devtoolset-9/root/usr/bin: --build-arg LD_LIBRARY_PATH_ARG=/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64/dyninst:/opt/rh/devtoolset-9/root/usr/lib/dyninst:/usr/local/lib64 --build-arg BUILD_UID=$( id -u )"
      Repository: onnxruntimecuda11build

  - task: CmdLine@2
    inputs:
      script: |
        mkdir -p $HOME/.onnx
        docker run -e CC=/opt/rh/devtoolset-9/root/usr/bin/cc -e CXX=/opt/rh/devtoolset-9/root/usr/bin/c++ -e CFLAGS="-Wp,-D_FORTIFY_SOURCE=2 -Wp,-D_GLIBCXX_ASSERTIONS -fstack-protector-strong -fstack-clash-protection -fcf-protection -O3 -Wl,--strip-all" -e CXXFLAGS="-Wp,-D_FORTIFY_SOURCE=2 -Wp,-D_GLIBCXX_ASSERTIONS -fstack-protector-strong -fstack-clash-protection -fcf-protection -O3 -Wl,--strip-all" --rm \
          --volume /data/onnx:/data/onnx:ro \
          --volume $(Build.SourcesDirectory):/onnxruntime_src \
          --volume $(Build.BinariesDirectory):/build \
          --volume /data/models:/build/models:ro \
          --volume $HOME/.onnx:/home/onnxruntimedev/.onnx \
          -e ALLOW_RELEASED_ONNX_OPSET_ONLY=0 \
          -e NIGHTLY_BUILD \
          -e BUILD_BUILDNUMBER \
          onnxruntimecuda11build \
            /opt/python/cp36-cp36m/bin/python3 /onnxruntime_src/tools/ci_build/build.py \
              --build_dir /build --cmake_generator Ninja \
              --config Release --update --build \
              --skip_submodule_sync \
              --build_shared_lib \
              --parallel \
              --build_wheel \
              --enable_onnx_tests --use_cuda --cuda_version=11.1 --cuda_home=/usr/local/cuda-11.1 --cudnn_home=/usr/local/cuda-11.1 \
              --enable_pybind --build_java \
              --cmake_extra_defines CMAKE_CUDA_HOST_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/cc  CMAKE_CUDA_ARCHITECTURES=52
      workingDirectory: $(Build.SourcesDirectory)

  - task: CmdLine@2
    inputs:
      script: |
        rm -rf $(Build.BinariesDirectory)/Release/onnxruntime $(Build.BinariesDirectory)/Release/pybind11
        rm -f $(Build.BinariesDirectory)/Release/models
        cd $(Build.BinariesDirectory)/Release
        find -executable -type f > $(Build.BinariesDirectory)/Release/perms.txt

  - task: PublishPipelineArtifact@0
    displayName: 'Publish Pipeline Artifact'
    inputs:
      artifactName: 'drop-linux'
      targetPath: '$(Build.BinariesDirectory)/Release'

  - task: PublishTestResults@2
    displayName: 'Publish unit test results'
    inputs:
      testResultsFiles: '**/*.results.xml'
      searchFolder: '$(Build.BinariesDirectory)'
      testRunTitle: 'Unit Test Run'
    condition: succeededOrFailed()

  - template: templates/component-governance-component-detection-steps.yml
    parameters:
      condition: 'succeeded'

  - task: mspremier.PostBuildCleanup.PostBuildCleanup-task.PostBuildCleanup@3
    displayName: 'Clean Agent Directories'
    condition: always()

- job: Linux_Test
  timeoutInMinutes: 60
  workspace:
    clean: all
  pool: centos7gpu
  dependsOn:
  - Linux_Build
  steps:
  - task: DownloadPipelineArtifact@2
    displayName: 'Download Pipeline Artifact'
    inputs:
      buildType: 'current'
      artifactName: 'drop-linux'
      targetPath: '$(Build.BinariesDirectory)/Release'

  - task: CmdLine@2
    inputs:
      script: |
         set -e -x
         #We assume the machine doesn't gcc and python development header files
         sudo rm -f /build /onnxruntime_src
         sudo ln -s $(Build.SourcesDirectory) /onnxruntime_src
         python3 -m pip uninstall -y ort-gpu-nightly ort_nightly onnxruntime onnxruntime-gpu -qq
         cp $(Build.SourcesDirectory)/tools/ci_build/github/linux/docker/scripts/manylinux/requirements.txt $(Build.BinariesDirectory)/requirements.txt
         #Test ORT with the latest ONNX release.
         sed -i 's/git+http:\/\/github\.com\/onnx\/onnx.*/onnx==1.9.0/' $(Build.BinariesDirectory)/requirements.txt
         python3 -m pip install -r $(Build.BinariesDirectory)/requirements.txt
         python3 -m pip install $(Build.BinariesDirectory)/Release/dist/*.whl
         ln -s /data/models $(Build.BinariesDirectory)
         cd $(Build.BinariesDirectory)/Release
         #Restore file permissions
         xargs -a $(Build.BinariesDirectory)/Release/perms.txt chmod a+x
         cd $(Build.SourcesDirectory)/java
         /usr/local/gradle/bin/gradle "cmakeCheck" "-DcmakeBuildDir=$(Build.BinariesDirectory)/Release" "-DUSE_CUDA=1"
         cd /tmp         
         python3 $(Build.SourcesDirectory)/tools/ci_build/build.py \
              --build_dir $(Build.BinariesDirectory) --cmake_generator Ninja \
              --config Release --test \
              --skip_submodule_sync \
              --build_shared_lib \
              --parallel \
              --build_wheel \
              --enable_onnx_tests --use_cuda --cuda_version=11.1 --cuda_home=/usr/local/cuda-11.1 --cudnn_home=/usr/local/cuda-11.1 \
              --enable_pybind --build_java --ctest_path ''

  - template: templates/component-governance-component-detection-steps.yml
    parameters:
      condition: 'succeeded'