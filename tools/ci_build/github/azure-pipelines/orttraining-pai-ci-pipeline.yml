trigger: none

name: 'orttraining_ci_$(Date:yyyyMMdd)_$(Rev:r)'
pool: 'AMD-GPU'

steps:
- checkout: self
  clean: true

- script: |-
    echo "##vso[task.prependpath]/home/ciagent/conda/bin/"
    echo "##vso[task.prependpath]/home/ciagent/pkg/openmpi-4.0.5/bin/"
    echo '##vso[task.setvariable variable=LD_LIBRARY_PATH]/home/ciagent/pkg/openmpi-4.0.5/lib/'
    eval "$('/home/ciagent/conda/bin/conda' 'shell.bash' 'hook' 2> /dev/null)"
    echo "Selecting GPU based on HIP_VISIBLE_DEVICES=$HIP_VISIBLE_DEVICES"
  displayName: 'Initialize environment'

# update these if the E2E test data changes
- script: |-
    sed -i 's|session_options.use_deterministic_compute = False|session_options.use_deterministic_compute = True|g' \
       orttraining/orttraining/python/training/ortmodule/_graph_execution_manager.py
  displayName: 'Toggle ON deterministic compute mode for ORTModule'

- script: |-
    export ROCM_HOME=/opt/rocm
    python tools/ci_build/build.py \
    --config RelWithDebInfo \
    --enable_training \
    --mpi_home /home/ciagent/pkg/openmpi-4.0.5 \
    --use_rocm \
    --rocm_version=4.2 \
    --rocm_home /opt/rocm \
    --nccl_home /opt/rocm \
    --update \
    --build_dir ./build \
    --build \
    --parallel 8 \
    --build_wheel \
    --skip_tests
  displayName: 'Build onnxruntime'

- script: |-
    cd ./build/RelWithDebInfo &&\
    ../../tools/ci_build/github/pai/pai_test_launcher.sh
  displayName: 'Run onnxruntime unit tests'

- script: |-
    cd ./build/RelWithDebInfo
    export PYTHONPATH=$PWD
    python -m onnxruntime.training.ortmodule.torch_cpp_extensions.install
  displayName: 'Compile torch extensions into build directory'
  condition: succeededOrFailed() # ensure all tests are run

- script: |-
    cd ./build/RelWithDebInfo
    export PYTHONPATH=$PWD
    python \
      /home/ciagent/huggingface-transformers/examples/pytorch/language-modeling/run_mlm.py \
      --model_name_or_path bert-large-uncased \
      --dataset_name wikitext \
      --dataset_config_name wikitext-2-raw-v1 \
      --do_train \
      --max_steps 260 \
      --logging_steps 20 \
      --output_dir ./test-mlm-bbu \
      --overwrite_output_dir \
      --per_device_train_batch_size 8 \
      --fp16 \
      --dataloader_num_workers 1 \
      --ort \
      --skip_memory_metrics
    python ../../orttraining/tools/ci_test/compare_huggingface.py \
      ci-pipeline-actual.json \
      ../../orttraining/tools/ci_test/results/ci-mi100.huggingface.bert-large.json 
  displayName: 'Run Python Hugging-Face BERT-L test'
  condition: succeededOrFailed() # ensure all tests are run

- script: |-
    cd ./build/RelWithDebInfo
    export PYTHONPATH=$PWD
    python \
      /home/ciagent/huggingface-transformers/examples/pytorch/language-modeling/run_clm.py \
      --model_name_or_path gpt2 \
      --dataset_name wikitext \
      --dataset_config_name wikitext-2-raw-v1 \
      --do_train \
      --label_smoothing 0.1 \
      --max_steps 260 \
      --logging_steps 20 \
      --overwrite_output_dir \
      --output_dir ./test-clm \
      --per_device_train_batch_size 8 \
      --fp16 \
      --dataloader_num_workers 1 \
      --ort \
      --skip_memory_metrics
    python ../../orttraining/tools/ci_test/compare_huggingface.py \
      ci-pipeline-actual.json \
      ../../orttraining/tools/ci_test/results/ci-mi100.huggingface.gpt2.json 
  displayName: 'Run Python Hugging-Face GPT2 test'
  condition: succeededOrFailed() # ensure all tests are run

- script: |-
    cd ./build/RelWithDebInfo
    export PYTHONPATH=$PWD
    python \
      /home/ciagent/huggingface-transformers/examples/pytorch/translation/run_translation.py \
      --dataset_name wmt16 \
      --dataset_config ro-en \
      --model_name_or_path facebook/bart-large \
      --output_dir ./tst-translation \
      --do_train \
      --label_smoothing 0.1 \
      --logging_steps 20 \
      --overwrite_output_dir \
      --per_device_train_batch_size 16 \
      --predict_with_generate \
      --source_lang en --target_lang ro \
      --warmup_steps 5 \
      --fp16 \
      --max_steps 260 \
      --dataloader_num_workers 1 \
      --ort \
      --skip_memory_metrics 
    python ../../orttraining/tools/ci_test/compare_huggingface.py \
      ci-pipeline-actual.json \
      ../../orttraining/tools/ci_test/results/ci-mi100.huggingface.bart-large.json 
  displayName: 'Run Python Hugging-Face BART-L test'
  condition: succeededOrFailed() # ensure all tests are run

- script: |-
    cd ./build/RelWithDebInfo
    export PYTHONPATH=$PWD
    python \
      /home/ciagent/huggingface-transformers/examples/pytorch/question-answering/run_qa.py \
      --model_name_or_path roberta-large \
      --dataset_name squad \
      --do_train \
      --per_device_train_batch_size 16 \
      --learning_rate 3e-5 \
      --max_steps 260 \
      --max_seq_length 384 \
      --doc_stride 128 \
      --output_dir ./roberta_res \
      --overwrite_output_dir \
      --logging_steps 20 \
      --fp16 \
      --dataloader_num_workers 1 \
      --ort \
      --skip_memory_metrics
    python ../../orttraining/tools/ci_test/compare_huggingface.py \
      ci-pipeline-actual.json \
      ../../orttraining/tools/ci_test/results/ci-mi100.huggingface.roberta-large.json 
  displayName: 'Run Python Hugging-Face RoBERTa-L test'
  condition: succeededOrFailed() # ensure all tests are run

- script: |-
    cd ./build/RelWithDebInfo
    export PYTHONPATH=$PWD
    python \
      /home/ciagent/huggingface-transformers/examples/pytorch/language-modeling/run_mlm.py \
      --model_name_or_path distilbert-base-uncased \
      --dataset_name wikitext \
      --dataset_config_name wikitext-2-raw-v1 \
      --do_train \
      --max_steps 260 \
      --logging_steps 20 \
      --output_dir ./test-mlm-bbu \
      --overwrite_output_dir \
      --per_device_train_batch_size 32 \
      --fp16 \
      --dataloader_num_workers 1 \
      --ort \
      --skip_memory_metrics
    python ../../orttraining/tools/ci_test/compare_huggingface.py \
      ci-pipeline-actual.json \
      ../../orttraining/tools/ci_test/results/ci-mi100.huggingface.distilbert-base.json 
  displayName: 'Run Python Hugging-Face DistilBERT test'
  condition: succeededOrFailed() # ensure all tests are run

#- script: |-
#    cd ./build/RelWithDebInfo
#    export PYTHONPATH=$PWD
#    python \
#      /home/ciagent/huggingface-transformers/examples/pytorch/text-classification/run_glue.py \
#      --model_name_or_path microsoft/deberta-v2-xxlarge \
#      --task_name MRPC \
#      --do_train \
#      --max_seq_length 128 \
#      --per_device_train_batch_size 4 \
#      --learning_rate 3e-6 \
#      --max_steps 260 \
#      --output_dir ./deberta_res \
#      --overwrite_output_dir \
#      --logging_steps 20 \
#      --fp16 \
#      --dataloader_num_workers 1 \
#      --ort \
#      --skip_memory_metrics 
#  displayName: 'Run Python Hugging-Face DeBERTa-XXL v2 test'
#  condition: succeededOrFailed() # ensure all tests are run

#- script: |-
#    cd ./build/RelWithDebInfo
#    export PYTHONPATH=$PWD
#    python \
#      /home/ciagent/huggingface-transformers/examples/pytorch/translation/run_translation.py \
#      --source_prefix '"translate English to Romanian:"' \
#      --dataset_name wmt16 \
#      --dataset_config ro-en \
#      --model_name_or_path t5-large \
#      --output_dir ./tst-translation \
#      --do_train \
#      --label_smoothing 0.1 \
#      --logging_steps 20 \
#      --overwrite_output_dir \
#      --per_device_train_batch_size 16 \
#      --predict_with_generate \
#      --source_lang en \
#      --target_lang ro \
#      --warmup_steps 5 \
#      --fp16 \
#      --max_steps 260 \
#      --dataloader_num_workers 1 \
#      --ort \
#      --skip_memory_metrics
#    python ../../orttraining/tools/ci_test/compare_huggingface.py \
#      ci-pipeline-actual.json \
#      ../../orttraining/tools/ci_test/results/ci-mi100.huggingface.t5-large.json 
#  displayName: 'Run Python Hugging-Face T5-L test'
#  condition: succeededOrFailed() # ensure all tests are run

# update these if the E2E test data changes
- script: |-
    python orttraining/tools/ci_test/download_azure_blob_archive.py \
      --azure_blob_url https://onnxruntimetestdata.blob.core.windows.net/training/onnxruntime_training_data.zip?snapshot=2020-06-15T23:17:35.8314853Z \
      --target_dir training_e2e_test_data \
      --archive_sha256_digest B01C169B6550D1A0A6F1B4E2F34AE2A8714B52DBB70AC04DA85D371F691BDFF9
  displayName: 'Download onnxruntime_training_data.zip data'

- script: |-
   python orttraining/tools/ci_test/run_batch_size_test.py \
      --binary_dir build/RelWithDebInfo \
      --model_root training_e2e_test_data/models \
     --gpu_sku MI100_32G
  displayName: 'Run C++ BERT-L batch size test'
  condition: succeededOrFailed() # ensure all tests are run

- script: |-
    python orttraining/tools/ci_test/run_bert_perf_test.py \
      --binary_dir build/RelWithDebInfo \
      --model_root training_e2e_test_data/models \
      --training_data_root training_e2e_test_data/data \
      --gpu_sku MI100_32G
  displayName: 'Run C++ BERT-L performance test'
  condition: succeededOrFailed() # ensure all tests are run

- script: |-
    python orttraining/tools/ci_test/run_convergence_test.py \
      --binary_dir build/RelWithDebInfo \
      --model_root training_e2e_test_data/models \
      --training_data_root training_e2e_test_data/data \
      --gpu_sku MI100_32G
  displayName: 'Run C++ BERT-L convergence test'
  condition: succeededOrFailed() # ensure all tests are run
