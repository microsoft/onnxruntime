<!DOCTYPE html>
<html lang="en">

<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-156955408-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());
        gtag('config', 'UA-156955408-1');
    </script>

    <script type="text/javascript" src="https://js.monitor.azure.com/scripts/c/ms.analytics-web-3.min.js">
    </script>

    <script type="text/javascript">
        const analytics = new oneDS.ApplicationInsights();
        var config = {
            instrumentationKey: "360b0e675e0044398fd28c8bdf711b8e-1fe5434d-ee99-4837-99cc-a3a16462d82d-7262",
            channelConfiguration: { // Post channel configuration
                eventsLimitInMem: 50
            },
            propertyConfiguration: { // Properties Plugin configuration 
                env: "PROD" // Environment can be set to PPE or PROD as needed. 
            },
            webAnalyticsConfiguration: { // Web Analytics Plugin configuration
                //urlCollectQuery:true, 
                autoCapture: {
                    scroll: true,
                    pageView: true,
                    onLoad: true,
                    onUnload: true,
                    click: true,
                    resize: true,
                    jsError: true
                }
            }
        };
        //Initialize SDK
        analytics.initialize(config, []);
    </script>

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>ONNX Runtime | Windows Dev Kit 2023</title>
    <link rel="icon" href="./images/ONNXRuntime-Favicon.png" type="image/gif" sizes="16x16">
    <link rel="stylesheet" href="css/fonts.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/4.6.2/css/bootstrap.min.css"
        integrity="sha512-rt/SrQ4UNIaGfDyEXZtNcyWvQeOq0QLygHluFQcSjaGB04IxWhal71tKuzP6K8eYXYB6vJV4pHkXcmFGGQ1/0w=="
        crossorigin="anonymous" referrerpolicy="no-referrer" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css"
        integrity="sha512-iecdLmaskl7CVkqkXNQ/ZH/XLlvWZOJyj7Yy7tcenmpD1ypASozpmT/E0iPtmFIB46ZmdtAc9eNBvH0H/ZpiBw=="
        crossorigin="anonymous" referrerpolicy="no-referrer" />
    <link rel="stylesheet" href="css/custom.css">
    <link rel="stylesheet" href="css/responsive.css">
</head>

<body>
    <a class="skip-main" href="#skipMain">Skip to main content</a>
    <div class="main-wrapper">
        <div class="top-banner-bg">

            <!-- Partial header.html Start-->
            <div w3-include-html="header.html"></div>
            <!-- Partial header.html End-->

            <div role="main" id="skipMain" tabindex="-1">

                <div class="container px-md-4 px-lg-5 pt-5 mx-auto text-center">
                    <h1 class="pt-3 pb-3 pt-md-5 pb-lg-3 px-md-4 px-lg-5 mt-5 mb-0">ONNX Runtime + Windows Dev Kit 2023
                        = NPU powered AI</h1>
                </div>

                <div class="outer-container mx-auto">
                    <section class="pt-4 blue-title-columns">
                        <div class="container-fluid">
                            <div class="row">
                                <div class="col-12 col-md-12 pr-5">
                                    <h2 class="blue-text">Delivering NPU powered AI capabilities in your apps</h2>
                                    <p>Windows Dev Kit 2023, aka Project Volterra, enables developers to build apps that
                                        unlock the power of the NPU hardware to accelerate AI/ML workloads delivering
                                        AI-enhanced features & experiences without compromising app performance.</p>
                                    <p>You can get started now and access the power of the NPU through the open source
                                        and cross-platform ONNX Runtime inference engine making it easy to run AI/ML
                                        models from popular machine learning frameworks like PyTorch and TensorFlow.</p>
                                </div>
                            </div>
                        </div>
                    </section>
                    <hr>
                    <section class="pb-4 pt-4 blue-title-columns">
                        <div class="container-fluid">
                            <div class="row pb-4 pb-md-5">

                                <div class="col-12 col-md-6 mb-4 mb-md-0 text-center pr-10">
                                    <img src="./images/windowsdevkit2023.png" class="img-fluid">
                                </div>
                                <div class="col-12 col-md-6 pr-5">
                                    <h2 class="blue-text">Get started on your Windows Dev Kit 2023 today</h2>
                                    <p>Follow these steps to setup your device to use ONNX Runtime (ORT) with the built
                                        in NPU:
                                    <ol>
                                        <li><a href="https://qpm.qualcomm.com/main/tools/details/qualcomm_ai_engine_direct"
                                                target="_blank">Download</a> the Qualcomm AI Engine Direct SDK (QNN SDK) </li>
                                        <li><a href="https://www.nuget.org/packages/Microsoft.ML.OnnxRuntime.QNN"
                                                target="_blank">Download</a> and install the ONNX Runtime with QNN
                                            package</li>
                                        <li>Start using the ONNX Runtime API in your application.</li>
                                    </ol>
                                    </p><br />
                                    <h2 class="blue-text">Optimizing models for the NPU</h2>
                                    <p><a href="https://onnx.ai" target="_blank">ONNX</a> is a standard format for
                                        representing ML models authored in frameworks like PyTorch, TensorFlow, and
                                        others. ONNX Runtime can run any ONNX model, however to make use of the NPU, you
                                        currently need to quantize the ONNX model to QDQ model.  
                                    <p>See our <a
                                            href="https://github.com/microsoft/onnxruntime-inference-examples/tree/main/c_cxx/QNN_EP/mobilenetv2_classification"
                                            target="_blank">C# tutorial<a> for an example of how this is done.</p>
                                    <p>Many models can be optimized for the NPU using this process. Even if a model
                                        cannot be optimized for NPU, it can still be run by ONNX Runtime
                                        on the CPU.</p>
                                    <br />
                                    <h2 class="blue-text">Getting help</h2>
                                    <p>For help with ONNX Runtime, you can <a
                                            href="https://github.com/microsoft/onnxruntime/discussions"
                                            target="_blank">start a discussion</a> on GitHub or <a
                                            href="https://github.com/microsoft/onnxruntime/issues" target="_blank">file
                                            an issue</a>.
                                    <p>
                                </div>
                            </div>
                        </div>
                    </section>


                </div>
            </div>
        </div>
    </div>
    <!-- Partial footer.html Start-->
    <div w3-include-html="footer.html"></div>
    <!-- Partial footer.html End-->

    <a id="back-to-top" href="JavaScript:void(0);" class="btn btn-lg back-to-top" role="button"
        aria-label="Back to top"><span class="fa fa-angle-up"></span></a>

    <script src="./js/w3.js"></script>
    <script>w3.includeHTML();</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.0/jquery.min.js"
        integrity="sha512-3gJwYpMe3QewGELv8k/BX9vcqhryRdzRMxVfq6ngyWXwo03GFEzjsUm8Q7RZcHPHksttq7/GFoxjCVUjkjvPdw=="
        crossorigin="anonymous" referrerpolicy="no-referrer"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/4.6.2/js/bootstrap.min.js"
        integrity="sha512-7rusk8kGPFynZWu26OKbTeI+QPoYchtxsmPeBqkHIEXJxeun4yJ4ISYe7C6sz9wdxeE1Gk3VxsIWgCZTc+vX3g=="
        crossorigin="anonymous" referrerpolicy="no-referrer"></script>
    <script src="./js/custom.js"></script>

</body>

</html>f
