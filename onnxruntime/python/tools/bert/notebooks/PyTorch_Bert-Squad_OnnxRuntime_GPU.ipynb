{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference PyTorch Bert Model with ONNX Runtime on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you'll be introduced to how to load a Bert model from PyTorch, convert it to ONNX, and inference it for high performance using ONNX Runtime and NVIDIA GPU. In the following sections, we are going to use the Bert model trained with Stanford Question Answering Dataset (SQuAD) dataset as an example. Bert SQuAD model is used in question answering scenarios, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n",
    "\n",
    "This notebook is for GPU inference. For CPU inference, please look at another notebook [Inference PyTorch Bert Model with ONNX Runtime on CPU](PyTorch_Bert-Squad_OnnxRuntime_CPU.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites ##\n",
    "It requires your machine to have a GPU, and a python environment with [PyTorch](https://pytorch.org/) and [OnnxRuntime](https://microsoft.github.io/onnxruntime/) installed before running this notebook.\n",
    "\n",
    "#### GPU Environment Setup using AnaConda\n",
    "\n",
    "First, we install [AnaConda](https://www.anaconda.com/distribution/) in a target machine and open an AnaConda prompt window when it is done. Then run the following commands to create a conda environment. This notebook is tested with PyTorch 1.4 and OnnxRuntime 1.2.0.\n",
    "\n",
    "```console\n",
    "conda create -n gpu_env python=3.6\n",
    "conda activate gpu_env\n",
    "conda install pytorch torchvision cudatoolkit=10.1 -c pytorch\n",
    "pip install onnxruntime-gpu\n",
    "pip install transformers==2.5.1\n",
    "pip install wget psutil onnx pytz pandas py-cpuinfo py3nvml netron\n",
    "conda install jupyter\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "Onnxruntime-gpu need specified version of CUDA and cuDNN. You can find the corresponding version in [release note](https://github.com/microsoft/onnxruntime/releases). If the version is different from above cudatoolkit version, you have to install them separately, and add their bin directories to PATH environment variable (See [CUDA and cuDNN Path](#CUDA-and-cuDNN-Path) below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Pretrained Bert model ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by downloading the SQuAD data file and store them in the specified location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cache_dir = \"./squad\"\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "predict_file_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\"\n",
    "predict_file = os.path.join(cache_dir, \"dev-v1.1.json\")\n",
    "if not os.path.exists(predict_file):\n",
    "    import wget\n",
    "    print(\"Start downloading predict file.\")\n",
    "    wget.download(predict_file_url, predict_file)\n",
    "    print(\"Predict file downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define some constant variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether allow overwriting existing ONNX model and download the latest script from GitHub\n",
    "enable_overwrite = True\n",
    "\n",
    "# Total samples to inference, so that we can get average latency\n",
    "total_samples = 1000\n",
    "\n",
    "# ONNX opset version: 10 or 11\n",
    "opset_version=11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify some model configuration variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fine-tuned large model, the model name is \"bert-large-uncased-whole-word-masking-finetuned-squad\". Here we use bert-base for demo.\n",
    "model_name_or_path = \"bert-base-cased\"\n",
    "max_seq_length = 128\n",
    "doc_stride = 128\n",
    "max_query_length = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start to load model from pretrained. This step could take a few minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 48/48 [00:04<00:00, 10.72it/s]\n",
      "convert squad examples to features: 100%|█████████████████████████████████████████| 1000/1000 [00:08<00:00, 123.84it/s]\n",
      "add example index and unique id: 100%|█████████████████████████████████████████| 1000/1000 [00:00<00:00, 500274.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# The following code is adapted from HuggingFace transformers\n",
    "# https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\n",
    "\n",
    "from transformers import (BertConfig, BertForQuestionAnswering, BertTokenizer)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "config_class, model_class, tokenizer_class = (BertConfig, BertForQuestionAnswering, BertTokenizer)\n",
    "config = config_class.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name_or_path, do_lower_case=True, cache_dir=cache_dir)\n",
    "model = model_class.from_pretrained(model_name_or_path,\n",
    "                                    from_tf=False,\n",
    "                                    config=config,\n",
    "                                    cache_dir=cache_dir)\n",
    "# load some examples\n",
    "from transformers.data.processors.squad import SquadV1Processor\n",
    "\n",
    "processor = SquadV1Processor()\n",
    "examples = processor.get_dev_examples(None, filename=predict_file)\n",
    "\n",
    "from transformers import squad_convert_examples_to_features\n",
    "features, dataset = squad_convert_examples_to_features( \n",
    "            examples=examples[:total_samples], # convert enough examples for this notebook\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length=max_seq_length,\n",
    "            doc_stride=doc_stride,\n",
    "            max_query_length=max_query_length,\n",
    "            is_training=False,\n",
    "            return_dataset='pt'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Export the loaded model ##\n",
    "Once the model is loaded, we can export the loaded PyTorch model to ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported at  ./onnx\\bert-base-cased-squad_opset11.onnx\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"./onnx\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)   \n",
    "export_model_path = os.path.join(output_dir, 'bert-base-cased-squad_opset{}.onnx'.format(opset_version))\n",
    "\n",
    "import torch\n",
    "use_gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
    "\n",
    "# Get the first example data to run the model and export it to ONNX\n",
    "data = dataset[0]\n",
    "inputs = {\n",
    "    'input_ids':      data[0].to(device).reshape(1, max_seq_length),\n",
    "    'attention_mask': data[1].to(device).reshape(1, max_seq_length),\n",
    "    'token_type_ids': data[2].to(device).reshape(1, max_seq_length)\n",
    "}\n",
    "\n",
    "# Set model to inference mode, which is required before exporting the model because some operators behave differently in \n",
    "# inference and training mode.\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "if enable_overwrite or not os.path.exists(export_model_path):\n",
    "    with torch.no_grad():\n",
    "        symbolic_names = {0: 'batch_size', 1: 'max_seq_len'}\n",
    "        torch.onnx.export(model,                                            # model being run\n",
    "                          args=tuple(inputs.values()),                      # model input (or a tuple for multiple inputs)\n",
    "                          f=export_model_path,                              # where to save the model (can be a file or file-like object)\n",
    "                          opset_version=opset_version,                      # the ONNX version to export the model to\n",
    "                          do_constant_folding=True,                         # whether to execute constant folding for optimization\n",
    "                          input_names=['input_ids',                         # the model's input names\n",
    "                                       'input_mask', \n",
    "                                       'segment_ids'],\n",
    "                          output_names=['start', 'end'],                    # the model's output names\n",
    "                          dynamic_axes={'input_ids': symbolic_names,        # variable length axes\n",
    "                                        'input_mask' : symbolic_names,\n",
    "                                        'segment_ids' : symbolic_names,\n",
    "                                        'start' : symbolic_names,\n",
    "                                        'end' : symbolic_names})\n",
    "        print(\"Model exported at \", export_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PyTorch Inference ##\n",
    "Use PyTorch to evaluate an example input for comparison purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch cuda Inference time = 16.54 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Measure the latency. It is not accurate using Jupyter Notebook, it is recommended to use standalone python script.\n",
    "latency = []\n",
    "with torch.no_grad():\n",
    "    for i in range(total_samples):\n",
    "        data = dataset[i]\n",
    "        inputs = {\n",
    "            'input_ids':      data[0].to(device).reshape(1, max_seq_length),\n",
    "            'attention_mask': data[1].to(device).reshape(1, max_seq_length),\n",
    "            'token_type_ids': data[2].to(device).reshape(1, max_seq_length)\n",
    "        }\n",
    "        start = time.time()\n",
    "        outputs = model(**inputs)\n",
    "        latency.append(time.time() - start)\n",
    "print(\"PyTorch {} Inference time = {} ms\".format(device.type, format(sum(latency) * 1000 / len(latency), '.2f')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference ONNX Model with ONNX Runtime ##\n",
    "\n",
    "### CUDA and cuDNN Path\n",
    "onnxruntime-gpu has dependency on [CUDA](https://developer.nvidia.com/cuda-downloads) and [cuDNN](https://developer.nvidia.com/cudnn):\n",
    "\n",
    "* [onnxruntime-gpu v1.2.0](https://github.com/microsoft/onnxruntime/releases/tag/v1.2.0) requires CUDA Runtime 10.1.243 and CUDNN 7.6.5.32.\n",
    "* [onnxruntime-gpu v1.0.0](https://github.com/microsoft/onnxruntime/releases/tag/v1.0.0) ~ v1.1.2 requires CUDA Runtime 10.0 and CUDNN 7.6.\n",
    "\n",
    "During installing PyTorch 1.4, we installed cudatoolkit 10.1.243 in this conda environment. That shall be good for onnxruntime-gpu 1.2.0 in Jupyter Notebook.\n",
    "\n",
    "If you use onnxruntime-gpu 1.0.0 ~ 1.1.2, you will have to install CUDA and CUDNN, then add them to path like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to True when onnxruntime (like onnxruntime-gpu 1.0.0 ~ 1.1.2) cannot be imported.\n",
    "add_cuda_path = False\n",
    "\n",
    "if add_cuda_path:\n",
    "    # Add path of CUDA 10.0 and CUDNN 7.6 for onnxruntime-gpu 1.0.0 ~ 1.1.2\n",
    "    cuda_dir = 'D:/NVidia/CUDA/v10.0/bin'\n",
    "    cudnn_dir = 'D:/NVidia/CUDA/v10.0/bin'\n",
    "    if not (os.path.exists(cuda_dir) and os.path.exists(cudnn_dir)):\n",
    "        raise ValueError(\"Please specify correct path for CUDA and cuDNN. Otherwise onnxruntime cannot be imported.\")\n",
    "    else:\n",
    "        if cuda_dir == cudnn_dir:\n",
    "            os.environ[\"PATH\"] = cuda_dir + ';' + os.environ[\"PATH\"]\n",
    "        else:\n",
    "            os.environ[\"PATH\"] = cuda_dir + ';' + cudnn_dir + ';' + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenMP Environment Variable\n",
    "\n",
    "OpenMP environment variables are optional for GPU inference of standard Bert model. It has little performance impact on Bert model since most nodes are executed in GPU. \n",
    "\n",
    "You can find the best setting based on [Performance Test Tool](#Performance-Test-Tool) result in later part of this notebook.\n",
    "\n",
    "**Attention: Setting environment variables shall be done before importing onnxruntime**. Otherwise, they might not take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional. You can change them according to Performance Test Tool result.\n",
    "#os.environ[\"OMP_NUM_THREADS\"] = '1'\n",
    "#os.environ[\"OMP_WAIT_POLICY\"] = 'PASSIVE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to inference the model with ONNX Runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OnnxRuntime gpu Inference time = 3.72 ms\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import onnxruntime\n",
    "import numpy\n",
    "\n",
    "assert 'CUDAExecutionProvider' in onnxruntime.get_available_providers()\n",
    "device_name = 'gpu'\n",
    "\n",
    "sess_options = onnxruntime.SessionOptions()\n",
    "\n",
    "# Optional: store the optimized graph and view it using Netron to verify that model is fully optimized.\n",
    "# Note that this will increase session creation time so enable it for debugging only.\n",
    "sess_options.optimized_model_filepath = os.path.join(output_dir, \"optimized_model_{}.onnx\".format(device_name))\n",
    "\n",
    "# Please change the value according to best setting in Performance Test Tool result.\n",
    "sess_options.intra_op_num_threads=psutil.cpu_count(logical=True)\n",
    "\n",
    "session = onnxruntime.InferenceSession(export_model_path, sess_options)\n",
    "\n",
    "latency = []\n",
    "for i in range(total_samples):\n",
    "    data = dataset[i]\n",
    "    # Use contiguous array as input might improve performance\n",
    "    ort_inputs = {\n",
    "        'input_ids':  numpy.ascontiguousarray(data[0].cpu().reshape(1, max_seq_length).numpy()),\n",
    "        'input_mask': numpy.ascontiguousarray(data[1].cpu().reshape(1, max_seq_length).numpy()),\n",
    "        'segment_ids': numpy.ascontiguousarray(data[2].cpu().reshape(1, max_seq_length).numpy())\n",
    "    }\n",
    "    start = time.time()\n",
    "    ort_outputs = session.run(None, ort_inputs)\n",
    "    latency.append(time.time() - start)\n",
    "    \n",
    "print(\"OnnxRuntime {} Inference time = {} ms\".format(device_name, format(sum(latency) * 1000 / len(latency), '.2f')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the output of PyTorch and ONNX Runtime. We can see some results are not close. It is because ONNX Runtime uses some approximation in CUDA optimization. Based on our evaluation on SQuAD data set, F1 score is on par for models before and after optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Verifying correctness *****\n",
      "PyTorch and ONNX Runtime output 0 are close: True\n",
      "maximum_diff=0.005700558423995972 average_diff=0.0005014391499571502\n",
      "PyTorch and ONNX Runtime output 1 are close: True\n",
      "maximum_diff=0.0023995935916900635 average_diff=0.00045171938836574554\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Verifying correctness *****\")\n",
    "for i in range(2):    \n",
    "    print('PyTorch and ONNX Runtime output {} are close:'.format(i), numpy.allclose(ort_outputs[i], outputs[i].cpu(), rtol=1e-02, atol=1e-02))\n",
    "    diff = ort_outputs[i] - outputs[i].cpu().numpy()\n",
    "    max_diff = numpy.max(numpy.abs(diff))\n",
    "    avg_diff = numpy.average(numpy.abs(diff))\n",
    "    print(f'maximum_diff={max_diff} average_diff={avg_diff}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with Actual Sequence Length\n",
    "Note that ONNX model is exported using dynamic length axis. It is recommended to use actual sequence input without padding instead of fixed length input for best performance. Let's see how it can be applied to this model.\n",
    "\n",
    "From an example input below, we can see zero padding at the end of each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1293,  1242,  2557,  1127,  1226,  1104,  1103,  3613, 16429,\n",
       "           5235,   136,   102,  3613, 16429,  5988,   170,   107,  1353,  1671,\n",
       "           1992,  1342,   107,  5235,   117,  1107,  1134,  1473,  3683,  3538,\n",
       "           1125,   170,  1476,   118,  1248,  2595,  4086,  1714,  1104,  2965,\n",
       "          15897,  1104,  3613, 16429,   119,  1473,  3683,  3538,  3222,  1149,\n",
       "           2551,  1168, 23759,  1116,  1121,  1506,  1103, 10280,  2231,  1111,\n",
       "           1103,  1714, 16355,   119,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0]],\n",
       "        device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example input (we can see padding). From attention_mask, we can deduce the actual length.\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original sequence length is 128. After removing paddings, the sequence length is reduced. Input with smaller sequence length need less computation, thus we can see there is improvement on inference latency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length 101\n",
      "OnnxRuntime gpu Inference time with actual sequence length = 3.44 ms\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "latency = []\n",
    "lengths = []\n",
    "for i in range(total_samples):\n",
    "    data = dataset[i]\n",
    "    # Instead of using fixed length (128), we can use actual sequence length (less than 128), which helps to get better performance.\n",
    "    actual_sequence_length = sum(data[1].numpy())\n",
    "    lengths.append(actual_sequence_length)\n",
    "    opt_inputs = {\n",
    "        'input_ids':  data[0].numpy()[:actual_sequence_length].reshape(1, actual_sequence_length),\n",
    "        'input_mask': data[1].numpy()[:actual_sequence_length].reshape(1, actual_sequence_length),\n",
    "        'segment_ids': data[2].numpy()[:actual_sequence_length].reshape(1, actual_sequence_length)\n",
    "    }\n",
    "    start = time.time()\n",
    "    opt_outputs = session.run(None, opt_inputs)\n",
    "    latency.append(time.time() - start)\n",
    "print(\"Average length\", statistics.mean(lengths))\n",
    "print(\"OnnxRuntime {} Inference time with actual sequence length = {} ms\".format(device_name, format(sum(latency) * 1000 / len(latency), '.2f')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the output and see whether the results are close.\n",
    "\n",
    "**Note**: Need end-to-end evaluation on performance and accuracy if you use this strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Comparing results with/without paddings *****\n",
      "Output 0 are close: True\n",
      "Output 1 are close: True\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Comparing results with/without paddings *****\")\n",
    "for i in range(2):\n",
    "    print('Output {} are close:'.format(i), numpy.allclose(opt_outputs[i], ort_outputs[i][:,:len(opt_outputs[i][0])], rtol=1e-03, atol=1e-03))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Offline Optimization and Test Tools\n",
    "\n",
    "It is recommended to download the [OnnxRuntime Python Tools for BERT](https://github.com/microsoft/onnxruntime/tree/master/onnxruntime/python/tools/bert), and try them on the exported ONNX models. It could help verify whether the model is fully optimized, and get performance test results.\n",
    "\n",
    "### Download OnnxRuntime Python Tools for Bert\n",
    "You may copy the whole [directory](https://github.com/microsoft/onnxruntime/tree/master/onnxruntime/python/tools/bert) to a sub-directory named bert_scripts for this notebook. The list of script files might need update if import error happens when you run some script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [..............................................................................] 15310 / 15310Downloaded bert_perf_test.py\n",
      "100% [................................................................................] 9571 / 9571Downloaded bert_test_data.py\n",
      "100% [................................................................................] 7272 / 7272Downloaded compare_bert_results.py\n",
      "100% [..............................................................................] 44905 / 44905Downloaded BertOnnxModel.py\n",
      "100% [..............................................................................] 21565 / 21565Downloaded BertOnnxModelKeras.py\n",
      "100% [..............................................................................] 26114 / 26114Downloaded BertOnnxModelTF.py\n",
      "100% [..............................................................................] 22773 / 22773Downloaded OnnxModel.py\n",
      "100% [................................................................................] 7795 / 7795Downloaded bert_model_optimization.py\n",
      "100% [................................................................................] 5885 / 5885Downloaded MachineInfo.py\n"
     ]
    }
   ],
    "source": [
      "import os\n",
      "import wget\n",
      "\n",
      "url_prfix = \"https://raw.githubusercontent.com/microsoft/onnxruntime/master/onnxruntime/python/tools/bert/\"\n",
      "script_files = ['bert_perf_test.py', 'bert_test_data.py', 'compare_bert_results.py', 'BertOnnxModel.py', 'BertOnnxModelKeras.py', 'BertOnnxModelTF.py', 'Gpt2OnnxModel.py', 'OnnxModel.py', 'bert_model_optimization.py', 'MachineInfo.py']\n",
      "\n",
      "script_dir = './bert_scripts'\n",
      "if not os.path.exists(script_dir):\n",
      "    os.makedirs(script_dir)\n",
      "\n",
      "for filename in script_files:\n",
      "    target_file = os.path.join(script_dir, filename)\n",
      "    if enable_overwrite and os.path.exists(target_file):\n",
      "        os.remove(target_file)\n",
      "    if not os.path.exists(target_file):\n",
      "        wget.download(url_prfix + filename, target_file)\n",
      "        print(\"Downloaded\", filename)"
    ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Optimization Script\n",
    "\n",
    "Sometime, some optimization of OnnxRuntime cannot be applied to a Bert model due to different reasons:\n",
    "* A new subgraph pattern is exported, which is not covered by the onnxruntime version users are using. For example, Gelu from PyTorch 1.4 is not fused by OnnxRuntime 1.1.2 (Note: it is covered in OnnxRuntime v1.2.0).\n",
    "* The exported model uses dynamic axis. That impacts shape inference. Without enough shape information, some optimization cannot be applied due to the constraint on the input shape.\n",
    "* Some optimization are not supported by OnnxRuntime, but it is feasible in offline script. Like changing input tensor type from int64 to int32 to avoid extra Cast nodes, or converting model to float16 to achieve better performance in V100 or T4 GPU.\n",
    "\n",
    "We have python script **bert_model_optimization.py**, which is flexible in graph pattern matching and model conversions to tackle these problems.\n",
    "\n",
    "In below example, we can see that the tool provide an extra optimization - SkipLayerNormalization and bias (Add) are not fused in OnnxRuntime due to shape inference.\n",
    "\n",
    "The tool will tell whether a model is fully optimized or not. If not, that means you might need change the script to handle some new subgraph patern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Float32 Model\n",
    "Let us optimize the ONNX model using the script. The first example will output model with float32 to store weights. This is the choice for most GPUs without Tensor Core.\n",
    "\n",
    "If your GPU (like V100 or T4) has Tensor Core, jump to [Float16 Model](#6.-Model-Optimization-with-Float16) section since that will give you better performance than Float32 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_model_optimization.py: Save optimized model by onnxruntime to ./onnx\\bert-base-cased-squad_opset11_ort_gpu.onnx\n",
      "bert_model_optimization.py: Use OnnxRuntime to optimize and save the optimized model to ./onnx\\bert-base-cased-squad_opset11_ort_gpu.onnx\n",
      "    BertOnnxModel.py: Fused LayerNormalization count: 0\n",
      "    BertOnnxModel.py: Fused Reshape count:0\n",
      "    BertOnnxModel.py: Fused SkipLayerNormalization count: 24\n",
      "    BertOnnxModel.py: Fused Attention count:0\n",
      "    BertOnnxModel.py: skip embed layer fusion since mask input is not found\n",
      "    BertOnnxModel.py: Fused SkipLayerNormalization with Bias count:24\n",
      "    BertOnnxModel.py: opset verion: 11\n",
      "        OnnxModel.py: Output model to ./onnx/bert-base-cased-squad_opt_gpu_fp32.onnx\n",
      "    BertOnnxModel.py: EmbedLayer=1, Attention=12, Gelu=12, LayerNormalization=24, Succesful=True\n",
      "bert_model_optimization.py: The output model is fully optimized.\n"
     ]
    }
   ],
   "source": [
    "GPU_OPTION = '--gpu_only' if use_gpu else ''\n",
    "optimized_fp32_model_path = './onnx/bert-base-cased-squad_opt_{}_fp32.onnx'.format('gpu' if use_gpu else 'cpu')\n",
    "%run ./bert_scripts/bert_model_optimization.py --input $export_model_path --output $optimized_fp32_model_path $GPU_OPTION --input_int32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimized Graph\n",
    "We can open the optimized model using [Netron](https://github.com/lutzroeder/netron) to visualize.\n",
    "\n",
    "The graph is like the following:\n",
    "<img src='images/optimized_bert_gpu.png'>\n",
    "\n",
    "Sometime, optimized graph is slightly different. For example, FastGelu is replaced by BiasGelu for CPU inference; When the option --input_int32 is used, Cast nodes for inputs are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netron\n",
    "\n",
    "# change it to True if want to view the optimized model in browser\n",
    "enable_netron = False\n",
    "if enable_netron:\n",
    "    # If you encounter error \"access a socket in a way forbidden by its access permissions\", install Netron as standalone application instead.\n",
    "    netron.start(optimized_fp32_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Test Tool\n",
    "\n",
    "The following will create 1000 random inputs of batch_size 1 and sequence length 128, then measure the average latency and throughput numbers.\n",
    "\n",
    "Note that the test uses fixed sequence length. If you use [dynamic sequence length](#Inference-with-Actual-Sequence-Length), actual performance depends on the distribution of sequence length.\n",
    "\n",
    "Note that this tool measures performance of inference using OnnxRuntime Python API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1000 samples for batch_size=1 sequence_length=128\n",
      "Extra latency for converting inputs to contiguous: 0.01 ms\n",
      "Test summary is saved to onnx\\perf_results_GPU_B1_S128_20200313-152329.txt\n"
     ]
    }
   ],
   "source": [
    "GPU_OPTION = '--use_gpu' if use_gpu else ''\n",
    "\n",
    "%run ./bert_scripts/bert_perf_test.py --model $optimized_fp32_model_path --batch_size 1 --sequence_length 128 --samples 1000 --test_times 1 --inclusive --all $GPU_OPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the summary file and take a look. Note that blank value in OMP_NUM_THREADS or OMP_WAIT_POLICY means the environment variable does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32 model perf results from ./onnx\\perf_results_GPU_B1_S128_20200313-152329.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latency(ms)</th>\n",
       "      <th>Latency_P50</th>\n",
       "      <th>Latency_P75</th>\n",
       "      <th>Latency_P90</th>\n",
       "      <th>Latency_P95</th>\n",
       "      <th>Latency_P99</th>\n",
       "      <th>Throughput(QPS)</th>\n",
       "      <th>intra_op_num_threads</th>\n",
       "      <th>OMP_NUM_THREADS</th>\n",
       "      <th>OMP_WAIT_POLICY</th>\n",
       "      <th>contiguous</th>\n",
       "      <th>warmup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.43</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.44</td>\n",
       "      <td>3.46</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.58</td>\n",
       "      <td>291.59</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.43</td>\n",
       "      <td>3.41</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.45</td>\n",
       "      <td>3.46</td>\n",
       "      <td>3.54</td>\n",
       "      <td>291.58</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.43</td>\n",
       "      <td>3.41</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.45</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.81</td>\n",
       "      <td>291.27</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.44</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.44</td>\n",
       "      <td>3.45</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.57</td>\n",
       "      <td>290.92</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.44</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.45</td>\n",
       "      <td>3.46</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.55</td>\n",
       "      <td>290.46</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.44</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.45</td>\n",
       "      <td>3.46</td>\n",
       "      <td>3.48</td>\n",
       "      <td>3.94</td>\n",
       "      <td>290.31</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.45</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.45</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.49</td>\n",
       "      <td>3.66</td>\n",
       "      <td>290.23</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.45</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.44</td>\n",
       "      <td>3.46</td>\n",
       "      <td>3.48</td>\n",
       "      <td>3.56</td>\n",
       "      <td>289.90</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.45</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.45</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.49</td>\n",
       "      <td>4.07</td>\n",
       "      <td>289.74</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.46</td>\n",
       "      <td>3.45</td>\n",
       "      <td>3.46</td>\n",
       "      <td>3.48</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3.58</td>\n",
       "      <td>289.17</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.46</td>\n",
       "      <td>3.45</td>\n",
       "      <td>3.46</td>\n",
       "      <td>3.48</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3.68</td>\n",
       "      <td>289.01</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.46</td>\n",
       "      <td>3.44</td>\n",
       "      <td>3.45</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.49</td>\n",
       "      <td>3.84</td>\n",
       "      <td>288.98</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.48</td>\n",
       "      <td>3.46</td>\n",
       "      <td>3.48</td>\n",
       "      <td>3.49</td>\n",
       "      <td>3.51</td>\n",
       "      <td>3.67</td>\n",
       "      <td>287.47</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.53</td>\n",
       "      <td>3.52</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.55</td>\n",
       "      <td>3.58</td>\n",
       "      <td>3.80</td>\n",
       "      <td>282.93</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Latency(ms)  Latency_P50  Latency_P75  Latency_P90  Latency_P95  \\\n",
       "0          3.43         3.42         3.44         3.46         3.47   \n",
       "1          3.43         3.41         3.43         3.45         3.46   \n",
       "2          3.43         3.41         3.43         3.45         3.47   \n",
       "3          3.44         3.42         3.44         3.45         3.47   \n",
       "4          3.44         3.43         3.45         3.46         3.47   \n",
       "5          3.44         3.43         3.45         3.46         3.48   \n",
       "6          3.45         3.43         3.45         3.47         3.49   \n",
       "7          3.45         3.43         3.44         3.46         3.48   \n",
       "8          3.45         3.43         3.45         3.47         3.49   \n",
       "9          3.46         3.45         3.46         3.48         3.50   \n",
       "10         3.46         3.45         3.46         3.48         3.50   \n",
       "11         3.46         3.44         3.45         3.47         3.49   \n",
       "12         3.48         3.46         3.48         3.49         3.51   \n",
       "13         3.53         3.52         3.53         3.55         3.58   \n",
       "\n",
       "    Latency_P99  Throughput(QPS)  intra_op_num_threads OMP_NUM_THREADS  \\\n",
       "0          3.58           291.59                     6               1   \n",
       "1          3.54           291.58                     6               1   \n",
       "2          3.81           291.27                     6               6   \n",
       "3          3.57           290.92                     0                   \n",
       "4          3.55           290.46                     1               6   \n",
       "5          3.94           290.31                     1               6   \n",
       "6          3.66           290.23                     6               1   \n",
       "7          3.56           289.90                     1               6   \n",
       "8          4.07           289.74                     6               6   \n",
       "9          3.58           289.17                     1               6   \n",
       "10         3.68           289.01                     6               6   \n",
       "11         3.84           288.98                     6               6   \n",
       "12         3.67           287.47                     6               1   \n",
       "13         3.80           282.93                     0                   \n",
       "\n",
       "   OMP_WAIT_POLICY  contiguous  warmup  \n",
       "0           ACTIVE       False    True  \n",
       "1          PASSIVE       False    True  \n",
       "2           ACTIVE       False    True  \n",
       "3                        False    True  \n",
       "4           ACTIVE        True    True  \n",
       "5          PASSIVE       False    True  \n",
       "6           ACTIVE        True    True  \n",
       "7          PASSIVE        True    True  \n",
       "8          PASSIVE        True    True  \n",
       "9           ACTIVE       False    True  \n",
       "10          ACTIVE        True    True  \n",
       "11         PASSIVE       False    True  \n",
       "12         PASSIVE        True    True  \n",
       "13                        True    True  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment import os\n",
    "import glob     \n",
    "import pandas\n",
    "latest_result_file = max(glob.glob(\"./onnx/perf_results_GPU_B1_S128_*.txt\"), key=os.path.getmtime)\n",
    "result_data = pandas.read_table(latest_result_file, converters={'OMP_NUM_THREADS': str, 'OMP_WAIT_POLICY':str})\n",
    "print(\"Float32 model perf results from\", latest_result_file)\n",
    "# Remove some columns that have same values for all rows.\n",
    "columns_to_remove = ['model', 'graph_optimization_level', 'batch_size', 'sequence_length', 'test_cases', 'test_times', 'use_gpu']\n",
    "result_data.drop(columns_to_remove, axis=1, inplace=True)\n",
    "result_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above result, we can see that latency is very close for different settings. The default setting (intra_op_num_threads=0, OMP_NUM_THREADS and OMP_WAIT_POLICY does not exist) performs almost on par with the best setting. \n",
    "\n",
    "### Model Results Comparison Tool\n",
    "\n",
    "When a BERT model is optimized, some approximation is used in calculation. If your BERT model has three inputs, a script compare_bert_results.py can be used to do a quick verification. The tool will generate some fake input data, and compare the inference outputs of the original and optimized models. If outputs are all close, it is safe to use the optimized model.\n",
    "\n",
    "For GPU inference, the absolute or relative difference is larger than those numbers of CPU inference. Note that slight difference in output will not impact final result. We did end-to-end evaluation using SQuAD data set using a fine-tuned squad model, and F1 score is almost the same before/after optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 out of 100 results not passed for thresholds (rtol=0.01, atol=0.01).\n",
      "maximum absolute difference=0.033498868346214294\n",
      "maximum relative difference=20.89048957824707\n"
     ]
    }
   ],
   "source": [
    "%run ./bert_scripts/compare_bert_results.py --baseline_model $export_model_path --optimized_model $optimized_fp32_model_path --batch_size 1 --sequence_length 128 --samples 100 --rtol 0.01 --atol 0.01 $GPU_OPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Optimization with Float16\n",
    "\n",
    "The bert_model_optimization.py script have an option **--float16** to convert model to use float16 to store weights. After the conversion, it could be faster to run in GPU with tensor cores like V100 or T4.\n",
    "\n",
    "Let's run tools to measure the performance on V100. The results show significant performance improvement: latency is about 3.4 ms for float32 model, and 1.8 ms for float16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_model_optimization.py: Save optimized model by onnxruntime to ./onnx\\bert-base-cased-squad_opset11_ort_gpu.onnx\n",
      "bert_model_optimization.py: Use OnnxRuntime to optimize and save the optimized model to ./onnx\\bert-base-cased-squad_opset11_ort_gpu.onnx\n",
      "    BertOnnxModel.py: Fused LayerNormalization count: 0\n",
      "    BertOnnxModel.py: Fused Reshape count:0\n",
      "    BertOnnxModel.py: Fused SkipLayerNormalization count: 24\n",
      "    BertOnnxModel.py: Fused Attention count:0\n",
      "    BertOnnxModel.py: skip embed layer fusion since mask input is not found\n",
      "    BertOnnxModel.py: Fused SkipLayerNormalization with Bias count:24\n",
      "    BertOnnxModel.py: opset verion: 11\n",
      "        OnnxModel.py: Output model to ./onnx/bert-base-cased-squad_opt_gpu_fp16.onnx\n",
      "    BertOnnxModel.py: EmbedLayer=1, Attention=12, Gelu=12, LayerNormalization=24, Succesful=True\n",
      "bert_model_optimization.py: The output model is fully optimized.\n"
     ]
    }
   ],
   "source": [
    "GPU_OPTION = '--gpu_only' if use_gpu else ''\n",
    "optimized_fp16_model_path = './onnx/bert-base-cased-squad_opt_{}_fp16.onnx'.format('gpu' if use_gpu else 'cpu')\n",
    "%run  ./bert_scripts/bert_model_optimization.py --input $export_model_path --output $optimized_fp16_model_path $GPU_OPTION --float16 --input_int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1000 samples for batch_size=1 sequence_length=128\n",
      "Extra latency for converting inputs to contiguous: 0.00 ms\n",
      "Test summary is saved to onnx\\perf_results_GPU_B1_S128_20200313-152527.txt\n"
     ]
    }
   ],
   "source": [
    "GPU_OPTION = '--use_gpu' if use_gpu else ''\n",
    "%run ./bert_scripts/bert_perf_test.py --model $optimized_fp16_model_path --batch_size 1 --sequence_length 128 --samples 1000 --test_times 1 --inclusive --all $GPU_OPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32 model perf results from ./onnx\\perf_results_GPU_B1_S128_20200313-152527.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latency(ms)</th>\n",
       "      <th>Latency_P50</th>\n",
       "      <th>Latency_P75</th>\n",
       "      <th>Latency_P90</th>\n",
       "      <th>Latency_P95</th>\n",
       "      <th>Latency_P99</th>\n",
       "      <th>Throughput(QPS)</th>\n",
       "      <th>intra_op_num_threads</th>\n",
       "      <th>OMP_NUM_THREADS</th>\n",
       "      <th>OMP_WAIT_POLICY</th>\n",
       "      <th>contiguous</th>\n",
       "      <th>warmup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.84</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.00</td>\n",
       "      <td>544.94</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.85</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.88</td>\n",
       "      <td>2.11</td>\n",
       "      <td>541.89</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.85</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1.84</td>\n",
       "      <td>1.87</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.12</td>\n",
       "      <td>541.84</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.85</td>\n",
       "      <td>1.84</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.87</td>\n",
       "      <td>1.89</td>\n",
       "      <td>2.12</td>\n",
       "      <td>541.64</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.85</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1.84</td>\n",
       "      <td>1.86</td>\n",
       "      <td>1.89</td>\n",
       "      <td>2.09</td>\n",
       "      <td>541.61</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.85</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1.84</td>\n",
       "      <td>1.86</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.07</td>\n",
       "      <td>541.00</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.85</td>\n",
       "      <td>1.84</td>\n",
       "      <td>1.84</td>\n",
       "      <td>1.86</td>\n",
       "      <td>1.89</td>\n",
       "      <td>2.18</td>\n",
       "      <td>540.81</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.85</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1.84</td>\n",
       "      <td>1.86</td>\n",
       "      <td>1.89</td>\n",
       "      <td>2.08</td>\n",
       "      <td>540.38</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.85</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1.84</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.88</td>\n",
       "      <td>2.16</td>\n",
       "      <td>540.01</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.86</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1.84</td>\n",
       "      <td>1.87</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.14</td>\n",
       "      <td>538.67</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.86</td>\n",
       "      <td>1.84</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.87</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.24</td>\n",
       "      <td>538.10</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.86</td>\n",
       "      <td>1.84</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.87</td>\n",
       "      <td>1.89</td>\n",
       "      <td>2.29</td>\n",
       "      <td>537.35</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.86</td>\n",
       "      <td>1.88</td>\n",
       "      <td>1.93</td>\n",
       "      <td>2.31</td>\n",
       "      <td>533.32</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.88</td>\n",
       "      <td>1.84</td>\n",
       "      <td>1.87</td>\n",
       "      <td>1.96</td>\n",
       "      <td>2.11</td>\n",
       "      <td>2.47</td>\n",
       "      <td>530.87</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>PASSIVE</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Latency(ms)  Latency_P50  Latency_P75  Latency_P90  Latency_P95  \\\n",
       "0          1.84         1.83         1.83         1.85         1.87   \n",
       "1          1.85         1.82         1.83         1.85         1.88   \n",
       "2          1.85         1.83         1.84         1.87         1.90   \n",
       "3          1.85         1.84         1.85         1.87         1.89   \n",
       "4          1.85         1.83         1.84         1.86         1.89   \n",
       "5          1.85         1.83         1.84         1.86         1.90   \n",
       "6          1.85         1.84         1.84         1.86         1.89   \n",
       "7          1.85         1.83         1.84         1.86         1.89   \n",
       "8          1.85         1.83         1.84         1.85         1.88   \n",
       "9          1.86         1.83         1.84         1.87         1.90   \n",
       "10         1.86         1.84         1.85         1.87         1.90   \n",
       "11         1.86         1.84         1.85         1.87         1.89   \n",
       "12         1.88         1.85         1.86         1.88         1.93   \n",
       "13         1.88         1.84         1.87         1.96         2.11   \n",
       "\n",
       "    Latency_P99  Throughput(QPS)  intra_op_num_threads OMP_NUM_THREADS  \\\n",
       "0          2.00           544.94                     6               1   \n",
       "1          2.11           541.89                     0                   \n",
       "2          2.12           541.84                     6               1   \n",
       "3          2.12           541.64                     1               6   \n",
       "4          2.09           541.61                     0                   \n",
       "5          2.07           541.00                     6               6   \n",
       "6          2.18           540.81                     1               6   \n",
       "7          2.08           540.38                     6               1   \n",
       "8          2.16           540.01                     1               6   \n",
       "9          2.14           538.67                     1               6   \n",
       "10         2.24           538.10                     6               6   \n",
       "11         2.29           537.35                     6               1   \n",
       "12         2.31           533.32                     6               6   \n",
       "13         2.47           530.87                     6               6   \n",
       "\n",
       "   OMP_WAIT_POLICY  contiguous  warmup  \n",
       "0          PASSIVE       False    True  \n",
       "1                        False    True  \n",
       "2           ACTIVE       False    True  \n",
       "3           ACTIVE        True    True  \n",
       "4                         True    True  \n",
       "5           ACTIVE       False    True  \n",
       "6          PASSIVE        True    True  \n",
       "7           ACTIVE        True    True  \n",
       "8           ACTIVE       False    True  \n",
       "9          PASSIVE       False    True  \n",
       "10         PASSIVE        True    True  \n",
       "11         PASSIVE        True    True  \n",
       "12          ACTIVE        True    True  \n",
       "13         PASSIVE       False    True  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob     \n",
    "import pandas\n",
    "latest_result_file = max(glob.glob(\"./onnx/perf_results_GPU_B1_S128_*.txt\"), key=os.path.getmtime)\n",
    "result_data = pandas.read_table(latest_result_file, converters={'OMP_NUM_THREADS': str, 'OMP_WAIT_POLICY':str})\n",
    "print(\"Float32 model perf results from\", latest_result_file)\n",
    "# Remove some columns that have same values for all rows.\n",
    "columns_to_remove = ['model', 'graph_optimization_level', 'batch_size', 'sequence_length', 'test_cases', 'test_times', 'use_gpu']\n",
    "result_data.drop(columns_to_remove, axis=1, inplace=True)\n",
    "result_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Throughput Tuning\n",
    "\n",
    "Some application need best throughput under some constraint on latency. This can be done by testing performance of different batch sizes. The tool could help on this.\n",
    "\n",
    "Here is an example that check the performance of multiple batch sizes (1, 2, 4, 8, 16, 32 and 64) using default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1000 samples for batch_size=32 sequence_length=128\n",
      "Generating 1000 samples for batch_size=1 sequence_length=128\n",
      "Generating 1000 samples for batch_size=2 sequence_length=128\n",
      "Generating 1000 samples for batch_size=64 sequence_length=128\n",
      "Generating 1000 samples for batch_size=4 sequence_length=128\n",
      "Generating 1000 samples for batch_size=8 sequence_length=128\n",
      "Generating 1000 samples for batch_size=16 sequence_length=128\n",
      "Test summary is saved to onnx\\perf_results_GPU_B1-2-4-8-16-32-64_S128_20200313-153014.txt\n"
     ]
    }
   ],
   "source": [
    "GPU_OPTION = '--use_gpu' if use_gpu else ''\n",
    "%run ./bert_scripts/bert_perf_test.py --model $optimized_fp16_model_path --batch_size 1 2 4 8 16 32 64 --sequence_length 128 --samples 1000 --test_times 1 --inclusive $GPU_OPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float16 model summary from ./onnx\\perf_results_GPU_B1-2-4-8-16-32-64_S128_20200313-153014.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latency(ms)</th>\n",
       "      <th>Latency_P50</th>\n",
       "      <th>Latency_P75</th>\n",
       "      <th>Latency_P90</th>\n",
       "      <th>Latency_P95</th>\n",
       "      <th>Latency_P99</th>\n",
       "      <th>Throughput(QPS)</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.84</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.84</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.09</td>\n",
       "      <td>544.95</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.19</td>\n",
       "      <td>2.18</td>\n",
       "      <td>2.19</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.40</td>\n",
       "      <td>914.56</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.89</td>\n",
       "      <td>2.87</td>\n",
       "      <td>2.89</td>\n",
       "      <td>2.90</td>\n",
       "      <td>2.92</td>\n",
       "      <td>3.04</td>\n",
       "      <td>1383.38</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.79</td>\n",
       "      <td>4.76</td>\n",
       "      <td>4.78</td>\n",
       "      <td>4.80</td>\n",
       "      <td>4.84</td>\n",
       "      <td>5.19</td>\n",
       "      <td>1671.42</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8.16</td>\n",
       "      <td>8.15</td>\n",
       "      <td>8.17</td>\n",
       "      <td>8.20</td>\n",
       "      <td>8.22</td>\n",
       "      <td>8.35</td>\n",
       "      <td>1960.21</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14.69</td>\n",
       "      <td>14.66</td>\n",
       "      <td>14.70</td>\n",
       "      <td>14.75</td>\n",
       "      <td>14.80</td>\n",
       "      <td>14.97</td>\n",
       "      <td>2178.49</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>28.43</td>\n",
       "      <td>28.42</td>\n",
       "      <td>28.50</td>\n",
       "      <td>28.61</td>\n",
       "      <td>28.68</td>\n",
       "      <td>28.78</td>\n",
       "      <td>2251.36</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Latency(ms)  Latency_P50  Latency_P75  Latency_P90  Latency_P95  \\\n",
       "0          1.84         1.82         1.84         1.85         1.87   \n",
       "3          2.19         2.18         2.19         2.20         2.22   \n",
       "7          2.89         2.87         2.89         2.90         2.92   \n",
       "11         4.79         4.76         4.78         4.80         4.84   \n",
       "12         8.16         8.15         8.17         8.20         8.22   \n",
       "15        14.69        14.66        14.70        14.75        14.80   \n",
       "18        28.43        28.42        28.50        28.61        28.68   \n",
       "\n",
       "    Latency_P99  Throughput(QPS)  batch_size  \n",
       "0          2.09           544.95           1  \n",
       "3          2.40           914.56           2  \n",
       "7          3.04          1383.38           4  \n",
       "11         5.19          1671.42           8  \n",
       "12         8.35          1960.21          16  \n",
       "15        14.97          2178.49          32  \n",
       "18        28.78          2251.36          64  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob     \n",
    "import pandas\n",
    "latest_result_file = max(glob.glob(\"./onnx/perf_results_*.txt\"), key=os.path.getmtime)\n",
    "result_data = pandas.read_table(latest_result_file, converters={'OMP_NUM_THREADS': str, 'OMP_WAIT_POLICY':str})\n",
    "print(\"Float16 model summary from\", latest_result_file)\n",
    "columns_to_remove = ['model', 'graph_optimization_level', 'test_cases', 'test_times', 'use_gpu', 'warmup', 'sequence_length']\n",
    "\n",
    "# Set it True to see all rows. Here we only see one setting of intra_op_num_threads==0 and no OMP environment variables\n",
    "show_all_rows = False\n",
    "if not show_all_rows:\n",
    "    result_data = result_data.loc[result_data['intra_op_num_threads']==0]\n",
    "    columns_to_remove.extend(['intra_op_num_threads', 'OMP_NUM_THREADS', 'OMP_WAIT_POLICY', 'contiguous'])\n",
    "result_data.drop(columns_to_remove, axis=1, inplace=True)\n",
    "result_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Additional Info\n",
    "\n",
    "Note that running Jupyter Notebook has slight impact on performance result since Jupyter Notebook is using system resources like CPU etc. You can close Jupyter Notebook and other applications, then run the performance test in a console to get more accurate performance numbers.\n",
    "\n",
    "[OnnxRuntime C API](https://github.com/microsoft/onnxruntime/blob/master/docs/C_API.md) could get slightly better performance than python API. If you use C API in inference, you can use OnnxRuntime_Perf_Test.exe built from source to measure performance instead.\n",
    "\n",
    "Here is the machine configuration that generated the above results. You might get slower or faster result according to your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"gpu\": {\n",
      "    \"driver_version\": \"419.69\",\n",
      "    \"devices\": [\n",
      "      {\n",
      "        \"memory_total\": 17048272896,\n",
      "        \"memory_available\": 12573147136,\n",
      "        \"name\": \"Tesla V100-PCIE-16GB\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"cpu\": {\n",
      "    \"brand\": \"Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz\",\n",
      "    \"cores\": 6,\n",
      "    \"logical_cores\": 6,\n",
      "    \"hz\": \"2.5940 GHz\",\n",
      "    \"l2_cache\": \"64\",\n",
      "    \"l3_cache\": \"0 KB\",\n",
      "    \"processor\": \"Intel64 Family 6 Model 79 Stepping 1, GenuineIntel\"\n",
      "  },\n",
      "  \"memory\": {\n",
      "    \"total\": 120258613248,\n",
      "    \"available\": 104351649792\n",
      "  },\n",
      "  \"python\": \"3.6.10.final.0 (64 bit)\",\n",
      "  \"os\": \"Windows-10-10.0.14393-SP0\",\n",
      "  \"onnxruntime\": {\n",
      "    \"version\": \"1.2.0\",\n",
      "    \"support_gpu\": true\n",
      "  },\n",
      "  \"pytorch\": {\n",
      "    \"version\": \"1.4.0\",\n",
      "    \"support_gpu\": true\n",
      "  },\n",
      "  \"tensorflow\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%run ./bert_scripts/MachineInfo.py --silent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch14_gpu",
   "language": "python",
   "name": "torch14_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
