{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python370jvsc74a57bd081098997110362167705b61d21e46dda767ff2050d805c22b6ba90fec7e1aa35",
   "display_name": "Python 3.7.0 64-bit ('py37athena': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "81098997110362167705b61d21e46dda767ff2050d805c22b6ba90fec7e1aa35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "Licensed under the MIT License."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Optimizing runtime performance on GPT-2 model inference with ONNXRuntime on CPU\n",
    "\n",
    "In this tutorial, you'll be introduced to how to load a GPT2 model from PyTorch, convert it to ONNX with one step search, and inference it using ONNX Runtime with/without IO Binding. GPT-2 model inference is optimized by compiling one-step beam search into the onnx compute graph, which speeds up the runtime significantly. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Prerequisites\n",
    "If you have Jupyter Notebook, you may directly run this notebook. We will use pip to install or upgrade [PyTorch](https://pytorch.org/), [OnnxRuntime](https://microsoft.github.io/onnxruntime/) and other required packages.\n",
    "\n",
    "Otherwise, you can setup a new environment. First, we install [Anaconda](https://www.anaconda.com/distribution/). Then open an AnaConda prompt window and run the following commands:\n",
    "\n",
    "```console\n",
    "conda create -n cpu_env python=3.8\n",
    "conda activate cpu_env\n",
    "conda install jupyter\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "The last command will launch Jupyter Notebook and we can open this notebook in browser to continue."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch 1.7.0 and OnnxRuntime 1.7.0 for CPU-only.\n",
    "import sys\n",
    "if sys.platform == 'darwin': # Mac\n",
    "    !{sys.executable} -m pip install --upgrade torch torchvision\n",
    "else:\n",
    "    !{sys.executable} -m pip install --upgrade torch==1.7.0+cpu torchvision==0.8.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!{sys.executable} -m pip install onnxruntime==1.7.2\n",
    "\n",
    "# Install other packages used in this notebook.\n",
    "!{sys.executable} -m pip install transformers==4.3.1\n",
    "!{sys.executable} -m pip install onnx onnxconverter_common psutil pytz pandas py-cpuinfo py3nvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a cache directory to store pretrained model.\n",
    "cache_dir = os.path.join(\".\", \"cache_models\")\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)"
   ]
  },
  {
   "source": [
    "## Convert GPT2 model from PyTorch to ONNX with one step search ##\n",
    "\n",
    "We have a script [convert_to_onnx.py](https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/transformers/convert_to_onnx.py) that could help you to convert GPT2 with past state to ONNX. \n",
    "\n",
    "The script accepts a pretrained model name or path of a checkpoint directory as input, and converts the model to ONNX. It also verifies that the ONNX model could generate same input as the pytorch model. The usage is like \n",
    "```\n",
    "python -m onnxruntime.transformers.convert_to_onnx -m model_name_or_path \\ \n",
    "--model_class=GPT2LMHeadModel_BeamSearchStep|GPT2LMHeadModel_ConfigurableOneStepSearch \\ \n",
    "--output gpt2_onestepsearch.onnx -o -p fp32|fp16|int8\n",
    "```\n",
    "The -p option can be used to choose the precision: fp32 (float32), fp16 (mixed precision) or int8 (quantization). The -o option will generate optimized model, which is required for fp16 or int8.\n",
    "\n",
    "Here we use a pretrained model as example:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GPT2Config {\n  \"_name_or_path\": \"gpt2\",\n  \"activation_function\": \"gelu_new\",\n  \"architectures\": [\n    \"GPT2LMHeadModel\"\n  ],\n  \"attn_pdrop\": 0.1,\n  \"batch_size\": 1,\n  \"beam_size\": 4,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"gradient_checkpointing\": false,\n  \"initializer_range\": 0.02,\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 1024,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 12,\n  \"n_positions\": 1024,\n  \"resid_pdrop\": 0.1,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"task_specific_params\": {\n    \"text-generation\": {\n      \"do_sample\": true,\n      \"max_length\": 50\n    }\n  },\n  \"transformers_version\": \"4.3.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 50257\n}\n\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.transformers.gpt2_beamsearch_helper import Gpt2BeamSearchHelper, GPT2LMHeadModel_BeamSearchStep\n",
    "from transformers import AutoConfig\n",
    "import torch\n",
    "\n",
    "model_name_or_path = \"gpt2\"\n",
    "config = AutoConfig.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "model = GPT2LMHeadModel_BeamSearchStep.from_pretrained(model_name_or_path, config=config, batch_size=1, beam_size=4, cache_dir=cache_dir)\n",
    "device = torch.device(\"cpu\")\n",
    "model.eval().to(device)\n",
    "\n",
    "print(model.config)\n",
    "\n",
    "num_attention_heads = model.config.n_head\n",
    "hidden_size = model.config.n_embd\n",
    "num_layer = model.config.n_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/data/anaconda/envs/py37athena/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py:654: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n  assert batch_size > 0, \"batch_size has to be defined and > 0\"\n/data/anaconda/envs/py37athena/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py:169: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n  w = w / (float(v.size(-1)) ** 0.5)\n/data/anaconda/envs/py37athena/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py:174: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n  mask = self.bias[:, :, ns - nd : ns, :ns]\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = \"gpt2_one_step_search.onnx\"\n",
    "Gpt2BeamSearchHelper.export_onnx(model, device, onnx_model_path) # add parameter use_external_data_format=True when model size > 2 GB"
   ]
  },
  {
   "source": [
    "## ONNX Runtime Inference ##\n",
    "\n",
    "We can use ONNX Runtime to inference. The inputs are dictionary with name and numpy array as value, and the output is list of numpy array. Note that both input and output are in CPU. When you run the inference in GPU, it will involve data copy between CPU and GPU for input and output.\n",
    "\n",
    "Let's create an inference session for ONNX Runtime given the exported ONNX model, and see the output."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "import numpy\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "EXAMPLE_Text = ['best hotel in bay area.']\n",
    "\n",
    "def get_tokenizer(model_name_or_path, cache_dir):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    #okenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    return tokenizer\n",
    "\n",
    "def get_example_inputs(prompt_text=EXAMPLE_Text):    \n",
    "    tokenizer = get_tokenizer(model_name_or_path, cache_dir)\n",
    "    encodings_dict = tokenizer.batch_encode_plus(prompt_text, padding=True)\n",
    "\n",
    "    input_ids = torch.tensor(encodings_dict['input_ids'], dtype=torch.int64)\n",
    "    attention_mask = torch.tensor(encodings_dict['attention_mask'], dtype=torch.float32)\n",
    "    position_ids = (attention_mask.long().cumsum(-1) - 1)\n",
    "    position_ids.masked_fill_(position_ids < 0, 0)\n",
    "\n",
    "    #Empty Past State for generating first word\n",
    "    empty_past = []\n",
    "    batch_size = input_ids.size(0)\n",
    "    sequence_length = input_ids.size(1)\n",
    "    past_shape = [2, batch_size, num_attention_heads, 0, hidden_size // num_attention_heads]\n",
    "    for i in range(num_layer):\n",
    "        empty_past.append(torch.empty(past_shape).type(torch.float32).to(device))\n",
    "       \n",
    "    return input_ids, attention_mask, position_ids, empty_past\n",
    "\n",
    "input_ids, attention_mask, position_ids, empty_past = get_example_inputs()\n",
    "beam_select_idx = torch.zeros([1, input_ids.shape[0]]).long()\n",
    "input_log_probs = torch.zeros([input_ids.shape[0], 1])\n",
    "input_unfinished_sents = torch.ones([input_ids.shape[0], 1], dtype=torch.bool)\n",
    "prev_step_scores = torch.zeros([input_ids.shape[0], 1])\n",
    "\n",
    "onnx_model_path = \"gpt2_one_step_search.onnx\"\n",
    "session = onnxruntime.InferenceSession(onnx_model_path)\n",
    "ort_inputs = {\n",
    "              'input_ids': numpy.ascontiguousarray(input_ids.cpu().numpy()),\n",
    "              'attention_mask' : numpy.ascontiguousarray(attention_mask.cpu().numpy()),\n",
    "              'position_ids': numpy.ascontiguousarray(position_ids.cpu().numpy()),\n",
    "              'beam_select_idx': numpy.ascontiguousarray(beam_select_idx.cpu().numpy()),\n",
    "              'input_log_probs': numpy.ascontiguousarray(input_log_probs.cpu().numpy()),\n",
    "              'input_unfinished_sents': numpy.ascontiguousarray(input_unfinished_sents.cpu().numpy()),\n",
    "              'prev_step_results': numpy.ascontiguousarray(input_ids.cpu().numpy()),\n",
    "              'prev_step_scores': numpy.ascontiguousarray(prev_step_scores.cpu().numpy()),\n",
    "             }\n",
    "for i, past_i in enumerate(empty_past):\n",
    "    ort_inputs[f'past_{i}'] = numpy.ascontiguousarray(past_i.cpu().numpy())\n",
    "ort_outputs = session.run(None, ort_inputs)"
   ]
  },
  {
   "source": [
    "## ONNX Runtime Inference with IO Binding ##\n",
    "\n",
    "To avoid data copy for input and output, ONNX Runtime also supports IO Binding. User could provide some buffer for input and outputs. For GPU inference, the buffer can be in GPU to reduce memory copy between CPU and GPU. This is helpful for high performance inference in GPU. For GPT-2, IO Binding might help the performance when batch size or (past) sequence length is large."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_with_io_binding(session, config, input_ids, position_ids, attention_mask, past, beam_select_idx, input_log_probs, input_unfinished_sents, prev_step_results, prev_step_scores, step, context_len):\n",
    "    output_shapes = Gpt2BeamSearchHelper.get_output_shapes(batch_size=1,\n",
    "                                                           context_len=context_len,\n",
    "                                                           past_sequence_length=past[0].size(3),\n",
    "                                                           sequence_length=input_ids.size(1),\n",
    "                                                           beam_size=4,\n",
    "                                                           step=step,\n",
    "                                                           config=config,\n",
    "                                                           model_class=\"GPT2LMHeadModel_BeamSearchStep\")\n",
    "    output_buffers = Gpt2BeamSearchHelper.get_output_buffers(output_shapes, device)\n",
    "\n",
    "    io_binding = Gpt2BeamSearchHelper.prepare_io_binding(session, input_ids, position_ids, attention_mask, past, output_buffers, output_shapes, beam_select_idx, input_log_probs, input_unfinished_sents, prev_step_results, prev_step_scores)\n",
    "    session.run_with_iobinding(io_binding)\n",
    "\n",
    "    outputs = Gpt2BeamSearchHelper.get_outputs_from_io_binding_buffer(session, output_buffers, output_shapes, return_numpy=False)\n",
    "    return outputs"
   ]
  },
  {
   "source": [
    "We can see that the result is exactly same with/without IO Binding:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "IO Binding result is good\n"
     ]
    }
   ],
   "source": [
    "input_ids, attention_mask, position_ids, empty_past = get_example_inputs()\n",
    "beam_select_idx = torch.zeros([1, input_ids.shape[0]]).long()\n",
    "input_log_probs = torch.zeros([input_ids.shape[0], 1])\n",
    "input_unfinished_sents = torch.ones([input_ids.shape[0], 1], dtype=torch.bool)\n",
    "prev_step_scores = torch.zeros([input_ids.shape[0], 1])\n",
    "outputs = inference_with_io_binding(session, config, input_ids, position_ids, attention_mask, empty_past, beam_select_idx, input_log_probs, input_unfinished_sents, input_ids, prev_step_scores, 0, input_ids.shape[-1])\n",
    "assert torch.eq(outputs[-2], torch.from_numpy(ort_outputs[-2])).all()\n",
    "print(\"IO Binding result is good\")"
   ]
  },
  {
   "source": [
    "## Batch Text Generation ##\n",
    "\n",
    "Here is an example for text generation using ONNX Runtime with/without IO Binding."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(output, step, batch_size, beam_size, context_length, prev_attention_mask, device):\n",
    "    \"\"\"\n",
    "    Update the inputs for next inference.\n",
    "    \"\"\"\n",
    "    last_state = (torch.from_numpy(output[0]).to(device)\n",
    "                        if isinstance(output[0], numpy.ndarray) else output[0].clone().detach().cpu())\n",
    "\n",
    "    input_ids = last_state.view(batch_size * beam_size, -1).to(device)\n",
    "\n",
    "    input_unfinished_sents_id = -3\n",
    "    prev_step_results = (torch.from_numpy(output[-2]).to(device) if isinstance(output[-2], numpy.ndarray)\n",
    "                                else output[-2].clone().detach().to(device))\n",
    "    position_ids = (torch.tensor([context_length + step - 1\n",
    "                                        ]).unsqueeze(0).repeat(batch_size * beam_size, 1).to(device))\n",
    "\n",
    "    if prev_attention_mask.shape[0] != (batch_size * beam_size):\n",
    "        prev_attention_mask = prev_attention_mask.repeat(batch_size * beam_size, 1)\n",
    "    attention_mask = torch.cat(\n",
    "        [\n",
    "            prev_attention_mask,\n",
    "            torch.ones([batch_size * beam_size, 1]).type_as(prev_attention_mask),\n",
    "        ],\n",
    "        1,\n",
    "    ).to(device)\n",
    "\n",
    "    beam_select_idx = (torch.from_numpy(output[input_unfinished_sents_id - 2]).to(device) if isinstance(\n",
    "        output[input_unfinished_sents_id - 2], numpy.ndarray) else output[input_unfinished_sents_id - 2].clone().detach().to(device))\n",
    "    input_log_probs = (torch.from_numpy(output[input_unfinished_sents_id - 1]).to(device) if isinstance(\n",
    "        output[input_unfinished_sents_id - 1], numpy.ndarray) else output[input_unfinished_sents_id - 1].clone().detach().to(device))\n",
    "    input_unfinished_sents = (torch.from_numpy(output[input_unfinished_sents_id]).to(device) if isinstance(\n",
    "        output[input_unfinished_sents_id], numpy.ndarray) else\n",
    "                                    output[input_unfinished_sents_id].clone().detach().to(device))\n",
    "    prev_step_scores = (torch.from_numpy(output[-1]).to(device)\n",
    "                                if isinstance(output[-1], numpy.ndarray) else output[-1].clone().detach().to(device))\n",
    "\n",
    "    past = []\n",
    "    if isinstance(output[1], tuple):  # past in torch output is tuple\n",
    "        past = list(output[1])\n",
    "    else:\n",
    "        for i in range(model.config.n_layer):\n",
    "            past_i = (torch.from_numpy(output[i + 1])\n",
    "                        if isinstance(output[i + 1], numpy.ndarray) else output[i + 1].clone().detach())\n",
    "            past.append(past_i.to(device)) \n",
    "\n",
    "    inputs = {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask' : attention_mask,\n",
    "        'position_ids': position_ids,\n",
    "        'beam_select_idx': beam_select_idx,\n",
    "        'input_log_probs': input_log_probs,\n",
    "        'input_unfinished_sents': input_unfinished_sents,\n",
    "        'prev_step_results': prev_step_results,\n",
    "        'prev_step_scores': prev_step_scores,\n",
    "    }\n",
    "    ort_inputs = {\n",
    "        'input_ids': numpy.ascontiguousarray(input_ids.cpu().numpy()),\n",
    "        'attention_mask' : numpy.ascontiguousarray(attention_mask.cpu().numpy()),\n",
    "        'position_ids': numpy.ascontiguousarray(position_ids.cpu().numpy()),\n",
    "        'beam_select_idx': numpy.ascontiguousarray(beam_select_idx.cpu().numpy()),\n",
    "        'input_log_probs': numpy.ascontiguousarray(input_log_probs.cpu().numpy()),\n",
    "        'input_unfinished_sents': numpy.ascontiguousarray(input_unfinished_sents.cpu().numpy()),\n",
    "        'prev_step_results': numpy.ascontiguousarray(prev_step_results.cpu().numpy()),\n",
    "        'prev_step_scores': numpy.ascontiguousarray(prev_step_scores.cpu().numpy()),\n",
    "    }\n",
    "    for i, past_i in enumerate(past):\n",
    "        ort_inputs[f'past_{i}'] = numpy.ascontiguousarray(past_i.cpu().numpy())\n",
    "    \n",
    "    return inputs, ort_inputs, past\n",
    "\n",
    "def test_generation(tokenizer, input_text, use_onnxruntime_io, ort_session = None, num_tokens_to_produce = 30):\n",
    "    print(\"Text generation using\", \"OnnxRuntime with IO binding\" if use_onnxruntime_io else \"OnnxRuntime\", \"...\")    \n",
    "    input_ids, attention_mask, position_ids, past = get_example_inputs(input_text)\n",
    "    beam_select_idx = torch.zeros([1, input_ids.shape[0]]).long()\n",
    "    input_log_probs = torch.zeros([input_ids.shape[0], 1])\n",
    "    input_unfinished_sents = torch.ones([input_ids.shape[0], 1], dtype=torch.bool)\n",
    "    prev_step_scores = torch.zeros([input_ids.shape[0], 1])\n",
    "    inputs = {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask' : attention_mask,\n",
    "        'position_ids': position_ids,\n",
    "        'beam_select_idx': beam_select_idx,\n",
    "        'input_log_probs': input_log_probs,\n",
    "        'input_unfinished_sents': input_unfinished_sents,\n",
    "        'prev_step_results': input_ids,\n",
    "        'prev_step_scores': prev_step_scores,\n",
    "    }\n",
    "    ort_inputs = {\n",
    "        'input_ids': numpy.ascontiguousarray(input_ids.cpu().numpy()),\n",
    "        'attention_mask' : numpy.ascontiguousarray(attention_mask.cpu().numpy()),\n",
    "        'position_ids': numpy.ascontiguousarray(position_ids.cpu().numpy()),\n",
    "        'beam_select_idx': numpy.ascontiguousarray(beam_select_idx.cpu().numpy()),\n",
    "        'input_log_probs': numpy.ascontiguousarray(input_log_probs.cpu().numpy()),\n",
    "        'input_unfinished_sents': numpy.ascontiguousarray(input_unfinished_sents.cpu().numpy()),\n",
    "        'prev_step_results': numpy.ascontiguousarray(input_ids.cpu().numpy()),\n",
    "        'prev_step_scores': numpy.ascontiguousarray(prev_step_scores.cpu().numpy()),\n",
    "    }\n",
    "    for i, past_i in enumerate(past):\n",
    "        ort_inputs[f'past_{i}'] = numpy.ascontiguousarray(past_i.cpu().numpy())\n",
    "    batch_size = input_ids.size(0)\n",
    "    beam_size = 4\n",
    "    context_length = input_ids.size(-1)\n",
    "\n",
    "    for step in range(num_tokens_to_produce):\n",
    "        if use_onnxruntime_io:\n",
    "            outputs = inference_with_io_binding(ort_session, config, inputs['input_ids'], inputs['position_ids'], inputs['attention_mask'], past, inputs['beam_select_idx'], inputs['input_log_probs'], inputs['input_unfinished_sents'], inputs['prev_step_results'], inputs['prev_step_scores'], step, context_length)\n",
    "        else:\n",
    "            outputs = ort_session.run(None, ort_inputs) \n",
    "        inputs, ort_inputs, past = update(outputs, step, batch_size, beam_size, context_length, inputs['attention_mask'], device)\n",
    "\n",
    "        if not inputs['input_unfinished_sents'].any():\n",
    "            break\n",
    "\n",
    "    print(\"------------\")\n",
    "    print(tokenizer.decode(inputs['prev_step_results'][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Text generation using OnnxRuntime ...\n",
      "------------\n",
      "best hotel in bay area.\n",
      "\n",
      "\"It's a great place to stay,\" he said. \"It's a great place to live. It's a great place to work\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(model_name_or_path, cache_dir)\n",
    "input_text = EXAMPLE_Text\n",
    "test_generation(tokenizer, input_text, use_onnxruntime_io=False, ort_session=session)"
   ]
  },
  {
   "source": [
    "Next, we use ONNX Runtime with IO binding to run again and we can see that the result is exactly same."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Text generation using OnnxRuntime with IO binding ...\n",
      "------------\n",
      "best hotel in bay area.\n",
      "\n",
      "\"It's a great place to stay,\" he said. \"It's a great place to live. It's a great place to work\n"
     ]
    }
   ],
   "source": [
    "test_generation(tokenizer, input_text, use_onnxruntime_io=True, ort_session=session)"
   ]
  }
 ]
}