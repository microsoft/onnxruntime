{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference PyTorch GPT2 Model with ONNX Runtime on CPU\n",
    "\n",
    "In this tutorial, you'll be introduced to how to load a GPT2 model from PyTorch, convert it to ONNX, and inference it using ONNX Runtime.\n",
    "\n",
    "**Note: this work is still in progresss. Need install ort_nightly package before onnxruntime 1.3.0 is ready. The performance number of ort_nightly does not reflect the final result for onnxruntime 1.3.0. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites ##\n",
    "\n",
    "If you have Jupyter Notebook, you may directly run this notebook. We will use pip to install or upgrade [PyTorch](https://pytorch.org/), [OnnxRuntime](https://microsoft.github.io/onnxruntime/) and other required packages.\n",
    "\n",
    "Otherwise, you can setup a new environment. First, we install [AnaConda](https://www.anaconda.com/distribution/). Then open an AnaConda prompt window and run the following commands:\n",
    "\n",
    "```console\n",
    "conda create -n cpu_env python=3.6\n",
    "conda activate cpu_env\n",
    "\n",
    "conda install pytorch torchvision cpuonly -c pytorch\n",
    "pip install onnxruntime\n",
    "pip install transformers==2.5.1\n",
    "pip install onnx psutil pytz pandas py-cpuinfo py3nvml netron\n",
    "\n",
    "conda install jupyter\n",
    "jupyter notebook\n",
    "```\n",
    "The last command will launch Jupyter Notebook and we can open this notebook in browser to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable pass state in input.\n",
    "enable_past_input = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cache_dir = \"./gpt2\"\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "output_dir = './gpt2_onnx'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark ##\n",
    "\n",
    "You will need git clone the onnxruntime repository like\n",
    "```console\n",
    "git clone https://github.com/microsoft/onnxruntime.git\n",
    "```\n",
    "Then update the bert_tools_dir according to the path in your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume you have git clone the repository of onnxruntime from github.\n",
    "bert_tools_dir = r'D:\\Git\\onnxruntime\\onnxruntime\\python\\tools\\bert'\n",
    "benchmark_script = os.path.join(bert_tools_dir, 'benchmark_gpt2.py')\n",
    "\n",
    "if enable_past_input:\n",
    "    %run $benchmark_script --model_type gpt2 --cache_dir $cache_dir --output_dir $output_dir --enable_optimization --enable_past_input\n",
    "else:\n",
    "    %run $benchmark_script --model_type gpt2 --cache_dir $cache_dir --output_dir $output_dir --enable_optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only need the benchmark results. You can skip the remaining parts.\n",
    "\n",
    "In the following, we will introduce the benchmark script.\n",
    "\n",
    "### Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "model_class, tokenizer_class,  model_name_or_path = (GPT2Model,  GPT2Tokenizer,  'gpt2')\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "model = model_class.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "model.eval().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import time\n",
    "\n",
    "def pytorch_inference(model, input_ids, past=None, total_runs = 100):\n",
    "    latency = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(total_runs):\n",
    "            start = time.time()\n",
    "            outputs = model(input_ids=input_ids, past=past)\n",
    "            latency.append(time.time() - start)\n",
    "            \n",
    "    if total_runs > 1:\n",
    "        print(\"PyTorch Inference time = {} ms\".format(format(sum(latency) * 1000 / len(latency), '.2f')))\n",
    "    \n",
    "    return outputs\n",
    "    \n",
    "def onnxruntime_inference(ort_session, input_ids, past=None, total_runs=100):    \n",
    "    # Use contiguous array as input might improve performance.\n",
    "    # You can check the results from performance test tool to see whether you need it.\n",
    "    ort_inputs = {\n",
    "        'input_ids':  numpy.ascontiguousarray(input_ids.cpu().numpy())\n",
    "    }\n",
    "    \n",
    "    if past is not None:\n",
    "        for i, past_i in enumerate(past):\n",
    "            ort_inputs[f'past_{i}'] = numpy.ascontiguousarray(past[i].cpu().numpy())\n",
    "            \n",
    "    latency = []\n",
    "    for _ in range(total_runs):\n",
    "        start = time.time()\n",
    "        ort_outputs = ort_session.run(None, ort_inputs)\n",
    "        latency.append(time.time() - start)\n",
    "        \n",
    "    if total_runs > 1:\n",
    "        print(\"OnnxRuntime Inference time = {} ms\".format(format(sum(latency) * 1000 / len(latency), '.2f')))\n",
    "    \n",
    "    return ort_outputs\n",
    "\n",
    "def inference(model, ort_session, input_ids, past=None, total_runs=100, verify_outputs=True):\n",
    "    outputs = pytorch_inference(model, input_ids, past, total_runs)\n",
    "    ort_outputs = onnxruntime_inference(ort_session, input_ids, past, total_runs)\n",
    "    if verify_outputs:\n",
    "        print('PyTorch and OnnxRuntime output 0 (last_state) are close:'.format(0), numpy.allclose(ort_outputs[0], outputs[0].cpu(), rtol=1e-05, atol=1e-04))\n",
    "\n",
    "        if enable_past_input:\n",
    "            for layer in range(model.config.n_layer):\n",
    "                print('PyTorch and OnnxRuntime layer {} state (present_{}) are close:'.format(layer, layer), numpy.allclose(ort_outputs[1 + layer], outputs[1][layer].cpu(), rtol=1e-05, atol=1e-04))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "inputs = tokenizer.encode_plus(\"Here is an example input for GPT2 model\", add_special_tokens=True, return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "# run without past so that we can know the shape of past from output.\n",
    "outputs = model(input_ids=input_ids, past=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layer = model.config.n_layer    \n",
    "present_names = [f'present_{i}' for i in range(num_layer)]\n",
    "output_names = [\"last_state\"] + present_names\n",
    "\n",
    "input_names = ['input_ids']\n",
    "dynamic_axes= {'input_ids': {0: 'batch_size', 1: 'seq_len'},\n",
    "               #'token_type_ids' : {0: 'batch_size', 1: 'seq_len'},\n",
    "               #'attention_mask' : {0: 'batch_size', 1: 'seq_len'},\n",
    "               'last_state' : {0: 'batch_size', 1: 'seq_len'}\n",
    "              }\n",
    "for name in present_names:\n",
    "        dynamic_axes[name] = {1: 'batch_size', 3: 'seq_len'}\n",
    "        \n",
    "if enable_past_input:\n",
    "    past_names = [f'past_{i}' for i in range(num_layer)]\n",
    "    input_names = ['input_ids'] + past_names  #+ ['token_type_ids', 'attention_mask']\n",
    "    dummy_past = [torch.zeros(list(outputs[1][0].shape)) for _ in range(num_layer)]\n",
    "    for name in past_names:\n",
    "        dynamic_axes[name] = {1: 'batch_size', 3: 'seq_len'}\n",
    "    export_inputs = (inputs['input_ids'], tuple(dummy_past)) #, inputs['token_type_ids'], inputs['attention_mask'])\n",
    "else:\n",
    "    export_inputs = (inputs['input_ids'])\n",
    "\n",
    "export_model_path = os.path.join(output_dir, 'gpt2_past{}.onnx'.format(int(enable_past_input)))\n",
    "\n",
    "torch.onnx.export(model,\n",
    "                  args=export_inputs,\n",
    "                  f=export_model_path,\n",
    "                  input_names=input_names,\n",
    "                  output_names=output_names,\n",
    "                  dynamic_axes=dynamic_axes,\n",
    "                  opset_version=11,\n",
    "                  do_constant_folding = True,\n",
    "                  verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_past_outputs(export_model_path, output_model_path):\n",
    "    from onnx import ModelProto\n",
    "    from OnnxModel import OnnxModel\n",
    "\n",
    "    model = ModelProto()\n",
    "    with open(export_model_path, \"rb\") as f:\n",
    "        model.ParseFromString(f.read())\n",
    "    bert_model = OnnxModel(model)\n",
    "\n",
    "    # remove past state outputs and only keep the first output.\n",
    "    keep_output_names = [bert_model.model.graph.output[0].name]\n",
    "    logger.info(f\"Prune graph to keep the first output and drop past state outputs:{keep_output_names}\")\n",
    "    bert_model.prune_graph(keep_output_names)\n",
    "\n",
    "    bert_model.save_model_to_file(output_model_path)\n",
    "    \n",
    "if enable_past_input:\n",
    "    onnx_model_path = export_model_path\n",
    "else:\n",
    "    onnx_model_path = os.path.join(output_dir, 'gpt2_past{}_out1.onnx'.format(int(enable_past_input)))\n",
    "    remove_past_outputs(export_model_path, onnx_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with ONNX Runtime\n",
    "\n",
    "### OpenMP Environment Variable\n",
    "\n",
    "OpenMP environment variables are very important for CPU inference of GPT2 model. It has large performance impact on GPT2 model so you might need set it carefully according to benchmark script.\n",
    "\n",
    "Setting environment variables shall be done before importing onnxruntime. Otherwise, they might not take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "# You may change the settings in this cell according to Performance Test Tool result.\n",
    "use_openmp = True\n",
    "\n",
    "# ATTENTION: these environment variables must be set before importing onnxruntime.\n",
    "if use_openmp:\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(psutil.cpu_count(logical=True))\n",
    "else:\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = '1'\n",
    "\n",
    "os.environ[\"OMP_WAIT_POLICY\"] = 'ACTIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "import numpy\n",
    "\n",
    "# Print warning if user uses onnxruntime-gpu instead of onnxruntime package.\n",
    "if 'CUDAExecutionProvider' in onnxruntime.get_available_providers():\n",
    "    print(\"warning: onnxruntime-gpu is not built with OpenMP. You might try onnxruntime package to test CPU inference.\")\n",
    "\n",
    "sess_options = onnxruntime.SessionOptions()\n",
    "\n",
    "# Optional: store the optimized graph and view it using Netron to verify that model is fully optimized.\n",
    "# Note that this will increase session creation time, so it is for debugging only.\n",
    "#sess_options.optimized_model_filepath = os.path.join(output_dir, \"optimized_model_cpu.onnx\")\n",
    "   \n",
    "if use_openmp:\n",
    "    sess_options.intra_op_num_threads=1\n",
    "else:\n",
    "    sess_options.intra_op_num_threads=psutil.cpu_count(logical=True)\n",
    "\n",
    "# Specify providers when you use onnxruntime-gpu for CPU inference.\n",
    "session = onnxruntime.InferenceSession(onnx_model_path, sess_options, providers=['CPUExecutionProvider'])\n",
    "\n",
    "# Compare PyTorch and OnnxRuntime inference performance and results\n",
    "%time inference(model, session, input_ids, past=dummy_past if enable_past_input else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del session\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model = os.path.join(output_dir, 'gpt2_past{}_optimized.onnx'.format(int(enable_past_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_opt_script = os.path.join(bert_tools_dir, 'optimizer.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local directory corresponding to https://github.com/microsoft/onnxruntime/tree/master/onnxruntime/python/tools/transformers/\n",
    "%run $bert_opt_script --model_type gpt2 --input $onnx_model_path --output $optimized_model --opt_level 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = onnxruntime.InferenceSession(optimized_model, sess_options, providers=['CPUExecutionProvider'])\n",
    "\n",
    "%time inference(model, session, input_ids, past=dummy_past if enable_past_input else None, verify_outputs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Info\n",
    "\n",
    "Note that running Jupyter Notebook has slight impact on performance result since Jupyter Notebook is using system resources like CPU and memory etc. It is recommended to close Jupyter Notebook and other applications, then run the benchmark script in a console to get more accurate performance numbers.\n",
    "\n",
    "[OnnxRuntime C API](https://github.com/microsoft/onnxruntime/blob/master/docs/C_API.md) could get slightly better performance than python API. If you use C API in inference, you can use OnnxRuntime_Perf_Test.exe built from source to measure performance instead.\n",
    "\n",
    "Here is the machine configuration that generated the above results. The machine has GPU but not used in CPU inference.\n",
    "You might get slower or faster result based on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_info_script = os.path.join(bert_tools_dir, 'MachineInfo.py')\n",
    "%run $machine_info_script --silent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpu_env",
   "language": "python",
   "name": "cpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
