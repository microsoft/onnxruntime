---
title: Accelerate Hugging Face
grand_parent: Tutorials
parent: Inferencing
nav_order: 4
---
# Accelerate Hugging Face models
{: .no_toc }

ONNX Runtime can accelerate training and inferencing popular Hugging Face NLP models. 

<!-- ## Contents
{: .no_toc }

* TOC placeholder
{:toc} -->


## Accelerate Hugging Face model inferencing

* [General export and inference: Hugging Face Transformers](https://github.com/huggingface/transformers/blob/master/notebooks/04-onnx-export.ipynb)
* [Accelerate GPT2 model on CPU](https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/transformers/notebooks/Inference_GPT2_with_OnnxRuntime_on_CPU.ipynb)
* [Accelerate BERT model on CPU](https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/transformers/notebooks/PyTorch_Bert-Squad_OnnxRuntime_CPU.ipynb)
* [Accelerate BERT model on GPU](https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/transformers/notebooks/PyTorch_Bert-Squad_OnnxRuntime_GPU.ipynb)


## Additional resources
* [Blog post: Faster and smaller quantized NLP with Hugging Face and ONNX Runtime](https://medium.com/microsoftazure/faster-and-smaller-quantized-nlp-with-hugging-face-and-onnx-runtime-ec5525473bb7)
* [Blog post: Accelerate your NLP pipelines using Hugging Face Transformers and ONNX Runtime](https://medium.com/microsoftazure/accelerate-your-nlp-pipelines-using-hugging-face-transformers-and-onnx-runtime-2443578f4333)
