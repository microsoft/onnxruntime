<script>
	// @ts-nocheck

	import Blog from './blog-post.svelte';
	import FeaturedBlog from './blog-post-featured.svelte';
	import anime from 'animejs';
	import { onMount } from 'svelte';
	import ImageBlogs from '../../images/undraw/image_blogs.svelte';
	import HFImage from '../../images/blogs/hugging-face-blog-img.png';
	import LlamaImage from '../../images/blogs/accelerating-llama-2/Figure1-LLaMA-2-7B-E2E-Throughput.png';
	import SDXLTurboImage from '../../images/blogs/sdxl_blog_thumbnail.png';
	import Phi2Image from '../../routes/blogs/accelerating-phi-2/Phi2_Int4_TokenGenerationTP.png';
	import Phi3Image from '../../routes/blogs/accelerating-phi-3/Phi3_Thumbnail.png';
	import { createEventDispatcher } from 'svelte';
	import ORT117Thumbnail from '../../images/blogs/ort-1-17-thumbnail.png';
	import WebGPUImage from '../../images/blogs/webgpu_blog_thumbnail.jpg';
	import WebTrainingImage from '../../images/blogs/webtraining_blog_thumbnail.png';
	import Phi3OnDeviceImage from '../../images/blogs/phi-3-on-device_blog_thumbnail.png';
	import Phi3SmallMediumImage from '../../images/blogs/accelerating-phi-3-medium-thumbnail.png';
	import LightGlueImage from '../../images/blogs/lightglue-community-blog.png';
	import OliveSharedCache from '../../images/blogs/olive-shared-cache-user-flow.png';
	import GoodnotesThumbnail from '../../images/blogs/goodnotes-scribble-to-erase/Thumbnail.png';
	import OliveCli from '../../images/blogs/olive-flow.png';
	import QuantizeFinetune from '../../images/blogs/Quantize-finetune.jpg';
	import MultiLoraThumbnail from '../../images/blogs/multilora.png';
	import ORTLogo from '../../images/ONNX-Icon.svg';
	onMount(() => {
		anime({
			targets: '.border-primary',
			translateY: -5,
			direction: 'alternate',
			loop: false,
			delay: function (el, i, l) {
				return 500 + i * 50;
			},
			endDelay: function (el, i, l) {
				return (l - i) * 50;
			}
		});
	});

	let showBlogs = true;
	const dispatch = createEventDispatcher();

	onMount(() => {
		dispatch('switchTab', 'blogs');
	});

	function switchTab(tab) {
		showBlogs = tab === 'blogs';
		dispatch('switchTab', tab);
	}
	let featuredblog = [
		{
			title:
				'Cross-Platform Edge AI Made Easy with ONNX Runtime',
			date: '19th November, 2024',
			blurb:
				"Driven by the growing demand for user privacy, real-time performance, and cost efficiency, edge AI is transforming the AI landscape. At Ignite, we're excited to announce four new features in the ONNX Runtime ecosystem designed to make edge AI more accessible.",
			link: 'https://techcommunity.microsoft.com/blog/aiplatformblog/cross-platform-edge-ai-made-easy-with-onnx-runtime/4303521',
			image: ORTLogo,
			imgalt: 'ONNX Runtime Logo'
		},	
		{
			title:
				'Announcing MultiLoRA with ONNX Runtime: Revolutionizing AI Customization',
			date: '20th November, 2024',
			blurb:
				'MultiLoRA with ONNX Runtime brings flexible, efficient AI customization by enabling easy integration of LoRA adapters for dynamic, personalized models with minimal resource demands.',
			link: 'blogs/multilora',
			image: MultiLoraThumbnail,
			imgalt: 'Serving LoRA models separately vs with MultiLoRA'
		},
		{
			title:
				'Is it better to quantize before or after finetuning?',
			date: '19th November, 2024',
			blurb:
				'Learn how to quickly and easily experiment in your model optimization workflow using Olive.',
			link: 'blogs/olive-quant-ft',
			image: QuantizeFinetune,
			imgalt: 'Quantize or finetune first for better model performance?'
		},
	];
	let blogs = [
		{
			title:
				'Scribble to Erase on Goodnotes for Windows, Web, and Android, Powered by ONNX Runtime',
			date: '18th November, 2024',
			blurb:
				'Discover how Goodnotes brings the popular scribble-to-erase feature from iPad to Windows, Web, and Android with the help of ONNX Runtime, enabling seamless, high-performance AI integration across platforms.',
			link: 'blogs/goodnotes-scribble-to-erase',
			image: GoodnotesThumbnail,
			imgalt: 'Scribble to Erase feature on Goodnotes for Windows, Web, and Android'
		},
		{
			title: 'Democratizing AI Model optimization with the new Olive CLI',
			date: 'November 11th, 2024',
			blurb:
				'Learn how to use the new Olive CLI to easily optimize AI Models for on-device inference',
			link: 'blogs/olive-cli',
			image: OliveCli,
			imgalt: 'Olive Flow'
		},
		{
			title:
				'Enhancing team collaboration during AI model optimization with the Olive Shared Cache',
			date: 'October 30th, 2024',
			blurb:
				"Learn how to use Olive's shared cache to enhance team collaboration when optimizing AI models",
			link: 'blogs/olive-shared-cache',
			image: OliveSharedCache,
			imgalt: 'Team Flow for Olive shared cache'
		},
		{
			title: 'Accelerating LightGlue Inference with ONNX Runtime and TensorRT',
			date: 'July 17th, 2024',
			blurb:
				'Outperform torch.compile significantly using ONNX Runtime with TensorRT for LightGlue inference.',
			link: 'https://fabio-sim.github.io/blog/accelerating-lightglue-inference-onnx-runtime-tensorrt/',
			image: LightGlueImage,
			imgalt:
				'Speedup for ONNX Runtime with TensorRT and CUDA vs. torch.compile for difference batch sizes and sequence lengths.'
		},
		{
			title: 'High performance on-device real-time ML with NimbleEdge, using ONNX Runtime',
			date: 'June 17th, 2024',
			blurb:
				'Using NimbleEdge with ONNX Runtime delivers millisecond latency and minimal resource use, enabling real-time and privacy-preserving personalization in mobile apps.',
			link: 'blogs/nimbleedge-x-onnxruntime',
			image: 'https://iili.io/d95Pwcx.png',
			imgalt:
				'Image of the different steps of an ML pipeline on a mobile device, running using NimbleEdge and ONNX Runtime.'
		},
		{
			title: 'Background Removal in the Browser Using ONNX Runtime with WebGPU',
			date: 'June 12th, 2024',
			blurb:
				'Using ONNX Runtime with WebGPU and WebAssembly leads to 20x speedup over multi-threaded and 550x speedup over single-threaded CPU performance. Thus achieving interactive speeds for state-of-the-art background removal directly in the browser.',
			link: 'https://img.ly/blog/browser-background-removal-using-onnx-runtime-webgpu/',
			image: 'https://imgly-blog-prod.storage.googleapis.com/2024/06/onnx-runtime-imgly.jpg',
			imgalt:
				'Image of a skateboarder with a sky background, with half of the background being alternating grey and white squares indicating it has been removed.'
		},
		{
			title: 'Phi-3 Small and Medium Models are now Optimized with ONNX Runtime and DirectML',
			date: 'May 21th, 2024',
			blurb: 'You can now run the Phi-3 medium, small models on device of your choice.',
			link: 'blogs/accelerating-phi-3-small-medium',
			image: Phi3SmallMediumImage,
			imgalt:
				'Chart comparing model size (in GB) of ONNX Phi-3-medium between PyTorch and ONNX Runtime'
		},
		{
			title: 'Enjoy the Power of Phi-3 with ONNX Runtime on your device',
			date: 'May 20th, 2024',
			blurb: 'Harness ONNX Runtime to run Phi-3-mini on mobile phones and in the browser.',
			link: 'https://huggingface.co/blog/Emma-N/enjoy-the-power-of-phi-3-with-onnx-runtime',
			image: Phi3OnDeviceImage,
			imgalt:
				'Chart comparing model size (in GB) of ONNX Phi-3-mini for web and mobile with original Phi-3-mini'
		},
		{
			title: 'ONNX Runtime supports Phi-3 mini models across platforms and devices',
			date: 'April 22nd, 2024',
			blurb:
				"You can now run Microsoft's latest home-grown Phi-3 models across a huge range of devices and platforms thanks to ONNX Runtime and DirectML.",
			link: 'blogs/accelerating-phi-3',
			image: Phi3Image,
			imgalt:
				'Phi-3 + ONNX Runtime with the prompt "Tell me a joke" and Phi-3 answering: "Why don\'t scientists trust atoms?" "Because they make up everything!"'
		},
		{
			title: 'ONNX Runtime Web unleashes generative AI in the browser using WebGPU',
			date: 'February 29th, 2024',
			blurb:
				'We are thrilled to announce the official launch of ONNX Runtime Web featuring WebGPU, which is now available in the ONNX Runtime 1.17 release.',
			link: 'https://cloudblogs.microsoft.com/opensource/2024/02/29/onnx-runtime-web-unleashes-generative-ai-in-the-browser-using-webgpu/',
			image: WebGPUImage,
			imgalt:
				'Comparison of ONNX Runtime Web with WebGPU EP on GPU vs. WASM EP on CPU for segment anything example'
		},
		{
			title: 'ONNX Runtime 1.17: CUDA 12 support, Phi-2 optimizations, WebGPU, and more!',
			date: 'February 28th, 2024',
			blurb:
				'From Phi-2 model optimizations to CUDA 12 support, read this post to learn more about some of the exciting new functionality introduced in the ONNX Runtime 1.17 release.',
			link: 'blogs/ort-1-17-release',
			image: ORT117Thumbnail,
			imgalt: 'ONNX Runtime 1.17 release logo'
		},
		{
			title: 'Accelerating Phi-2, CodeLlama, Gemma and other Gen AI models with ONNX Runtime',
			date: 'February 26th, 2024',
			blurb: 'Improvements with ONNX Runtime for inferencing popular Gen AI models.',
			link: 'blogs/accelerating-phi-2',
			image: Phi2Image,
			imgalt: 'Phi2 float16 token generation throughput comparison'
		},
		{
			title: 'On-Device Training: Training a model in browser',
			date: 'February 6th, 2024',
			blurb:
				'Want to do ML training for your website in-browser? Learn more about what web training with ONNX Runtime has to offer in our blog below and experiment with your own applications through our easy-to-follow tutorials and demo.',
			link: 'https://cloudblogs.microsoft.com/opensource/2024/02/06/on-device-training-training-a-model-in-browser',
			image: WebTrainingImage,
			imgalt: 'Components of the onnxruntime-web JS package'
		},
		{
			title: 'Accelerating SD Turbo and SDXL Turbo Inference with ONNX Runtime and Olive',
			date: 'January 15th, 2024',
			blurb:
				'With ONNX Runtime and Olive, users can easily accelerate SD Turbo and SDXL Turbo models to generate viable images in as little as one step!',
			link: 'https://huggingface.co/blog/sdxl_ort_inference',
			image: SDXLTurboImage,
			imgalt: 'SD Turbo and SDXL Turbo models with ONNX Runtime and Olive'
		},
		{
			title: 'Accelerating LLaMA-2 Inference with ONNX Runtime',
			date: 'November 14th, 2023',
			blurb: 'Learn how ONNX Runtime can speed up LLaMA-2 inference by up to 4.5X',
			link: 'blogs/accelerating-llama-2',
			image: LlamaImage,
			imgalt: 'LLaMA-2 e2e throughput'
		},
		{
			title: 'Run PyTorch models on the edge',
			date: 'October 12th, 2023',
			blurb:
				'Everything you need to know about running PyTorch models on the edge with ONNX Runtime.',
			link: 'blogs/pytorch-on-the-edge',
			image:
				'https://onnxruntime.ai/_app/immutable/assets/pytorch-on-the-edge-with-ort.cdaa9c84.png',
			imgalt: 'Run PyTorch models on the edge'
		},
		{
			title: 'Accelerating over 130,000 Hugging Face models with ONNX Runtime',
			date: 'October 4th, 2023',
			blurb:
				'Learn more on how ONNX Runtime helps users accelerate open source machine learning models from Hugging Face.',
			link: 'https://cloudblogs.microsoft.com/opensource/2023/10/04/accelerating-over-130000-hugging-face-models-with-onnx-runtime/',
			image: HFImage,
			imgalt: 'Hugging Face models with ONNX Runtime'
		},
		{
			title: 'On-Device Training with ONNX Runtime: A deep dive',
			date: 'July 5th, 2023',
			blurb:
				'This blog presents technical details of On-Device training with ONNX Runtime. It explains how On-Device Training works and what are the different steps and artifacts involved in the training process. This information will help you train your models on edge devices.',
			link: 'https://cloudblogs.microsoft.com/opensource/2023/07/05/on-device-training-with-onnx-runtime-a-deep-dive/',
			image:
				'https://cloudblogs.microsoft.com/opensource/wp-content/uploads/sites/37/2023/06/Open-Source-1.webp'
		},
		{
			title:
				'Build and deploy fast and portable speech recognition applications with ONNX Runtime and Whisper',
			date: 'June 7th, 2023',
			blurb:
				'Learn how ONNX Runtime accelerates Whisper and makes it easy to deploy on desktop, mobile, in the cloud, and even in the browser.',
			link: 'https://medium.com/microsoftazure/build-and-deploy-fast-and-portable-speech-recognition-applications-with-onnx-runtime-and-whisper-5bf0969dd56b',
			image: 'https://miro.medium.com/v2/resize:fit:1100/format:webp/1*DJH8_6GS06-N32tkVhdTOw.png'
		},
		{
			title: 'On-Device Training: Efficient training on the edge with ONNX Runtime',
			date: 'May 31st, 2023',
			blurb:
				'This blog introduces On-Device Training to enable training models on edge devices with the data available on-edge. It extends ORT Inference on edge to include federated learning and personalization scenarios.',
			link: 'https://cloudblogs.microsoft.com/opensource/2023/05/31/on-device-training-efficient-training-on-the-edge-with-onnx-runtime/'
		},
		{
			title:
				'Unlocking the end-to-end Windows AI developer experience using ONNX runtime and Olive',
			date: 'May 23th, 2023',
			blurb:
				'This blog reviews the new capabilities of ONNX Runtime and the Olive toolchain to support hybrid inferencing, NPU EPs, and hardware aware model optimizations on Windows and other platforms',
			link: 'https://blogs.windows.com/windowsdeveloper/2023/05/23/unlocking-the-end-to-end-windows-ai-developer-experience-using-onnx-runtime-and-olive'
		},
		{
			title:
				'Bringing the power of AI to Windows 11 - unlocking a new era of productivity for customers and developers with Windows Copilot and Dev Home',
			date: 'May 23th, 2023',
			blurb:
				'This blog reviews AI in Windows 11, including ONNX Runtime as the gateway to Windows AI and new ONNX Runtime capabilities on Windows',
			link: 'https://blogs.windows.com/windowsdeveloper/2023/05/23/bringing-the-power-of-ai-to-windows-11-unlocking-a-new-era-of-productivity-for-customers-and-developers-with-windows-copilot-and-dev-home'
		},
		{
			title: 'Optimize DirectML performance with Olive',
			date: 'May 23th, 2023',
			blurb: 'This blog shows how to use Olive to optimize models for DML EP in ONNX Runtime',
			link: 'https://devblogs.microsoft.com/windowsai/optimize-directml-performance-with-olive'
		},
		{
			title: 'DirectML ❤ Stable Diffusion',
			date: 'May 23th, 2023',
			blurb:
				'This blog shows how to use the Stable Diffusion model on DML EP using Olive to optimize the Stable Diffusion model',
			link: 'https://devblogs.microsoft.com/windowsai/dml-stable-diffusion/'
		},
		{
			title: 'Accelerating Stable Diffusion Inference with ONNX Runtime',
			date: 'May 10th, 2023',
			blurb:
				'This blog shows how to accelerate the Stable Diffusion models from Hugging Face on NVIDIA and AMD GPUs with ONNX Runtime. It includes benchmark results obtained on A100 and RTX3060 and MI250X.',
			link: 'https://medium.com/microsoftazure/accelerating-stable-diffusion-inference-with-onnx-runtime-203bd7728540'
		},
		{
			title: 'Azure Container for PyTorch is now Generally Available in Azure Machine Learning!',
			date: 'March 22nd, 2023',
			blurb:
				'ACPT provides a ready-to-use distributed training environment for users to run on the latest multi-node GPU infrastructure offered in Azure. With Nebula, a new fast checkpointing capability in ACPT, you can save your checkpoints 1000 times faster with a simple API that works asynchronously with your training process.',
			link: 'https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/azure-container-for-pytorch-is-now-generally-available-in-azure/ba-p/3774616'
		},
		{
			title: 'High-performance deep learning in Oracle Cloud with ONNX Runtime',
			date: 'March 15th, 2023',
			blurb:
				'Enabling scenarios through the usage of Deep Neural Network (DNN) models is critical to our AI strategy at Oracle, and our Cloud AI Services team has built a solution to serve DNN models for customers in the healthcare sector. In this blog post, we’ll share challenges our team faced, and how ONNX Runtime solves these as the backbone of success for high-performance inferencing.',
			link: 'https://cloudblogs.microsoft.com/opensource/2023/03/15/high-performance-deep-learning-in-oracle-cloud-with-onnx-runtime/'
		},
		{
			title: 'Inference Stable Diffusion with C# and ONNX Runtime',
			date: 'March 9th, 2023',
			blurb:
				'In this tutorial we will learn how to do inferencing for the popular Stable Diffusion deep learning model in C#. Stable Diffusion models take a text prompt and create an image that represents the text. ',
			link: 'https://onnxruntime.ai/docs/tutorials/csharp/stable-diffusion-csharp.html'
		},
		{
			title: 'Video super resolution in Microsoft Edge',
			date: 'March 8th, 2023',
			blurb:
				'VSR in Microsoft Edge builds on top of ONNX Runtime and DirectML making our solution portable across GPU vendors and allowing VSR to be available to more users. Additional graphics cards which support these technologies and have sufficient computing power will receive support in the future. The ONNX Runtime and DirectML teams have fine-tuned their technology over many years, resulting in VSR making the most of the performance and capabilities of your graphics card’s processing power.',
			link: 'https://blogs.windows.com/msedgedev/2023/03/08/video-super-resolution-in-microsoft-edge/'
		},
		{
			title:
				'OctoML drives down production AI inference costs at Microsoft through new integration with ONNX Runtime ecosystem',
			date: 'March 2nd, 2023',
			blurb:
				'Over the past year, OctoML engineers worked closely with Watch For to design and implement the TVM Execution Provider (EP) for ONNX Runtime - bringing the model optimization potential of Apache TVM to all ONNX Runtime users. This builds upon the collaboration we began in 2021, to bring the benefits of TVM’s code generation and flexible quantization support to production scale at Microsoft.',
			link: 'https://octoml.ai/blog/octoml-drives-down-costs-at-microsoft-through-new-integration-with-onnx-runtime/'
		},
		{
			title: 'Performant on-device inferencing with ONNX Runtime',
			date: 'February 8th, 2023',
			blurb:
				'On-device machine learning model serving is a difficult task, especially given the limited bandwidth of early-stage startups. This guest post from the team at Pieces shares the problems and solutions evaluated for their on-device model serving stack and how ONNX Runtime serves as their backbone of success.',
			link: 'https://cloudblogs.microsoft.com/opensource/2023/02/08/performant-on-device-inferencing-with-onnx-runtime/'
		},
		{
			title:
				'Improve BERT inference speed by combining the power of Optimum, OpenVINO™, ONNX Runtime, and Azure',
			date: 'January 25th, 2023',
			blurb:
				'In this blog, we will discuss one of the ways to make huge models like BERT smaller and faster with OpenVINO™ Neural Networks Compression Framework (NNCF) and ONNX Runtime with OpenVINO™ Execution Provider through Azure Machine Learning.',
			link: 'https://cloudblogs.microsoft.com/opensource/2023/01/25/improve-bert-inference-speed-by-combining-the-power-of-optimum-openvino-onnx-runtime-and-azure/'
		},
		{
			title: 'Optimum + ONNX Runtime: Easier, Faster training for your Hugging Face models',
			date: 'January 24th, 2023',
			blurb:
				'Hugging Face’s Optimum library, through its integration with ONNX Runtime for training, provides an open solution to improve training times by 35% or more for many popular Hugging Face models. We present details of both Hugging Face Optimum and the ONNX Runtime Training ecosystem, with performance numbers highlighting the benefits of using the Optimum library.',
			link: 'https://huggingface.co/blog/optimum-onnxruntime-training/'
		},
		{
			title: 'Live demos of machine learning models with ONNX and Hugging Face Spaces',
			date: 'June 6, 2022',
			blurb:
				'Choosing which machine learning model to use, sharing a model with a colleague, and quickly trying out a model are all reasons why you may find yourself wanting to quickly run inference on a model. You can configure your environment and download Jupyter notebooks, but it would be nicer if there was a way to run a model with even less effort...',
			link: 'https://cloudblogs.microsoft.com/opensource/2022/06/06/live-demos-of-machine-learning-models-with-onnx-and-hugging-face-spaces/'
		},
		{
			title:
				'Optimizing and deploying transformer INT8 inference with ONNX Runtime-TensorRT on NVIDIA GPUs',
			date: 'May 2, 2022',
			blurb:
				'Transformer-based models have revolutionized the natural language processing (NLP) domain. Ever since its inception, transformer architecture has been integrated into models like Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-trained Transformer (GPT) for performing tasks such as text generation or summarization and question and answering to name a few...',
			link: 'https://cloudblogs.microsoft.com/opensource/2022/05/02/optimizing-and-deploying-transformer-int8-inference-with-onnx-runtime-tensorrt-on-nvidia-gpus/'
		},
		{
			title:
				'Scaling-up PyTorch inference: Serving billions of daily NLP inferences with ONNX Runtime',
			date: 'April 19, 2022',
			blurb:
				'Scale, performance, and efficient deployment of state-of-the-art Deep Learning models are ubiquitous challenges as applied machine learning grows across the industry. We’re happy to see that the ONNX Runtime Machine Learning model inferencing solution we’ve built and use in high-volume Microsoft products and services also resonates with our open source community, enabling new capabilities that drive content relevance and productivity...',
			link: 'https://cloudblogs.microsoft.com/opensource/2022/04/19/scaling-up-pytorch-inference-serving-billions-of-daily-nlp-inferences-with-onnx-runtime/'
		},
		{
			title: 'Add AI to mobile applications with Xamarin and ONNX Runtime',
			date: 'December 14, 2021',
			blurb:
				'ONNX Runtime now supports building mobile applications in C# with Xamarin. Support for Android and iOS is included in the ONNX Runtime release 1.10 NuGet package. This enables C# developers to build AI applications for Android and iOS to execute ONNX models on mobile devices with ONNX Runtime...',
			link: 'https://cloudblogs.microsoft.com/opensource/2021/12/14/add-ai-to-mobile-applications-with-xamarin-and-onnx-runtime/'
		},
		{
			title: 'ONNX Runtime Web—running your machine learning model in browser',
			date: 'September 2, 2021',
			blurb:
				'We are introducing ONNX Runtime Web (ORT Web), a new feature in ONNX Runtime to enable JavaScript developers to run and deploy machine learning models in browsers. It also helps enable new classes of on-device computation. ORT Web will be replacing the soon to be deprecated onnx.js...',
			link: 'https://cloudblogs.microsoft.com/opensource/2021/09/02/onnx-runtime-web-running-your-machine-learning-model-in-browser/'
		},
		{
			title: 'Accelerate PyTorch transformer model training with ONNX Runtime – a deep dive',
			date: 'July 13, 2021',
			blurb:
				'ONNX Runtime (ORT) for PyTorch accelerates training large scale models across multiple GPUs with up to 37% increase in training throughput over PyTorch and up to 86% speed up when combined with DeepSpeed...',
			link: 'https://techcommunity.microsoft.com/t5/azure-ai/accelerate-pytorch-transformer-model-training-with-onnx-runtime/ba-p/2540471'
		},
		{
			title: 'Accelerate PyTorch training with torch-ort',
			date: 'July 13, 2021',
			blurb:
				'With a simple change to your PyTorch training script, you can now speed up training large language models with torch_ort.ORTModule, running on the target hardware of your choice. Training deep learning models requires ever-increasing compute and memory resources. Today we release torch_ort.ORTModule, to accelerate distributed training of PyTorch models, reducing the time and resources needed for training...',
			link: 'https://cloudblogs.microsoft.com/opensource/2021/07/13/accelerate-pytorch-training-with-torch-ort/'
		},
		{
			title:
				'ONNX Runtime release 1.8.1 previews support for accelerated training on AMD GPUs with the AMD ROCm™ Open Software Platform',
			date: 'July 13, 2021',
			blurb:
				'ONNX Runtime is an open-source project that is designed to accelerate machine learning across a wide range of frameworks, operating systems, and hardware platforms. Today, we are excited to announce a preview version of ONNX Runtime in release 1.8.1 featuring support for AMD Instinct™ GPUs facilitated by the AMD ROCm™ open software platform...',
			link: 'https://cloudblogs.microsoft.com/opensource/2021/07/13/onnx-runtime-release-1-8-1-previews-support-for-accelerated-training-on-amd-gpus-with-the-amd-rocm-open-software-platform/'
		},
		{
			title: 'Journey to optimize large scale transformer model inference with ONNX Runtime',
			date: 'June 30, 2021',
			blurb:
				'Large-scale transformer models, such as GPT-2 and GPT-3, are among the most useful self-supervised transformer language models for natural language processing tasks such as language translation, question answering, passage summarization, text generation, and so on...',
			link: 'https://cloudblogs.microsoft.com/opensource/2021/06/30/journey-to-optimize-large-scale-transformer-model-inference-with-onnx-runtime/'
		}
	];
	let blogsCommunity = [
		{
			title: 'Sentence Transformers 3.2.0: 2x-3x Faster Inference with ONNX Runtime',
			date: 'October 10, 2024',
			link: 'https://github.com/UKPLab/sentence-transformers/releases/tag/v3.2.0',
			blurb:
				'This update brings 2x-3x speedups with a new ONNX backends, plus static embeddings offering 50x-500x faster performance with a slight accuracy trade-off. Install with pip install sentence-transformers==3.2.0.'
		},
		{
			title: 'Running Phi-3 Mistral 7B LLMs on Raspberry Pi 5: A Step-by-Step Guide',
			date: 'September 5, 2024',
			link: 'https://medium.com/@vadikus/running-phi-3-mistral-7b-llms-on-raspberry-pi-5-a-step-by-step-guide-185e8102e35b',
			blurb:
				'Learn how to run Phi-3 Mistral 7B on Raspberry Pi 5 using the ONNX Runtime Gen AI library.'
		},
		{
			title: 'Deploying a Production-Ready RAG Server: A Comprehensive Guide with LlamaIndex',
			date: 'March 27, 2024',
			link: 'https://python.plainenglish.io/deploying-a-production-ready-rag-server-a-comprehensive-guide-with-llamaindex-dbe57cc960df',
			blurb:
				'Leveraging ONNX Runtime to use FastEmbed for a serverless deployment of a RAG server with LlamaIndex.'
		},
		{
			title:
				'Efficient image generation with Stable Diffusion models and ONNX Runtime using AMD GPUs',
			date: 'February 23, 2024',
			link: 'https://rocm.blogs.amd.com/artificial-intelligence/stable-diffusion-onnx-runtime/README.html',
			blurb:
				'Use pre-trained Stable Diffusion models to generate images from text (text-to-image), transform existing visuals (image-to-image), and restore damaged pictures (inpainting) on AMD GPUs using ONNX Runtime.'
		},
		{
			title: 'AMD expands its AI and ML development tools with ROCm 6.0',
			date: 'February 15, 2024',
			link: 'https://overclock3d.net/news/software/amd-expands-its-ai-and-machine-learning-development-tools-with-rocm-6-0-with-expanded-gpu-support/',
			blurb:
				'ROCm 6.0 features support for the ONNX runtime. This support enhances AI development capabilities and interoperability.'
		},
		{
			title: 'UC San Diego Students Win MLPerf Contest at SC23',
			date: 'February 2, 2024',
			blurb:
				'During the annual Student Cluster Competition (SCC), UC San Diego undergraduate students achieved third place. Their success was fueled by optimizing performance using industry benchmarks, including the MLPerf Inference Benchmark. The seamless support for PyTorch and ONNX Runtime enabled them to port and fine-tune their code efficiently.',
			link: 'https://www.hpcwire.com/off-the-wire/uc-san-diego-students-win-mlperf-contest-at-sc23/'
		},
		{
			title: 'Human Capital Management (HCM) - Sentence Similarity Language Model using Java',
			date: 'December 5, 2023',
			blurb:
				'Using ONNX Runtime, the HCM team was able to deploy a sentence similarity language model using Java, demonstrating how easy it is to use with multiple languages.',
			link: 'https://www.linkedin.com/pulse/hcm-sentence-similarity-language-model-using-java-jonathon-palmieri-tdlpc%3FtrackingId=CN2PPVO4Toqh8r6JsAYMIw%253D%253D/?trackingId=ByNomo0pQFKM%2F%2BWEknVs7Q%3D%3D'
		}
	];
	let description =
		'ONNX Runtime Blogs - your source for the latest ONNX Runtime updates and information.';
	let image = 'https://i.ibb.co/0YBy62j/ORT-icon-for-light-bg.png';
	let imageSquare = 'https://i.ibb.co/0YBy62j/ORT-icon-for-light-bg.png';
	let authors = [''];
	let keywords =
		'onnxruntime, onnx runtime blogs, onnx runtime community blogs, onnx runtime community posts, onnx runtime community announcements';
</script>

<svelte:head>
	<!-- Dynamic meta tags -->
	<meta name="description" content={description} />
	<meta name="image" content={image} />
	<meta name="author" content={authors.join(', ')} />
	<meta name="keywords" content={keywords} />
	<!-- Open Graph / Facebook -->
	<meta property="og:description" content={description} />
	<meta property="og:image" content={image} />

	<!-- Twitter -->
	<meta property="twitter:description" content={description} />
	<meta property="twitter:image" content={image} />
	<meta property="twitter:card" content={imageSquare} />
</svelte:head>
<div class="container mx-auto">
	<div class="flex">
		<h1 class="text-5xl my-auto mx-4">Blogs & Announcements</h1>
		<div class="ml-5 hidden md:flex">
			<ImageBlogs />
		</div>
	</div>
	<div class="pt-5 mx-4 md:mx-10">
		<h3 class="text-3xl pb-8">Featured posts</h3>
		<div class="grid gap-4 grid-cols-1 lg:grid-cols-3">
			{#each featuredblog as blog}
				<FeaturedBlog
					title={blog.title}
					description={blog.blurb}
					imgalt={blog.imgalt}
					date={blog.date}
					link={blog.link}
					image={blog.image}
				/>
			{/each}
		</div>
	</div>
	<div class="mx-4">
		<div role="tablist" class="tabs tabs-bordered tabs-lg mb-10">
			<button
				role="tab"
				tabindex="0"
				class="tab text-3xl {showBlogs ? 'tab-active' : ''}"
				on:click={() => switchTab('blogs')}
				on:keypress={() => switchTab('blogs')}>Recent posts</button
			>
			<button
				tabindex="0"
				role="tab"
				class="tab text-3xl {!showBlogs ? 'tab-active' : ''}"
				on:click={() => switchTab('community')}
				on:keypress={() => switchTab('community')}>Community Posts</button
			>
		</div>
		{#if showBlogs}
			<div class="grid gap-4 grid-cols-1 md:grid-cols-2 mx-0 md:mx-6">
				{#each blogs as blog, i}
					<Blog title={blog.title} description={blog.blurb} date={blog.date} link={blog.link} />
				{/each}
			</div>
		{:else}
			<div class="grid gap-4 grid-cols-1 md:grid-cols-2 mx-0 md:mx-6">
				{#each blogsCommunity as blog, i}
					<Blog title={blog.title} description={blog.blurb} date={blog.date} link={blog.link} />
				{/each}
			</div>
		{/if}
	</div>
</div>
