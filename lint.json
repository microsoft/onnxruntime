{"path":"\\\\?\\C:\\Users\\Naomi\\Desktop\\ONNX\\onnxruntime\\include\\onnxruntime\\core\\session\\onnxruntime_c_api.h","line":null,"char":null,"code":"CLANGFORMAT","severity":"warning","name":"format","description":"See https://clang.llvm.org/docs/ClangFormat.html.\nRun `lintrunner -a` to apply this patch.","original":"ï»¿// Copyright (c) Microsoft Corporation. All rights reserved.\r\n// Licensed under the MIT License.\r\n\r\n// See docs\\c_cxx\\README.md on generating the Doxygen documentation from this file\r\n\r\n/** \\mainpage ONNX Runtime\r\n *\r\n * ONNX Runtime is a high-performance inference and training graph execution engine for deep learning models.\r\n *\r\n * ONNX Runtime's C, C++ APIs offer an easy to use interface to onboard and execute onnx models.\r\n * - \\subpage c_cpp_api \"Core C, C++ APIs\"\r\n * - \\subpage training_c_cpp_api \"Training C, C++ APIs for on-device training\"\r\n *\r\n * \\page c_cpp_api Core C, C++ APIs\r\n * <h1>C</h1>\r\n *\r\n * ::OrtApi - Click here to go to the structure with all C API functions.\r\n *\r\n * <h1>C++</h1>\r\n *\r\n * ::Ort - Click here to go to the namespace holding all of the C++ wrapper classes\r\n *\r\n * It is a set of header only wrapper classes around the C API. The goal is to turn the C style return value error codes into C++ exceptions, and to\r\n * automate memory management through standard C++ RAII principles.\r\n *\r\n * \\addtogroup Global\r\n * ONNX Runtime C API\r\n * @{\r\n */\r\n\r\n#pragma once\r\n#include <stdbool.h>\r\n#include <stdint.h>\r\n#include <stdlib.h>\r\n#include <string.h>\r\n\r\n/** \\brief The API version defined in this header\r\n *\r\n * This value is used by some API functions to behave as this version of the header expects.\r\n */\r\n#define ORT_API_VERSION 24\r\n\r\n#ifdef __cplusplus\r\nextern \"C\" {\r\n#endif\r\n\r\n//! @}\r\n// SAL2 Definitions\r\n#ifndef _MSC_VER\r\n#define _In_\r\n#define _In_z_\r\n#define _In_opt_\r\n#define _In_opt_z_\r\n#define _Out_\r\n#define _Out_opt_\r\n#define _Outptr_\r\n#define _Outptr_opt_\r\n#define _Inout_\r\n#define _Inout_opt_\r\n#define _Frees_ptr_opt_\r\n#define _Ret_maybenull_\r\n#define _Ret_notnull_\r\n#define _Check_return_\r\n#define _Outptr_result_maybenull_\r\n#define _Outptr_result_maybenull_z_\r\n#define _In_reads_(X)\r\n#define _In_reads_opt_\r\n#define _Inout_updates_(X)\r\n#define _Out_writes_(X)\r\n#define _Out_writes_opt_(X)\r\n#define _Inout_updates_all_(X)\r\n#define _Out_writes_bytes_all_(X)\r\n#define _Out_writes_all_(X)\r\n#define _Success_(X)\r\n#define _Outptr_result_buffer_maybenull_(X)\r\n#define ORT_ALL_ARGS_NONNULL __attribute__((nonnull))\r\n#else\r\n#include <specstrings.h>\r\n#define ORT_ALL_ARGS_NONNULL\r\n#endif\r\n\r\n#ifdef _WIN32\r\n// Define ORT_DLL_IMPORT if your program is dynamically linked to Ort.\r\n// dllexport is not used, we use a .def file.\r\n#ifdef ORT_DLL_IMPORT\r\n#define ORT_EXPORT __declspec(dllimport)\r\n#else\r\n#define ORT_EXPORT\r\n#endif\r\n#define ORT_API_CALL __stdcall\r\n#define ORT_MUST_USE_RESULT\r\n#define ORTCHAR_T wchar_t\r\n#else\r\n// To make symbols visible on macOS/iOS\r\n#ifdef __APPLE__\r\n#define ORT_EXPORT __attribute__((visibility(\"default\")))\r\n#else\r\n#define ORT_EXPORT\r\n#endif\r\n#define ORT_API_CALL\r\n#define ORT_MUST_USE_RESULT __attribute__((warn_unused_result))\r\n#define ORTCHAR_T char\r\n#endif\r\n\r\n/// ORTCHAR_T, ORT_TSTR are reserved specifically for path handling.\r\n/// All other strings are UTF-8 encoded, use char and std::string\r\n#ifndef ORT_TSTR\r\n#ifdef _WIN32\r\n#define ORT_TSTR(X) L##X\r\n// When X is a macro, L##X is not defined. In this case, we need to use ORT_TSTR_ON_MACRO.\r\n#define ORT_TSTR_ON_MACRO(X) L\"\" X\r\n#else\r\n#define ORT_TSTR(X) X\r\n#define ORT_TSTR_ON_MACRO(X) X\r\n#endif\r\n#endif\r\n\r\n// On Windows, ORT_FILE is a wchar_t version of the __FILE__ macro.\r\n// Otherwise, ORT_FILE is equivalent to __FILE__.\r\n#ifndef ORT_FILE\r\n#define ORT_FILE_INTERNAL(x) ORT_TSTR(x)\r\n#define ORT_FILE ORT_FILE_INTERNAL(__FILE__)\r\n#endif\r\n\r\n// Any pointer marked with _In_ or _Out_, cannot be NULL.\r\n\r\n// Windows users should use unicode paths when possible to bypass the MAX_PATH limitation\r\n// Every pointer marked with _In_ or _Out_, cannot be NULL. Caller should ensure that.\r\n// for ReleaseXXX(...) functions, they can accept NULL pointer.\r\n\r\n#ifdef __cplusplus\r\n// For any compiler with C++11 support, MSVC 2015 and greater, or Clang version supporting noexcept.\r\n// Such complex condition is needed because compilers set __cplusplus value differently.\r\n#ifndef __has_feature\r\n#define __has_feature(x) 0\r\n#endif\r\n#if ((__cplusplus >= 201103L) || (_MSC_VER >= 1900) || (defined(__has_feature) && __has_feature(cxx_noexcept)))\r\n#define NO_EXCEPTION noexcept\r\n#else\r\n#define NO_EXCEPTION throw()\r\n#endif\r\n#else\r\n#define NO_EXCEPTION\r\n#endif\r\n\r\n// __VA_ARGS__ on Windows and Linux are different\r\n#define ORT_API(RETURN_TYPE, NAME, ...) RETURN_TYPE ORT_API_CALL NAME(__VA_ARGS__) NO_EXCEPTION\r\n\r\n#define ORT_API_T(RETURN_TYPE, NAME, ...) \\\r\n  RETURN_TYPE(ORT_API_CALL* NAME)(__VA_ARGS__) NO_EXCEPTION\r\n\r\n#define ORT_API_STATUS(NAME, ...)                                                                   \\\r\n  _Success_(return == 0) _Check_return_ _Ret_maybenull_ OrtStatusPtr ORT_API_CALL NAME(__VA_ARGS__) \\\r\n  NO_EXCEPTION ORT_MUST_USE_RESULT\r\n\r\n// XXX: Unfortunately, SAL annotations are known to not work with function pointers\r\n#define ORT_API2_STATUS(NAME, ...) \\\r\n  _Check_return_ _Ret_maybenull_ OrtStatusPtr(ORT_API_CALL* NAME)(__VA_ARGS__) NO_EXCEPTION ORT_MUST_USE_RESULT\r\n\r\n// Used in *.cc files. Almost as same as ORT_API_STATUS, except without ORT_MUST_USE_RESULT and ORT_EXPORT\r\n#define ORT_API_STATUS_IMPL(NAME, ...) \\\r\n  _Success_(return == 0) _Check_return_ _Ret_maybenull_ OrtStatusPtr ORT_API_CALL NAME(__VA_ARGS__) NO_EXCEPTION\r\n\r\n#define ORT_CLASS_RELEASE(X) void(ORT_API_CALL * Release##X)(_Frees_ptr_opt_ Ort##X * input)\r\n\r\n#ifdef __DOXYGEN__\r\n#undef ORT_API_STATUS\r\n#define ORT_API_STATUS(NAME, ...) OrtStatus* NAME(__VA_ARGS__)\r\n#undef ORT_API2_STATUS\r\n#define ORT_API2_STATUS(NAME, ...) OrtStatus* NAME(__VA_ARGS__)\r\n#undef ORT_CLASS_RELEASE\r\n#define ORT_CLASS_RELEASE(X) void Release##X(Ort##X* input)\r\n#undef NO_EXCEPTION\r\n#define NO_EXCEPTION\r\n#endif\r\n/** \\addtogroup Global\r\n * ONNX Runtime C API\r\n * @{\r\n */\r\n\r\n/** Copied from TensorProto::DataType\r\n * Currently, Ort doesn't support complex64, complex128\r\n */\r\ntypedef enum ONNXTensorElementDataType {\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_UNDEFINED,\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT,   // maps to c type float\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_UINT8,   // maps to c type uint8_t\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_INT8,    // maps to c type int8_t\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_UINT16,  // maps to c type uint16_t\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_INT16,   // maps to c type int16_t\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_INT32,   // maps to c type int32_t\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_INT64,   // maps to c type int64_t\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_STRING,  // maps to c++ type std::string\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_BOOL,\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16,\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_DOUBLE,      // maps to c type double\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_UINT32,      // maps to c type uint32_t\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_UINT64,      // maps to c type uint64_t\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_COMPLEX64,   // complex with float32 real and imaginary components\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_COMPLEX128,  // complex with float64 real and imaginary components\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_BFLOAT16,    // Non-IEEE floating-point format based on IEEE754 single-precision\r\n  // float 8 types were introduced in onnx 1.14, see https://onnx.ai/onnx/technical/float8.html\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT8E4M3FN,    // Non-IEEE floating-point format based on IEEE754 single-precision\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT8E4M3FNUZ,  // Non-IEEE floating-point format based on IEEE754 single-precision\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT8E5M2,      // Non-IEEE floating-point format based on IEEE754 single-precision\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT8E5M2FNUZ,  // Non-IEEE floating-point format based on IEEE754 single-precision\r\n  // Int4 types were introduced in ONNX 1.16. See https://onnx.ai/onnx/technical/int4.html\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_UINT4,  // maps to a pair of packed uint4 values (size == 1 byte)\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_INT4,   // maps to a pair of packed int4 values (size == 1 byte)\r\n  // Float4 types were introduced in ONNX 1.18. See https://onnx.ai/onnx/technical/float4.html\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT4E2M1,  // maps to a pair of packed float4 values (size == 1 byte)\r\n} ONNXTensorElementDataType;\r\n\r\n// Synced with onnx TypeProto oneof\r\ntypedef enum ONNXType {\r\n  ONNX_TYPE_UNKNOWN,\r\n  ONNX_TYPE_TENSOR,\r\n  ONNX_TYPE_SEQUENCE,\r\n  ONNX_TYPE_MAP,\r\n  ONNX_TYPE_OPAQUE,\r\n  ONNX_TYPE_SPARSETENSOR,\r\n  ONNX_TYPE_OPTIONAL\r\n} ONNXType;\r\n\r\n// These types are synced with internal\r\n// SparseFormatFlags\r\ntypedef enum OrtSparseFormat {\r\n  ORT_SPARSE_UNDEFINED = 0,\r\n  ORT_SPARSE_COO = 0x1,\r\n  ORT_SPARSE_CSRC = 0x2,\r\n  ORT_SPARSE_BLOCK_SPARSE = 0x4\r\n} OrtSparseFormat;\r\n\r\n// Enum allows to query sparse tensor indices\r\nenum OrtSparseIndicesFormat {\r\n  ORT_SPARSE_COO_INDICES,\r\n  ORT_SPARSE_CSR_INNER_INDICES,\r\n  ORT_SPARSE_CSR_OUTER_INDICES,\r\n  ORT_SPARSE_BLOCK_SPARSE_INDICES\r\n};\r\n\r\n/** \\brief Logging severity levels\r\n *\r\n * In typical API usage, specifying a logging severity level specifies the minimum severity of log messages to show.\r\n */\r\ntypedef enum OrtLoggingLevel {\r\n  ORT_LOGGING_LEVEL_VERBOSE,  ///< Verbose informational messages (least severe).\r\n  ORT_LOGGING_LEVEL_INFO,     ///< Informational messages.\r\n  ORT_LOGGING_LEVEL_WARNING,  ///< Warning messages.\r\n  ORT_LOGGING_LEVEL_ERROR,    ///< Error messages.\r\n  ORT_LOGGING_LEVEL_FATAL,    ///< Fatal error messages (most severe).\r\n} OrtLoggingLevel;\r\n\r\ntypedef enum OrtErrorCode {\r\n  ORT_OK,\r\n  ORT_FAIL,\r\n  ORT_INVALID_ARGUMENT,\r\n  ORT_NO_SUCHFILE,\r\n  ORT_NO_MODEL,\r\n  ORT_ENGINE_ERROR,\r\n  ORT_RUNTIME_EXCEPTION,\r\n  ORT_INVALID_PROTOBUF,\r\n  ORT_MODEL_LOADED,\r\n  ORT_NOT_IMPLEMENTED,\r\n  ORT_INVALID_GRAPH,\r\n  ORT_EP_FAIL,\r\n  ORT_MODEL_LOAD_CANCELED,\r\n  ORT_MODEL_REQUIRES_COMPILATION,\r\n  ORT_NOT_FOUND,\r\n} OrtErrorCode;\r\n\r\ntypedef enum OrtOpAttrType {\r\n  ORT_OP_ATTR_UNDEFINED = 0,\r\n  ORT_OP_ATTR_INT,\r\n  ORT_OP_ATTR_INTS,\r\n  ORT_OP_ATTR_FLOAT,\r\n  ORT_OP_ATTR_FLOATS,\r\n  ORT_OP_ATTR_STRING,\r\n  ORT_OP_ATTR_STRINGS,\r\n  ORT_OP_ATTR_GRAPH,\r\n  ORT_OP_ATTR_TENSOR,\r\n} OrtOpAttrType;\r\n\r\n//! @}\r\n#define ORT_RUNTIME_CLASS(X) \\\r\n  struct Ort##X;             \\\r\n  typedef struct Ort##X Ort##X\r\n\r\n/** \\addtogroup Global\r\n * ONNX Runtime C API\r\n * @{\r\n */\r\n// The actual types defined have an Ort prefix\r\nORT_RUNTIME_CLASS(Env);\r\nORT_RUNTIME_CLASS(Status);  // nullptr for Status* indicates success\r\nORT_RUNTIME_CLASS(MemoryInfo);\r\nORT_RUNTIME_CLASS(IoBinding);\r\nORT_RUNTIME_CLASS(Session);  // Don't call ReleaseSession from Dllmain (because session owns a thread pool)\r\nORT_RUNTIME_CLASS(Value);\r\nORT_RUNTIME_CLASS(RunOptions);\r\nORT_RUNTIME_CLASS(TypeInfo);\r\nORT_RUNTIME_CLASS(TensorTypeAndShapeInfo);\r\nORT_RUNTIME_CLASS(MapTypeInfo);\r\nORT_RUNTIME_CLASS(SequenceTypeInfo);\r\nORT_RUNTIME_CLASS(OptionalTypeInfo);\r\nORT_RUNTIME_CLASS(SessionOptions);\r\nORT_RUNTIME_CLASS(CustomOpDomain);\r\nORT_RUNTIME_CLASS(ModelMetadata);\r\nORT_RUNTIME_CLASS(ThreadPoolParams);\r\nORT_RUNTIME_CLASS(ThreadingOptions);\r\nORT_RUNTIME_CLASS(ArenaCfg);\r\nORT_RUNTIME_CLASS(PrepackedWeightsContainer);\r\nORT_RUNTIME_CLASS(TensorRTProviderOptionsV2);\r\nORT_RUNTIME_CLASS(NvTensorRtRtxProviderOptions);\r\nORT_RUNTIME_CLASS(CUDAProviderOptionsV2);\r\nORT_RUNTIME_CLASS(CANNProviderOptions);\r\nORT_RUNTIME_CLASS(DnnlProviderOptions);\r\nORT_RUNTIME_CLASS(Op);\r\nORT_RUNTIME_CLASS(OpAttr);\r\nORT_RUNTIME_CLASS(Logger);\r\nORT_RUNTIME_CLASS(ShapeInferContext);\r\nORT_RUNTIME_CLASS(LoraAdapter);\r\nORT_RUNTIME_CLASS(ValueInfo);\r\nORT_RUNTIME_CLASS(Node);\r\nORT_RUNTIME_CLASS(Graph);\r\nORT_RUNTIME_CLASS(Model);\r\nORT_RUNTIME_CLASS(ModelCompilationOptions);\r\nORT_RUNTIME_CLASS(HardwareDevice);\r\nORT_RUNTIME_CLASS(EpDevice);\r\nORT_RUNTIME_CLASS(KeyValuePairs);\r\nORT_RUNTIME_CLASS(SyncStream);  // Opaque class to create an onnxruntime::Stream.\r\nORT_RUNTIME_CLASS(ExternalInitializerInfo);\r\n\r\n#ifdef _MSC_VER\r\ntypedef _Return_type_success_(return == 0) OrtStatus* OrtStatusPtr;\r\n#else\r\ntypedef OrtStatus* OrtStatusPtr;\r\n#endif\r\n\r\n/** \\brief Memory allocation interface\r\n *\r\n * Structure of function pointers that defines a memory allocator. This can be created and filled in by the user for custom allocators.\r\n *\r\n * When an allocator is passed to any function, be sure that the allocator object is not destroyed until the last allocated object using it is freed.\r\n */\r\ntypedef struct OrtAllocator {\r\n  uint32_t version;  ///< Must be initialized to ORT_API_VERSION\r\n\r\n  /// Returns a pointer to an allocated block of `size` bytes\r\n  void*(ORT_API_CALL* Alloc)(struct OrtAllocator* this_, size_t size);\r\n\r\n  /// Free a block of memory previously allocated with OrtAllocator::Alloc\r\n  void(ORT_API_CALL* Free)(struct OrtAllocator* this_, void* p);\r\n\r\n  /// Return a pointer to an ::OrtMemoryInfo that describes this allocator\r\n  const struct OrtMemoryInfo*(ORT_API_CALL* Info)(const struct OrtAllocator* this_);\r\n  /**\r\n   * @brief Optional allocation function to use for memory allocations made during session initialization.\r\n   * Use this function if you want to separate allocations made by ORT during Run() calls from\r\n   * those made during session initialization. This allows for separate memory management strategies for these\r\n   * allocations.\r\n   *\r\n   * \\return pointer to an allocated block of `size` bytes. nullptr if size was 0 or allocation failed.\r\n   *\r\n   * \\since 1.18\r\n   */\r\n  void*(ORT_API_CALL* Reserve)(struct OrtAllocator* this_, size_t size);\r\n\r\n  /**\r\n   * @brief Function used to get the statistics of the allocator.\r\n   *\r\n   * Return a pointer to the OrtKeyValuePairs structure that contains the statistics of the allocator.\r\n   * The user should call OrtApi::ReleaseKeyValuePairs when done.\r\n   *\r\n   * Current known keys are:\r\n   * - Limit: Bytes limit of the allocator. -1 if no limit is set.\r\n   * - InUse: Number of bytes in use.\r\n   * - TotalAllocated: The total number of allocated bytes by the allocator.\r\n   * - MaxInUse: The maximum bytes in use.\r\n   * - NumAllocs: Number of allocations.\r\n   * - NumReserves: Number of reserves. (Number of calls to Reserve() in arena-based allocators)\r\n   * - NumArenaExtensions: Number of arena extensions (Relevant only for arena based allocators)\r\n   * - NumArenaShrinkages: Number of arena shrinkages (Relevant only for arena based allocators)\r\n   * - MaxAllocSize: The max single allocation seen.\r\n   *\r\n   * The allocator is free to add other entries as appropriate.\r\n   *\r\n   * \\note Implementation of this function is optional and GetStats may be set to a nullptr.\r\n   *       If the OrtAllocator is wrapping an internal ORT allocator that does not implement GetStats\r\n   *       the returned OrtKeyValuePairs instance will be empty.\r\n   *\r\n   * \\since 1.23\r\n   */\r\n  ORT_API2_STATUS(GetStats, _In_ const struct OrtAllocator* this_, _Outptr_ OrtKeyValuePairs** out);\r\n\r\n  /** \\brief Allocate using a stream.\r\n   *\r\n   * If the allocator is stream aware this performs allocation using a stream.\r\n   *\r\n   * Alloc will be used if this is nullptr.\r\n   *\r\n   * \\param[in] this_ OrtAllocator instance\r\n   * \\param[in] size Size of the allocation in bytes. nullptr if size was 0 or allocation failed.\r\n   * \\param[in] stream The stream to allocate on.\r\n   *\r\n   * \\return pointer to an allocated block of `size` bytes\r\n   *\r\n   * \\note Implementation of this function is optional and AllocOnStream may be set to a nullptr.\r\n   * \\since 1.23\r\n   */\r\n  void*(ORT_API_CALL* AllocOnStream)(struct OrtAllocator* this_, size_t size, OrtSyncStream* stream);\r\n} OrtAllocator;\r\n\r\ntypedef void(ORT_API_CALL* OrtLoggingFunction)(\r\n    void* param, OrtLoggingLevel severity, const char* category, const char* logid, const char* code_location,\r\n    const char* message);\r\n\r\n/** \\brief Graph optimization level\r\n *\r\n * Refer to https://www.onnxruntime.ai/docs/performance/graph-optimizations.html#graph-optimization-levels\r\n * for an in-depth understanding of the Graph Optimization Levels.\r\n */\r\ntypedef enum GraphOptimizationLevel {\r\n  ORT_DISABLE_ALL = 0,\r\n  ORT_ENABLE_BASIC = 1,\r\n  ORT_ENABLE_EXTENDED = 2,\r\n  ORT_ENABLE_LAYOUT = 3,\r\n  ORT_ENABLE_ALL = 99\r\n} GraphOptimizationLevel;\r\n\r\ntypedef enum ExecutionMode {\r\n  ORT_SEQUENTIAL = 0,\r\n  ORT_PARALLEL = 1,\r\n} ExecutionMode;\r\n\r\n/** \\brief Language projection identifiers\r\n * /see OrtApi::SetLanguageProjection\r\n */\r\ntypedef enum OrtLanguageProjection {\r\n  ORT_PROJECTION_C = 0,\r\n  ORT_PROJECTION_CPLUSPLUS = 1,\r\n  ORT_PROJECTION_CSHARP = 2,\r\n  ORT_PROJECTION_PYTHON = 3,\r\n  ORT_PROJECTION_JAVA = 4,\r\n  ORT_PROJECTION_WINML = 5,\r\n  ORT_PROJECTION_NODEJS = 6,\r\n} OrtLanguageProjection;\r\n\r\nstruct OrtKernelInfo;\r\ntypedef struct OrtKernelInfo OrtKernelInfo;\r\nstruct OrtKernelContext;\r\ntypedef struct OrtKernelContext OrtKernelContext;\r\nstruct OrtCustomOp;\r\ntypedef struct OrtCustomOp OrtCustomOp;\r\n\r\ntypedef enum OrtAllocatorType {\r\n  OrtInvalidAllocator = -1,\r\n  OrtDeviceAllocator = 0,\r\n  OrtArenaAllocator = 1,\r\n  OrtReadOnlyAllocator = 2,\r\n} OrtAllocatorType;\r\n\r\n/** \\brief Memory types for allocated memory, execution provider specific types should be extended in each provider.\r\n */\r\n// Whenever this struct is updated, please also update the MakeKey function in onnxruntime / core / framework / execution_provider.cc\r\ntypedef enum OrtMemType {\r\n  /// Any CPU memory used by non-CPU execution provider\r\n  OrtMemTypeCPUInput = -2,\r\n  /// CPU accessible memory outputted by non-CPU execution provider, i.e. HOST_ACCESSIBLE\r\n  OrtMemTypeCPUOutput = -1,\r\n  /// CPU accessible memory allocated by non-CPU execution provider, i.e. HOST_ACCESSIBLE\r\n  OrtMemTypeCPU = OrtMemTypeCPUOutput,\r\n  /// The default allocator for execution provider\r\n  OrtMemTypeDefault = 0,\r\n} OrtMemType;\r\n\r\n/** \\brief This matches OrtDevice::MemoryType values */\r\ntypedef enum OrtDeviceMemoryType {\r\n  OrtDeviceMemoryType_DEFAULT = 0,          ///< Device memory\r\n  OrtDeviceMemoryType_HOST_ACCESSIBLE = 5,  ///< Shared/pinned memory for transferring between CPU and the device\r\n} OrtDeviceMemoryType;\r\n\r\n/** \\brief This mimics OrtDevice type constants so they can be returned in the API\r\n */\r\ntypedef enum OrtMemoryInfoDeviceType {\r\n  OrtMemoryInfoDeviceType_CPU = 0,\r\n  OrtMemoryInfoDeviceType_GPU = 1,\r\n  OrtMemoryInfoDeviceType_FPGA = 2,\r\n  OrtMemoryInfoDeviceType_NPU = 3,\r\n} OrtMemoryInfoDeviceType;\r\n\r\ntypedef enum OrtHardwareDeviceType {\r\n  OrtHardwareDeviceType_CPU,\r\n  OrtHardwareDeviceType_GPU,\r\n  OrtHardwareDeviceType_NPU\r\n} OrtHardwareDeviceType;\r\n\r\n/** \\brief These are the default EP selection policies used by ORT when doing automatic EP selection.\r\n */\r\ntypedef enum OrtExecutionProviderDevicePolicy {\r\n  OrtExecutionProviderDevicePolicy_DEFAULT,\r\n  OrtExecutionProviderDevicePolicy_PREFER_CPU,\r\n  OrtExecutionProviderDevicePolicy_PREFER_NPU,\r\n  OrtExecutionProviderDevicePolicy_PREFER_GPU,\r\n  OrtExecutionProviderDevicePolicy_MAX_PERFORMANCE,\r\n  OrtExecutionProviderDevicePolicy_MAX_EFFICIENCY,\r\n  OrtExecutionProviderDevicePolicy_MIN_OVERALL_POWER,\r\n} OrtExecutionProviderDevicePolicy;\r\n\r\n/** \\brief Delegate to allow providing custom OrtEpDevice selection logic\r\n *\r\n * This delegate is called by the EP selection code to allow the user to provide custom device selection logic.\r\n * The user can use this to select OrtEpDevice instances from the list of available devices.\r\n *\r\n * \\param ep_devices The list of available devices.\r\n * \\param num_devices The number of available devices.\r\n * \\param model_metadata The model metadata.\r\n * \\param runtime_metadata The runtime metadata. May be nullptr.\r\n * \\param selected Pre-allocated array to populate with selected OrtEpDevice pointers from ep_devices.\r\n * \\param max_selected The maximum number of devices that can be selected in the pre-allocated array.\r\n                       Currently the maximum is 8.\r\n * \\param num_selected The number of selected devices.\r\n * \\param state Opaque pointer. Required to use the delegate from other languages like C# and python.\r\n *\r\n * \\return OrtStatus* Selection status. Return nullptr on success.\r\n *                    Use CreateStatus to provide error info. Use ORT_FAIL as the error code.\r\n *                    ORT will release the OrtStatus* if not null.\r\n */\r\ntypedef OrtStatus*(ORT_API_CALL* EpSelectionDelegate)(_In_ const OrtEpDevice** ep_devices,\r\n                                                      _In_ size_t num_devices,\r\n                                                      _In_ const OrtKeyValuePairs* model_metadata,\r\n                                                      _In_opt_ const OrtKeyValuePairs* runtime_metadata,\r\n                                                      _Inout_ const OrtEpDevice** selected,\r\n                                                      _In_ size_t max_selected,\r\n                                                      _Out_ size_t* num_selected,\r\n                                                      _In_ void* state);\r\n\r\n/** \\brief Function called by ORT to write a buffer to a custom destination (e.g., file, stream, etc.).\r\n *\r\n * \\param state Opaque pointer holding the user's state.\r\n * \\param buffer The buffer to write.\r\n * \\param buffer_num_bytes The size of the buffer in bytes.\r\n *\r\n * \\return OrtStatus* Write status. Return nullptr on success.\r\n *                    Use CreateStatus to provide error info. Use ORT_FAIL as the error code.\r\n *                    ORT will release the OrtStatus* if not null.\r\n */\r\ntypedef OrtStatus*(ORT_API_CALL* OrtWriteBufferFunc)(_In_ void* state,\r\n                                                     _In_ const void* buffer,\r\n                                                     _In_ size_t buffer_num_bytes);\r\n\r\n/** \\brief Function called by ORT to allow user to specify how an initializer should be saved, that is, either\r\n * written to an external file or stored within the model. ORT calls this function for every initializer when\r\n * generating a model.\r\n *\r\n * If the function implementation sets the `new_external_info` output parameter to NULL, ORT stores the initializer data\r\n * within the generated model.\r\n *\r\n * Otherwise, if the function implementation sets `new_external_info` to a valid OrtExternalInitializerInfo instance,\r\n * ORT assumes that this function stores the initializer data in a file. In this case, ORT configures the model's\r\n * initializer to point to the location specified by the `new_external_info` output parameter.\r\n *\r\n * \\param[in] state Opaque pointer holding the user's state.\r\n * \\param[in] initializer_name The initializer's name as a null-terminated string.\r\n * \\param[in] initializer_value OrtValue containing the initializer's data, type, and shape.\r\n * \\param[in] external_info If the initializer is originally stored in an external file, `external_info` contains\r\n *                          the file path, file offset, and the data's byte size within the file. Otherwise,\r\n *                          `external_info` is NULL if the initializer is not originally stored in a file.\r\n * \\param[out] new_external_info Output parameter set to a new OrtExternalInitializerInfo instance indicating the\r\n *                               location where the function implementation stored the initializer data.\r\n *                               The function implementation must use `OrtApi::CreateExternalInitializerInfo()` to\r\n *                               create the instance.\r\n *                               If the function implementation sets `new_external_info` to NULL,\r\n *                               ORT stores the initializers within the model.\r\n *\r\n * \\note ORT takes ownership of the `new_external_info` output parameter.\r\n *\r\n * \\return OrtStatus* Write status. Return nullptr on success.\r\n *                    Use CreateStatus to provide error info. Use ORT_FAIL as the error code.\r\n *                    ORT will release the OrtStatus* if not null.\r\n */\r\ntypedef OrtStatus*(ORT_API_CALL* OrtGetInitializerLocationFunc)(\r\n    _In_ void* state,\r\n    _In_ const char* initializer_name,\r\n    _In_ const OrtValue* initializer_value,\r\n    _In_opt_ const OrtExternalInitializerInfo* external_info,\r\n    _Outptr_result_maybenull_ OrtExternalInitializerInfo** new_external_info);\r\n\r\n/** \\brief Algorithm to use for cuDNN Convolution Op\r\n */\r\ntypedef enum OrtCudnnConvAlgoSearch {\r\n  OrtCudnnConvAlgoSearchExhaustive,  // expensive exhaustive benchmarking using cudnnFindConvolutionForwardAlgorithmEx\r\n  OrtCudnnConvAlgoSearchHeuristic,   // lightweight heuristic based search using cudnnGetConvolutionForwardAlgorithm_v7\r\n  OrtCudnnConvAlgoSearchDefault,     // default algorithm using CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM\r\n} OrtCudnnConvAlgoSearch;\r\n\r\n/** \\brief CUDA Provider Options\r\n *\r\n * \\see OrtApi::SessionOptionsAppendExecutionProvider_CUDA\r\n */\r\ntypedef struct OrtCUDAProviderOptions {\r\n#ifdef __cplusplus\r\n  OrtCUDAProviderOptions()\r\n      : device_id{},\r\n        cudnn_conv_algo_search{OrtCudnnConvAlgoSearchExhaustive},\r\n        gpu_mem_limit{SIZE_MAX},\r\n        arena_extend_strategy{},\r\n        do_copy_in_default_stream{1},\r\n        has_user_compute_stream{},\r\n        user_compute_stream{},\r\n        default_memory_arena_cfg{},\r\n        tunable_op_enable{false},\r\n        tunable_op_tuning_enable{false},\r\n        tunable_op_max_tuning_duration_ms{} {}\r\n#endif\r\n\r\n  /** \\brief CUDA device Id\r\n   *   Defaults to 0.\r\n   */\r\n  int device_id;\r\n\r\n  /** \\brief CUDA Convolution algorithm search configuration.\r\n   *   See enum OrtCudnnConvAlgoSearch for more details.\r\n   *   Defaults to OrtCudnnConvAlgoSearchExhaustive.\r\n   */\r\n  OrtCudnnConvAlgoSearch cudnn_conv_algo_search;\r\n\r\n  /** \\brief CUDA memory limit (To use all possible memory pass in maximum size_t)\r\n   *   Defaults to SIZE_MAX.\r\n   *   \\note If a ::OrtArenaCfg has been applied, it will override this field\r\n   */\r\n  size_t gpu_mem_limit;\r\n\r\n  /** \\brief Strategy used to grow the memory arena\r\n   *   0 = kNextPowerOfTwo<br>\r\n   *   1 = kSameAsRequested<br>\r\n   *   Defaults to 0.\r\n   *   \\note If a ::OrtArenaCfg has been applied, it will override this field\r\n   */\r\n  int arena_extend_strategy;\r\n\r\n  /** \\brief Flag indicating if copying needs to take place on the same stream as the compute stream in the CUDA EP\r\n   *   0 = Use separate streams for copying and compute.\r\n   *   1 = Use the same stream for copying and compute.\r\n   *   Defaults to 1.\r\n   *   WARNING: Setting this to 0 may result in data races for some models.\r\n   *   Please see issue #4829 for more details.\r\n   */\r\n  int do_copy_in_default_stream;\r\n\r\n  /** \\brief Flag indicating if there is a user provided compute stream\r\n   *   Defaults to 0.\r\n   */\r\n  int has_user_compute_stream;\r\n\r\n  /** \\brief User provided compute stream.\r\n   *   If provided, please set `has_user_compute_stream` to 1.\r\n   */\r\n  void* user_compute_stream;\r\n\r\n  /** \\brief CUDA memory arena configuration parameters\r\n   */\r\n  OrtArenaCfg* default_memory_arena_cfg;\r\n\r\n  /** \\brief Enable TunableOp for using.\r\n   *   Set it to 1/0 to enable/disable TunableOp. Otherwise, it is disabled by default.\r\n   *   This option can be overridden by environment variable ORT_CUDA_TUNABLE_OP_ENABLE.\r\n   */\r\n  int tunable_op_enable;\r\n\r\n  /** \\brief Enable TunableOp for tuning.\r\n   *   Set it to 1/0 to enable/disable TunableOp tuning. Otherwise, it is disabled by default.\r\n   *   This option can be overridden by environment variable ORT_CUDA_TUNABLE_OP_TUNING_ENABLE.\r\n   */\r\n  int tunable_op_tuning_enable;\r\n\r\n  /** \\brief Max tuning duration time limit for each instance of TunableOp.\r\n   *   Defaults to 0 to disable the limit.\r\n   */\r\n  int tunable_op_max_tuning_duration_ms;\r\n\r\n} OrtCUDAProviderOptions;\r\n\r\n/** \\brief ROCM Provider Options\r\n *\r\n * \\see OrtApi::SessionOptionsAppendExecutionProvider_ROCM\r\n */\r\ntypedef struct OrtROCMProviderOptions {\r\n#ifdef __cplusplus\r\n  OrtROCMProviderOptions()\r\n      : device_id{},\r\n        miopen_conv_exhaustive_search{0},\r\n        gpu_mem_limit{SIZE_MAX},\r\n        arena_extend_strategy{},\r\n        do_copy_in_default_stream{1},\r\n        has_user_compute_stream{},\r\n        user_compute_stream{},\r\n        default_memory_arena_cfg{},\r\n        enable_hip_graph{false},\r\n        tunable_op_enable{false},\r\n        tunable_op_tuning_enable{false},\r\n        tunable_op_max_tuning_duration_ms{} {}\r\n#endif\r\n\r\n  /** \\brief ROCM device Id\r\n   *   Defaults to 0.\r\n   */\r\n  int device_id;\r\n\r\n  /** \\brief ROCM MIOpen Convolution algorithm exhaustive search option.\r\n   *   Defaults to 0 (false).\r\n   */\r\n  int miopen_conv_exhaustive_search;\r\n\r\n  /** \\brief ROCM memory limit (To use all possible memory pass in maximum size_t)\r\n   *   Defaults to SIZE_MAX.\r\n   *   \\note If a ::OrtArenaCfg has been applied, it will override this field\r\n   */\r\n  size_t gpu_mem_limit;\r\n\r\n  /** \\brief Strategy used to grow the memory arena\r\n   *   0 = kNextPowerOfTwo<br>\r\n   *   1 = kSameAsRequested<br>\r\n   *   Defaults to 0.\r\n   *   \\note If a ::OrtArenaCfg has been applied, it will override this field\r\n   */\r\n  int arena_extend_strategy;\r\n\r\n  /** \\brief Flag indicating if copying needs to take place on the same stream as the compute stream in the ROCM EP\r\n   *   0 = Use separate streams for copying and compute.\r\n   *   1 = Use the same stream for copying and compute.\r\n   *   Defaults to 1.\r\n   *   WARNING: Setting this to 0 may result in data races for some models.\r\n   *   Please see issue #4829 for more details.\r\n   */\r\n  int do_copy_in_default_stream;\r\n\r\n  /** \\brief Flag indicating if there is a user provided compute stream\r\n   *   Defaults to 0.\r\n   */\r\n  int has_user_compute_stream;\r\n\r\n  /** \\brief User provided compute stream.\r\n   *   If provided, please set `has_user_compute_stream` to 1.\r\n   */\r\n  void* user_compute_stream;\r\n\r\n  /** \\brief ROCM memory arena configuration parameters\r\n   */\r\n  OrtArenaCfg* default_memory_arena_cfg;\r\n\r\n  int enable_hip_graph;\r\n\r\n  /** \\brief Enable TunableOp for using.\r\n   *   Set it to 1/0 to enable/disable TunableOp. Otherwise, it is disabled by default.\r\n   *   This option can be overridden by environment variable ORT_ROCM_TUNABLE_OP_ENABLE.\r\n   */\r\n  int tunable_op_enable;\r\n\r\n  /** \\brief Enable TunableOp for tuning.\r\n   *   Set it to 1/0 to enable/disable TunableOp tuning. Otherwise, it is disabled by default.\r\n   *   This option can be overridden by environment variable ORT_ROCM_TUNABLE_OP_TUNING_ENABLE.\r\n   */\r\n  int tunable_op_tuning_enable;\r\n\r\n  /** \\brief Max tuning duration time limit for each instance of TunableOp.\r\n   *   Defaults to 0 to disable the limit.\r\n   */\r\n  int tunable_op_max_tuning_duration_ms;\r\n\r\n} OrtROCMProviderOptions;\r\n\r\n/** \\brief TensorRT Provider Options\r\n *\r\n * \\see OrtApi::SessionOptionsAppendExecutionProvider_TensorRT\r\n */\r\ntypedef struct OrtTensorRTProviderOptions {\r\n  int device_id;                                ///< CUDA device id (0 = default device)\r\n  int has_user_compute_stream;                  // indicator of user specified CUDA compute stream.\r\n  void* user_compute_stream;                    // user specified CUDA compute stream.\r\n  int trt_max_partition_iterations;             // maximum iterations for TensorRT parser to get capability\r\n  int trt_min_subgraph_size;                    // minimum size of TensorRT subgraphs\r\n  size_t trt_max_workspace_size;                // maximum workspace size for TensorRT.\r\n  int trt_fp16_enable;                          // enable TensorRT FP16 precision. Default 0 = false, nonzero = true\r\n  int trt_int8_enable;                          // enable TensorRT INT8 precision. Default 0 = false, nonzero = true\r\n  const char* trt_int8_calibration_table_name;  // TensorRT INT8 calibration table name.\r\n  int trt_int8_use_native_calibration_table;    // use native TensorRT generated calibration table. Default 0 = false, nonzero = true\r\n  int trt_dla_enable;                           // enable DLA. Default 0 = false, nonzero = true\r\n  int trt_dla_core;                             // DLA core number. Default 0\r\n  int trt_dump_subgraphs;                       // dump TRT subgraph. Default 0 = false, nonzero = true\r\n  int trt_engine_cache_enable;                  // enable engine caching. Default 0 = false, nonzero = true\r\n  const char* trt_engine_cache_path;            // specify engine cache path\r\n  int trt_engine_decryption_enable;             // enable engine decryption. Default 0 = false, nonzero = true\r\n  const char* trt_engine_decryption_lib_path;   // specify engine decryption library path\r\n  int trt_force_sequential_engine_build;        // force building TensorRT engine sequentially. Default 0 = false, nonzero = true\r\n  // This is the legacy struct and don't add new fields here.\r\n  // For new field that can be represented by string, please add it in include/onnxruntime/core/providers/tensorrt/tensorrt_provider_options.h\r\n  // For non-string field, need to create a new separate api to handle it.\r\n} OrtTensorRTProviderOptions;\r\n\r\n/** \\brief MIGraphX Provider Options\r\n *\r\n * \\see OrtApi::SessionOptionsAppendExecutionProvider_MIGraphX\r\n */\r\ntypedef struct OrtMIGraphXProviderOptions {\r\n  int device_id;                                     // hip device id.\r\n  int migraphx_fp16_enable;                          // MIGraphX FP16 precision. Default 0 = false, nonzero = true\r\n  int migraphx_fp8_enable;                           // MIGraphX FP8 precision. Default 0 = false, nonzero = true\r\n  int migraphx_int8_enable;                          // MIGraphX INT8 precision. Default 0 = false, nonzero = true\r\n  int migraphx_use_native_calibration_table;         // MIGraphx INT8 cal table. Default 0 = false, nonzero = true\r\n  const char* migraphx_int8_calibration_table_name;  // MIGraphx INT8 calibration table name\r\n  int migraphx_save_compiled_model;                  // migraphx save compiled model. Default 0 = false, nonzero = true\r\n  const char* migraphx_save_model_path;              // migraphx model path name\r\n  int migraphx_load_compiled_model;                  // migraphx int8 cal table. Default 0 = false, nonzero = true\r\n  const char* migraphx_load_model_path;              // migraphx model path name\r\n  bool migraphx_exhaustive_tune;                     // MIGraphX tuned compile. Default = false, nonzero = true\r\n\r\n  /** \\brief MIGraphX memory limit (To use all possible memory pass in maximum size_t)\r\n   *   Defaults to SIZE_MAX.\r\n   *   \\note If a ::OrtArenaCfg has been applied, it will override this field\r\n   */\r\n  size_t migraphx_mem_limit;\r\n\r\n  /** \\brief Strategy used to grow the memory arena\r\n   *   0 = kNextPowerOfTwo<br>\r\n   *   1 = kSameAsRequested<br>\r\n   *   Defaults to 0.\r\n   *   \\note If a ::OrtArenaCfg has been applied, it will override this field\r\n   */\r\n  int migraphx_arena_extend_strategy;\r\n\r\n  // This is the legacy struct and don't add new fields here.\r\n} OrtMIGraphXProviderOptions;\r\n\r\n/** \\brief OpenVINO Provider Options\r\n *  \\brief This Struct is frozen since ORT 1.13.0. Its maintained part of Legacy API for compatibility.\r\n *  \\brief For latest OpenVINO Provider Options update to the ProviderOptions map.\r\n *  \\brief Latest OpenVINO Provider Options are listed in the\r\n *  \\htmlonly\r\n *  <a href=\"https://onnxruntime.ai/docs/execution-providers/OpenVINO-ExecutionProvider.html#summary-of-options\">onnxruntime document.</a>\r\n *  \\endhtmlonly\r\n * \\see OrtApi::SessionOptionsAppendExecutionProvider()\r\n */\r\ntypedef struct OrtOpenVINOProviderOptions {\r\n#ifdef __cplusplus\r\n  OrtOpenVINOProviderOptions() : device_type{},\r\n                                 enable_npu_fast_compile{},\r\n                                 device_id{},\r\n                                 num_of_threads{},\r\n                                 cache_dir{},\r\n                                 context{},\r\n                                 enable_opencl_throttling{},\r\n                                 enable_dynamic_shapes{} {}\r\n#endif\r\n  /** \\brief Device type string\r\n   *\r\n   * Valid settings are one of: \"CPU_FP32\", \"CPU_FP16\", \"GPU_FP32\", \"GPU_FP16\"\r\n   */\r\n  const char* device_type;\r\n  unsigned char enable_npu_fast_compile;  ///< 0 = disabled, nonzero = enabled\r\n  const char* device_id;\r\n  size_t num_of_threads;  ///< 0 = Use default number of threads\r\n  const char* cache_dir;  // path is set to empty by default\r\n  void* context;\r\n  unsigned char enable_opencl_throttling;  ///< 0 = disabled, nonzero = enabled\r\n  unsigned char enable_dynamic_shapes;     ///< 0 = disabled, nonzero = enabled\r\n} OrtOpenVINOProviderOptions;\r\n\r\nstruct OrtApi;\r\ntypedef struct OrtApi OrtApi;\r\n\r\nstruct OrtTrainingApi;\r\ntypedef struct OrtTrainingApi OrtTrainingApi;\r\n\r\nstruct OrtModelEditorApi;\r\ntypedef struct OrtModelEditorApi OrtModelEditorApi;\r\n\r\nstruct OrtCompileApi;\r\ntypedef struct OrtCompileApi OrtCompileApi;\r\n\r\nstruct OrtEpApi;\r\ntypedef struct OrtEpApi OrtEpApi;\r\n\r\n/** \\brief The helper interface to get the right version of OrtApi\r\n *\r\n * Get a pointer to this structure through ::OrtGetApiBase\r\n */\r\nstruct OrtApiBase {\r\n  /** \\brief Get a pointer to the requested version of the ::OrtApi\r\n   *\r\n   * \\param[in] version Must be ::ORT_API_VERSION\r\n   * \\return The ::OrtApi for the version requested, nullptr will be returned if this version is unsupported, for example when using a runtime\r\n   *   older than the version created with this header file.\r\n   *\r\n   * One can call GetVersionString() to get the version of the Onnxruntime library for logging\r\n   * and error reporting purposes.\r\n   */\r\n  const OrtApi*(ORT_API_CALL* GetApi)(uint32_t version)NO_EXCEPTION;\r\n\r\n  /** \\brief Returns a null terminated string of the version of the Onnxruntime library (eg: \"1.8.1\")\r\n   *\r\n   *  \\return UTF-8 encoded version string. Do not deallocate the returned buffer.\r\n   */\r\n  const char*(ORT_API_CALL* GetVersionString)(void)NO_EXCEPTION;\r\n};\r\n\r\ntypedef struct OrtApiBase OrtApiBase;\r\n\r\n/** \\brief The Onnxruntime library's entry point to access the C API\r\n *\r\n * Call this to get the a pointer to an ::OrtApiBase\r\n */\r\nORT_EXPORT const OrtApiBase* ORT_API_CALL OrtGetApiBase(void) NO_EXCEPTION;\r\n\r\n/** \\brief Thread work loop function\r\n *\r\n * Onnxruntime will provide the working loop on custom thread creation\r\n * Argument is an onnxruntime built-in type which will be provided when thread pool calls OrtCustomCreateThreadFn\r\n */\r\ntypedef void (*OrtThreadWorkerFn)(void* ort_worker_fn_param);\r\n\r\ntypedef const struct OrtCustomHandleType {\r\n  char __place_holder;\r\n}* OrtCustomThreadHandle;\r\n\r\n/** \\brief Ort custom thread creation function\r\n *\r\n * The function should return a thread handle to be used in onnxruntime thread pools\r\n * Onnxruntime will throw exception on return value of nullptr or 0, indicating that the function failed to create a thread\r\n */\r\ntypedef OrtCustomThreadHandle (*OrtCustomCreateThreadFn)(void* ort_custom_thread_creation_options, OrtThreadWorkerFn ort_thread_worker_fn, void* ort_worker_fn_param);\r\n\r\n/** \\brief Custom thread join function\r\n *\r\n * Onnxruntime thread pool destructor will call the function to join a custom thread.\r\n * Argument ort_custom_thread_handle is the value returned by OrtCustomCreateThreadFn\r\n */\r\ntypedef void (*OrtCustomJoinThreadFn)(OrtCustomThreadHandle ort_custom_thread_handle);\r\n\r\ntypedef OrtStatus*(ORT_API_CALL* RegisterCustomOpsFn)(OrtSessionOptions* options, const OrtApiBase* api);\r\n\r\n/** \\brief Callback function for RunAsync\r\n *\r\n * \\param[in] user_data User specific data that passed back to the callback\r\n * \\param[out] outputs On succeed, outputs host inference results, on error, the value will be nullptr\r\n * \\param[out] num_outputs Number of outputs, on error, the value will be zero\r\n * \\param[out] status On error, status will provide details\r\n */\r\ntypedef void (*RunAsyncCallbackFn)(void* user_data, OrtValue** outputs, size_t num_outputs, OrtStatusPtr status);\r\n\r\n/** \\brief The C API\r\n *\r\n * All C API functions are defined inside this structure as pointers to functions.\r\n * Call OrtApiBase::GetApi to get a pointer to it\r\n *\r\n * \\nosubgrouping\r\n */\r\n/*\r\n * Public enum for compiled model compatibility across EPs.\r\n */\r\ntypedef enum OrtCompiledModelCompatibility {\r\n  OrtCompiledModelCompatibility_EP_NOT_APPLICABLE = 0,\r\n  OrtCompiledModelCompatibility_EP_SUPPORTED_OPTIMAL,\r\n  OrtCompiledModelCompatibility_EP_SUPPORTED_PREFER_RECOMPILATION,\r\n  OrtCompiledModelCompatibility_EP_UNSUPPORTED,\r\n} OrtCompiledModelCompatibility;\r\n\r\nstruct OrtApi {\r\n  /// \\name OrtStatus\r\n  /// @{\r\n\r\n  /**\r\n   * \\brief Create an OrtStatus from a null terminated string\r\n   *\r\n   * \\param[in] code\r\n   * \\param[in] msg A null-terminated string. Its contents will be copied.\r\n   * \\return A new OrtStatus object, must be destroyed with OrtApi::ReleaseStatus\r\n   */\r\n  OrtStatus*(ORT_API_CALL* CreateStatus)(OrtErrorCode code, _In_ const char* msg)NO_EXCEPTION ORT_ALL_ARGS_NONNULL;\r\n\r\n  /** \\brief Get OrtErrorCode from OrtStatus\r\n   *\r\n   * \\param[in] status\r\n   * \\return OrtErrorCode that \\p status was created with\r\n   */\r\n  OrtErrorCode(ORT_API_CALL* GetErrorCode)(_In_ const OrtStatus* status) NO_EXCEPTION ORT_ALL_ARGS_NONNULL;\r\n\r\n  /** \\brief Get error string from OrtStatus\r\n   *\r\n   * \\param[in] status\r\n   * \\return The error message inside the `status`. Do not free the returned value.\r\n   */\r\n  const char*(ORT_API_CALL* GetErrorMessage)(_In_ const OrtStatus* status)NO_EXCEPTION ORT_ALL_ARGS_NONNULL;\r\n\r\n  /// @}\r\n  /// \\name OrtEnv\r\n  /// @{\r\n\r\n  /** \\brief Create an OrtEnv\r\n   *\r\n   * \\note Invoking this function will return the same instance of the environment as that returned by a previous call\r\n   * to another env creation function; all arguments to this function will be ignored.\r\n   * \\param[in] log_severity_level The log severity level.\r\n   * \\param[in] logid The log identifier.\r\n   * \\param[out] out Returned newly created OrtEnv. Must be freed with OrtApi::ReleaseEnv\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateEnv, OrtLoggingLevel log_severity_level, _In_ const char* logid, _Outptr_ OrtEnv** out);\r\n\r\n  /** \\brief Create an OrtEnv\r\n   *\r\n   * \\note Invoking this function will return the same instance of the environment as that returned by a previous call\r\n   * to another env creation function; all arguments to this function will be ignored. If you want to provide your\r\n   * own logging function, consider setting it using the SetUserLoggingFunction API instead.\r\n   * \\param[in] logging_function A pointer to a logging function.\r\n   * \\param[in] logger_param A pointer to arbitrary data passed as the ::OrtLoggingFunction `param` parameter to\r\n   *                         `logging_function`. This parameter is optional.\r\n   * \\param[in] log_severity_level The log severity level.\r\n   * \\param[in] logid The log identifier.\r\n   * \\param[out] out Returned newly created OrtEnv. Must be freed with OrtApi::ReleaseEnv\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateEnvWithCustomLogger, _In_ OrtLoggingFunction logging_function, _In_opt_ void* logger_param,\r\n                  _In_ OrtLoggingLevel log_severity_level, _In_ const char* logid, _Outptr_ OrtEnv** out);\r\n\r\n  /** \\brief Enable Telemetry\r\n   *\r\n   * \\note Telemetry events are on by default since they are lightweight\r\n   * \\param[in] env\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(EnableTelemetryEvents, _In_ const OrtEnv* env);\r\n  /** \\brief Disable Telemetry\r\n   *\r\n   * \\see OrtApi::EnableTelemetryEvents\r\n   * \\param[in] env\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(DisableTelemetryEvents, _In_ const OrtEnv* env);\r\n\r\n  /// @}\r\n  /// \\name OrtSession\r\n  /// @{\r\n\r\n  /** \\brief Create an OrtSession from a model file\r\n   *\r\n   * \\param[in] env\r\n   * \\param[in] model_path\r\n   * \\param[in] options\r\n   * \\param[out] out Returned newly created OrtSession. Must be freed with OrtApi::ReleaseSession\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  // TODO: document the path separator convention? '/' vs '\\'\r\n  // TODO: should specify the access characteristics of model_path. Is this read only during the\r\n  // execution of CreateSession, or does the OrtSession retain a handle to the file/directory\r\n  // and continue to access throughout the OrtSession lifetime?\r\n  //  What sort of access is needed to model_path : read or read/write?\r\n  ORT_API2_STATUS(CreateSession, _In_ const OrtEnv* env, _In_ const ORTCHAR_T* model_path,\r\n                  _In_ const OrtSessionOptions* options, _Outptr_ OrtSession** out);\r\n\r\n  /** \\brief Create an OrtSession from memory\r\n   *\r\n   * \\param[in] env\r\n   * \\param[in] model_data\r\n   * \\param[in] model_data_length\r\n   * \\param[in] options\r\n   * \\param[out] out Returned newly created OrtSession. Must be freed with OrtApi::ReleaseSession\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateSessionFromArray, _In_ const OrtEnv* env,\r\n                  _In_ const void* model_data, size_t model_data_length,\r\n                  _In_ const OrtSessionOptions* options, _Outptr_ OrtSession** out);\r\n\r\n  /** \\brief Run the model in an ::OrtSession\r\n   *\r\n   * Will not return until the model run has completed. Multiple threads might be used to run the model based on\r\n   * the options in the ::OrtSession and settings used when creating the ::OrtEnv\r\n   *\r\n   * \\param[in] session\r\n   * \\param[in] run_options If nullptr, will use a default ::OrtRunOptions\r\n   * \\param[in] input_names Array of null terminated UTF8 encoded strings of the input names\r\n   * \\param[in] inputs Array of ::OrtValue%s of the input values\r\n   * \\param[in] input_len Number of elements in the input_names and inputs arrays\r\n   * \\param[in] output_names Array of null terminated UTF8 encoded strings of the output names\r\n   * \\param[in] output_names_len Number of elements in the output_names and outputs array\r\n   * \\param[out] outputs Array of ::OrtValue%s that the outputs are stored in. This can also be\r\n   *     an array of nullptr values, in this case ::OrtValue objects will be allocated and pointers\r\n   *     to them will be set into the `outputs` array.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(Run, _Inout_ OrtSession* session, _In_opt_ const OrtRunOptions* run_options,\r\n                  _In_reads_(input_len) const char* const* input_names,\r\n                  _In_reads_(input_len) const OrtValue* const* inputs, size_t input_len,\r\n                  _In_reads_(output_names_len) const char* const* output_names, size_t output_names_len,\r\n                  _Inout_updates_all_(output_names_len) OrtValue** outputs);\r\n\r\n  /// @}\r\n  /// \\name OrtSessionOptions\r\n  /// @{\r\n\r\n  /** \\brief Create an ::OrtSessionOptions object\r\n   *\r\n   * To use additional providers, you must build ORT with the extra providers enabled. Then call one of these\r\n   * functions to enable them in the session:<br>\r\n   *   OrtSessionOptionsAppendExecutionProvider_CPU<br>\r\n   *   OrtSessionOptionsAppendExecutionProvider_CUDA<br>\r\n   *   OrtSessionOptionsAppendExecutionProvider_(remaining providers...)<br>\r\n   * The order they are called indicates the preference order as well. In other words call this method\r\n   * on your most preferred execution provider first followed by the less preferred ones.\r\n   * If none are called Ort will use its internal CPU execution provider.\r\n   *\r\n   * \\param[out] options The newly created OrtSessionOptions. Must be freed with OrtApi::ReleaseSessionOptions\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateSessionOptions, _Outptr_ OrtSessionOptions** options);\r\n\r\n  /** \\brief Set filepath to save optimized model after graph level transformations\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] optimized_model_filepath\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetOptimizedModelFilePath, _Inout_ OrtSessionOptions* options,\r\n                  _In_ const ORTCHAR_T* optimized_model_filepath);\r\n\r\n  /** \\brief Create a copy of an existing ::OrtSessionOptions\r\n   *\r\n   * \\param[in] in_options OrtSessionOptions to copy\r\n   * \\param[out] out_options Returned newly created ::OrtSessionOptions. Must be freed with OrtApi::ReleaseSessionOptions\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CloneSessionOptions, _In_ const OrtSessionOptions* in_options,\r\n                  _Outptr_ OrtSessionOptions** out_options);\r\n\r\n  /** \\brief Set execution mode\r\n   *\r\n   * Controls whether you want to execute operators in your graph sequentially or in parallel. Usually when the model\r\n   *  has many branches, setting this option to ExecutionMode.ORT_PARALLEL will give you better performance.\r\n   *  See [docs/ONNX_Runtime_Perf_Tuning.md] for more details.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] execution_mode\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetSessionExecutionMode, _Inout_ OrtSessionOptions* options, ExecutionMode execution_mode);\r\n\r\n  /** \\brief Enable profiling for a session\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] profile_file_prefix\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(EnableProfiling, _Inout_ OrtSessionOptions* options, _In_ const ORTCHAR_T* profile_file_prefix);\r\n\r\n  /** \\brief Disable profiling for a session\r\n   *\r\n   * \\param[in] options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(DisableProfiling, _Inout_ OrtSessionOptions* options);\r\n\r\n  /** \\brief Enable the memory pattern optimization\r\n   *\r\n   * The idea is if the input shapes are the same, we could trace the internal memory allocation\r\n   * and generate a memory pattern for future request. So next time we could just do one allocation\r\n   * with a big chunk for all the internal memory allocation.\r\n   * \\note Memory pattern optimization is only available when Sequential Execution mode is enabled (see OrtApi::SetSessionExecutionMode)\r\n   *\r\n   * \\see OrtApi::DisableMemPattern\r\n   *\r\n   * \\param[in] options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(EnableMemPattern, _Inout_ OrtSessionOptions* options);\r\n\r\n  /** \\brief Disable the memory pattern optimization\r\n   *\r\n   * \\see OrtApi::EnableMemPattern\r\n   *\r\n   * \\param[in] options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(DisableMemPattern, _Inout_ OrtSessionOptions* options);\r\n\r\n  /** \\brief Enable the memory arena on CPU\r\n   *\r\n   * Arena may pre-allocate memory for future usage.\r\n   *\r\n   * \\param[in] options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(EnableCpuMemArena, _Inout_ OrtSessionOptions* options);\r\n\r\n  /** \\brief Disable the memory arena on CPU\r\n   *\r\n   * \\param[in] options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(DisableCpuMemArena, _Inout_ OrtSessionOptions* options);\r\n\r\n  /** \\brief Set session log id\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] logid The log identifier.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetSessionLogId, _Inout_ OrtSessionOptions* options, const char* logid);\r\n\r\n  /** \\brief Set session log verbosity level\r\n   *\r\n   * Applies to session load, initialization, etc\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] session_log_verbosity_level \\snippet{doc} snippets.dox Log Verbosity Level\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetSessionLogVerbosityLevel, _Inout_ OrtSessionOptions* options, int session_log_verbosity_level);\r\n\r\n  /** \\brief Set session log severity level\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] session_log_severity_level The log severity level (refer to ::OrtLoggingLevel for possible values).\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetSessionLogSeverityLevel, _Inout_ OrtSessionOptions* options, int session_log_severity_level);\r\n\r\n  /** \\brief Set the optimization level to apply when loading a graph\r\n   *\r\n   * Please see https://onnxruntime.ai/docs/performance/model-optimizations/graph-optimizations.html for an in-depth explanation\r\n   * \\param[in,out] options The session options object\r\n   * \\param[in] graph_optimization_level The optimization level\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetSessionGraphOptimizationLevel, _Inout_ OrtSessionOptions* options,\r\n                  GraphOptimizationLevel graph_optimization_level);\r\n\r\n  /** \\brief Sets the number of threads used to parallelize the execution within nodes\r\n   *\r\n   * When running a single node operation, ex. add, this sets the maximum number of threads to use.\r\n   *\r\n   * \\note If built with OpenMP, this has no effect on the number of threads used. In this case\r\n   *       use the OpenMP env variables to configure the number of intra op num threads.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] intra_op_num_threads Number of threads to use<br>\r\n   *   A value of 0 will use the default number of threads<br>\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetIntraOpNumThreads, _Inout_ OrtSessionOptions* options, int intra_op_num_threads);\r\n\r\n  /** \\brief Sets the number of threads used to parallelize the execution of the graph\r\n   *\r\n   * If nodes can be run in parallel, this sets the maximum number of threads to use to run them in parallel.\r\n   *\r\n   * \\note If sequential execution is enabled this value is ignored, it acts as if it was set to 1.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] inter_op_num_threads Number of threads to use<br>\r\n   *   A value of 0 will use the default number of threads<br>\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetInterOpNumThreads, _Inout_ OrtSessionOptions* options, int inter_op_num_threads);\r\n\r\n  /// @}\r\n  /// \\name OrtCustomOpDomain\r\n  /// @{\r\n\r\n  /** \\brief Create a custom op domain\r\n   *\r\n   * \\param[in] domain\r\n   * \\param[out] out Newly created domain. Must be freed with OrtApi::ReleaseCustomOpDomain\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateCustomOpDomain, _In_ const char* domain, _Outptr_ OrtCustomOpDomain** out);\r\n\r\n  /** \\brief Add a custom op to a custom op domain\r\n   *\r\n   * \\note The OrtCustomOp* pointer must remain valid until the ::OrtCustomOpDomain using it is released\r\n   *\r\n   * \\param[in] custom_op_domain\r\n   * \\param[in] op\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CustomOpDomain_Add, _Inout_ OrtCustomOpDomain* custom_op_domain, _In_ const OrtCustomOp* op);\r\n\r\n  /// @}\r\n  /// \\name OrtSessionOptions\r\n  /// @{\r\n\r\n  /** \\brief Add custom op domain to a session options\r\n   *\r\n   * \\note The OrtCustomOpDomain* must not be deleted until all sessions using it are released\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] custom_op_domain\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(AddCustomOpDomain, _Inout_ OrtSessionOptions* options, _In_ OrtCustomOpDomain* custom_op_domain);\r\n\r\n  /** \\deprecated Use OrtApi::RegisterCustomOpsLibrary_V2.\r\n   *\r\n   * Registers custom ops from a shared library.\r\n   *\r\n   * Loads a shared library (dll on windows, so on linux, etc) named 'library_path' and looks for this entry point:\r\n   *\t\tOrtStatus* RegisterCustomOps(OrtSessionOptions * options, const OrtApiBase* api);\r\n   * It then passes in the provided session options to this function along with the api base.\r\n   * The handle to the loaded library is returned in library_handle. It can be freed by the caller after all sessions using the passed in\r\n   * session options are destroyed, or if an error occurs and it is non null.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] library_path\r\n   * \\param[out] library_handle OS specific handle to the loaded library (Use FreeLibrary on Windows, dlclose on Linux, etc.. to unload)\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(RegisterCustomOpsLibrary, _Inout_ OrtSessionOptions* options, _In_ const char* library_path, _Outptr_ void** library_handle);\r\n\r\n  /// @}\r\n  /// \\name OrtSession\r\n  /// @{\r\n\r\n  /** \\brief Get input count for a session\r\n   *\r\n   * This number must also match the number of inputs passed to OrtApi::Run\r\n   *\r\n   * \\see OrtApi::SessionGetInputTypeInfo, OrtApi::SessionGetInputName, OrtApi::Session\r\n   *\r\n   * \\param[in] session\r\n   * \\param[out] out Number of inputs\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionGetInputCount, _In_ const OrtSession* session, _Out_ size_t* out);\r\n\r\n  /** \\brief Get output count for a session\r\n   *\r\n   * This number must also match the number of outputs returned by OrtApi::Run\r\n   *\r\n   * \\see OrtApi::SessionGetOutputTypeInfo, OrtApi::SessionGetOutputName, OrtApi::Session\r\n   *\r\n   * \\param[in] session\r\n   * \\param[out] out Number of outputs\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionGetOutputCount, _In_ const OrtSession* session, _Out_ size_t* out);\r\n\r\n  /** \\brief Get overridable initializer count\r\n   *\r\n   * \\see OrtApi::SessionGetOverridableInitializerTypeInfo, OrtApi::SessionGetOverridableInitializerName\r\n   *\r\n   * \\param[in] session\r\n   * \\param[in] out\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionGetOverridableInitializerCount, _In_ const OrtSession* session, _Out_ size_t* out);\r\n\r\n  /** \\brief Get input type information\r\n   *\r\n   * \\param[in] session\r\n   * \\param[in] index Must be between 0 (inclusive) and what OrtApi::SessionGetInputCount returns (exclusive)\r\n   * \\param[out] type_info Must be freed with OrtApi::ReleaseTypeInfo\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionGetInputTypeInfo, _In_ const OrtSession* session, size_t index, _Outptr_ OrtTypeInfo** type_info);\r\n\r\n  /** \\brief Get output type information\r\n   *\r\n   * \\param[in] session\r\n   * \\param[in] index Must be between 0 (inclusive) and what OrtApi::SessionGetOutputCount returns (exclusive)\r\n   * \\param[out] type_info Must be freed with OrtApi::ReleaseTypeInfo\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionGetOutputTypeInfo, _In_ const OrtSession* session, size_t index, _Outptr_ OrtTypeInfo** type_info);\r\n\r\n  /** \\brief Get overridable initializer type information\r\n   *\r\n   * \\param[in] session\r\n   * \\param[in] index Must be between 0 (inclusive) and what OrtApi::SessionGetOverridableInitializerCount returns (exclusive)\r\n   * \\param[out] type_info Must be freed with OrtApi::ReleaseTypeInfo\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionGetOverridableInitializerTypeInfo, _In_ const OrtSession* session, size_t index, _Outptr_ OrtTypeInfo** type_info);\r\n\r\n  /** \\brief Get input name\r\n   *\r\n   * \\param[in] session\r\n   * \\param[in] index Must be between 0 (inclusive) and what OrtApi::SessionGetInputCount returns (exclusive)\r\n   * \\param[in] allocator\r\n   * \\param[out] value Set to a null terminated UTF-8 encoded string allocated using `allocator`. Must be freed using `allocator`.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionGetInputName, _In_ const OrtSession* session, size_t index, _Inout_ OrtAllocator* allocator, _Outptr_ char** value);\r\n\r\n  /** \\brief Get output name\r\n   *\r\n   * \\param[in] session\r\n   * \\param[in] index Must be between 0 (inclusive) and what OrtApi::SessionGetOutputCount returns (exclusive)\r\n   * \\param[in] allocator\r\n   * \\param[out] value Set to a null terminated UTF-8 encoded string allocated using `allocator`. Must be freed using `allocator`.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionGetOutputName, _In_ const OrtSession* session, size_t index, _Inout_ OrtAllocator* allocator, _Outptr_ char** value);\r\n\r\n  /** \\brief Get overridable initializer name\r\n   *\r\n   * \\param[in] session\r\n   * \\param[in] index Must be between 0 (inclusive) and what OrtApi::SessionGetOverridableInitializerCount returns (exclusive)\r\n   * \\param[in] allocator\r\n   * \\param[out] value Set to a null terminated UTF-8 encoded string allocated using `allocator`. Must be freed using `allocator`.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionGetOverridableInitializerName, _In_ const OrtSession* session, size_t index,\r\n                  _Inout_ OrtAllocator* allocator, _Outptr_ char** value);\r\n\r\n  /// @}\r\n  /// \\name OrtRunOptions\r\n  /// @{\r\n\r\n  /** \\brief Create an OrtRunOptions\r\n   *\r\n   * \\param[out] out Returned newly created ::OrtRunOptions. Must be freed with OrtApi::ReleaseRunOptions\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateRunOptions, _Outptr_ OrtRunOptions** out);\r\n\r\n  /** \\brief Set per-run log verbosity level\r\n   *\r\n   * \\see OrtApi::RunOptionsGetRunLogVerbosityLevel\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] log_verbosity_level \\snippet{doc} snippets.dox Log Verbosity Level\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(RunOptionsSetRunLogVerbosityLevel, _Inout_ OrtRunOptions* options, int log_verbosity_level);\r\n\r\n  /** \\brief Set per-run log severity level\r\n   *\r\n   * \\see OrtApi::RunOptionsGetRunLogSeverityLevel\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] log_severity_level The log severity level (refer to ::OrtLoggingLevel for possible values).\r\n   */\r\n  ORT_API2_STATUS(RunOptionsSetRunLogSeverityLevel, _Inout_ OrtRunOptions* options, int log_severity_level);\r\n\r\n  /** \\brief Set per-run tag\r\n   *\r\n   * This is used in a per-run log identifier.\r\n   *\r\n   * \\see OrtApi::RunOptionsGetRunTag\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] run_tag The run tag.\r\n   */\r\n  ORT_API2_STATUS(RunOptionsSetRunTag, _Inout_ OrtRunOptions* options, _In_ const char* run_tag);\r\n\r\n  /** \\brief Get per-run log verbosity level\r\n   *\r\n   * \\see OrtApi::RunOptionsSetRunLogVerbosityLevel\r\n   *\r\n   * \\param[in] options\r\n   * \\param[out] log_verbosity_level \\snippet{doc} snippets.dox Log Verbosity Level\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(RunOptionsGetRunLogVerbosityLevel, _In_ const OrtRunOptions* options,\r\n                  _Out_ int* log_verbosity_level);\r\n\r\n  /** \\brief Get per-run log severity level\r\n   *\r\n   * \\see OrtApi::RunOptionsSetRunLogSeverityLevel\r\n   *\r\n   * \\param[in] options\r\n   * \\param[out] log_severity_level The log severity level (refer to ::OrtLoggingLevel for possible values).\r\n   */\r\n  ORT_API2_STATUS(RunOptionsGetRunLogSeverityLevel, _In_ const OrtRunOptions* options, _Out_ int* log_severity_level);\r\n\r\n  /** \\brief Get per-run tag\r\n   *\r\n   * This is used in a per-run log identifier.\r\n   *\r\n   * \\see OrtApi::RunOptionsSetRunTag\r\n   *\r\n   * \\param[in] options\r\n   * \\param[out] run_tag The run tag.\r\n   *                     Do not free this value, it is owned by `options`. It will be invalidated if the run tag\r\n   *                     changes (i.e., with OrtApi::RunOptionsSetRunTag) or `options` is freed.\r\n   */\r\n  ORT_API2_STATUS(RunOptionsGetRunTag, _In_ const OrtRunOptions* options, _Out_ const char** run_tag);\r\n\r\n  /** \\brief Set terminate flag\r\n   *\r\n   * If a currently executing session needs to be force terminated, this can be called from another thread to force it to fail with an error.\r\n   *\r\n   * \\param[in] options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(RunOptionsSetTerminate, _Inout_ OrtRunOptions* options);\r\n\r\n  /** \\brief Clears the terminate flag\r\n   *\r\n   * Used so the OrtRunOptions instance can be used in a new OrtApi::Run call without it instantly terminating\r\n   *\r\n   * \\param[in] options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(RunOptionsUnsetTerminate, _Inout_ OrtRunOptions* options);\r\n\r\n  /// @}\r\n  /// \\name OrtValue\r\n  /// @{\r\n\r\n  /** \\brief Create a tensor\r\n   *\r\n   * Create a tensor using a supplied ::OrtAllocator\r\n   *\r\n   * \\param[in] allocator\r\n   * \\param[in] shape Pointer to the tensor shape dimensions.\r\n   * \\param[in] shape_len The number of tensor shape dimensions.\r\n   * \\param[in] type\r\n   * \\param[out] out Returns newly created ::OrtValue. Must be freed with OrtApi::ReleaseValue\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateTensorAsOrtValue, _Inout_ OrtAllocator* allocator, _In_ const int64_t* shape, size_t shape_len,\r\n                  ONNXTensorElementDataType type, _Outptr_ OrtValue** out);\r\n\r\n  /** \\brief Create a tensor backed by a user supplied buffer\r\n   *\r\n   * Create a tensor with user's buffer. You can fill the buffer either before calling this function or after.\r\n   * p_data is owned by caller. ReleaseValue won't release p_data.\r\n   *\r\n   * If you wish to transfer ownership of p_data to ORT use CreateTensorWithDataAndDeleterAsOrtValue.\r\n   *\r\n   * \\param[in] info Memory description of where the p_data buffer resides (CPU vs GPU etc).\r\n   * \\param[in] p_data Pointer to the data buffer.\r\n   * \\param[in] p_data_len The number of bytes in the data buffer.\r\n   * \\param[in] shape Pointer to the tensor shape dimensions.\r\n   * \\param[in] shape_len The number of tensor shape dimensions.\r\n   * \\param[in] type The data type.\r\n   * \\param[out] out Returns newly created ::OrtValue. Must be freed with OrtApi::ReleaseValue\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateTensorWithDataAsOrtValue, _In_ const OrtMemoryInfo* info, _Inout_ void* p_data,\r\n                  size_t p_data_len, _In_ const int64_t* shape, size_t shape_len, ONNXTensorElementDataType type,\r\n                  _Outptr_ OrtValue** out);\r\n\r\n  /** \\brief Return if an ::OrtValue is a tensor type\r\n   *\r\n   * \\param[in] value A tensor type (string tensors are not supported)\r\n   * \\param[out] out Set to 1 iff ::OrtValue is a tensor, 0 otherwise\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(IsTensor, _In_ const OrtValue* value, _Out_ int* out);\r\n\r\n  /** \\brief Get a pointer to the raw data inside a tensor\r\n   *\r\n   * Used to read/write/modify the internal tensor data directly.\r\n   * \\note The returned pointer is valid until the \\p value is destroyed.\r\n   *\r\n   * \\param[in] value A tensor type (string tensors are not supported)\r\n   * \\param[out] out Filled in with a pointer to the internal storage\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetTensorMutableData, _In_ OrtValue* value, _Outptr_ void** out);\r\n\r\n  /** \\brief Set all strings at once in a string tensor\r\n   *\r\n   * \\param[in,out] value A tensor of type ONNX_TENSOR_ELEMENT_DATA_TYPE_STRING\r\n   * \\param[in] s An array of strings. Each string in this array must be null terminated.\r\n   * \\param[in] s_len Count of strings in s (Must match the size of \\p value's tensor shape)\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(FillStringTensor, _Inout_ OrtValue* value, _In_ const char* const* s, size_t s_len);\r\n\r\n  /** \\brief Get total byte length for all strings in a string tensor\r\n   *\r\n   * Typically used with OrtApi::GetStringTensorContent\r\n   *\r\n   * \\param[in] value A tensor of type ONNX_TENSOR_ELEMENT_DATA_TYPE_STRING\r\n   * \\param[out] len Total byte length of all strings (does not include trailing nulls)\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetStringTensorDataLength, _In_ const OrtValue* value, _Out_ size_t* len);\r\n\r\n  /** \\brief Get all strings from a string tensor\r\n   *\r\n   * An example of the results:<br>\r\n   * Given \\p value is a string tensor with the strings { \"This\" \"is\" \"a\" \"test\" }<br>\r\n   * \\p s must have a size of 11 bytes<br>\r\n   * \\p offsets must have 4 elements<br>\r\n   * After the call, these values will be filled in:<br>\r\n   * \\p s will contain \"Thisisatest\"<br>\r\n   * \\p offsets will contain { 0, 4, 6, 7 }<br>\r\n   * The length of the last string is just s_len - offsets[last]\r\n   *\r\n   * \\param[in] value A tensor of type ONNX_TENSOR_ELEMENT_DATA_TYPE_STRING\r\n   * \\param[in] s Buffer to sequentially write all tensor strings to. Each string is NOT null-terminated.\r\n   * \\param[in] s_len Number of bytes of buffer pointed to by \\p s (Get it from OrtApi::GetStringTensorDataLength)\r\n   * \\param[out] offsets Array of start offsets into the strings written to \\p s\r\n   * \\param[in] offsets_len Number of elements in offsets\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetStringTensorContent, _In_ const OrtValue* value, _Out_writes_bytes_all_(s_len) void* s,\r\n                  size_t s_len, _Out_writes_all_(offsets_len) size_t* offsets, size_t offsets_len);\r\n\r\n  /// @}\r\n  /// \\name OrtTypeInfo\r\n  /// @{\r\n\r\n  /** \\brief Get ::OrtTensorTypeAndShapeInfo from an ::OrtTypeInfo\r\n   *\r\n   * \\param[in] type_info\r\n   * \\param[out] out Do not free this value, it will be valid until type_info is freed.\r\n   *             If type_info does not represent tensor, this value will be set to nullptr.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CastTypeInfoToTensorInfo, _In_ const OrtTypeInfo* type_info,\r\n                  _Outptr_result_maybenull_ const OrtTensorTypeAndShapeInfo** out);\r\n\r\n  /** \\brief Get ::ONNXType from ::OrtTypeInfo\r\n   *\r\n   * \\param[in] type_info\r\n   * \\param[out] out\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetOnnxTypeFromTypeInfo, _In_ const OrtTypeInfo* type_info, _Out_ enum ONNXType* out);\r\n\r\n  /// @}\r\n  /// \\name OrtTensorTypeAndShapeInfo\r\n  /// @{\r\n\r\n  /** \\brief Create an ::OrtTensorTypeAndShapeInfo object\r\n   *\r\n   * \\param[out] out Returns newly created ::OrtTensorTypeAndShapeInfo. Must be freed with OrtApi::ReleaseTensorTypeAndShapeInfo\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateTensorTypeAndShapeInfo, _Outptr_ OrtTensorTypeAndShapeInfo** out);\r\n\r\n  /** \\brief Set element type in ::OrtTensorTypeAndShapeInfo\r\n   *\r\n   * \\param[in] info\r\n   * \\param[in] type\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetTensorElementType, _Inout_ OrtTensorTypeAndShapeInfo* info, enum ONNXTensorElementDataType type);\r\n\r\n  /** \\brief Set shape information in ::OrtTensorTypeAndShapeInfo\r\n   *\r\n   * \\param[in] info\r\n   * \\param[in] dim_values Array with `dim_count` elements. Can contain negative values.\r\n   * \\param[in] dim_count Number of elements in `dim_values`\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetDimensions, OrtTensorTypeAndShapeInfo* info, _In_ const int64_t* dim_values, size_t dim_count);\r\n\r\n  /** \\brief Get element type in ::OrtTensorTypeAndShapeInfo\r\n   *\r\n   * \\see OrtApi::SetTensorElementType\r\n   *\r\n   * \\param[in] info\r\n   * \\param[out] out\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetTensorElementType, _In_ const OrtTensorTypeAndShapeInfo* info,\r\n                  _Out_ enum ONNXTensorElementDataType* out);\r\n\r\n  /** \\brief Get dimension count in ::OrtTensorTypeAndShapeInfo\r\n   *\r\n   * \\see OrtApi::GetDimensions\r\n   *\r\n   * \\param[in] info\r\n   * \\param[out] out\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetDimensionsCount, _In_ const OrtTensorTypeAndShapeInfo* info, _Out_ size_t* out);\r\n\r\n  /** \\brief Get dimensions in ::OrtTensorTypeAndShapeInfo\r\n   *\r\n   * \\param[in] info\r\n   * \\param[out] dim_values Array with `dim_values_length` elements. On return, filled with the dimensions stored in the ::OrtTensorTypeAndShapeInfo\r\n   * \\param[in] dim_values_length Number of elements in `dim_values`. Use OrtApi::GetDimensionsCount to get this value\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetDimensions, _In_ const OrtTensorTypeAndShapeInfo* info, _Out_ int64_t* dim_values,\r\n                  size_t dim_values_length);\r\n\r\n  /** \\brief Get symbolic dimension names in ::OrtTensorTypeAndShapeInfo\r\n   *\r\n   * \\param[in] info\r\n   * \\param[in] dim_params Array with `dim_params_length` elements. On return filled with pointers to null terminated strings of the dimension names\r\n   * \\param[in] dim_params_length Number of elements in `dim_params`. Use OrtApi::GetDimensionsCount to get this value\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetSymbolicDimensions, _In_ const OrtTensorTypeAndShapeInfo* info,\r\n                  _Out_writes_all_(dim_params_length) const char* dim_params[], size_t dim_params_length);\r\n\r\n  /** \\brief Get total number of elements in a tensor shape from an ::OrtTensorTypeAndShapeInfo\r\n   *\r\n   * Return the number of elements specified by the tensor shape (all dimensions multiplied by each other).\r\n   * For 0 dimensions, 1 is returned. If any dimension is less than 0, the result is always -1.\r\n   *\r\n   * Examples:<br>\r\n   * [] = 1<br>\r\n   * [1,3,4] = 12<br>\r\n   * [2,0,4] = 0<br>\r\n   * [-1,3,4] = -1<br>\r\n   *\r\n   * \\param[in] info\r\n   * \\param[out] out Number of elements\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetTensorShapeElementCount, _In_ const OrtTensorTypeAndShapeInfo* info, _Out_ size_t* out);\r\n\r\n  /// @}\r\n  /// \\name OrtValue\r\n  /// @{\r\n\r\n  /** \\brief Get type and shape information from a tensor ::OrtValue\r\n   *\r\n   * \\param[in] value Must be a tensor (not a map/sequence/etc) or will return failure\r\n   * \\param[out] out Newly created ::OrtTensorTypeAndShapeInfo. Must be freed with OrtApi::ReleaseTensorTypeAndShapeInfo\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetTensorTypeAndShape, _In_ const OrtValue* value, _Outptr_ OrtTensorTypeAndShapeInfo** out);\r\n\r\n  /** \\brief Get type information of an OrtValue\r\n   *\r\n   * \\param[in] value\r\n   * \\param[out] out Newly created ::OrtTypeInfo. Must be freed with OrtApi::ReleaseTypeInfo\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetTypeInfo, _In_ const OrtValue* value, _Outptr_result_maybenull_ OrtTypeInfo** out);\r\n\r\n  /** \\brief Get ONNXType of an ::OrtValue\r\n   *\r\n   * \\param[in] value\r\n   * \\param[out] out\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetValueType, _In_ const OrtValue* value, _Out_ enum ONNXType* out);\r\n\r\n  /// @}\r\n  /// \\name OrtMemoryInfo\r\n  /// @{\r\n\r\n  /** \\brief Create an ::OrtMemoryInfo\r\n   *\r\n   * \\param[in] name\r\n   * \\param[in] type\r\n   * \\param[in] id\r\n   * \\param[in] mem_type\r\n   * \\param[out] out Newly created ::OrtMemoryInfo. Must be freed with OrtAPi::ReleaseMemoryInfo\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateMemoryInfo, _In_ const char* name, enum OrtAllocatorType type, int id,\r\n                  enum OrtMemType mem_type, _Outptr_ OrtMemoryInfo** out);\r\n\r\n  /** \\brief Create an ::OrtMemoryInfo for CPU memory\r\n   *\r\n   * Special case version of OrtApi::CreateMemoryInfo for CPU based memory. Same as using OrtApi::CreateMemoryInfo with name = \"Cpu\" and id = 0.\r\n   *\r\n   * \\param[in] type\r\n   * \\param[in] mem_type\r\n   * \\param[out] out\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateCpuMemoryInfo, enum OrtAllocatorType type, enum OrtMemType mem_type,\r\n                  _Outptr_ OrtMemoryInfo** out);\r\n\r\n  /** \\brief Compare ::OrtMemoryInfo objects for equality\r\n   *\r\n   * Compares all settings of each ::OrtMemoryInfo for equality\r\n   *\r\n   * \\param[in] info1\r\n   * \\param[in] info2\r\n   * \\param[out] out Set to 0 if equal, -1 if not equal\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CompareMemoryInfo, _In_ const OrtMemoryInfo* info1, _In_ const OrtMemoryInfo* info2, _Out_ int* out);\r\n\r\n  /** \\brief Get name from ::OrtMemoryInfo\r\n   *\r\n   * \\param[in] ptr\r\n   * \\param[out] out Writes null terminated string to this pointer. Do NOT free the returned pointer. It is valid for the lifetime of the ::OrtMemoryInfo\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(MemoryInfoGetName, _In_ const OrtMemoryInfo* ptr, _Out_ const char** out);\r\n\r\n  /** \\brief Get the device id from ::OrtMemoryInfo\r\n   */\r\n  ORT_API2_STATUS(MemoryInfoGetId, _In_ const OrtMemoryInfo* ptr, _Out_ int* out);\r\n\r\n  /** \\brief Get the ::OrtMemType from ::OrtMemoryInfo\r\n   */\r\n  ORT_API2_STATUS(MemoryInfoGetMemType, _In_ const OrtMemoryInfo* ptr, _Out_ OrtMemType* out);\r\n\r\n  /** \\brief Get the ::OrtAllocatorType from ::OrtMemoryInfo\r\n   */\r\n  ORT_API2_STATUS(MemoryInfoGetType, _In_ const OrtMemoryInfo* ptr, _Out_ OrtAllocatorType* out);\r\n\r\n  /// @}\r\n  /// \\name OrtAllocator\r\n  /// @{\r\n\r\n  /// \\brief Calls OrtAllocator::Alloc function\r\n  ORT_API2_STATUS(AllocatorAlloc, _Inout_ OrtAllocator* ort_allocator, size_t size, _Outptr_ void** out);\r\n  /// \\brief Calls OrtAllocator::Free function\r\n  ORT_API2_STATUS(AllocatorFree, _Inout_ OrtAllocator* ort_allocator, void* p);\r\n  /// \\brief Calls OrtAllocator::Info function\r\n  ORT_API2_STATUS(AllocatorGetInfo, _In_ const OrtAllocator* ort_allocator, _Outptr_ const struct OrtMemoryInfo** out);\r\n\r\n  /** \\brief Get the default allocator\r\n   *\r\n   * The default allocator is a CPU based, non-arena. Always returns the same pointer to the same default allocator.\r\n   *\r\n   * \\param[out] out Returned value should NOT be freed\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetAllocatorWithDefaultOptions, _Outptr_ OrtAllocator** out);\r\n\r\n  /// @}\r\n  /// \\name OrtSessionOptions\r\n  /// @{\r\n\r\n  /** \\brief Override session symbolic dimensions\r\n   *\r\n   * Override symbolic dimensions (by specific denotation strings) with actual values if known at session initialization time to enable\r\n   * optimizations that can take advantage of fixed values (such as memory planning, etc)\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] dim_denotation\r\n   * \\param[in] dim_value\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(AddFreeDimensionOverride, _Inout_ OrtSessionOptions* options, _In_ const char* dim_denotation,\r\n                  _In_ int64_t dim_value);\r\n\r\n  /// @}\r\n  /// \\name OrtValue\r\n  /// @{\r\n\r\n  /* Internal information (not seen in Doxygen)\r\n   *\r\n   * APIs to support non-tensor types - map and sequence.\r\n   * Currently only the following types are supported\r\n   * Note: the following types should be kept in sync with data_types.h\r\n   * Map types\r\n   * =========\r\n   * std::map<std::string, std::string>\r\n   * std::map<std::string, int64_t>\r\n   * std::map<std::string, float>\r\n   * std::map<std::string, double>\r\n   * std::map<int64_t, std::string>\r\n   * std::map<int64_t, int64_t>\r\n   * std::map<int64_t, float>\r\n   * std::map<int64_t, double>\r\n   *\r\n   * Sequence types\r\n   * ==============\r\n   * std::vector<std::string>\r\n   * std::vector<int64_t>\r\n   * std::vector<float>\r\n   * std::vector<double>\r\n   * std::vector<std::map<std::string, float>>\r\n   * std::vector<std::map<int64_t, float>\r\n   */\r\n\r\n  /** \\brief Get non tensor data from an ::OrtValue\r\n   *\r\n   * If `value` is of type ONNX_TYPE_MAP, you need to retrieve the keys and values\r\n   * separately. Use index=0 to retrieve keys and index=1 to retrieve values.\r\n   * If `value` is of type ONNX_TYPE_SEQUENCE, use index to retrieve the index'th element\r\n   * of the sequence.\r\n   *\r\n   * \\param[in] value\r\n   * \\param[in] index See above for usage based on `value` type\r\n   * \\param[in] allocator Allocator used to allocate ::OrtValue\r\n   * \\param[out] out Created ::OrtValue that holds the element requested. Must be freed with OrtApi::ReleaseValue\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetValue, _In_ const OrtValue* value, int index, _Inout_ OrtAllocator* allocator,\r\n                  _Outptr_ OrtValue** out);\r\n\r\n  /** \\brief Get non tensor value count from an ::OrtValue\r\n   *\r\n   * If `value` is of type ONNX_TYPE_MAP 2 will always be returned. For ONNX_TYPE_SEQUENCE\r\n   * the number of elements in the sequence will be returned\r\n   *\r\n   * \\param[in] value\r\n   * \\param[out] out\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetValueCount, _In_ const OrtValue* value, _Out_ size_t* out);\r\n\r\n  /** \\brief Create a map or sequence ::OrtValue\r\n   *\r\n   * To construct a map (ONNX_TYPE_MAP), use num_values = 2 and `in` should be an array of 2 ::OrtValue%s\r\n   * representing keys and values.<br>\r\n   *\r\n   * To construct a sequence (ONNX_TYPE_SEQUENCE), use num_values = N where N is the number of the elements in the\r\n   * sequence. 'in' should be an array of N ::OrtValue%s.\r\n   *\r\n   * \\param[in] in See above for details\r\n   * \\param[in] num_values\r\n   * \\param[in] value_type Must be either ONNX_TYPE_MAP or ONNX_TYPE_SEQUENCE\r\n   * \\param[out] out Newly created ::OrtValue. Must be freed with OrtApi::ReleaseValue\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateValue, _In_reads_(num_values) const OrtValue* const* in, size_t num_values,\r\n                  enum ONNXType value_type, _Outptr_ OrtValue** out);\r\n\r\n  /** \\brief Create an opaque (custom user defined type) ::OrtValue\r\n   *\r\n   * Constructs an ::OrtValue that contains a value of non-standard type created for\r\n   * experiments or while awaiting standardization. ::OrtValue in this case would contain\r\n   * an internal representation of the Opaque type. Opaque types are distinguished from\r\n   * each other by two strings 1) domain and 2) type name. The combination of the two\r\n   * must be unique, so the type representation is properly identified internally. The combination\r\n   * must be properly registered from within ORT at both compile/run time or by another API.\r\n   *\r\n   * To construct the ::OrtValue pass domain and type names, also a pointer to a data container\r\n   * the type of which must be known to both ORT and the client program. That data container may or may\r\n   * not match the internal representation of the Opaque type. The sizeof(data_container) is passed for\r\n   * verification purposes.\r\n   *\r\n   * \\param[in] domain_name Null terminated string of the domain name\r\n   * \\param[in] type_name Null terminated string of the type name\r\n   * \\param[in] data_container User pointer Data to populate ::OrtValue\r\n   * \\param[in] data_container_size Size in bytes of what `data_container` points to\r\n   * \\param[out] out Newly created ::OrtValue. Must be freed with OrtApi::ReleaseValue\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateOpaqueValue, _In_z_ const char* domain_name, _In_z_ const char* type_name,\r\n                  _In_ const void* data_container, size_t data_container_size, _Outptr_ OrtValue** out);\r\n\r\n  /** \\brief Get internal data from an opaque (custom user defined type) ::OrtValue\r\n   *\r\n   * Copies internal data from an opaque value into a user provided buffer\r\n   *\r\n   * \\see OrtApi::CreateOpaqueValue\r\n   *\r\n   * \\param[in] domain_name Null terminated string of the domain name\r\n   * \\param[in] type_name Null terminated string of the type name\r\n   * \\param[in] in The opaque ::OrtValue\r\n   * \\param[out] data_container Buffer to copy data into\r\n   * \\param[out] data_container_size Size in bytes of the buffer pointed to by data_container. Must match the size of the internal buffer.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetOpaqueValue, _In_ const char* domain_name, _In_ const char* type_name, _In_ const OrtValue* in,\r\n                  _Out_ void* data_container, size_t data_container_size);\r\n\r\n  /// @}\r\n  /// \\name OrtKernelInfo\r\n  /// Custom operator APIs.\r\n  /// @{\r\n\r\n  /** \\brief Get a float stored as an attribute in the graph node\r\n   *\r\n   * \\param[in] info ::OrtKernelInfo instance\r\n   * \\param[in] name Null terminated string of the name of the attribute\r\n   * \\param[out] out Pointer to memory where the attribute will be stored\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(KernelInfoGetAttribute_float, _In_ const OrtKernelInfo* info, _In_ const char* name,\r\n                  _Out_ float* out);\r\n\r\n  /** \\brief Fetch a 64-bit int stored as an attribute in the graph node\r\n   *\r\n   * \\param[in] info ::OrtKernelInfo instance\r\n   * \\param[in] name Null terminated string of the name of the attribute\r\n   * \\param[out] out Pointer to memory where the attribute will be stored\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(KernelInfoGetAttribute_int64, _In_ const OrtKernelInfo* info, _In_ const char* name,\r\n                  _Out_ int64_t* out);\r\n\r\n  /** \\brief Fetch a string stored as an attribute in the graph node\r\n   *\r\n   * If `out` is nullptr, the value of `size` is set to the true size of the string\r\n   * attribute, and a success status is returned.\r\n   *\r\n   * If the `size` parameter is greater than or equal to the actual string attribute's size,\r\n   * the value of `size` is set to the true size of the string attribute, the provided memory\r\n   * is filled with the attribute's contents, and a success status is returned.\r\n   *\r\n   * If the `size` parameter is less than the actual string attribute's size and `out`\r\n   * is not nullptr, the value of `size` is set to the true size of the string attribute\r\n   * and a failure status is returned.)\r\n   *\r\n   * \\param[in] info ::OrtKernelInfo instance\r\n   * \\param[in] name Null terminated string of the name of the attribute\r\n   * \\param[out] out Pointer to memory where the attribute will be stored\r\n   * \\param[in,out] size See above comments for details\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(KernelInfoGetAttribute_string, _In_ const OrtKernelInfo* info, _In_ const char* name, _Out_ char* out,\r\n                  _Inout_ size_t* size);\r\n\r\n  /// @}\r\n  /// \\name OrtKernelContext\r\n  /// Custom operator APIs.\r\n  /// @{\r\n\r\n  /** \\brief Used for custom operators, get the input count of a kernel\r\n   *\r\n   * \\see ::OrtCustomOp\r\n   */\r\n  ORT_API2_STATUS(KernelContext_GetInputCount, _In_ const OrtKernelContext* context, _Out_ size_t* out);\r\n\r\n  /** \\brief Used for custom operators, get the output count of a kernel\r\n   *\r\n   * \\see ::OrtCustomOp\r\n   */\r\n  ORT_API2_STATUS(KernelContext_GetOutputCount, _In_ const OrtKernelContext* context, _Out_ size_t* out);\r\n\r\n  /** \\brief Used for custom operators, get an input of a kernel\r\n   *\r\n   * The function attempts fetches the input of the kernel. If the input is optional\r\n   * and not present, the function returns success and out is set to nullptr.\r\n   *\r\n   * \\param[in] context ::OrtKernelContext instance\r\n   * \\param[in] index See KernelContext_GetInputCount for boundaries check.\r\n   * \\param[out] out OrtValue if the input is present otherwise is set nullptr\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(KernelContext_GetInput, _In_ const OrtKernelContext* context, _In_ size_t index,\r\n                  _Out_ const OrtValue** out);\r\n\r\n  /** \\brief Used for custom operators, get an output of a kernel\r\n   *\r\n   * The function attempts fetches the output of the kernel. If the output is optional\r\n   * and not present, the function returns success and out is set to nullptr.\r\n   *\r\n   * \\param[in] context ::OrtKernelContext instance\r\n   * \\param[in] index See KernelContext_GetOutputCount for boundaries check.\r\n   * \\param[in] dim_values output dimensions\r\n   * \\param[in] dim_count number of dimensions\r\n   * \\param[out] out a ptr to OrtValue to output otherwise set to nullptr\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(KernelContext_GetOutput, _Inout_ OrtKernelContext* context, _In_ size_t index,\r\n                  _In_ const int64_t* dim_values, size_t dim_count, _Outptr_ OrtValue** out);\r\n\r\n  /// @}\r\n  /// \\name OrtEnv\r\n  /// @{\r\n  ORT_CLASS_RELEASE(Env);\r\n  /// @}\r\n  /// \\name OrtStatus\r\n  /// @{\r\n  ORT_CLASS_RELEASE(Status);\r\n  /// @}\r\n  /// \\name OrtMemoryInfo\r\n  /// @{\r\n  ORT_CLASS_RELEASE(MemoryInfo);\r\n  /// @}\r\n  /// \\name OrtSession\r\n  /// @{\r\n  ORT_CLASS_RELEASE(Session);  // Don't call ReleaseSession from Dllmain (because session owns a thread pool)\r\n  /// @}\r\n  /// \\name OrtValue\r\n  /// @{\r\n  ORT_CLASS_RELEASE(Value);\r\n  /// @}\r\n  /// \\name OrtRunOptions\r\n  /// @{\r\n  ORT_CLASS_RELEASE(RunOptions);\r\n  /// @}\r\n  /// \\name OrtTypeInfo\r\n  /// @{\r\n  ORT_CLASS_RELEASE(TypeInfo);\r\n  /// @}\r\n  /// \\name OrtTensorTypeAndShapeInfo\r\n  /// @{\r\n  ORT_CLASS_RELEASE(TensorTypeAndShapeInfo);\r\n  /// @}\r\n  /// \\name OrtSessionOptions\r\n  /// @{\r\n  ORT_CLASS_RELEASE(SessionOptions);\r\n  /// @}\r\n  /// \\name OrtCustomOpDomain\r\n  /// @{\r\n  ORT_CLASS_RELEASE(CustomOpDomain);\r\n\r\n  /// @}\r\n  /// \\name OrtTypeInfo\r\n  /// @{\r\n\r\n  /** \\brief Get denotation from type information\r\n   *\r\n   * Augments ::OrtTypeInfo to return denotations on the type.\r\n   *\r\n   * This is used by WinML to determine if an input/output is intended to be an Image or a Tensor.\r\n   *\r\n   * \\param[in] type_info\r\n   * \\param[out] denotation Pointer to the null terminated denotation string is written to this pointer. This pointer is valid until the object is destroyed or the name is changed, do not free.\r\n   * \\param[out] len Length in bytes of the string returned in `denotation`\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetDenotationFromTypeInfo, _In_ const OrtTypeInfo* type_info, _Out_ const char** const denotation,\r\n                  _Out_ size_t* len);\r\n\r\n  /** \\brief Get detailed map information from an ::OrtTypeInfo\r\n   *\r\n   * This augments ::OrtTypeInfo to return an ::OrtMapTypeInfo when the type is a map.\r\n   * The OrtMapTypeInfo has additional information about the map's key type and value type.\r\n   *\r\n   * This is used by WinML to support model reflection APIs.\r\n   *\r\n   * \\param[out] type_info\r\n   * \\param[out] out A pointer to the ::OrtMapTypeInfo. Do not free this value. If type_info\r\n   *             does not contain a map, this value will be set to nullptr.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CastTypeInfoToMapTypeInfo, _In_ const OrtTypeInfo* type_info,\r\n                  _Outptr_result_maybenull_ const OrtMapTypeInfo** out);\r\n\r\n  /** \\brief Cast ::OrtTypeInfo to an ::OrtSequenceTypeInfo\r\n   *\r\n   * This api augments ::OrtTypeInfo to return an ::OrtSequenceTypeInfo when the type is a sequence.\r\n   * The ::OrtSequenceTypeInfo has additional information about the sequence's element type.\r\n   *\r\n   * This is used by WinML to support model reflection APIs.\r\n   *\r\n   * \\param[in] type_info\r\n   * \\param[out] out A pointer to the OrtSequenceTypeInfo. Do not free this value. If type_info\r\n   *             doesn not contain a sequence, this value will be set to nullptr.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CastTypeInfoToSequenceTypeInfo, _In_ const OrtTypeInfo* type_info,\r\n                  _Outptr_result_maybenull_ const OrtSequenceTypeInfo** out);\r\n\r\n  /// @}\r\n  /// \\name OrtMapTypeInfo\r\n  /// @{\r\n\r\n  /** \\brief Get key type from an ::OrtMapTypeInfo\r\n   *\r\n   * Key types are restricted to being scalar types.\r\n   *\r\n   * This is used by WinML to support model reflection APIs.\r\n   *\r\n   * \\param[in] map_type_info\r\n   * \\param[out] out\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetMapKeyType, _In_ const OrtMapTypeInfo* map_type_info, _Out_ enum ONNXTensorElementDataType* out);\r\n\r\n  /** \\brief Get the value type from an ::OrtMapTypeInfo\r\n   *\r\n   * \\param[in] map_type_info\r\n   * \\param[out] type_info A copy of the OrtTypeInfo for the map value type.\r\n   *                       The user must free this value with ReleaseTypeInfo.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetMapValueType, _In_ const OrtMapTypeInfo* map_type_info, _Outptr_ OrtTypeInfo** type_info);\r\n\r\n  /// @}\r\n  /// \\name OrtSequenceTypeInfo\r\n  /// @{\r\n\r\n  /** \\brief Get element type from an ::OrtSequenceTypeInfo\r\n   *\r\n   * This is used by WinML to support model reflection APIs.\r\n   *\r\n   * \\param[in] sequence_type_info\r\n   * \\param[out] type_info A copy of the OrtTypeInfo for the sequence element type.\r\n   *                       The user must free this value with ReleaseTypeInfo.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetSequenceElementType, _In_ const OrtSequenceTypeInfo* sequence_type_info,\r\n                  _Outptr_ OrtTypeInfo** type_info);\r\n\r\n  /// @}\r\n  /// \\name OrtMapTypeInfo\r\n  /// @{\r\n  ORT_CLASS_RELEASE(MapTypeInfo);\r\n  /// @}\r\n  /// \\name OrtSequenceTypeInfo\r\n  /// @{\r\n  ORT_CLASS_RELEASE(SequenceTypeInfo);\r\n\r\n  /// @}\r\n  /// \\name OrtSession\r\n  /// @{\r\n\r\n  /** \\brief End profiling and return filename of the profile data\r\n   *\r\n   * Profiling is turned on through OrtApi::EnableProfiling\r\n   *\r\n   * \\param[in] session\r\n   * \\param[in] allocator\r\n   * \\param[out] out Null terminated string of the filename, allocated using `allocator`. Must be freed using `allocator`\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionEndProfiling, _In_ OrtSession* session, _Inout_ OrtAllocator* allocator, _Outptr_ char** out);\r\n\r\n  /** \\brief Get ::OrtModelMetadata from an ::OrtSession\r\n   *\r\n   * \\param[in] session\r\n   * \\param[out] out Newly created ::OrtModelMetadata. Must be freed using OrtApi::ReleaseModelMetadata\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionGetModelMetadata, _In_ const OrtSession* session, _Outptr_ OrtModelMetadata** out);\r\n\r\n  /// @}\r\n  /// \\name OrtModelMetadata\r\n  /// @{\r\n\r\n  /** \\brief Get `producer name` from an ::OrtModelMetadata\r\n   *\r\n   * \\param[in] model_metadata\r\n   * \\param[in] allocator\r\n   * \\param[out] value Set to a null terminated string allocated using `allocator`. Must be freed using `allocator`\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(ModelMetadataGetProducerName, _In_ const OrtModelMetadata* model_metadata,\r\n                  _Inout_ OrtAllocator* allocator, _Outptr_ char** value);\r\n\r\n  /** \\brief Get `graph name` from an ::OrtModelMetadata\r\n   *\r\n   * \\param[in] model_metadata\r\n   * \\param[in] allocator\r\n   * \\param[out] value Set to a null terminated string allocated using `allocator`. Must be freed using `allocator`\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(ModelMetadataGetGraphName, _In_ const OrtModelMetadata* model_metadata,\r\n                  _Inout_ OrtAllocator* allocator, _Outptr_ char** value);\r\n\r\n  /** \\brief Get `domain` from an ::OrtModelMetadata\r\n   *\r\n   * \\param[in] model_metadata\r\n   * \\param[in] allocator\r\n   * \\param[out] value Set to a null terminated string allocated using `allocator`. Must be freed using `allocator`\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(ModelMetadataGetDomain, _In_ const OrtModelMetadata* model_metadata, _Inout_ OrtAllocator* allocator,\r\n                  _Outptr_ char** value);\r\n\r\n  /** \\brief Get `description` from an ::OrtModelMetadata\r\n   *\r\n   * \\param[in] model_metadata\r\n   * \\param[in] allocator\r\n   * \\param[out] value Set to a null terminated string allocated using `allocator`. Must be freed using `allocator`\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(ModelMetadataGetDescription, _In_ const OrtModelMetadata* model_metadata,\r\n                  _Inout_ OrtAllocator* allocator, _Outptr_ char** value);\r\n\r\n  /** \\brief Return data for a key in the custom metadata map in an ::OrtModelMetadata\r\n   *\r\n   * \\param[in] model_metadata\r\n   * \\param[in] allocator\r\n   * \\param[in] key Null terminated string\r\n   * \\param[out] value Set to a null terminated string allocated using `allocator`. Must be freed using `allocator`\r\n   * `value` will be set to nullptr if the given key is not found in the custom metadata map.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(ModelMetadataLookupCustomMetadataMap, _In_ const OrtModelMetadata* model_metadata,\r\n                  _Inout_ OrtAllocator* allocator, _In_ const char* key, _Outptr_result_maybenull_ char** value);\r\n\r\n  /** \\brief Get version number from an ::OrtModelMetadata\r\n   *\r\n   * \\param[in] model_metadata\r\n   * \\param[out] value Set to the version number\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(ModelMetadataGetVersion, _In_ const OrtModelMetadata* model_metadata, _Out_ int64_t* value);\r\n\r\n  ORT_CLASS_RELEASE(ModelMetadata);\r\n\r\n  /// @}\r\n  /// \\name OrtEnv\r\n  /// @{\r\n\r\n  /** \\brief Create an OrtEnv\r\n   *\r\n   * Create an environment with global threadpools that will be shared across sessions.\r\n   * Use this in conjunction with OrtApi::DisablePerSessionThreads or else the session will use\r\n   * its own thread pools.\r\n   *\r\n   * \\param[in] log_severity_level The log severity level.\r\n   * \\param[in] logid The log identifier.\r\n   * \\param[in] tp_options\r\n   * \\param[out] out Returned newly created OrtEnv. Must be freed with OrtApi::ReleaseEnv\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateEnvWithGlobalThreadPools, OrtLoggingLevel log_severity_level, _In_ const char* logid,\r\n                  _In_ const OrtThreadingOptions* tp_options, _Outptr_ OrtEnv** out);\r\n\r\n  /// @}\r\n  /// \\name OrtSessionOptions\r\n  /// @{\r\n\r\n  /** \\brief Use global thread pool on a session\r\n   *\r\n   * Disable using per session thread pool and use the shared global threadpool.\r\n   * This should be used in conjunction with OrtApi::CreateEnvWithGlobalThreadPools.\r\n   *\r\n   * \\param[in] options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(DisablePerSessionThreads, _Inout_ OrtSessionOptions* options);\r\n\r\n  /// @}\r\n  /// \\name OrtThreadingOptions\r\n  /// @{\r\n\r\n  /** \\brief Create an ::OrtThreadingOptions\r\n   *\r\n   * \\param[out] out Newly created ::OrtThreadingOptions. Must be freed with OrtApi::ReleaseThreadingOptions\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateThreadingOptions, _Outptr_ OrtThreadingOptions** out);\r\n\r\n  ORT_CLASS_RELEASE(ThreadingOptions);\r\n\r\n  /// @}\r\n  /// \\name OrtModelMetadata\r\n  /// @{\r\n\r\n  /**\r\n   *\r\n   * \\param[in] model_metadata\r\n   * \\param[in] allocator\r\n   * \\param[out] keys Array of null terminated strings (array count = num_keys) allocated using `allocator`.\r\n   *  The strings and the pointer array must be freed using `allocator`\r\n   *  `keys` will be set to nullptr if the custom metadata map is empty.\r\n   * \\param[out] num_keys Set to the number of elements in the `keys` array\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(ModelMetadataGetCustomMetadataMapKeys, _In_ const OrtModelMetadata* model_metadata,\r\n                  _Inout_ OrtAllocator* allocator, _Outptr_result_buffer_maybenull_(*num_keys) char*** keys, _Out_ int64_t* num_keys);\r\n\r\n  /// @}\r\n  /// \\name OrtSessionOptions\r\n  /// @{\r\n\r\n  /**\r\n   *\r\n   * Override symbolic dimensions (by specific name strings) with actual values\r\n   * if known at session initialization time to enable optimizations that can\r\n   * take advantage of fixed values (such as memory planning, etc)\r\n   *\r\n   */\r\n  ORT_API2_STATUS(AddFreeDimensionOverrideByName,\r\n                  _Inout_ OrtSessionOptions* options, _In_ const char* dim_name,\r\n                  _In_ int64_t dim_value);\r\n\r\n  /// @}\r\n  /// \\name Misc\r\n  /// @{\r\n\r\n  /** \\brief Get the names of all available providers\r\n   *\r\n   * \\note The providers in the list are not guaranteed to be usable. They may fail to load due to missing system dependencies.\r\n   *    For example, if the CUDA/cuDNN libraries are not installed, the CUDA provider will report an error when it is added to the session options.\r\n   *\r\n   * \\param[out] out_ptr Set to a pointer to an array of null terminated strings of the available providers. The entries and the\r\n   *    array itself must be freed using OrtApi::ReleaseAvailableProviders\r\n   * \\param[out] provider_length Set to the number of entries in the `out_ptr` array\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetAvailableProviders, _Outptr_ char*** out_ptr, _Out_ int* provider_length);\r\n\r\n  /** \\brief Release data from OrtApi::GetAvailableProviders. This API will never fail\r\n   * so you can rely on it in a noexcept code.\r\n   *\r\n   * \\param[in] ptr The `out_ptr` result from OrtApi::GetAvailableProviders.\r\n   * \\param[in] providers_length The `provider_length` result from OrtApi::GetAvailableProviders\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(ReleaseAvailableProviders, _In_ char** ptr,\r\n                  _In_ int providers_length);\r\n\r\n  /// @}\r\n  /// \\name OrtValue\r\n  /// @{\r\n\r\n  /** \\brief Get the length of a single string in a string tensor\r\n   *\r\n   * \\param[in] value A string tensor\r\n   * \\param[in] index Index of the string in the tensor\r\n   * \\param[out] out Set to number of bytes of the string element\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetStringTensorElementLength, _In_ const OrtValue* value, size_t index, _Out_ size_t* out);\r\n\r\n  /** \\brief Get a single string from a string tensor\r\n   *\r\n   * \\param[in] value A string tensor\r\n   * \\param[in] s_len Number of bytes in the `s` buffer. Must match the value returned by OrtApi::GetStringTensorElementLength.\r\n   * \\param[in] index Index of the string in the tensor\r\n   * \\param[out] s The string element contents in UTF-8 encoding. The string is NOT null-terminated.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetStringTensorElement, _In_ const OrtValue* value, size_t s_len, size_t index, _Out_writes_bytes_all_(s_len) void* s);\r\n\r\n  /** \\brief Set a single string in a string tensor\r\n   *\r\n   * \\param[in] value A string tensor\r\n   * \\param[in] s A null terminated UTF-8 encoded string\r\n   * \\param[in] index Index of the string in the tensor to set\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(FillStringTensorElement, _Inout_ OrtValue* value, _In_ const char* s, size_t index);\r\n\r\n  /// @}\r\n  /// \\name OrtSessionOptions\r\n  /// @{\r\n\r\n  /** \\brief Set a session configuration entry as a pair of strings\r\n   *\r\n   * If a configuration with same key exists, this will overwrite the configuration with the given config_value.\r\n   *\r\n   * The config_key and the format of config_value are defined in onnxruntime_session_options_config_keys.h\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] config_key A null terminated string representation of the config key\r\n   * \\param[in] config_value A null terminated string representation of the config value\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(AddSessionConfigEntry, _Inout_ OrtSessionOptions* options,\r\n                  _In_z_ const char* config_key, _In_z_ const char* config_value);\r\n\r\n  /// @}\r\n  /// \\name OrtAllocator\r\n  /// @{\r\n\r\n  /** \\brief Create an allocator for an ::OrtSession following an ::OrtMemoryInfo\r\n   *\r\n   * The allocator wraps the internal allocator from the OrtSession and becomes invalid when the session does.\r\n   *\r\n   * \\param[in] session\r\n   * \\param[in] mem_info valid ::OrtMemoryInfo instance\r\n   * \\param[out] out Newly created ::OrtAllocator. Must be freed with OrtApi::ReleaseAllocator\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateAllocator, _In_ const OrtSession* session, _In_ const OrtMemoryInfo* mem_info,\r\n                  _Outptr_ OrtAllocator** out);\r\n\r\n  /** \\brief Release an ::OrtAllocator obtained from OrtApi::CreateAllocator\r\n   */\r\n  ORT_CLASS_RELEASE(Allocator);\r\n\r\n  /// @}\r\n  /// \\name OrtSession\r\n  /// @{\r\n\r\n  /** \\brief Run a model using Io Bindings for the inputs & outputs\r\n   *\r\n   * \\see OrtApi::Run\r\n   *\r\n   * \\param[in] session\r\n   * \\param[in] run_options\r\n   * \\param[in] binding_ptr\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(RunWithBinding, _Inout_ OrtSession* session, _In_ const OrtRunOptions* run_options, _In_ const OrtIoBinding* binding_ptr);\r\n\r\n  /** \\brief Create an ::OrtIoBinding instance\r\n   *\r\n   * An IoBinding object allows one to bind pre-allocated ::OrtValue%s to input names.\r\n   * Thus if you want to use a raw on device buffer as input or output you can avoid\r\n   * extra copy during runtime.\r\n   *\r\n   * \\param[in] session\r\n   * \\param[out] out Newly created ::OrtIoBinding. Must be freed with OrtApi::ReleaseIoBinding\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateIoBinding, _Inout_ OrtSession* session, _Outptr_ OrtIoBinding** out);\r\n\r\n  /// @}\r\n  /// \\name OrtIoBinding\r\n  /// @{\r\n\r\n  /** \\brief Release an ::OrtIoBinding obtained from OrtApi::CreateIoBinding\r\n   */\r\n  ORT_CLASS_RELEASE(IoBinding);\r\n\r\n  /** \\brief Bind an ::OrtValue to an ::OrtIoBinding input\r\n   *\r\n   * When using OrtApi::RunWithBinding this value is used for the named input\r\n   *\r\n   * \\param[in] binding_ptr\r\n   * \\param[in] name Name for the model input\r\n   * \\param[in] val_ptr ::OrtValue of Tensor type.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(BindInput, _Inout_ OrtIoBinding* binding_ptr, _In_ const char* name, _In_ const OrtValue* val_ptr);\r\n\r\n  /** \\brief Bind an ::OrtValue to an ::OrtIoBinding output\r\n   *\r\n   * When using OrtApi::RunWithBinding this value is used for the named output\r\n   *\r\n   * \\param[in] binding_ptr\r\n   * \\param[in] name Null terminated string of the model output name\r\n   * \\param[in] val_ptr ::OrtValue of Tensor type.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(BindOutput, _Inout_ OrtIoBinding* binding_ptr, _In_ const char* name, _In_ const OrtValue* val_ptr);\r\n\r\n  /** \\brief Bind an ::OrtIoBinding output to a device\r\n   *\r\n   * Binds the ::OrtValue to a device which is specified by ::OrtMemoryInfo.\r\n   * You can either create an instance of ::OrtMemoryInfo with a device id or obtain one from the allocator that you have created/are using\r\n   * This is useful when one or more outputs have dynamic shapes and, it is hard to pre-allocate and bind a chunk of\r\n   * memory within ::OrtValue ahead of time.\r\n   *\r\n   * \\see OrtApi::RunWithBinding\r\n   *\r\n   * \\param[in] binding_ptr\r\n   * \\param[in] name Null terminated string of the device name\r\n   * \\param[in] mem_info_ptr\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(BindOutputToDevice, _Inout_ OrtIoBinding* binding_ptr, _In_ const char* name, _In_ const OrtMemoryInfo* mem_info_ptr);\r\n\r\n  /** \\brief Get the names of an ::OrtIoBinding's outputs\r\n   *\r\n   * Returns the names of the outputs in the order they were bound. This is useful after running the model\r\n   * with bound outputs because the returned names are in order in which output ::OrtValue are returned. This is useful if\r\n   * the order of outputs and their names is not known.\r\n   *\r\n   * \\param[in] binding_ptr\r\n   * \\param[in] allocator Allocator used to allocate continuous buffers for output strings and lengths.\r\n   * \\param[out] buffer Returns an array of non-null terminated UTF-8 strings. The number of strings stored is returned in the count parameter.\r\n   *   This buffer is allocated using `allocator` and must be freed using it.\r\n   * \\param[out] lengths Returns an array of `count` lengths of the strings returned in `buffer`\r\n   *   This buffer is allocated using `allocator` and must be freed using it.\r\n   * \\param[out] count Number of strings returned. If `binding_ptr` has no bound outputs, zero is returned,\r\n   *              no memory allocation is performed and buffer and lengths are set to nullptr.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetBoundOutputNames, _In_ const OrtIoBinding* binding_ptr, _In_ OrtAllocator* allocator,\r\n                  _Out_ char** buffer, _Out_writes_all_(count) size_t** lengths, _Out_ size_t* count);\r\n\r\n  /** \\brief Get the output ::OrtValue objects from an ::OrtIoBinding\r\n   *\r\n   * Returns an array of pointers to individually allocated ::OrtValue%s that contain results of a model execution with OrtApi::RunWithBinding\r\n   * The array contains the same number of ::OrtValue%s and they are in the same order as they were bound with OrtApi::BindOutput\r\n   * or OrtApi::BindOutputToDevice.\r\n   *\r\n   * The returned ::OrtValue%s must be released using OrtApi::ReleaseValue after they are no longer needed.\r\n   * The array is allocated using the specified instance of the allocator and must be freed using the same allocator after\r\n   * all the ::OrtValue%s contained therein are individually released.\r\n   *\r\n   * \\param[in] binding_ptr\r\n   * \\param[in] allocator Allocator used to allocate output array\r\n   * \\param[out] output Set to the allocated array of allocated ::OrtValue outputs. Set to nullptr if there are 0 outputs.\r\n   * \\param[out] output_count Set to number of ::OrtValue%s returned\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetBoundOutputValues, _In_ const OrtIoBinding* binding_ptr, _In_ OrtAllocator* allocator,\r\n                  _Out_writes_all_(output_count) OrtValue*** output, _Out_ size_t* output_count);\r\n\r\n  /** \\brief Clears any previously set Inputs for an ::OrtIoBinding\r\n   */\r\n  void(ORT_API_CALL* ClearBoundInputs)(_Inout_ OrtIoBinding* binding_ptr) NO_EXCEPTION ORT_ALL_ARGS_NONNULL;\r\n\r\n  /** \\brief Clears any previously set Outputs for an ::OrtIoBinding\r\n   */\r\n  void(ORT_API_CALL* ClearBoundOutputs)(_Inout_ OrtIoBinding* binding_ptr) NO_EXCEPTION ORT_ALL_ARGS_NONNULL;\r\n\r\n  /// @}\r\n  /// \\name OrtValue\r\n  /// @{\r\n\r\n  /** \\brief Direct memory access to a specified tensor element\r\n   *\r\n   * For example, given a tensor with shape of [3,224,224], a pointer to the element at location [2,150,128] can be retrieved\r\n   *\r\n   * This function only works for numeric type tensors (No strings, etc).\r\n   * This is a no-copy method whose returned pointer is valid until the passed in ::OrtValue is free'd.\r\n   *\r\n   * \\param[in] value\r\n   * \\param[in] location_values Pointer to an array of index values that specify an element's location relative to its shape\r\n   * \\param[in] location_values_count Number of elements in location_values. Must match the number of elements in the tensor's shape.\r\n   * \\param[out] out Set to a pointer to the element specified\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(TensorAt, _Inout_ OrtValue* value, const int64_t* location_values, size_t location_values_count, _Outptr_ void** out);\r\n\r\n  /// @}\r\n  /// \\name OrtEnv\r\n  /// @{\r\n\r\n  /** \\brief Create an allocator and register it with the ::OrtEnv\r\n   *\r\n   * Enables sharing the allocator between multiple sessions that use the same env instance.\r\n   * Lifetime of the created allocator will be valid for the duration of the environment.\r\n   * Returns an error if an allocator with the same ::OrtMemoryInfo is already registered.\r\n   *\r\n   * See https://onnxruntime.ai/docs/get-started/with-c.html for details.\r\n   *\r\n   * \\param[in] env ::OrtEnv instance\r\n   * \\param[in] mem_info\r\n   * \\param[in] arena_cfg Pass nullptr for defaults\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateAndRegisterAllocator, _Inout_ OrtEnv* env, _In_ const OrtMemoryInfo* mem_info,\r\n                  _In_ const OrtArenaCfg* arena_cfg);\r\n\r\n  /** \\brief Set language projection\r\n   *\r\n   * Set the language projection for collecting telemetry data when Env is created.\r\n   *\r\n   * The default is ORT_PROJECTION_C, which means it will classify the language not in the list to C also.\r\n   *\r\n   * \\param[in] ort_env\r\n   * \\param[in] projection\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetLanguageProjection, _In_ const OrtEnv* ort_env, _In_ OrtLanguageProjection projection);\r\n\r\n  /// @}\r\n  /// \\name OrtSession\r\n  /// @{\r\n\r\n  /** \\brief Return the time that profiling was started\r\n   *\r\n   * \\note The timer precision varies per platform. On Windows and MacOS, the precision will be ~100ns\r\n   *\r\n   * \\param[in] session\r\n   * \\param[out] out nanoseconds of profiling's start time\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionGetProfilingStartTimeNs, _In_ const OrtSession* session, _Outptr_ uint64_t* out);\r\n\r\n  /// @}\r\n  /// \\name OrtThreadingOptions\r\n  /// @{\r\n\r\n  /** \\brief Set global intra-op thread count\r\n   *\r\n   * This configures the global thread pool options to be used in the call to OrtApi::CreateEnvWithGlobalThreadPools\r\n   *\r\n   * \\param[in] tp_options\r\n   * \\param[in] intra_op_num_threads Number of threads, special values:<br>\r\n   *    0 = Use default thread count<br>\r\n   *    1 = The invoking thread will be used; no threads will be created in the thread pool.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetGlobalIntraOpNumThreads, _Inout_ OrtThreadingOptions* tp_options, int intra_op_num_threads);\r\n\r\n  /** \\brief Set global inter-op thread count\r\n   *\r\n   * This configures the global thread pool options to be used in the call to OrtApi::CreateEnvWithGlobalThreadPools\r\n   *\r\n   * \\param[in] tp_options\r\n   * \\param[in] inter_op_num_threads Number of threads, special values:<br>\r\n   *    0 = Use default thread count<br>\r\n   *    1 = The invoking thread will be used; no threads will be created in the thread pool.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetGlobalInterOpNumThreads, _Inout_ OrtThreadingOptions* tp_options, int inter_op_num_threads);\r\n\r\n  /** \\brief Set global spin control options\r\n   *\r\n   * This will configure the global thread pool options to be used in the call to OrtApi::CreateEnvWithGlobalThreadPools.\r\n   * Allow spinning of thread pools when their queues are empty. This will set the value for both\r\n   * inter_op and intra_op threadpools.\r\n   *\r\n   * \\param[in] tp_options\r\n   * \\param[in] allow_spinning Valid values are 0 or 1.<br>\r\n   *   0 = It won't spin (recommended if CPU usage is high)<br>\r\n   *   1 = Threadpool will spin to wait for queue to become non-empty\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetGlobalSpinControl, _Inout_ OrtThreadingOptions* tp_options, int allow_spinning);\r\n\r\n  /// @}\r\n  /// \\name OrtSessionOptions\r\n  /// @{\r\n\r\n  /** \\brief Add a pre-allocated initializer to a session\r\n   *\r\n   * If a model contains an initializer with a name that is same as the name passed to this call,\r\n   * ORT will use this initializer instance instead of deserializing one from the model file. This\r\n   * is useful when you want to share the same initializer across sessions.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] name Null terminated string of the initializer name\r\n   * \\param[in] val ::OrtValue containing the initializer. Its lifetime and the underlying initializer buffer must be\r\n   *   managed by the user (created using the OrtApi::CreateTensorWithDataAsOrtValue) and it must outlive the session object\r\n   *   to which it is added.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(AddInitializer, _Inout_ OrtSessionOptions* options, _In_z_ const char* name,\r\n                  _In_ const OrtValue* val);\r\n\r\n  /// @}\r\n  /// \\name OrtEnv\r\n  /// @{\r\n\r\n  /**\r\n   * Create a custom environment with global threadpools and logger that will be shared across sessions.\r\n   * Use this in conjunction with OrtApi::DisablePerSessionThreads or else the session will use\r\n   * its own thread pools.\r\n   *\r\n   * \\param[in] logging_function A pointer to a logging function.\r\n   * \\param[in] logger_param A pointer to arbitrary data passed as the ::OrtLoggingFunction `param` parameter to\r\n   *                         `logging_function`.\r\n   * \\param[in] log_severity_level The log severity level.\r\n   * \\param[in] logid The log identifier.\r\n   * \\param[in] tp_options\r\n   * \\param[out] out Newly created OrtEnv. Must be freed with OrtApi::ReleaseEnv\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateEnvWithCustomLoggerAndGlobalThreadPools, OrtLoggingFunction logging_function, _In_opt_ void* logger_param, OrtLoggingLevel log_severity_level,\r\n                  _In_ const char* logid, _In_ const struct OrtThreadingOptions* tp_options, _Outptr_ OrtEnv** out);\r\n\r\n  /// @}\r\n  /// \\name OrtSessionOptions\r\n  /// @{\r\n\r\n  /** \\brief Append CUDA provider to session options\r\n   *\r\n   * If CUDA is not available (due to a non CUDA enabled build, or if CUDA is not installed on the system), this function will return failure.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] cuda_options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsAppendExecutionProvider_CUDA,\r\n                  _In_ OrtSessionOptions* options, _In_ const OrtCUDAProviderOptions* cuda_options);\r\n\r\n  /** \\brief Append ROCM execution provider to the session options\r\n   *\r\n   * If ROCM is not available (due to a non ROCM enabled build, or if ROCM is not installed on the system), this function will return failure.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] rocm_options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsAppendExecutionProvider_ROCM,\r\n                  _In_ OrtSessionOptions* options, _In_ const OrtROCMProviderOptions* rocm_options);\r\n\r\n  /** \\brief Append OpenVINO execution provider to the session options\r\n   *\r\n   * If OpenVINO is not available (due to a non OpenVINO enabled build, or if OpenVINO is not installed on the system), this function will fail.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] provider_options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsAppendExecutionProvider_OpenVINO,\r\n                  _In_ OrtSessionOptions* options, _In_ const OrtOpenVINOProviderOptions* provider_options);\r\n\r\n  /// @}\r\n  /// \\name OrtThreadingOptions\r\n  /// @{\r\n\r\n  /** \\brief Set threading flush-to-zero and denormal-as-zero\r\n   *\r\n   * Sets global thread pool options to be used in the call to OrtApi::CreateEnvWithGlobalThreadPools.\r\n   * Flush-to-zero and denormal-as-zero are applied to threads in both intra and inter global thread pool.\r\n   * \\note This option is not needed if the models used have no denormals. Having no denormals is recommended as this option may hurt model accuracy.\r\n   *\r\n   * \\param[in] tp_options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetGlobalDenormalAsZero, _Inout_ OrtThreadingOptions* tp_options);\r\n\r\n  /// @}\r\n  /// \\name OrtArenaCfg\r\n  /// @{\r\n\r\n  /** \\deprecated Use OrtApi::CreateArenaCfgV2\r\n   *\r\n   * This will create the configuration of an arena that can eventually be used to define an arena based allocator's behavior\r\n   *\r\n   * \\param[in] max_mem Use 0 to allow ORT to choose the default\r\n   * \\param[in] arena_extend_strategy Use -1 to allow ORT to choose the default, 0 = kNextPowerOfTwo, 1 = kSameAsRequested\r\n   * \\param[in] initial_chunk_size_bytes Use -1 to allow ORT to choose the default\r\n   * \\param[in] max_dead_bytes_per_chunk Use -1 to allow ORT to choose the default\r\n   * \\param[in] out A pointer to an OrtArenaCfg instance\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateArenaCfg, _In_ size_t max_mem, int arena_extend_strategy, int initial_chunk_size_bytes,\r\n                  int max_dead_bytes_per_chunk, _Outptr_ OrtArenaCfg** out);\r\n\r\n  ORT_CLASS_RELEASE(ArenaCfg);\r\n\r\n  /// @}\r\n  /// \\name OrtModelMetadata\r\n  /// @{\r\n\r\n  /**\r\n   * Use this to obtain the description of the graph present in the model\r\n   * (doc_string field of the GraphProto message within the ModelProto message).\r\n   * If it doesn't exist, an empty string will be returned.\r\n   *\r\n   * \\param[in] model_metadata An instance of ::OrtModelMetadata\r\n   * \\param[in] allocator Allocator used to allocate the string that will be returned back\r\n   * \\param[out] value Set to a null terminated string allocated using `allocator`.  The caller is responsible for freeing it using `allocator`\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(ModelMetadataGetGraphDescription, _In_ const OrtModelMetadata* model_metadata,\r\n                  _Inout_ OrtAllocator* allocator, _Outptr_ char** value);\r\n\r\n  /// @}\r\n  /// \\name OrtSessionOptions\r\n  /// @{\r\n\r\n  /** \\brief Append TensorRT provider to session options\r\n   *\r\n   * If TensorRT is not available (due to a non TensorRT enabled build, or if TensorRT is not installed on the system), this function will return failure.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] tensorrt_options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsAppendExecutionProvider_TensorRT,\r\n                  _In_ OrtSessionOptions* options, _In_ const OrtTensorRTProviderOptions* tensorrt_options);\r\n\r\n  /// @}\r\n  /// \\name Misc\r\n  /// @{\r\n\r\n  /** \\brief Set current GPU device ID\r\n   *\r\n   * Set the current device id of the GPU execution provider (CUDA/tensorrt/rocm). The device id should be less\r\n   * than the total number of devices available. This is only useful when multiple-GPUs are installed and it is\r\n   * required to restrict execution to a single GPU.\r\n   *\r\n   * \\param[in] device_id\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetCurrentGpuDeviceId, _In_ int device_id);\r\n\r\n  /** \\brief Get current GPU device ID\r\n   *\r\n   * Get the current device id of the GPU execution provider (CUDA/tensorrt/rocm).\r\n   *\r\n   * \\see OrtApi::SetCurrentGpuDeviceId\r\n   *\r\n   * \\param[out] device_id\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetCurrentGpuDeviceId, _In_ int* device_id);\r\n\r\n  /// @}\r\n  /// \\name OrtKernelInfo\r\n  /// Custom operator APIs.\r\n  /// @{\r\n\r\n  /** \\brief Fetch an array of int64_t values stored as an attribute in the graph node\r\n   *\r\n   *\r\n   * If `out` is nullptr, the value of `size` is set to the true size of the attribute\r\n   * array's size, and a success status is returned.\r\n   *\r\n   * If the `size` parameter is greater than or equal to the actual attribute array's size,\r\n   * the value of `size` is set to the true size of the attribute array's size,\r\n   * the provided memory is filled with the attribute's contents,\r\n   * and a success status is returned.\r\n   *\r\n   * If the `size` parameter is less than the actual attribute array's size and `out`\r\n   * is not nullptr, the value of `size` is set to the true size of the attribute array's size\r\n   * and a failure status is returned.)\r\n   *\r\n   * \\param[in] info instance\r\n   * \\param[in] name name of the attribute to be parsed\r\n   * \\param[out] out pointer to memory where the attribute's contents are to be stored\r\n   * \\param[in, out] size actual size of attribute array\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(KernelInfoGetAttributeArray_float, _In_ const OrtKernelInfo* info, _In_ const char* name,\r\n                  _Out_ float* out, _Inout_ size_t* size);\r\n\r\n  /** \\brief Fetch an array of int64_t values stored as an attribute in the graph node\r\n   *\r\n   * If `out` is nullptr, the value of `size` is set to the true size of the attribute\r\n   * array's size, and a success status is returned.\r\n   *\r\n   * If the `size` parameter is greater than or equal to the actual attribute array's size,\r\n   * the value of `size` is set to the true size of the attribute array's size,\r\n   * the provided memory is filled with the attribute's contents,\r\n   * and a success status is returned.\r\n   *\r\n   * If the `size` parameter is less than the actual attribute array's size and `out`\r\n   * is not nullptr, the value of `size` is set to the true size of the attribute array's size\r\n   * and a failure status is returned.)\r\n   *\r\n   * \\param[in] info instance\r\n   * \\param[in] name name of the attribute to be parsed\r\n   * \\param[out] out pointer to memory where the attribute's contents are to be stored\r\n   * \\param[in, out] size actual size of attribute array\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(KernelInfoGetAttributeArray_int64, _In_ const OrtKernelInfo* info, _In_ const char* name,\r\n                  _Out_ int64_t* out, _Inout_ size_t* size);\r\n\r\n  /// @}\r\n  /// \\name OrtArenaCfg\r\n  /// @{\r\n\r\n  /** \\brief Create an ::OrtArenaCfg\r\n   *\r\n   * Create the configuration of an arena that can eventually be used to define an arena based allocator's behavior.\r\n   *\r\n   * Supported keys are (See https://onnxruntime.ai/docs/get-started/with-c.html for details on what the\r\n   * following parameters mean and how to choose these values.):\r\n   * \"max_mem\": Maximum memory that can be allocated by the arena based allocator.\r\n   *  Use 0 for ORT to pick the best value. Default is 0.\r\n   * \"arena_extend_strategy\": 0 = kNextPowerOfTwo, 1 = kSameAsRequested.\r\n   *  Use -1 to allow ORT to choose the default.\r\n   * \"initial_chunk_size_bytes\": (Possible) Size of the first allocation in the arena.\r\n   *  Only relevant if arena strategy is `kNextPowerOfTwo`. Use -1 to allow ORT to choose the default.\r\n   *  Ultimately, the first allocation size is determined by the allocation memory request.\r\n   * \"max_dead_bytes_per_chunk\": Threshold of unused memory in an allocated chunk of arena memory after\r\n   *  crossing which the current chunk is chunked into 2.\r\n   * \"initial_growth_chunk_size_bytes\": (Possible) Size of the second allocation in the arena.\r\n   *  Only relevant if arena strategy is `kNextPowerOfTwo`. Use -1 to allow ORT to choose the default.\r\n   * \"max_power_of_two_extend_bytes\": The maximum extend size if arena strategy is `kNextPowerOfTwo`.\r\n   *  It is not an allocation limit, it is only a limit for extension when requested byte is less than the limit.\r\n   *  When requested bytes is more than the limit, allocator will still return as requested.\r\n   *  Use -1 to allow ORT to choose the default 1GB for max_power_of_two_extend_bytes.\r\n   *  Ultimately, the allocation size is determined by the allocation memory request.\r\n   *  Further allocation sizes are governed by the arena extend strategy.\r\n   *\r\n   * \\param[in] arena_config_keys Keys to configure the arena\r\n   * \\param[in] arena_config_values Values to configure the arena\r\n   * \\param[in] num_keys Number of keys in `arena_config_keys` and `arena_config_values`\r\n   * \\param[out] out Newly created ::OrtArenaCfg. Must be freed with OrtApi::ReleaseArenaCfg\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateArenaCfgV2, _In_reads_(num_keys) const char* const* arena_config_keys,\r\n                  _In_reads_(num_keys) const size_t* arena_config_values, _In_ size_t num_keys,\r\n                  _Outptr_ OrtArenaCfg** out);\r\n\r\n  /// @}\r\n  /// \\name OrtRunOptions\r\n  /// @{\r\n\r\n  /** \\brief Set a single run configuration entry as a pair of strings\r\n   *\r\n   * If a configuration with same key exists, this will overwrite the configuration with the given config_value\r\n   *\r\n   * The config_key and the format of config_value are defined in onnxruntime_run_options_config_keys.h\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] config_key A null terminated string representation of the config key\r\n   * \\param[in] config_value  A null terminated string representation of the config value\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(AddRunConfigEntry, _Inout_ OrtRunOptions* options,\r\n                  _In_z_ const char* config_key, _In_z_ const char* config_value);\r\n\r\n  /// @}\r\n  /// \\name OrtPrepackedWeightsContainer\r\n  /// @{\r\n\r\n  /** \\brief Create an ::OrtPrepackedWeightsContainer\r\n   *\r\n   * This container will hold pre-packed buffers of shared initializers for sharing between sessions\r\n   * (i.e.) if there are shared initializers that can be shared between sessions, the pre-packed buffers\r\n   * of these (if any) may possibly be shared to provide memory footprint savings. Pass this container\r\n   * to sessions that you would like to share pre-packed buffers of shared initializers at session\r\n   * creation time.\r\n   *\r\n   *  \\param[out] out Newly created ::OrtPrepackedWeightsContainer. Must be freed with OrtApi::ReleasePrepackedWeightsContainer\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreatePrepackedWeightsContainer, _Outptr_ OrtPrepackedWeightsContainer** out);\r\n\r\n  /** \\brief Release OrtPrepackedWeightsContainer instance\r\n   *\r\n   * \\note instance must not be released until the sessions using it are released\r\n   */\r\n  ORT_CLASS_RELEASE(PrepackedWeightsContainer);\r\n\r\n  /// @}\r\n  /// \\name OrtSession\r\n  /// @{\r\n\r\n  /** \\brief Create session with prepacked weights container\r\n   *\r\n   * Same functionality offered by OrtApi::CreateSession except that a container that contains\r\n   * pre-packed weights' buffers is written into/read from by the created session.\r\n   * This is useful when used in conjunction with OrtApi::AddInitializer which injects\r\n   * shared initializer info into sessions. Wherever possible, the pre-packed versions of these\r\n   * shared initializers are cached in this container so that multiple sessions can just re-use\r\n   * these instead of duplicating these in memory.\r\n   *\r\n   * \\param[in] env OrtEnv instance instance\r\n   * \\param[in] model_path Null terminated string of the path (wchar on Windows, char otherwise)\r\n   * \\param[in] options\r\n   * \\param[in] prepacked_weights_container\r\n   * \\param[out] out Newly created ::OrtSession. Must be freed with OrtApi::ReleaseSession\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateSessionWithPrepackedWeightsContainer, _In_ const OrtEnv* env, _In_ const ORTCHAR_T* model_path,\r\n                  _In_ const OrtSessionOptions* options,\r\n                  _Inout_ OrtPrepackedWeightsContainer* prepacked_weights_container,\r\n                  _Outptr_ OrtSession** out);\r\n\r\n  /** \\brief Create session from memory with prepacked weights container\r\n   *\r\n   * Same functionality offered by OrtApi::CreateSessionFromArray except that a container that contains\r\n   * pre-packed weights' buffers is written into/read from by the created session.\r\n   * This is useful when used in conjunction with OrtApi::AddInitializer which injects\r\n   * shared initializer info into sessions. Wherever possible, the pre-packed versions of these\r\n   * shared initializers are cached in this container so that multiple sessions can just re-use\r\n   * these instead of duplicating these in memory.\r\n   *\r\n   * \\param[in] env\r\n   * \\param[in] model_data Array of bytes holding the model\r\n   * \\param[in] model_data_length Number of bytes in `model_data_model`\r\n   * \\param[in] options\r\n   * \\param[in] prepacked_weights_container\r\n   * \\param[out] out Newly created ::OrtSession. Must be freed with OrtApi::ReleaseSession\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateSessionFromArrayWithPrepackedWeightsContainer, _In_ const OrtEnv* env,\r\n                  _In_ const void* model_data, size_t model_data_length,\r\n                  _In_ const OrtSessionOptions* options,\r\n                  _Inout_ OrtPrepackedWeightsContainer* prepacked_weights_container,\r\n                  _Outptr_ OrtSession** out);\r\n\r\n  /// @}\r\n  /// \\name OrtSessionOptions\r\n  /// @{\r\n\r\n  /** \\brief Append TensorRT execution provider to the session options\r\n   *\r\n   * If TensorRT is not available (due to a non TensorRT enabled build), this function will return failure.\r\n   *\r\n   * This is slightly different from OrtApi::SessionOptionsAppendExecutionProvider_TensorRT, it takes an\r\n   * ::OrtTensorRTProviderOptions which is publicly defined. This takes an opaque ::OrtTensorRTProviderOptionsV2\r\n   * which must be created with OrtApi::CreateTensorRTProviderOptions.\r\n   *\r\n   * For OrtApi::SessionOptionsAppendExecutionProvider_TensorRT, the user needs to instantiate ::OrtTensorRTProviderOptions\r\n   * as well as allocate/release buffers for some members of ::OrtTensorRTProviderOptions.\r\n   * Here, OrtApi::CreateTensorRTProviderOptions and Ortapi::ReleaseTensorRTProviderOptions will do the memory management for you.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] tensorrt_options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsAppendExecutionProvider_TensorRT_V2,\r\n                  _In_ OrtSessionOptions* options, _In_ const OrtTensorRTProviderOptionsV2* tensorrt_options);\r\n\r\n  /// @}\r\n  /// \\name OrtTensorRTProviderOptionsV2\r\n  /// @{\r\n\r\n  /** \\brief Create an OrtTensorRTProviderOptionsV2\r\n   *\r\n   * \\param[out] out Newly created ::OrtTensorRTProviderOptionsV2. Must be released with OrtApi::ReleaseTensorRTProviderOptions\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateTensorRTProviderOptions, _Outptr_ OrtTensorRTProviderOptionsV2** out);\r\n\r\n  /** \\brief Set options in a TensorRT Execution Provider.\r\n   *\r\n   * Please refer to https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html#cc\r\n   * to know the available keys and values. Key should be in null terminated string format of the member of ::OrtTensorRTProviderOptionsV2\r\n   * and value should be its related range. Recreates the options and only sets the supplied values.\r\n   *\r\n   * For example, key=\"trt_max_workspace_size\" and value=\"2147483648\"\r\n   *\r\n   * \\param[in] tensorrt_options\r\n   * \\param[in] provider_options_keys Array of UTF-8 null-terminated string for provider options keys\r\n   * \\param[in] provider_options_values Array of UTF-8 null-terminated string for provider options values\r\n   * \\param[in] num_keys Number of elements in the `provider_option_keys` and `provider_options_values` arrays\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(UpdateTensorRTProviderOptions, _Inout_ OrtTensorRTProviderOptionsV2* tensorrt_options,\r\n                  _In_reads_(num_keys) const char* const* provider_options_keys,\r\n                  _In_reads_(num_keys) const char* const* provider_options_values,\r\n                  _In_ size_t num_keys);\r\n\r\n  /** \\brief Get serialized TensorRT provider options string.\r\n   *\r\n   * For example, \"trt_max_workspace_size=2147483648;trt_max_partition_iterations=10;trt_int8_enable=1;......\"\r\n   *\r\n   * \\param tensorrt_options - OrtTensorRTProviderOptionsV2 instance\r\n   * \\param allocator - a ptr to an instance of OrtAllocator obtained with OrtApi::CreateAllocator or OrtApi::GetAllocatorWithDefaultOptions\r\n   *                      the specified allocator will be used to allocate continuous buffers for output strings and lengths.\r\n   * \\param ptr - is a UTF-8 null terminated string allocated using 'allocator'. The caller is responsible for using the same allocator to free it.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetTensorRTProviderOptionsAsString, _In_ const OrtTensorRTProviderOptionsV2* tensorrt_options, _Inout_ OrtAllocator* allocator, _Outptr_ char** ptr);\r\n\r\n  /** \\brief Release an ::OrtTensorRTProviderOptionsV2\r\n   *\r\n   * \\note This is an exception in the naming convention of other Release* functions, as the name of the method does not have the V2 suffix, but the type does\r\n   */\r\n  void(ORT_API_CALL* ReleaseTensorRTProviderOptions)(_Frees_ptr_opt_ OrtTensorRTProviderOptionsV2* input);\r\n\r\n  /// @}\r\n  /// \\name OrtSessionOptions\r\n  /// @{\r\n\r\n  /** \\brief Enable custom operators\r\n   *\r\n   * See onnxruntime-extensions: https://github.com/microsoft/onnxruntime-extensions.git\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(EnableOrtCustomOps, _Inout_ OrtSessionOptions* options);\r\n\r\n  /// @}\r\n  /// \\name OrtAllocator\r\n  /// @{\r\n\r\n  /** \\brief Register a custom allocator\r\n   *\r\n   * Enables sharing between multiple sessions that use the same env instance.\r\n   * Returns an error if an allocator with the same ::OrtMemoryInfo is already registered.\r\n   *\r\n   * The behavior of this is exactly the same as OrtApi::CreateAndRegisterAllocator except\r\n   * instead of ORT creating an allocator based on provided info, in this case\r\n   * ORT uses the user-provided custom allocator.\r\n   * See https://onnxruntime.ai/docs/get-started/with-c.html for details.\r\n   *\r\n   * \\param[in] env\r\n   * \\param[in] allocator User provided allocator\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(RegisterAllocator, _Inout_ OrtEnv* env, _In_ OrtAllocator* allocator);\r\n\r\n  /** \\brief Unregister a custom allocator\r\n   *\r\n   * It is an error if you provide an ::OrtMemoryInfo not corresponding to any\r\n   * registered allocators for sharing.\r\n   *\r\n   * \\param[in] env\r\n   * \\param[in] mem_info\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(UnregisterAllocator, _Inout_ OrtEnv* env,\r\n                  _In_ const OrtMemoryInfo* mem_info);\r\n\r\n  /// @}\r\n  /// \\name OrtValue\r\n  /// @{\r\n\r\n  /** \\brief Sets *out to 1 iff an ::OrtValue is a SparseTensor, and 0 otherwise\r\n   *\r\n   * \\param[in] value existing ::OrtValue\r\n   * \\param[out] out unless an error occurs, contains 1 iff the value contains an instance\r\n   *  of sparse tensor or 0 otherwise.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(IsSparseTensor, _In_ const OrtValue* value, _Out_ int* out);\r\n\r\n  /** \\brief Create an ::OrtValue with a sparse tensor that is empty.\r\n   *\r\n   * Use FillSparseTensor<Format>() functions to populate sparse tensor with non-zero values and\r\n   * format specific indices data.\r\n   * Use ReleaseValue to destroy the sparse tensor, this will also release the buffer inside the output value\r\n   * if any was allocated.\r\n   * \\param[in,out] allocator allocator to use when performing an allocation. Allocation will be performed\r\n   *   by FillSparseTensor<Format>() APIs. The lifespan of the allocator instance must eclipse the lifespan\r\n   *   this sparse tensor instance as the same allocator will be used to free memory.\r\n   * \\param[in] dense_shape shape of the original dense tensor\r\n   * \\param[in] dense_shape_len number of shape dimensions being passed\r\n   * \\param[in] type must be one of TENSOR_ELEMENT_DATA_TYPE_xxxx\r\n   * \\param[out] out Should be freed by calling ReleaseValue\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateSparseTensorAsOrtValue, _Inout_ OrtAllocator* allocator, _In_ const int64_t* dense_shape,\r\n                  size_t dense_shape_len, ONNXTensorElementDataType type, _Outptr_ OrtValue** out);\r\n\r\n  /**\r\n   * This fills populates an empty tensor that was created using OrtApi::CreateSparseTensorAsOrtValue.\r\n   * This will allocate required memory and copy the supplied NNZ values and COO indices into that memory allocation.\r\n   * Memory allocation is performed using the allocator that was specified with OrtApi::CreateSparseTensorAsOrtValue.\r\n   *\r\n   * \\param[in,out] ort_value ::OrtValue to populate with data\r\n   * \\param[in] data_mem_info serves to identify the location of the data to be copied. If the allocator specified\r\n   *  at the creation time has memory info that is not the same as mem_info argument to this function a X-device copy will be performed.\r\n   *  String data is assumed to be on CPU and will only be copied into a CPU allocated buffer.\r\n   * \\param[in] values_shape pointer to values shape array\r\n   * \\param[in] values_shape_len length of the values_shape\r\n   * \\param[in] values pointer to an array of values. For strings, pass const char**.\r\n   * \\param[in] indices_data pointer to a location of COO indices\r\n   * \\param[in] indices_num number of COO indices\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(FillSparseTensorCoo, _Inout_ OrtValue* ort_value, _In_ const OrtMemoryInfo* data_mem_info,\r\n                  _In_ const int64_t* values_shape, size_t values_shape_len, _In_ const void* values,\r\n                  _In_ const int64_t* indices_data, size_t indices_num);\r\n\r\n  /**\r\n   * This fills populates an empty tensor that was created using OrtApi::CreateSparseTensorAsOrtValue.\r\n   * This will allocate required memory and copy the supplied NNZ values and CSR indices into that memory allocation.\r\n   * Memory allocation is performed using the allocator that was specified with OrtApi::CreateSparseTensorAsOrtValue.\r\n   *\r\n   * \\param[in,out] ort_value ::OrtValue to populate with data\r\n   * \\param[in] data_mem_info serves to identify the location of the data to be copied. If the allocator specified\r\n   *  at the creation time has memory info that is not the same as mem_info argument to this function a X-device copy will be performed.\r\n   *  String data is assumed to be on CPU and will only be copied into a CPU allocated buffer.\r\n   * \\param[in] values_shape pointer to values shape array\r\n   * \\param[in] values_shape_len length of the values_shape\r\n   * \\param[in] values - pointer to an array of values. For strings, pass const char**.\r\n   * \\param[in] inner_indices_data pointer to a location of CSR inner indices\r\n   * \\param[in] inner_indices_num number of CSR inner indices\r\n   * \\param[in] outer_indices_data pointer to a location of CSR outer indices\r\n   * \\param[in] outer_indices_num number of CSR outer indices\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(FillSparseTensorCsr, _Inout_ OrtValue* ort_value, _In_ const OrtMemoryInfo* data_mem_info,\r\n                  _In_ const int64_t* values_shape, size_t values_shape_len, _In_ const void* values,\r\n                  _In_ const int64_t* inner_indices_data, size_t inner_indices_num,\r\n                  _In_ const int64_t* outer_indices_data, size_t outer_indices_num);\r\n\r\n  /**\r\n   * This fills populates an empty tensor that was created using OrtApi::CreateSparseTensorAsOrtValue.\r\n   * This will allocate required memory and copy the supplied NNZ values and BlockSparse indices into that memory allocation.\r\n   * Memory allocation is performed using the allocator that was specified with OrtApi::CreateSparseTensorAsOrtValue.\r\n   *\r\n   * \\param[in,out] ort_value ::OrtValue to populate with data\r\n   * \\param[in] data_mem_info serves to identify the location of the data to be copied. If the allocator specified\r\n   *  at the creation time has memory info that is not the same as mem_info argument to this function a X-device copy will be performed.\r\n   *  String data is assumed to be on CPU and will only be copied into a CPU allocated buffer.\r\n   * \\param[in] values_shape\r\n   * \\param[in] values_shape_len\r\n   * \\param[in] values structure with values information\r\n   * \\param[in] indices_shape_data pointer to a location of indices shape\r\n   * \\param[in] indices_shape_len length of the block sparse indices shape\r\n   * \\param[in] indices_data pointer to a location of indices data. Shape will determine the length of the indices data.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(FillSparseTensorBlockSparse, _Inout_ OrtValue* ort_value, _In_ const OrtMemoryInfo* data_mem_info,\r\n                  _In_ const int64_t* values_shape, size_t values_shape_len, _In_ const void* values,\r\n                  _In_ const int64_t* indices_shape_data, size_t indices_shape_len,\r\n                  _In_ const int32_t* indices_data);\r\n\r\n  /**\r\n   * Create an ::OrtValue with a sparse tensor. This is the first step.\r\n   * Next, use Use<Format>Indices() functions to supply sparse tensor with\r\n   * format specific indices data and set its sparse format to a specific enum value.\r\n   * This will not perform memory allocations. It will\r\n   * use supplied user buffer which should outlive the created sparse tensor.\r\n   * Use OrtApi::ReleaseValue to destroy the sparse tensor. It would not release the supplied values buffer.\r\n   * This function can not be used to map strings from the user allocated memory. Strings must always be copied\r\n   * and have UTF-8 encoding. Therefore, use OrtApi::CreateSparseTensorAsOrtValue above and then fill it with data\r\n   * using appropriate Make*() function.\r\n   *\r\n   * \\param[in] info memory info where sparse values reside.\r\n   * \\param[in,out] p_data pointer to a user allocated buffer with values. To create a full sparse tensor with no non-zero\r\n   *   values, pass nullptr\r\n   * \\param[in] dense_shape shape of the original dense tensor\r\n   * \\param[in] dense_shape_len number of shape dimensions being passed\r\n   * \\param[in] values_shape shape of the values data. To create a fully sparse tensor with no non-zero values,\r\n   *   pass {0} shape.\r\n   * \\param[in] values_shape_len number of values shape dimensions\r\n   * \\param[in] type must be one of TENSOR_ELEMENT_DATA_TYPE_xxxx\r\n   * \\param[out] out Should be freed by calling ReleaseValue\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateSparseTensorWithValuesAsOrtValue, _In_ const OrtMemoryInfo* info, _Inout_ void* p_data,\r\n                  _In_ const int64_t* dense_shape, size_t dense_shape_len,\r\n                  _In_ const int64_t* values_shape, size_t values_shape_len,\r\n                  ONNXTensorElementDataType type, _Outptr_ OrtValue** out);\r\n\r\n  /**\r\n   * This assigns Coo format indices to the SparseTensor that was created by\r\n   * OrtApi::CreateSparseTensorWithValuesAsOrtValue above. It also sets OrtSparseFormat to\r\n   * ORT_SPARSE_COO. This will not allocate any additional memory for data. The life span of\r\n   * indices_data buffer should eclipse the life span of this ::OrtValue.\r\n   *\r\n   * \\param[in,out] ort_value ::OrtValue instance constructed with OrtApi::CreateSparseTensorWithValuesAsOrtValue\r\n   * \\param[in,out] indices_data pointer to a user pre-allocated buffer or nullptr for fully sparse tensors.\r\n   * \\param[in] indices_num  number of COO indices. Should either be 0 for fully sparse tensors, be equal\r\n   *  to the number of nnz values specified to OrtApi::CreateSparseTensorWithValuesAsOrtValue for 1-D {nnz} indices or\r\n   *  be twice as number of nnz values for a  2-D indices {nnz, 2}\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(UseCooIndices, _Inout_ OrtValue* ort_value, _Inout_ int64_t* indices_data, size_t indices_num);\r\n\r\n  /**\r\n   * The assigns CSR format indices to the SparseTensor that was created by\r\n   * OrtApi::CreateSparseTensorWithValuesAsOrtValue above. It also sets OrtSparseFormat to\r\n   * ORT_SPARSE_CSRC. This will not allocate any additional memory for data. The life spans of\r\n   * inner_data and outer_data buffers should eclipse the life span of this ::OrtValue.\r\n   *\r\n   * \\param[in,out] ort_value ::OrtValue instance constructed with OrtApi::CreateSparseTensorWithValuesAsOrtValue\r\n   * \\param[in,out] inner_data pointer to a user pre-allocated buffer or nullptr for fully sparse tensors.\r\n   * \\param[in] inner_num  number of inner CSR indices. Should either be 0 for fully sparse tensors or be equal\r\n   * to the number of nnz values specified to OrtApi::CreateSparseTensorWithValuesAsOrtValue.\r\n   * \\param[in,out] outer_data pointer to user pre-allocated buffer or nullptr for fully sparse tensors.\r\n   * \\param[in] outer_num number of CSR outer indices. Should either be 0 for fully sparse tensors or\r\n   * equal to rows + 1 of the dense shape.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(UseCsrIndices, _Inout_ OrtValue* ort_value, _Inout_ int64_t* inner_data, size_t inner_num,\r\n                  _Inout_ int64_t* outer_data, size_t outer_num);\r\n\r\n  /**\r\n   * The assigns BlockSparse format indices to the SparseTensor that was created by\r\n   * OrtApi::CreateSparseTensorWithValuesAsOrtValue above. It also sets OrtSparseFormat to\r\n   * ORT_SPARSE_BLOCK_SPARSE. This will not allocate any additional memory for data. The life span of\r\n   * indices_data buffer must eclipse the lifespan of this ::OrtValue.\r\n   *\r\n   * \\param[in,out] ort_value OrtValue instance constructed with OrtApi::CreateSparseTensorWithValuesAsOrtValue\r\n   * \\param[in] indices_shape pointer to indices shape. Use {0} for fully sparse tensors\r\n   * \\param[in] indices_shape_len length of the indices shape\r\n   * \\param[in,out] indices_data pointer to user pre-allocated buffer or nullptr for fully sparse tensors.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(UseBlockSparseIndices, _Inout_ OrtValue* ort_value, const int64_t* indices_shape, size_t indices_shape_len, _Inout_ int32_t* indices_data);\r\n\r\n  /** \\brief Returns sparse tensor format enum iff a given ort value contains an instance of sparse tensor.\r\n   *\r\n   * \\param[in] ort_value ::OrtValue that contains an instance of sparse tensor\r\n   * \\param[out] out pointer to out parameter\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetSparseTensorFormat, _In_ const OrtValue* ort_value, _Out_ enum OrtSparseFormat* out);\r\n\r\n  /** \\brief Returns data type and shape of sparse tensor values (nnz) iff ::OrtValue contains a SparseTensor.\r\n   *\r\n   * \\param[in] ort_value An ::OrtValue that contains a fully constructed sparse tensor\r\n   * \\param[out] out Must be freed by OrtApi::ReleaseTensorTypeAndShapeInfo\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetSparseTensorValuesTypeAndShape, _In_ const OrtValue* ort_value, _Outptr_ OrtTensorTypeAndShapeInfo** out);\r\n\r\n  /** \\brief Returns numeric data for sparse tensor values (nnz). For string values use GetStringTensor*().\r\n   *\r\n   * \\param[in] ort_value an instance of ::OrtValue containing sparse tensor\r\n   * \\param[out] out returns a pointer to values data.  Do not attempt to free this ptr.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetSparseTensorValues, _In_ const OrtValue* ort_value, _Outptr_ const void** out);\r\n\r\n  /** \\brief Returns data type, shape for the type of indices specified by indices_format.\r\n   *\r\n   * \\param[in] ort_value ::OrtValue containing sparse tensor.\r\n   * \\param[in] indices_format One of the indices formats. It is an error to request a format that the sparse\r\n   * tensor does not contain.\r\n   * \\param[out] out an instance of ::OrtTensorTypeAndShapeInfo. Must be freed by OrtApi::ReleaseTensorTypeAndShapeInfo\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetSparseTensorIndicesTypeShape, _In_ const OrtValue* ort_value, enum OrtSparseIndicesFormat indices_format, _Outptr_ OrtTensorTypeAndShapeInfo** out);\r\n\r\n  /** \\brief Returns indices data for the type of the indices specified by indices_format\r\n   *\r\n   * \\param[in] ort_value ::OrtValue containing sparse tensor.\r\n   * \\param[in] indices_format One of the indices formats. It is an error to request a format that the sparse tensor does not contain.\r\n   * \\param[out] num_indices Pointer to where the number of indices entries is returned\r\n   * \\param[out] indices Returned pointer to the indices data. Do not free the returned pointer as it refers to internal data owned by the ::OrtValue\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetSparseTensorIndices, _In_ const OrtValue* ort_value, enum OrtSparseIndicesFormat indices_format, _Out_ size_t* num_indices, _Outptr_ const void** indices);\r\n  /// @}\r\n  /// \\name OrtSessionOptions\r\n  /// @{\r\n\r\n  /**\r\n   * \\brief Sets out to 1 iff an optional type OrtValue has an element, 0 otherwise (OrtValue is None)\r\n   * Use this API to find if the optional type OrtValue is None or not.\r\n   * If the optional type OrtValue is not None, use the OrtValue just like any other OrtValue.\r\n   * For example, if you get an OrtValue that corresponds to Optional(tensor) and\r\n   * if HasValue() returns true, use it as tensor and so on.\r\n\r\n   * \\param[in] value Input OrtValue.\r\n   * \\param[out] out indicating if the input OrtValue contains data (1) or if it is a None (0)\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(HasValue, _In_ const OrtValue* value, _Out_ int* out);\r\n\r\n  /// @}\r\n  /// \\name OrtKernelContext\r\n  /// Custom operator APIs.\r\n  /// @{\r\n\r\n  /** \\brief Used for custom operators, gets the GPU compute stream to use to launch the custom a GPU kernel\r\n   *   \\see ::OrtCustomOp\r\n   * \\param[in]  context OrtKernelContext instance\r\n   * \\param[out] out Returns pointer to a GPU compute stream that can be used to launch the custom GPU kernel.\r\n   *             If retrieving the GPU compute stream is not relevant (GPU not enabled in the build, kernel partitioned to\r\n   *             some other EP), then a nullptr is returned as the output param.\r\n   *             Do not free or mutate the returned pointer as it refers to internal data owned by the underlying session.\r\n   *             Only use it for custom kernel launching.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(KernelContext_GetGPUComputeStream, _In_ const OrtKernelContext* context, _Outptr_ void** out);\r\n\r\n  /// @}\r\n  /// \\name GetTensorMemoryInfo\r\n  /// @{\r\n  /** \\brief Returns a pointer to the ::OrtMemoryInfo of a Tensor\r\n   * \\param[in] value ::OrtValue containing tensor.\r\n   * \\param[out] mem_info ::OrtMemoryInfo of the tensor. Do NOT free the returned pointer. It is valid for the lifetime of the ::OrtValue\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetTensorMemoryInfo, _In_ const OrtValue* value, _Out_ const OrtMemoryInfo** mem_info);\r\n\r\n  /// @}\r\n  /// \\name GetExecutionProviderApi\r\n  /// @{\r\n  /** \\brief Get a pointer to the requested version of the Execution Provider specific\r\n   * API extensions to the OrtApi\r\n   * \\param[in] provider_name The name of the execution provider name. Currently only the following\r\n   * values are supported: \"DML\".\r\n   * \\param[in] version Must be ::ORT_API_VERSION.\r\n   * \\param[out] provider_api A void pointer containing a reference to the execution provider versioned api structure.\r\n   * For example, the provider_api pointer can be cast to the OrtDmlApi* when the provider_name is \"DML\".\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetExecutionProviderApi, _In_ const char* provider_name, _In_ uint32_t version, _Outptr_ const void** provider_api);\r\n\r\n  /// @}\r\n\r\n  /// \\name SessionOptions\r\n  /// @{\r\n  /** \\brief Set custom thread creation function\r\n   *\r\n   * \\param[in] options Session options\r\n   * \\param[in] ort_custom_create_thread_fn Custom thread creation function\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsSetCustomCreateThreadFn, _Inout_ OrtSessionOptions* options, _In_ OrtCustomCreateThreadFn ort_custom_create_thread_fn);\r\n\r\n  /** \\brief Set creation options for custom thread\r\n   *\r\n   * \\param[in] options Session options\r\n   * \\param[in] ort_custom_thread_creation_options Custom thread creation options (can be nullptr)\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsSetCustomThreadCreationOptions, _Inout_ OrtSessionOptions* options, _In_ void* ort_custom_thread_creation_options);\r\n\r\n  /** \\brief Set custom thread join function\r\n   *\r\n   * \\param[in] options Session options\r\n   * \\param[in] ort_custom_join_thread_fn Custom join thread function, must not be nullptr when ort_custom_create_thread_fn is set\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsSetCustomJoinThreadFn, _Inout_ OrtSessionOptions* options, _In_ OrtCustomJoinThreadFn ort_custom_join_thread_fn);\r\n  /// @}\r\n\r\n  /// \\name OrtThreadingOptions\r\n  /// @{\r\n  /** \\brief Set custom thread creation function for global thread pools\r\n   *\r\n   * \\param[inout] tp_options\r\n   * \\param[in] ort_custom_create_thread_fn Custom thread creation function\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetGlobalCustomCreateThreadFn, _Inout_ OrtThreadingOptions* tp_options, _In_ OrtCustomCreateThreadFn ort_custom_create_thread_fn);\r\n\r\n  /** \\brief Set custom thread creation options for global thread pools\r\n   *\r\n   * \\param[inout] tp_options\r\n   * \\param[in] ort_custom_thread_creation_options Custom thread creation options (can be nullptr)\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetGlobalCustomThreadCreationOptions, _Inout_ OrtThreadingOptions* tp_options, _In_ void* ort_custom_thread_creation_options);\r\n\r\n  /** \\brief Set custom thread join function for global thread pools\r\n   *\r\n   * \\param[inout] tp_options\r\n   * \\param[in] ort_custom_join_thread_fn Custom thread join function, must not be nullptr when global ort_custom_create_thread_fn is set\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetGlobalCustomJoinThreadFn, _Inout_ OrtThreadingOptions* tp_options, _In_ OrtCustomJoinThreadFn ort_custom_join_thread_fn);\r\n  /// @}\r\n\r\n  /** \\brief Synchronize bound inputs. The call may be necessary for some providers, such as cuda,\r\n   *   in case the system that allocated bound memory operated on a different stream. However, the\r\n   *   operation is provider specific and could be a no-op.\r\n   *\r\n   * \\param[inout] binding_ptr\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SynchronizeBoundInputs, _Inout_ OrtIoBinding* binding_ptr);\r\n\r\n  /** \\brief Synchronize bound outputs. The call may be necessary for some providers, such as cuda,\r\n   *   in case the system that allocated bound memory operated on a different stream. However, the\r\n   *   operation is provider specific and could be a no-op.\r\n   *\r\n   * \\param[inout] binding_ptr\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SynchronizeBoundOutputs, _Inout_ OrtIoBinding* binding_ptr);\r\n\r\n  /// \\name OrtSessionOptions\r\n  /// @{\r\n\r\n  /** \\brief Append CUDA execution provider to the session options\r\n   *\r\n   * If CUDA is not available (due to a non CUDA enabled build), this function will return failure.\r\n   *\r\n   * This is slightly different from OrtApi::SessionOptionsAppendExecutionProvider_CUDA, it takes an\r\n   * ::OrtCUDAProviderOptions which is publicly defined. This takes an opaque ::OrtCUDAProviderOptionsV2\r\n   * which must be created with OrtApi::CreateCUDAProviderOptions.\r\n   *\r\n   * For OrtApi::SessionOptionsAppendExecutionProvider_CUDA, the user needs to instantiate ::OrtCUDAProviderOptions\r\n   * as well as allocate/release buffers for some members of ::OrtCUDAProviderOptions.\r\n   * Here, OrtApi::CreateCUDAProviderOptions and Ortapi::ReleaseCUDAProviderOptions will do the memory management for you.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] cuda_options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.11.\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsAppendExecutionProvider_CUDA_V2,\r\n                  _In_ OrtSessionOptions* options, _In_ const OrtCUDAProviderOptionsV2* cuda_options);\r\n\r\n  /// @}\r\n  /// \\name OrtCUDAProviderOptionsV2\r\n  /// @{\r\n\r\n  /** \\brief Create an OrtCUDAProviderOptionsV2\r\n   *\r\n   * \\param[out] out Newly created ::OrtCUDAProviderOptionsV2. Must be released with OrtApi::ReleaseCudaProviderOptions\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.11.\r\n   */\r\n  ORT_API2_STATUS(CreateCUDAProviderOptions, _Outptr_ OrtCUDAProviderOptionsV2** out);\r\n\r\n  /** \\brief Set options in a CUDA Execution Provider.\r\n   *\r\n   * Please refer to https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#configuration-options\r\n   * to know the available keys and values. Key should be in null terminated string format of the member of ::OrtCUDAProviderOptionsV2\r\n   * and value should be its related range. Recreates the options and only sets the supplied values.\r\n   *\r\n   * For example, key=\"device_id\" and value=\"0\"\r\n   *\r\n   * \\param[in] cuda_options\r\n   * \\param[in] provider_options_keys Array of UTF-8 null-terminated string for provider options keys\r\n   * \\param[in] provider_options_values Array of UTF-8 null-terminated string for provider options values\r\n   * \\param[in] num_keys Number of elements in the `provider_option_keys` and `provider_options_values` arrays\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.11.\r\n   */\r\n  ORT_API2_STATUS(UpdateCUDAProviderOptions, _Inout_ OrtCUDAProviderOptionsV2* cuda_options,\r\n                  _In_reads_(num_keys) const char* const* provider_options_keys,\r\n                  _In_reads_(num_keys) const char* const* provider_options_values,\r\n                  _In_ size_t num_keys);\r\n\r\n  /**\r\n   * Get serialized CUDA provider options string.\r\n   *\r\n   * For example, \"device_id=0;arena_extend_strategy=0;......\"\r\n   *\r\n   * \\param cuda_options - OrtCUDAProviderOptionsV2 instance\r\n   * \\param allocator - a ptr to an instance of OrtAllocator obtained with CreateAllocator() or GetAllocatorWithDefaultOptions()\r\n   *                      the specified allocator will be used to allocate continuous buffers for output strings and lengths.\r\n   * \\param ptr - is a UTF-8 null terminated string allocated using 'allocator'. The caller is responsible for using the same allocator to free it.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.11.\r\n   */\r\n  ORT_API2_STATUS(GetCUDAProviderOptionsAsString, _In_ const OrtCUDAProviderOptionsV2* cuda_options, _Inout_ OrtAllocator* allocator, _Outptr_ char** ptr);\r\n\r\n  /** \\brief Release an ::OrtCUDAProviderOptionsV2\r\n   *\r\n   * \\note This is an exception in the naming convention of other Release* functions, as the name of the method does not have the V2 suffix, but the type does\r\n   *\r\n   * \\since Version 1.11.\r\n   */\r\n  void(ORT_API_CALL* ReleaseCUDAProviderOptions)(_Frees_ptr_opt_ OrtCUDAProviderOptionsV2* input);\r\n\r\n  /// @}\r\n\r\n  /** \\brief Append MIGraphX provider to session options\r\n   *\r\n   * If MIGraphX is not available (due to a non MIGraphX enabled build, or if MIGraphX is not installed on the system), this function will return failure.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] migraphx_options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.11.\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsAppendExecutionProvider_MIGraphX,\r\n                  _In_ OrtSessionOptions* options, _In_ const OrtMIGraphXProviderOptions* migraphx_options);\r\n\r\n  /** \\brief Replace initialized Tensors with external data with the data provided in initializers.\r\n   *\r\n   * The function will find the initialized TensorProtos with external data in the graph with the provided names and\r\n   * replace them with the provided tensors. The API verifies that the TensorProto being replaced\r\n   * has an external data reference and has the same name, dimensions and data type as its replacement. The replacement\r\n   * will occur before any of the optimizations take place. The data will be copied into the graph\r\n   * since TensorProto can't refer to the user provided buffers.\r\n   *\r\n   * Once the model has been loaded, the OrtValue(s) added to SessionOptions instance will be removed\r\n   * from the internal SessionOptions copy to save memory, the user provided buffers can then be deallocated\r\n   * and the SessionOptions instance that refers to them can be destroyed.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] initializer_names Array of null terminated UTF-8 encoded strings of the initializers names.\r\n   * \\param[in] initializers Array of ::OrtValue type\r\n   * \\param[in] num_initializers Number of elements in the initializer_names and initializers\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.12.\r\n   */\r\n  ORT_API2_STATUS(AddExternalInitializers, _In_ OrtSessionOptions* options,\r\n                  _In_reads_(num_initializers) const char* const* initializer_names,\r\n                  _In_reads_(num_initializers) const OrtValue* const* initializers, size_t num_initializers);\r\n\r\n  /** \\brief: Create attribute of onnxruntime operator\r\n   *\r\n   * \\param[in] name Name of the attribute\r\n   * \\param[in] data Data content of the attribute\r\n   * \\param[in] len Number of bytes stored in data for ORT_OP_ATTR_STRING.\r\n                    Number of elements if data represents an array (e.g., ORT_OP_ATTR_INTS). Otherwise, set to 1.\r\n   * \\param[in] type Data type\r\n   * \\param[out] op_attr Attribute that has been created, which must be released by OrtApi::ReleaseOpAttr\r\n   *\r\n   * \\since Version 1.12.\r\n   */\r\n  ORT_API2_STATUS(CreateOpAttr,\r\n                  _In_ const char* name,\r\n                  _In_ const void* data,\r\n                  _In_ int len,\r\n                  _In_ OrtOpAttrType type,\r\n                  _Outptr_ OrtOpAttr** op_attr);\r\n\r\n  /* \\brief: Release op attribute\r\n   *\r\n   * \\param[in] opAttr Attribute created by OrtApi::CreateOpAttr\r\n   *\r\n   * \\since Version 1.12.\r\n   */\r\n  ORT_CLASS_RELEASE(OpAttr);\r\n\r\n  /** \\brief: Create onnxruntime native operator\r\n   *\r\n   * \\param[in] info Kernel info\r\n   * \\param[in] op_name Operator name\r\n   * \\param[in] domain Operator domain\r\n   * \\param[in] version Operator opset version\r\n   * \\param[in] type_constraint_names Name of the type constraints, such as \"T\" or \"T1\"\r\n   * \\param[in] type_constraint_values Type of each constraints\r\n   * \\param[in] type_constraint_count Number of constraints\r\n   * \\param[in] attr_values Attributes used to initialize the operator\r\n   * \\param[in] attr_count Number of the attributes\r\n   * \\param[in] input_count Number of inputs\r\n   * \\param[in] output_count Number of outputs\r\n   * \\param[out] ort_op Operator that has been created\r\n   *\r\n   * \\since Version 1.12.\r\n   */\r\n  ORT_API2_STATUS(CreateOp,\r\n                  _In_ const OrtKernelInfo* info,\r\n                  _In_z_ const char* op_name,\r\n                  _In_z_ const char* domain,\r\n                  int version,\r\n                  _In_reads_(type_constraint_count) const char** type_constraint_names,\r\n                  _In_reads_(type_constraint_count) const ONNXTensorElementDataType* type_constraint_values,\r\n                  int type_constraint_count,\r\n                  _In_reads_(attr_count) const OrtOpAttr* const* attr_values,\r\n                  int attr_count,\r\n                  int input_count,\r\n                  int output_count,\r\n                  _Outptr_ OrtOp** ort_op);\r\n\r\n  /** \\brief: Invoke the operator created by OrtApi::CreateOp\r\n   * The inputs must follow the order as specified in onnx specification\r\n   *\r\n   * \\param[in] context Kernel context\r\n   * \\param[in] ort_op Operator that has been created\r\n   * \\param[in] input_values Array of inputs\r\n   * \\param[in] input_count Number of inputs\r\n   * \\param[in] output_values Array of outputs\r\n   * \\param[in] output_count Number of outputs\r\n   *\r\n   * \\since Version 1.12.\r\n   */\r\n  ORT_API2_STATUS(InvokeOp,\r\n                  _In_ const OrtKernelContext* context,\r\n                  _In_ const OrtOp* ort_op,\r\n                  _In_ const OrtValue* const* input_values,\r\n                  _In_ int input_count,\r\n                  _Inout_ OrtValue* const* output_values,\r\n                  _In_ int output_count);\r\n\r\n  /* \\brief: Release an onnxruntime operator\r\n   *\r\n   * \\param[in] Op Operator created by OrtApi::CreateOp\r\n   *\r\n   * \\since Version 1.12.\r\n   */\r\n  ORT_CLASS_RELEASE(Op);\r\n\r\n  /** \\brief: Append execution provider to the session options.\r\n   * \\param[in] options\r\n   * \\param[in] provider_name - provider to add.\r\n   * \\param[in] provider_options_keys - keys to configure the provider options\r\n   * \\param[in] provider_options_values - values to configure the provider options\r\n   * \\param[in] num_keys - number of keys passed in\r\n   *\r\n   * Currently supported provider names:\r\n   *   QNNExecutionProvider (or QNN)\r\n   *   OpenVINOExecutionProvider (or OpenVINO)\r\n   *   XnnpackExecutionProvider (or XNNPACK)\r\n   *   WebNNExecutionProvider (or WEBNN)\r\n   *   WebGpuExecutionProvider (or WebGPU)\r\n   *   AzureExecutionProvider (or AZURE)\r\n   *   JsExecutionProvider (or JS)\r\n   *   VitisAIExecutionProvider (or VitisAI)\r\n   *   CoreMLExecutionProvider (or CoreML)\r\n   *\r\n   * Note: If an execution provider has a dedicated SessionOptionsAppendExecutionProvider_<provider name> function\r\n   *       that should be used to add it.\r\n   *\r\n   * QNN supported keys:\r\n   *   \"backend_type\": Type of QNN backend. Specifies a backend path that is the associated QNN backend library file\r\n   *      name. E.g., given backend type \"htp\", on Windows, the backend path would be \"QnnHtp.dll\", and on other\r\n   *      platforms, it would be \"libQnnHtp.so\". Mutually exclusive with \"backend_path\".\r\n   *      Available options:\r\n   *      -# \"cpu\"\r\n   *      -# \"gpu\"\r\n   *      -# \"htp\": Default.\r\n   *      -# \"saver\"\r\n   *      -# \"ir\"\r\n   *   \"backend_path\": File path to QNN backend library. Mutually exclusive with \"backend_type\".\r\n   *   \"profiling_level\": QNN profiling level.\r\n   *      Available options:\r\n   *      -# \"off\": Default.\r\n   *      -# \"basic\"\r\n   *      -# \"detailed\"\r\n   *   \"profiling_file_path\": QNN profiling file path if ETW not enabled.\r\n   *   \"rpc_control_latency\": QNN RPC control latency.\r\n   *   \"vtcm_mb\": QNN VTCM size in MB. default to 0(not set).\r\n   *   \"htp_performance_mode\": QNN performance mode.\r\n   *      Available options:\r\n   *      -# \"burst\"\r\n   *      -# \"balanced\"\r\n   *      -# \"default\": Default.\r\n   *      -# \"high_performance\"\r\n   *      -# \"high_power_saver\"\r\n   *      -# \"low_balanced\"\r\n   *      -# \"extreme_power_saver\"\r\n   *      -# \"low_power_saver\"\r\n   *      -# \"power_saver\"\r\n   *      -# \"sustained_high_performance\"\r\n   *   \"dump_qnn_ir_dlc\": Use the QnnIr backend library to write .dlc files for each subgraph dispatched to QNN. When\r\n   *       enabled, inference results will be incorrect. Use only for debugging.\r\n   *      -# \"0\": Default: disabled\r\n   *      -# \"1\": enabled\r\n   *   \"dump_qnn_ir_dlc_dir\": Set the directory into which QnnIr will be configured to write QNN graphs as .dlc files.\r\n   *      Default is current working directory.\r\n   *   \"qnn_ir_backend_path\": File path to the QnnIr backend library. If \"dump_qnn_ir_dlc\" is enabled, use this path\r\n   *      instead of looking for the Ir backend in the standard location.\r\n   *   \"qnn_saver_path\": File path to the QNN Saver backend library. If specified, QNN Saver will be enabled and will\r\n   *      dump QNN API calls to disk for replay/debugging. QNN Saver produces incorrect model inference results and\r\n   *      may alter model/EP partitioning. Use only for debugging.\r\n   *   \"qnn_context_priority\": QNN context priority.\r\n   *      Available options:\r\n   *      -# \"low\"\r\n   *      -# \"normal\": Default.\r\n   *      -# \"normal_high\"\r\n   *      -# \"high\"\r\n   *   \"htp_graph_finalization_optimization_mode\": Set the optimization mode for graph finalization on the HTP backend.\r\n   *      Available options:\r\n   *      -# \"0\": Default.\r\n   *      -# \"1\": Faster preparation time, less optimal graph.\r\n   *      -# \"2\": Longer preparation time, more optimal graph.\r\n   *      -# \"3\": Longest preparation time, most likely even more optimal graph. See QNN SDK documentation for specific\r\n   *        details.\r\n   *   \"soc_model\": The SoC model number. Refer to the QNN SDK documentation for valid values.\r\n   *      Defaults to \"0\" (unknown).\r\n   *   \"htp_arch\": The minimum HTP architecture the driver will use to select compatible QNN operators.\r\n   *      Available options:\r\n   *      -# \"0\": Default (none).\r\n   *      -# \"68\"\r\n   *      -# \"69\"\r\n   *      -# \"73\"\r\n   *      -# \"75\"\r\n   *   \"device_id\": The ID of the device to use when setting 'htp_arch'. Defaults to \"0\" (for single device).\r\n   *   \"enable_htp_fp16_precision\": Used for float32 model for HTP backend.\r\n   *      Enable the float32 model to be inferenced with fp16 precision. Otherwise, it will be fp32 precision.\r\n   *      -# \"0\": With fp32 precision.\r\n   *      -# \"1\": Default. With fp16 precision.\r\n   *   \"offload_graph_io_quantization\": Offload graph input quantization and graph output dequantization to another\r\n   *      execution provider (typically CPU EP).\r\n   *      -# \"0\": Disabled. QNN EP will handle quantization and dequantization of graph I/O.\r\n   *      -# \"1\": Enabled. This is the default value.\r\n   *   \"enable_htp_spill_fill_buffer\": Enable HTP spill fill buffer setting. The flag is used while generating context\r\n   *      binary.\r\n   *      -# \"0\": Default. Disabled.\r\n   *      -# \"1\": Enabled.\r\n   *   \"enable_htp_shared_memory_allocator\": Enable the QNN HTP shared memory allocator. Requires libcdsprpc.so/dll to\r\n   *      be available.\r\n   *      -# \"0\": Default. Disabled.\r\n   *      -# \"1\": Enabled.\r\n   *   \"dump_json_qnn_graph\": Set to \"1\" to dump QNN graphs generated by QNN EP as JSON files. Each graph partition\r\n   *      assigned to QNN EP is dumped to a separate file.\r\n   *   \"json_qnn_graph_dir\": Directory in which to dump QNN JSON graphs. If not specified, QNN graphs are dumped in the\r\n   *      program's current working directory. Ignored if \"dump_json_qnn_graph\" is not set.\r\n   *   \"op_packages\": QNN UDO op_package for QNN EP, allowed format:\r\n   *Â Â    \"<op_type>:<op_package_path>:<interface>[:<target>],<op_type2>:<op_package_path2>:<interface2>[:<target>]\",\r\n   *Â Â    where op_type is the name of the operation, op_package_path is the path to the op package shared library,\r\n   *     interface is the symbol name to register the op life cycle functions, and target is the backend type. For more\r\n   *     details, refer to: https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/op_packages.html\r\n   *\r\n   * XNNPACK supported keys:\r\n   *   \"intra_op_num_threads\": number of thread-pool size to use for XNNPACK execution provider.\r\n   *      default value is 0, which means to use the session thread-pool size.\r\n   *\r\n   * \\since Version 1.12.\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsAppendExecutionProvider, _In_ OrtSessionOptions* options,\r\n                  _In_ const char* provider_name,\r\n                  _In_reads_(num_keys) const char* const* provider_options_keys,\r\n                  _In_reads_(num_keys) const char* const* provider_options_values,\r\n                  _In_ size_t num_keys);\r\n\r\n  /* \\brief: Get a copy of kernel info\r\n   *\r\n   * \\param[in] info Kernel info\r\n   * \\param[out] info_copy Copy of kernel info\r\n   *\r\n   * \\since Version 1.12.\r\n   */\r\n  ORT_API2_STATUS(CopyKernelInfo,\r\n                  _In_ const OrtKernelInfo* info,\r\n                  _Outptr_ OrtKernelInfo** info_copy);\r\n\r\n  /* \\brief: Release kernel info\r\n   *\r\n   * \\param[in] KernelInfo A copy of kernel info returned by CopyKernelInfo\r\n   *\r\n   * \\since Version 1.12.\r\n   */\r\n  ORT_CLASS_RELEASE(KernelInfo);\r\n\r\n  /// \\name Ort Training\r\n  /// @{\r\n  /** \\brief Gets the Training C Api struct\r\n   *\r\n   * Call this function to access the ::OrtTrainingApi structure that holds pointers to functions that enable\r\n   * training with onnxruntime.\r\n   * \\note A NULL pointer will be returned and no error message will be printed if the training api\r\n   * is not supported with this build. A NULL pointer will be returned and an error message will be\r\n   * printed if the provided version is unsupported, for example when using a runtime older than the\r\n   * version created with this header file.\r\n   *\r\n   * \\param[in] version Must be ::ORT_API_VERSION\r\n   * \\return The ::OrtTrainingApi struct for the version requested.\r\n   *\r\n   * \\since Version 1.13\r\n   */\r\n  const OrtTrainingApi*(ORT_API_CALL* GetTrainingApi)(uint32_t version)NO_EXCEPTION;\r\n\r\n  /// @}\r\n\r\n  /** \\brief Append CANN provider to session options\r\n   *\r\n   * If CANN is not available (due to a non CANN enabled build, or if CANN is not installed on the system), this function will return failure.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] cann_options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.13.\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsAppendExecutionProvider_CANN,\r\n                  _In_ OrtSessionOptions* options, _In_ const OrtCANNProviderOptions* cann_options);\r\n\r\n  /** \\brief Create an OrtCANNProviderOptions\r\n   *\r\n   * \\param[out] out created ::OrtCANNProviderOptions. Must be released with OrtApi::ReleaseCANNProviderOptions\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.13.\r\n   */\r\n  ORT_API2_STATUS(CreateCANNProviderOptions, _Outptr_ OrtCANNProviderOptions** out);\r\n\r\n  /** \\brief Set options in a CANN Execution Provider.\r\n   *\r\n   * \\param[in] cann_options\r\n   * \\param[in] provider_options_keys Array of UTF-8 null-terminated string for provider options keys\r\n   * \\param[in] provider_options_values Array of UTF-8 null-terminated string for provider options values\r\n   * \\param[in] num_keys Number of elements in the `provider_option_keys` and `provider_options_values` arrays\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.13.\r\n   */\r\n  ORT_API2_STATUS(UpdateCANNProviderOptions, _Inout_ OrtCANNProviderOptions* cann_options,\r\n                  _In_reads_(num_keys) const char* const* provider_options_keys,\r\n                  _In_reads_(num_keys) const char* const* provider_options_values,\r\n                  _In_ size_t num_keys);\r\n\r\n  /** \\brief Get serialized CANN provider options string.\r\n   *\r\n   * \\param[in] cann_options OrtCANNProviderOptions instance\r\n   * \\param[in] allocator a ptr to an instance of OrtAllocator obtained with CreateAllocator()\r\n   *                      or GetAllocatorWithDefaultOptions(), the specified allocator will be used to allocate\r\n   *                      continuous buffers for output strings and lengths.\r\n   * \\param[out] ptr is a UTF-8 null terminated string allocated using 'allocator'.\r\n   *                 The caller is responsible for using the same allocator to free it.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.13.\r\n   */\r\n  ORT_API2_STATUS(GetCANNProviderOptionsAsString, _In_ const OrtCANNProviderOptions* cann_options,\r\n                  _Inout_ OrtAllocator* allocator, _Outptr_ char** ptr);\r\n\r\n  /** \\brief Release an OrtCANNProviderOptions\r\n   *\r\n   * \\param[in] input The pointer of OrtCANNProviderOptions which will been deleted\r\n   *\r\n   * \\since Version 1.13.\r\n   */\r\n  void(ORT_API_CALL* ReleaseCANNProviderOptions)(_Frees_ptr_opt_ OrtCANNProviderOptions* input);\r\n\r\n  /*  \\brief Get OrtDevice type from MemoryInfo\r\n   *\r\n   *  \\since Version 1.14\r\n   */\r\n  void(ORT_API_CALL* MemoryInfoGetDeviceType)(_In_ const OrtMemoryInfo* ptr, _Out_ OrtMemoryInfoDeviceType* out);\r\n\r\n  /* \\brief Update the OrtEnv instance with custom log severity level\r\n   *\r\n   * \\param[in] ort_env The OrtEnv instance being used\r\n   * \\param[in] log_severity_level The log severity level.\r\n   *\r\n   * \\since Version 1.14.\r\n   */\r\n  ORT_API2_STATUS(UpdateEnvWithCustomLogLevel, _In_ OrtEnv* ort_env, OrtLoggingLevel log_severity_level);\r\n\r\n  /*  \\brief Set affinities for intra op threads\r\n   *\r\n   * Affinity string follows format:\r\n   * logical_processor_id,logical_processor_id;logical_processor_id,logical_processor_id\r\n   * Semicolon isolates configurations among threads, while comma split processors where ith thread expected to attach to.\r\n   * e.g. 1,2,3;4,5\r\n   * specifies affinities for two threads, with the 1st thread attach to the 1st, 2nd, and 3rd processor, and 2nd thread to the 4th and 5th.\r\n   * To ease the configuration, an \"interval\" is also allowed:\r\n   * e.g. 1-8;8-16;17-24\r\n   * orders that the 1st thread runs on first eight processors, 2nd thread runs on next eight processors, and so forth.\r\n   * Note:\r\n   * 1. Once set, the number of thread affinities must equal to intra_op_num_threads - 1,\r\n   *    ort does not set affinity on the main thread which is started and managed by the calling app;\r\n   * 2. For windows, ort will infer the group id from a logical processor id, for example, assuming there are two groups with each has 64 logical processors,\r\n   *    an id of 64 will be inferred as the last processor of the 1st group, while 65 will be interpreted as the 1st processor of the second group.\r\n   *    Hence 64-65 is an invalid configuration, because a windows thread cannot be attached to processors across group boundary.\r\n   *\r\n   *  \\since Version 1.14\r\n   */\r\n  ORT_API2_STATUS(SetGlobalIntraOpThreadAffinity, _Inout_ OrtThreadingOptions* tp_options, const char* affinity_string);\r\n\r\n  /** \\brief Register custom ops from a shared library.\r\n   *\r\n   * Loads a shared library (.dll on windows, .so on linux, etc) named 'library_name' and looks for this entry point:\r\n   *\t\tOrtStatus* RegisterCustomOps(OrtSessionOptions * options, const OrtApiBase* api);\r\n   * It then passes in the provided session options to this function along with the api base.\r\n   *\r\n   * The handle to the loaded library is automatically released by ORT when the last OrtSession that references the\r\n   * library handle is released. If no OrtSession is created, then the library handle is released when the provided\r\n   * OrtSessionOptions is released.\r\n   *\r\n   * \\param[in] options The session options.\r\n   * \\param[in] library_name The name of the shared library to load and register. Refer to OS-specific dynamic library\r\n   *                         loading utilities (e.g., LoadLibraryEx on Windows or dlopen on Linux/MacOS) for information\r\n   *                         on the format of library names and search paths.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.14\r\n   */\r\n  ORT_API2_STATUS(RegisterCustomOpsLibrary_V2, _Inout_ OrtSessionOptions* options, _In_ const ORTCHAR_T* library_name);\r\n\r\n  /** \\brief Register custom ops by calling a RegisterCustomOpsFn function.\r\n   *\r\n   * Searches for registration_func_name and if found calls it.\r\n   *\r\n   * The library containing the function must either be linked against or previously loaded by the executable.\r\n   *\r\n   * If you want ONNX Runtime to load the library and manage its lifetime, use RegisterCustomOpsLibrary_V2.\r\n   *\r\n   * RegisterCustomOpsUsingFunction can be used in scenarios where it may not be possible for ONNX Runtime to load\r\n   * the library from a path. e.g. mobile platforms where the library must be linked into the app.\r\n   *\r\n   * The registration function must have the signature of RegisterCustomOpsFn:\r\n   *    OrtStatus* (*fn)(OrtSessionOptions* options, const OrtApiBase* api);\r\n   *\r\n   * See https://onnxruntime.ai/docs/reference/operators/add-custom-op.html for details on how the registration\r\n   * function should be implemented.\r\n   *\r\n   * \\param[in] options OrtSessionOptions that is passed through as the first argument in the call to the\r\n   *                    registration function.\r\n   * \\param[in] registration_func_name Name of registration function to use.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.14\r\n   */\r\n  ORT_API2_STATUS(RegisterCustomOpsUsingFunction, _Inout_ OrtSessionOptions* options,\r\n                  _In_ const char* registration_func_name);\r\n\r\n  /// \\name OrtKernelInfo\r\n  /// Custom operator APIs.\r\n  /// @{\r\n\r\n  /** \\brief Get the number of inputs from ::OrtKernelInfo.\r\n   *\r\n   * Used in the CreateKernel callback of an OrtCustomOp to query the number of inputs\r\n   * during kernel/session creation.\r\n   *\r\n   * \\param[in] info Instance of ::OrtKernelInfo.\r\n   * \\param[out] out Pointer to variable assigned with the result on success.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.14\r\n   */\r\n  ORT_API2_STATUS(KernelInfo_GetInputCount, _In_ const OrtKernelInfo* info, _Out_ size_t* out);\r\n\r\n  /** \\brief Get the number of outputs from ::OrtKernelInfo.\r\n   *\r\n   * Used in the CreateKernel callback of an OrtCustomOp to query the number of outputs\r\n   * during kernel/session creation.\r\n   *\r\n   * \\param[in] info Instance of ::OrtKernelInfo.\r\n   * \\param[out] out Pointer to variable assigned with the result on success.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.14\r\n   */\r\n  ORT_API2_STATUS(KernelInfo_GetOutputCount, _In_ const OrtKernelInfo* info, _Out_ size_t* out);\r\n\r\n  /** \\brief Get the name of a ::OrtKernelInfo's input.\r\n   *\r\n   * Used in the CreateKernel callback of an OrtCustomOp to query an input's name\r\n   * during kernel/session creation.\r\n   *\r\n   * If `out` is nullptr, the value of `size` is set to the size of the name\r\n   * string (including null-terminator), and a success status is returned.\r\n   *\r\n   * If the `size` parameter is greater than or equal to the name string's size,\r\n   * the value of `size` is set to the true size of the string (including null-terminator),\r\n   * the provided memory is filled with the string's contents, and a success status is returned.\r\n   *\r\n   * If the `size` parameter is less than the actual string's size and `out`\r\n   * is not nullptr, the value of `size` is set to the true size of the string\r\n   * and a failure status is returned.\r\n   *\r\n   * \\param[in] info An instance of ::OrtKernelInfo.\r\n   * \\param[in] index The index of the input name to get. Returns a failure status if out-of-bounds.\r\n   * \\param[out] out Memory location into which to write the UTF-8 null-terminated string representing the input's name.\r\n   * \\param[in,out] size Pointer to the size of the `out` buffer. See above comments for details.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.14\r\n   */\r\n  ORT_API2_STATUS(KernelInfo_GetInputName, _In_ const OrtKernelInfo* info, size_t index, _Out_ char* out,\r\n                  _Inout_ size_t* size);\r\n\r\n  /** \\brief Get the name of a ::OrtKernelInfo's output.\r\n   *\r\n   * Used in the CreateKernel callback of an OrtCustomOp to query an output's name\r\n   * during kernel/session creation.\r\n   *\r\n   * If `out` is nullptr, the value of `size` is set to the size of the name\r\n   * string (including null-terminator), and a success status is returned.\r\n   *\r\n   * If the `size` parameter is greater than or equal to the name string's size,\r\n   * the value of `size` is set to the true size of the string (including null-terminator),\r\n   * the provided memory is filled with the string's contents, and a success status is returned.\r\n   *\r\n   * If the `size` parameter is less than the actual string's size and `out`\r\n   * is not nullptr, the value of `size` is set to the true size of the string\r\n   * and a failure status is returned.\r\n   *\r\n   * \\param[in] info An instance of ::OrtKernelInfo.\r\n   * \\param[in] index The index of the output name to get. Returns a failure status if out-of-bounds.\r\n   * \\param[out] out Memory location into which to write the UTF-8 null-terminated string representing the output's\r\n   *                 name.\r\n   * \\param[in,out] size Pointer to the size of the `out` buffer. See above comments for details.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.14\r\n   */\r\n  ORT_API2_STATUS(KernelInfo_GetOutputName, _In_ const OrtKernelInfo* info, size_t index, _Out_ char* out,\r\n                  _Inout_ size_t* size);\r\n\r\n  /** \\brief Get the type information for a ::OrtKernelInfo's input.\r\n   *\r\n   * Used in the CreateKernel callback of an OrtCustomOp to query the shape and type information\r\n   * of an input during kernel/session creation.\r\n   *\r\n   * \\param[in] info An instance of ::OrtKernelInfo.\r\n   * \\param[in] index Which input to get the type information for\r\n   * \\param[out] type_info Pointer set to the resulting ::OrtTypeInfo. Must be freed with OrtApi::ReleaseTypeInfo.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.14\r\n   */\r\n  ORT_API2_STATUS(KernelInfo_GetInputTypeInfo, _In_ const OrtKernelInfo* info, size_t index,\r\n                  _Outptr_ OrtTypeInfo** type_info);\r\n\r\n  /** \\brief Get the type information for a ::OrtKernelInfo's output.\r\n   *\r\n   * Used in the CreateKernel callback of an OrtCustomOp to query the shape and type information\r\n   * of an output during kernel/session creation.\r\n   *\r\n   * \\param[in] info An instance of ::OrtKernelInfo.\r\n   * \\param[in] index Which input to get the type information for\r\n   * \\param[out] type_info Pointer set to the resulting ::OrtTypeInfo. Must be freed with OrtApi::ReleaseTypeInfo.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.14\r\n   */\r\n  ORT_API2_STATUS(KernelInfo_GetOutputTypeInfo, _In_ const OrtKernelInfo* info, size_t index,\r\n                  _Outptr_ OrtTypeInfo** type_info);\r\n\r\n  /** \\brief Get a ::OrtValue tensor stored as an attribute in the graph node.\r\n   *\r\n   * Used in the CreateKernel callback of an OrtCustomOp to get a tensor attribute.\r\n   *\r\n   * \\param[in] info ::OrtKernelInfo instance.\r\n   * \\param[in] name UTF-8 null-terminated string representing the attribute's name.\r\n   * \\param[in] allocator Allocator used to allocate the internal tensor state.\r\n   * \\param[out] out Returns newly created ::OrtValue. Must be freed with OrtApi::ReleaseValue,\r\n   *                 which will also free internal tensor state allocated with the provided allocator.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(KernelInfoGetAttribute_tensor, _In_ const OrtKernelInfo* info, _In_z_ const char* name,\r\n                  _Inout_ OrtAllocator* allocator, _Outptr_ OrtValue** out);\r\n\r\n  /// @}\r\n  /// \\name OrtSessionOptions\r\n  /// Custom operator APIs\r\n  /// @{\r\n\r\n  /** \\brief Checks if the given session configuration entry exists.\r\n   *\r\n   * The config_key formats are defined in onnxruntime_session_options_config_keys.h\r\n   *\r\n   * Can be used in a custom operator library to check for session configuration entries\r\n   * that target one or more custom operators in the library. Example: The config entry\r\n   * custom_op.myop.some_key targets a custom op named \"myop\".\r\n   *\r\n   * \\param[in] options The ::OrtSessionOptions instance.\r\n   * \\param[in] config_key A null-terminated UTF-8 string representation of the configuration key.\r\n   * \\param[out] out Pointer set to 1 if the entry exists and 0 otherwise.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.14\r\n   */\r\n  ORT_API2_STATUS(HasSessionConfigEntry, _In_ const OrtSessionOptions* options,\r\n                  _In_z_ const char* config_key, _Out_ int* out);\r\n\r\n  /** \\brief Get a session configuration value.\r\n   *\r\n   * Returns a failure status if the configuration key does not exist.\r\n   * The config_key and the format of config_value are defined in onnxruntime_session_options_config_keys.h\r\n   *\r\n   * If `config_value` is nullptr, the value of `size` is set to the true size of the string\r\n   * value (including null-terminator), and a success status is returned.\r\n   *\r\n   * If the `size` parameter is greater than or equal to the actual string value's size,\r\n   * the value of `size` is set to the true size of the string value, the provided memory\r\n   * is filled with the value's contents, and a success status is returned.\r\n   *\r\n   * If the `size` parameter is less than the actual string value's size and `config_value`\r\n   * is not nullptr, the value of `size` is set to the true size of the string value\r\n   * and a failure status is returned.\r\n   *\r\n   * Can be used in a custom operator library to get session configuration entries\r\n   * that target one or more custom operators in the library. Example: The config entry\r\n   * custom_op.myop.some_key targets a custom op named \"myop\".\r\n   *\r\n   * \\param[in] options The session options.\r\n   * \\param[in] config_key A null-terminated UTF-8 string representation of the config key.\r\n   * \\param[in] config_value Pointer to memory where the null-terminated UTF-8 string value will be stored.\r\n   * \\param[in,out] size Pointer to the size of the `config_value` buffer. See above comments for details.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.14\r\n   */\r\n  ORT_API2_STATUS(GetSessionConfigEntry, _In_ const OrtSessionOptions* options,\r\n                  _In_z_ const char* config_key, _Out_ char* config_value, _Inout_ size_t* size);\r\n\r\n  /// @}\r\n\r\n  /** \\brief Append dnnl provider to session options\r\n   *\r\n   * If oneDNN is not available, this function will return failure.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] dnnl_options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.15.\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsAppendExecutionProvider_Dnnl,\r\n                  _In_ OrtSessionOptions* options, _In_ const OrtDnnlProviderOptions* dnnl_options);\r\n\r\n  /** \\brief Create an OrtDnnlProviderOptions\r\n   *\r\n   * \\param[out] out Newly created ::OrtDnnlProviderOptions. Must be released with OrtApi::ReleaseDnnlProviderOptions\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.15.\r\n   */\r\n  ORT_API2_STATUS(CreateDnnlProviderOptions, _Outptr_ OrtDnnlProviderOptions** out);\r\n\r\n  /** \\brief Set options in a oneDNN Execution Provider.\r\n   *\r\n   * Key should be in null terminated string format of the member of ::OrtDnnlProviderOptions\r\n   * and value should be its related range.\r\n   *\r\n   * For example, key=\"use_arena\" and value=\"1\"\r\n   *\r\n   * \\param[in] dnnl_options\r\n   * \\param[in] provider_options_keys Array of UTF-8 null-terminated string for provider options keys\r\n   * \\param[in] provider_options_values Array of UTF-8 null-terminated string for provider options values\r\n   * \\param[in] num_keys Number of elements in the `provider_option_keys` and `provider_options_values` arrays\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.15.\r\n   */\r\n  ORT_API2_STATUS(UpdateDnnlProviderOptions, _Inout_ OrtDnnlProviderOptions* dnnl_options,\r\n                  _In_reads_(num_keys) const char* const* provider_options_keys,\r\n                  _In_reads_(num_keys) const char* const* provider_options_values,\r\n                  _In_ size_t num_keys);\r\n\r\n  /**\r\n   * Get serialized oneDNN provider options string.\r\n   *\r\n   * For example, \"use_arena=1;......\"\r\n   *\r\n   * \\param dnnl_options - OrtDnnlProviderOptions instance\r\n   * \\param allocator - a ptr to an instance of OrtAllocator obtained with CreateAllocator() or GetAllocatorWithDefaultOptions()\r\n   *                      the specified allocator will be used to allocate continuous buffers for output strings and lengths.\r\n   * \\param ptr - is a UTF-8 null terminated string allocated using 'allocator'. The caller is responsible for using the same allocator to free it.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.15.\r\n   */\r\n  ORT_API2_STATUS(GetDnnlProviderOptionsAsString, _In_ const OrtDnnlProviderOptions* dnnl_options, _Inout_ OrtAllocator* allocator, _Outptr_ char** ptr);\r\n\r\n  /** \\brief Release an ::OrtDnnlProviderOptions\r\n   *\r\n   * \\since Version 1.15.\r\n   */\r\n  void(ORT_API_CALL* ReleaseDnnlProviderOptions)(_Frees_ptr_opt_ OrtDnnlProviderOptions* input);\r\n\r\n  /// \\name OrtKernelInfo\r\n  /// Custom operator APIs.\r\n  /// @{\r\n\r\n  /** \\brief Get the graph node name from ::OrtKernelInfo.\r\n   *\r\n   * If `out` is nullptr, the value of `size` is set to the size of the name\r\n   * string (including null-terminator), and a success status is returned.\r\n   *\r\n   * If the `size` parameter is greater than or equal to the name string's size,\r\n   * the value of `size` is set to the true size of the string (including null-terminator),\r\n   * the provided memory is filled with the string's contents, and a success status is returned.\r\n   *\r\n   * If the `size` parameter is less than the actual string's size and `out`\r\n   * is not nullptr, the value of `size` is set to the true size of the string\r\n   * and a failure status is returned.\r\n   *\r\n   * Can be used in a custom operator's CreateKernel callback to get the name of the operator's node name in the graph.\r\n   *\r\n   * \\param[in] info An instance of ::OrtKernelInfo.\r\n   * \\param[out] out Memory location into which to write the UTF-8 null-terminated string representing the name.\r\n   * \\param[in,out] size Pointer to the size of the `out` buffer. See above comments for details.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.15\r\n   */\r\n  ORT_API2_STATUS(KernelInfo_GetNodeName, _In_ const OrtKernelInfo* info, _Out_ char* out, _Inout_ size_t* size);\r\n\r\n  /** \\brief Get the session logger from ::OrtKernelInfo.\r\n   *\r\n   * Used in the CreateKernel callback of an OrtCustomOp to get a logger that can be used to log\r\n   * messages.\r\n   *\r\n   * \\param[in] info An instance of ::OrtKernelInfo.\r\n   * \\param[out] logger Pointer set to the session's ::OrtLogger. Owned by ONNX Runtime, so do not free.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.15\r\n   */\r\n  ORT_API2_STATUS(KernelInfo_GetLogger, _In_ const OrtKernelInfo* info, _Outptr_ const OrtLogger** logger);\r\n\r\n  /// @}\r\n  /// \\name OrtKernelContext\r\n  /// Custom operator APIs.\r\n  /// @{\r\n\r\n  /** \\brief Get the runtime logger from ::OrtKernelContext.\r\n   *\r\n   * Used in the KernelCompute callback of an OrtCustomOp to get a logger that can be used to log\r\n   * messages during inference.\r\n   *\r\n   * \\param[in] context An instance of ::OrtKernelContext.\r\n   * \\param[out] logger Pointer set to the kernel context's ::OrtLogger. Owned by ONNX Runtime, so do not free.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.15\r\n   */\r\n  ORT_API2_STATUS(KernelContext_GetLogger, _In_ const OrtKernelContext* context, _Outptr_ const OrtLogger** logger);\r\n\r\n  /// @}\r\n  /// \\name OrtLogger\r\n  /// Custom operator APIs.\r\n  /// @{\r\n\r\n  /** \\brief Logs a message at the given severity level using the provided ::OrtLogger.\r\n   *\r\n   * Only messages with a severity level equal or greater than the ::OrtLogger's logging severity level\r\n   * are logged. Use OrtApi::Logger_GetLoggingSeverityLevel to get the ::OrtLogger's logging severity\r\n   * level.\r\n   *\r\n   * Can be used in custom operators to log messages with the logger retrieved via OrtApi::KernelInfo_GetLogger.\r\n   *\r\n   * \\param[in] logger The ::OrtLogger instance.\r\n   * \\param[in] log_severity_level The message's severity level.\r\n   * \\param[in] message The message to log.\r\n   * \\param[in] file_path The filepath of the file in which the message is logged. Usually the value of ORT_FILE.\r\n   * \\param[in] line_number The file line number in which the message is logged. Usually the value of __LINE__.\r\n   * \\param[in] func_name The name of the function in which the message is logged. Usually the value of __FUNCTION__.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.15\r\n   */\r\n  ORT_API2_STATUS(Logger_LogMessage, _In_ const OrtLogger* logger, OrtLoggingLevel log_severity_level,\r\n                  _In_z_ const char* message, _In_z_ const ORTCHAR_T* file_path, int line_number,\r\n                  _In_z_ const char* func_name);\r\n\r\n  /** \\brief Get the logging severity level of the ::OrtLogger.\r\n   *\r\n   * Can be used in a custom operator to get the logging severity level of the ::OrtLogger associated with\r\n   * the ::OrtKernelInfo.\r\n   *\r\n   * \\param[in] logger The ::OrtLogger instance.\r\n   * \\param[out] out Pointer to variable assigned with the logging severity level on success.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.15\r\n   */\r\n  ORT_API2_STATUS(Logger_GetLoggingSeverityLevel, _In_ const OrtLogger* logger, _Out_ OrtLoggingLevel* out);\r\n\r\n  /// @}\r\n\r\n  /** \\brief Get a ::OrtValue tensor stored as a constant initializer in the graph node.\r\n   *\r\n   * Used in the CreateKernel callback of an OrtCustomOp to get a tensor value.\r\n   *\r\n   * \\param[in] info ::OrtKernelInfo instance.\r\n   * \\param[in] index The node index.\r\n   * \\param[out] is_constant Is it a constant node input or not.\r\n   * \\param[out] out The OrtValue tensor value.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.15.\r\n   */\r\n  ORT_API2_STATUS(KernelInfoGetConstantInput_tensor, _In_ const OrtKernelInfo* info, size_t index, _Out_ int* is_constant, _Outptr_ const OrtValue** out);\r\n\r\n  /** \\brief Get Optional Type information from an ::OrtTypeInfo\r\n   *\r\n   * This augments ::OrtTypeInfo to return an ::OrtOptionalTypeInfo when the type is optional.\r\n   * The OrtOptionalTypeInfo also has a nested ::OrtTypeInfo that describes the type of the optional value.\r\n   * ::OrtOptionalTypeInfo type can only appear within model metadata to describe inputs/outputs.\r\n   * The actual OrtValues that are supplied in place of optional type inputs should contain\r\n   * specific type that is described by ::OrtOptionalTypeInfo.\r\n   *\r\n   * So the picture: ::OrtTypeInfo -> ::OrtOptionalTypeInfo -> ::OrtTypeInfo (describes the type that can be supplied\r\n   * in place of the optional type when creating the actual ::OrtValue).\r\n   *\r\n   * \\param[in] type_info\r\n   * \\param[out] out A pointer to the ::OrtOptionalTypeInfo. Do not free this value,\r\n   *                 it is owned by OrtTypeInfo instance. When the type_info does not represent\r\n   *                 optional type, nullptr is returned in out.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.15.\r\n   */\r\n  ORT_API2_STATUS(CastTypeInfoToOptionalTypeInfo, _In_ const OrtTypeInfo* type_info,\r\n                  _Outptr_result_maybenull_ const OrtOptionalTypeInfo** out);\r\n\r\n  /** \\brief Get OrtTypeInfo for the allowed contained type from an ::OrtOptionalTypeInfo.\r\n   *\r\n   * This augments ::OrtOptionalTypeInfo to return an ::OrtTypeInfo for the contained type.\r\n   * The OrtOptionalTypeInfo has a nested ::OrtTypeInfo that describes the type of the optional value.\r\n   * ::OrtOptionalTypeInfo type can only appear within model metadata to describe inputs/outputs.\r\n   * The actual OrtValues that are supplied in place of optional type inputs should contain\r\n   * specific type that is described by the returned ::OrtTypeInfo.\r\n   *\r\n   * \\param[in] optional_type_info\r\n   * \\param[out] out A copy of ::OrtTypeInfo for what the optional value could be.\r\n   *                 The user must free this value with ReleaseTypeInfo.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.15.\r\n   */\r\n  ORT_API2_STATUS(GetOptionalContainedTypeInfo, _In_ const OrtOptionalTypeInfo* optional_type_info,\r\n                  _Outptr_ OrtTypeInfo** out);\r\n\r\n  /** \\brief Set a single string in a string tensor\r\n   *  Do not zero terminate the string data.\r\n   *\r\n   * \\param[in] value A string tensor\r\n   * \\param[in] index - flat index of the element\r\n   * \\param[in] length_in_bytes length of the buffer in utf-8 bytes (without the null terminator)\r\n   * \\param[inout] buffer - address of return value\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetResizedStringTensorElementBuffer, _Inout_ OrtValue* value, _In_ size_t index, _In_ size_t length_in_bytes, _Inout_ char** buffer);\r\n\r\n  /** \\brief Get Allocator from KernelContext for a specific memoryInfo. Please use C API ReleaseAllocator to release out object\r\n   *\r\n   * \\param[in] context OrtKernelContext instance\r\n   * \\param[in] mem_info OrtMemoryInfo instance\r\n   * \\param[out] out A pointer to OrtAllocator.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.15.\r\n   */\r\n  ORT_API2_STATUS(KernelContext_GetAllocator, _In_ const OrtKernelContext* context, _In_ const OrtMemoryInfo* mem_info, _Outptr_ OrtAllocator** out);\r\n\r\n  /** \\brief Returns a null terminated string of the build info including git info and cxx flags\r\n   *\r\n   * \\return UTF-8 encoded version string. Do not deallocate the returned buffer.\r\n   *\r\n   * \\since Version 1.15.\r\n   */\r\n  const char*(ORT_API_CALL* GetBuildInfoString)(void);\r\n\r\n  /// \\name OrtROCMProviderOptions\r\n  /// @{\r\n\r\n  /** \\brief Create an OrtROCMProviderOptions\r\n   *\r\n   * \\param[out] out Newly created ::OrtROCMProviderOptions. Must be released with OrtApi::ReleaseROCMProviderOptions\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.16.\r\n   */\r\n  ORT_API2_STATUS(CreateROCMProviderOptions, _Outptr_ OrtROCMProviderOptions** out);\r\n\r\n  /** \\brief Set options in a ROCm Execution Provider.\r\n   *\r\n   * Please refer to https://onnxruntime.ai/docs/execution-providers/ROCm-ExecutionProvider.html\r\n   * to know the available keys and values. Key should be in null terminated string format of the member of\r\n   * ::OrtROCMProviderOptions and value should be its related range.\r\n   *\r\n   * For example, key=\"device_id\" and value=\"0\"\r\n   *\r\n   * \\param[in] rocm_options\r\n   * \\param[in] provider_options_keys Array of UTF-8 null-terminated string for provider options keys\r\n   * \\param[in] provider_options_values Array of UTF-8 null-terminated string for provider options values\r\n   * \\param[in] num_keys Number of elements in the `provider_option_keys` and `provider_options_values` arrays\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.16.\r\n   */\r\n  ORT_API2_STATUS(UpdateROCMProviderOptions, _Inout_ OrtROCMProviderOptions* rocm_options,\r\n                  _In_reads_(num_keys) const char* const* provider_options_keys,\r\n                  _In_reads_(num_keys) const char* const* provider_options_values,\r\n                  _In_ size_t num_keys);\r\n\r\n  /**\r\n   * Get serialized ROCm provider options string.\r\n   *\r\n   * For example, \"device_id=0;arena_extend_strategy=0;......\"\r\n   *\r\n   * \\param rocm_options - OrtROCMProviderOptions instance\r\n   * \\param allocator - a ptr to an instance of OrtAllocator obtained with CreateAllocator() or GetAllocatorWithDefaultOptions()\r\n   *                      the specified allocator will be used to allocate continuous buffers for output strings and lengths.\r\n   * \\param ptr - is a UTF-8 null terminated string allocated using 'allocator'. The caller is responsible for using the same allocator to free it.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.16.\r\n   */\r\n  ORT_API2_STATUS(GetROCMProviderOptionsAsString, _In_ const OrtROCMProviderOptions* rocm_options, _Inout_ OrtAllocator* allocator, _Outptr_ char** ptr);\r\n\r\n  /** \\brief Release an ::OrtROCMProviderOptions\r\n   *\r\n   * \\note This is an exception in the naming convention of other Release* functions, as the name of the method does not have the V2 suffix, but the type does\r\n   *\r\n   * \\since Version 1.16.\r\n   */\r\n  void(ORT_API_CALL* ReleaseROCMProviderOptions)(_Frees_ptr_opt_ OrtROCMProviderOptions* input);\r\n\r\n  /** \\brief Create an allocator with specific type and register it with the ::OrtEnv\r\n   *  This API enhance CreateAndRegisterAllocator that it can create an allocator with specific type, not just CPU allocator\r\n   *  Enables sharing the allocator between multiple sessions that use the same env instance.\r\n   *  Lifetime of the created allocator will be valid for the duration of the environment.\r\n   *  Returns an error if an allocator with the same ::OrtMemoryInfo is already registered.\r\n   *  \\param[in] env OrtEnv instance\r\n   *  \\param[in] provider_type ExecutionProvider type\r\n   *  \\param[in] mem_info OrtMemoryInfo instance\r\n   *  \\param[in] arena_cfg Arena configuration\r\n   *  \\param[in] provider_options_keys key of the provider options map\r\n   *  \\param[in] provider_options_values value of the provider options map\r\n   *  \\param[in] num_keys Length of the provider options map\r\n   */\r\n  ORT_API2_STATUS(CreateAndRegisterAllocatorV2, _Inout_ OrtEnv* env, _In_ const char* provider_type,\r\n                  _In_ const OrtMemoryInfo* mem_info, _In_ const OrtArenaCfg* arena_cfg,\r\n                  _In_reads_(num_keys) const char* const* provider_options_keys, _In_reads_(num_keys) const char* const* provider_options_values, _In_ size_t num_keys);\r\n\r\n  /** \\brief Run the model asynchronously in a thread owned by intra op thread pool\r\n   *\r\n   * \\param[in] session\r\n   * \\param[in] run_options If nullptr, will use a default ::OrtRunOptions\r\n   * \\param[in] input_names Array of null terminated UTF8 encoded strings of the input names\r\n   * \\param[in] input Array of ::OrtValue%s of the input values\r\n   * \\param[in] input_len Number of elements in the input_names and inputs arrays\r\n   * \\param[in] output_names Array of null terminated UTF8 encoded strings of the output names\r\n   * \\param[in] output_names_len Number of elements in the output_names and outputs array\r\n   * \\param[out] output OrtValue* array of size output_names_len.\r\n   *             On calling RunAsync, output[i] could either be a null or a pointer to a preallocated OrtValue.\r\n   *             Later, the output array will be passed to run_async_callback with all null(s) filled with valid\r\n   *             OrtValue pointer(s) allocated by onnxruntime.\r\n   *             NOTE: it is customer's duty to finally release the output array and each of its member,\r\n   *             regardless of whether the member (OrtValue*) is allocated by onnxruntime or preallocated by the customer.\r\n   * \\param[in] run_async_callback Callback function on model run completion\r\n   * \\param[in] user_data User data that pass back to run_async_callback\r\n   */\r\n  ORT_API2_STATUS(RunAsync, _Inout_ OrtSession* session, _In_opt_ const OrtRunOptions* run_options,\r\n                  _In_reads_(input_len) const char* const* input_names,\r\n                  _In_reads_(input_len) const OrtValue* const* input, size_t input_len,\r\n                  _In_reads_(output_names_len) const char* const* output_names, size_t output_names_len,\r\n                  _Inout_updates_all_(output_names_len) OrtValue** output,\r\n                  _In_ RunAsyncCallbackFn run_async_callback, _In_opt_ void* user_data);\r\n\r\n  /**\r\n   * Update TensorRT EP provider option where its data type is pointer, for example 'user_compute_stream'.\r\n   * If the data type of the provider option can be represented by string please use UpdateTensorRTProviderOptions.\r\n   *\r\n   * Note: It's caller's responsibility to properly manage the lifetime of the instance pointed by this pointer.\r\n   *\r\n   * \\param tensorrt_options - OrtTensorRTProviderOptionsV2 instance\r\n   * \\param key - Name of the provider option\r\n   * \\param value - A pointer to the instance that will be assigned to this provider option\r\n   *\r\n   * \\since Version 1.16.\r\n   */\r\n  ORT_API2_STATUS(UpdateTensorRTProviderOptionsWithValue, _Inout_ OrtTensorRTProviderOptionsV2* tensorrt_options, _In_ const char* key, _In_ void* value);\r\n\r\n  /**\r\n   * Get TensorRT EP provider option where its data type is pointer.\r\n   * If the data type of the provider option can be represented by string please use GetTensorRTProviderOptionsAsString.\r\n   *\r\n   * \\param tensorrt_options - OrtTensorRTProviderOptionsV2 instance\r\n   * \\param key - Name of the provider option\r\n   * \\param ptr - A pointer to the instance that is kept by the provider option\r\n   *\r\n   * \\since Version 1.16.\r\n   */\r\n  ORT_API2_STATUS(GetTensorRTProviderOptionsByName, _In_ const OrtTensorRTProviderOptionsV2* tensorrt_options, _In_ const char* key, _Outptr_ void** ptr);\r\n\r\n  /**\r\n   * Update CUDA EP provider option where its data type is pointer, for example 'user_compute_stream'.\r\n   * If the data type of the provider option can be represented by string please use UpdateCUDAProviderOptions.\r\n   *\r\n   * Note: It's caller's responsibility to properly manage the lifetime of the instance pointed by this pointer.\r\n   *\r\n   * \\param cuda_options - OrtCUDAProviderOptionsV2 instance\r\n   * \\param key - Name of the provider option\r\n   * \\param value - A pointer to the instance that will be assigned to this provider option\r\n   *\r\n   * \\since Version 1.16.\r\n   */\r\n  ORT_API2_STATUS(UpdateCUDAProviderOptionsWithValue, _Inout_ OrtCUDAProviderOptionsV2* cuda_options, _In_ const char* key, _In_ void* value);\r\n\r\n  /**\r\n   * Get CUDA EP provider option where its data type is pointer.\r\n   * If the data type of the provider option can be represented by string please use GetCUDAProviderOptionsAsString.\r\n   *\r\n   * \\param cuda_options - OrtCUDAProviderOptionsV2 instance\r\n   * \\param key - Name of the provider option\r\n   * \\param ptr - A pointer to the instance that is kept by the provider option\r\n   *\r\n   * \\since Version 1.16.\r\n   */\r\n  ORT_API2_STATUS(GetCUDAProviderOptionsByName, _In_ const OrtCUDAProviderOptionsV2* cuda_options, _In_ const char* key, _Outptr_ void** ptr);\r\n\r\n  /**\r\n   * Get a EP resource.\r\n   * E.g. a cuda stream or a cublas handle\r\n   *\r\n   * \\param context - Kernel context\r\n   * \\param resource_version - Version of the resource\r\n   * \\param resource_id - Type of resource\r\n   * \\param resource - A pointer to returned resource\r\n   *\r\n   * \\since Version 1.16.\r\n   */\r\n  ORT_API2_STATUS(KernelContext_GetResource, _In_ const OrtKernelContext* context, _In_ int resource_version,\r\n                  _In_ int resource_id, _Outptr_ void** resource);\r\n\r\n  /** \\brief Set user logging function\r\n   *\r\n   *  By default the logger created by the CreateEnv* functions is used to create the session logger as well.\r\n   *  This function allows a user to override this default session logger with a logger of their own choosing. This way\r\n   *  the user doesn't have to create a separate environment with a custom logger. This addresses the problem when\r\n   *  the user already created an env but now wants to use a different logger for a specific session (for debugging or\r\n   *  other reasons).\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] user_logging_function A pointer to a logging function.\r\n   * \\param[in] user_logging_param A pointer to arbitrary data passed as the ::OrtLoggingFunction `param` parameter to\r\n   *                         `user_logging_function`. This parameter is optional.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.17.\r\n   */\r\n  ORT_API2_STATUS(SetUserLoggingFunction, _Inout_ OrtSessionOptions* options,\r\n                  _In_ OrtLoggingFunction user_logging_function, _In_opt_ void* user_logging_param);\r\n\r\n  /**\r\n   * Get number of input from OrtShapeInferContext\r\n   *\r\n   * \\param[in] context\r\n   * \\param[out] out The number of inputs\r\n   *\r\n   * \\since Version 1.17.\r\n   */\r\n  ORT_API2_STATUS(ShapeInferContext_GetInputCount, _In_ const OrtShapeInferContext* context, _Out_ size_t* out);\r\n\r\n  /**\r\n   * Get type and shape info of an input\r\n   *\r\n   * \\param[in] context\r\n   * \\param[in] index The index of the input\r\n   * \\param[out] info Type shape info of the input\r\n   *\r\n   * \\since Version 1.17.\r\n   */\r\n  ORT_API2_STATUS(ShapeInferContext_GetInputTypeShape, _In_ const OrtShapeInferContext* context, _In_ size_t index, _Outptr_ OrtTensorTypeAndShapeInfo** info);\r\n\r\n  /**\r\n   * Get attribute from OrtShapeInferContext. Note that OrtShapeInferContext is a per-node context, one could only read attribute from current node.\r\n   *\r\n   * \\param[in] context\r\n   * \\param[in] attr_name Name of the attribute\r\n   * \\param[out] attr Handle of the attribute fetched\r\n   *\r\n   * \\since Version 1.17.\r\n   */\r\n  ORT_API2_STATUS(ShapeInferContext_GetAttribute, _In_ const OrtShapeInferContext* context, _In_ const char* attr_name, _Outptr_ const OrtOpAttr** attr);\r\n\r\n  /**\r\n   * Set type and shape info of an output\r\n   *\r\n   * \\param[in] context\r\n   * \\param[in] index The index of the output\r\n   * \\param[out] info Type shape info of the output\r\n   *\r\n   * \\since Version 1.17.\r\n   */\r\n  ORT_API2_STATUS(ShapeInferContext_SetOutputTypeShape, _In_ const OrtShapeInferContext* context, _In_ size_t index, _In_ const OrtTensorTypeAndShapeInfo* info);\r\n\r\n  /**\r\n   * Set symbolic shape to type shape info\r\n   *\r\n   * \\param[in] info Type shape info\r\n   * \\param[in] dim_params Symbolic strings\r\n   * \\param[in] dim_params_length Number of strings\r\n   *\r\n   * \\since Version 1.17.\r\n   */\r\n  ORT_API2_STATUS(SetSymbolicDimensions, _In_ OrtTensorTypeAndShapeInfo* info, _In_ const char* dim_params[], _In_ size_t dim_params_length);\r\n\r\n  /**\r\n   * Read contents of an attribute to data\r\n   *\r\n   * \\param[in] op_attr\r\n   * \\param[in] type Attribute type\r\n   * \\param[out] data Memory address to save raw content of the attribute\r\n   * \\param[in] len Number of bytes allowed to store in data\r\n   * \\param[out] out Number of bytes required to save the data when the call failed, or the real number of bytes saved to data on success\r\n   *\r\n   * \\note Does not support reading graph attributes. Refer to Node_GetSubgraphs.\r\n   *\r\n   * \\since Version 1.17.\r\n   */\r\n  ORT_API2_STATUS(ReadOpAttr, _In_ const OrtOpAttr* op_attr, _In_ OrtOpAttrType type, _Inout_ void* data, _In_ size_t len, _Out_ size_t* out);\r\n\r\n  /** \\brief Set whether to use deterministic compute.\r\n   *\r\n   * Default is false. If set to true, this will enable deterministic compute for GPU kernels where possible.\r\n   * Note that this most likely will have a performance cost.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] value\r\n   *\r\n   * \\since Version 1.17.\r\n   */\r\n  ORT_API2_STATUS(SetDeterministicCompute, _Inout_ OrtSessionOptions* options, bool value);\r\n\r\n  /**\r\n   * Run fn in parallel\r\n   *\r\n   * \\param[in] context\r\n   * \\param[in] fn Function accepting usr_data and an integer as iterator\r\n   * \\param[in] total The number of times fn is to be invoked\r\n   * \\param[in] num_batch Number of batches by which the \"total\" is to be divided in maximum. When zero, there is no limit\r\n   * \\param[in] usr_data User data to be passed back to fn\r\n   *\r\n   * \\since Version 1.17.\r\n   */\r\n  ORT_API2_STATUS(KernelContext_ParallelFor, _In_ const OrtKernelContext* context, _In_ void (*fn)(void*, size_t), _In_ size_t total, _In_ size_t num_batch, _In_ void* usr_data);\r\n\r\n  /** \\brief Append OpenVINO execution provider to the session options\r\n   *\r\n   * If OpenVINO is not available (due to a non OpenVINO enabled build, or if OpenVINO is not installed on the system), this function will fail.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] provider_options_keys\r\n   * \\param[in] provider_options_values\r\n   * \\param[in] num_keys\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.17.\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsAppendExecutionProvider_OpenVINO_V2,\r\n                  _In_ OrtSessionOptions* options,\r\n                  _In_reads_(num_keys) const char* const* provider_options_keys,\r\n                  _In_reads_(num_keys) const char* const* provider_options_values,\r\n                  _In_ size_t num_keys);\r\n\r\n  /** \\brief Append VitisAI provider to session options\r\n   *\r\n   * If VitisAI is not available (due to a non VitisAI enabled build, or if VitisAI is not installed on the system), this function will return failure.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] provider_options_keys\r\n   * \\param[in] provider_options_values\r\n   * \\param[in] num_keys\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.18.\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsAppendExecutionProvider_VitisAI,\r\n                  _In_ OrtSessionOptions* options,\r\n                  _In_reads_(num_keys) const char* const* provider_options_keys,\r\n                  _In_reads_(num_keys) const char* const* provider_options_values,\r\n                  _In_ size_t num_keys);\r\n\r\n  /** \\brief Get scratch buffer from the corresponding allocator under the specific OrtMemoryInfo object.\r\n   *         NOTE: callers are responsible to release this scratch buffer from the corresponding allocator\r\n   *  \\param[in] context OrtKernelContext instance\r\n   *  \\param[in] mem_info OrtMemoryInfo instance\r\n   *  \\param[in] count_or_bytes How many bytes is this scratch buffer\r\n   *  \\param[out] out A pointer to the scratch buffer\r\n   *\r\n   *  \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.18.\r\n   */\r\n  ORT_API2_STATUS(KernelContext_GetScratchBuffer, _In_ const OrtKernelContext* context, _In_ const OrtMemoryInfo* mem_info, _In_ size_t count_or_bytes, _Outptr_ void** out);\r\n\r\n  /** \\brief Get allocator from KernelInfo for a specific memory type. Please use C API ReleaseAllocator to release out object\r\n   *\r\n   * \\param[in] info OrtKernelInfo instance\r\n   * \\param[in] mem_type OrtMemType object\r\n   * \\param[out] out A pointer to OrtAllocator\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.18.\r\n   */\r\n  ORT_API2_STATUS(KernelInfoGetAllocator, _In_ const OrtKernelInfo* info, _In_ OrtMemType mem_type, _Outptr_ OrtAllocator** out);\r\n\r\n  /** \\brief Replace initialized Tensors with external data with the provided files in memory\r\n   *\r\n   * The function will find the initialized TensorProtos with external data in the graph with the provided\r\n   * external file names and the file content in memory. The API gets the external file name, offset, data length\r\n   * from TensorProto, and locate the tensor data from the file in memory buffer.\r\n   * It creates a Tensor to replace the existing Tensor in graph. The replacement\r\n   * will occur before any of the optimizations take place. The data will be copied into the graph\r\n   * since TensorProto can't refer to the user provided buffers.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] external_initializer_file_names Array of null terminated UTF-8 encoded strings of the file names\r\n   *            which holds the external initializers.\r\n   * \\param[in] external_initializer_file_buffer_array Array of pointers to the buffer of the file content.\r\n   *            The buffer can be freed after session creation.\r\n   * \\param[in] external_initializer_file_lengths Array of size_t to indicate the length of file content\r\n   * \\param[in] num_external_initializer_files Number of external files\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.18.\r\n   */\r\n  ORT_API2_STATUS(AddExternalInitializersFromFilesInMemory, _In_ OrtSessionOptions* options,\r\n                  _In_reads_(num_external_initializer_files) const ORTCHAR_T* const* external_initializer_file_names,\r\n                  _In_reads_(num_external_initializer_files) char* const* external_initializer_file_buffer_array,\r\n                  _In_reads_(num_external_initializer_files) const size_t* external_initializer_file_lengths,\r\n                  size_t num_external_initializer_files);\r\n\r\n  /** \\brief Create an OrtLoraAdapter\r\n   *\r\n   * The function attempts to locate file specified by adapter_file_path, read it and create an OrtLoraAdapter\r\n   * instance. The adapter_file_path should be a valid path to a file that contains a valid Lora Adapter\r\n   * format. The function attempts to validate the format at load time. The file will always be memory mapped, unless\r\n   * the platform does not support memory mapping, in which case the file will be read into memory.\r\n   *\r\n   * \\param[in] adapter_file_path adapter file path.\r\n   * \\param[in] allocator optional pointer to a device allocator. If specified\r\n   *            data is copied to the device at some point before Run() is invoked. If nullptr, data stays on CPU.\r\n   *            The data would still be copied to device if required by the model at inference time.\r\n   * \\param[out] out A pointer to a newly created OrtLoraAdapter instance. Must be released with\r\n   *                  OrtApi::ReleaseLoraAdapter.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.20.\r\n   */\r\n  ORT_API2_STATUS(CreateLoraAdapter, const ORTCHAR_T* adapter_file_path, _In_ OrtAllocator* allocator,\r\n                  _Outptr_ OrtLoraAdapter** out);\r\n\r\n  /** \\brief Create an OrtLoraAdapter\r\n   *\r\n   * The function copies the bytes from the array and creates an OrtLoraAdapter instance.\r\n   *\r\n   *\r\n   * \\param[in] bytes pointer to a valid Lora Adapter format buffer.\r\n   * \\param[in] num_bytes length of bytes buffer.\r\n   * \\param[in] allocator optional pointer to a device allocator. If specified\r\n   *            data is copied to the device at some point before Run() is invoked. If nullptr, data stays on CPU.\r\n   *            The data would still be copied to device if required by the model at inference time.\r\n   * \\param[out] out A pointer to a newly created OrtLoraAdapter instance. Must be released with\r\n   *                  OrtApi::ReleaseLoraAdapter.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.20.\r\n   */\r\n  ORT_API2_STATUS(CreateLoraAdapterFromArray, _In_ const void* bytes, size_t num_bytes, _In_ OrtAllocator* allocator,\r\n                  _Outptr_ OrtLoraAdapter** out);\r\n\r\n  /** \\brief Release an ::OrtLoraAdapter obtained from OrtApi::CreateLoraAdapter\r\n   */\r\n  ORT_CLASS_RELEASE(LoraAdapter);\r\n\r\n  /** \\brief Add the Lora Adapter to the list of active adapters.\r\n   *\r\n   * The function adds the Lora Adapter to the list of active adapters. The Lora Adapter must be created with\r\n   * OrtApi::CreateLoraAdapter or FromArray. The Lora Adapter will be used by the session to run the model.\r\n   * The instance of the OrtRunOptions can then be used to customize the Run() calls.\r\n   * More than one OrtLoraAdapter can be active at the same time. Lora Parameters that belong to different\r\n   * Lora adapters that will be active at the same time must not overlap.\r\n   * This setting does not affect RunWithBinding.\r\n   *\r\n   * \\param[in] options OrtRunOptions instance\r\n   * \\param[in] adapter OrtLoraAdapter instance\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.20.\r\n   */\r\n  ORT_API2_STATUS(RunOptionsAddActiveLoraAdapter, _Inout_ OrtRunOptions* options, _In_ const OrtLoraAdapter* adapter);\r\n\r\n  /// @}\r\n  /// \\name OrtEpDynamicOptions\r\n  /// @{\r\n\r\n  /** \\brief Set DynamicOptions for EPs (Execution Providers)\r\n   *\r\n   * Valid options can be found in `include\\onnxruntime\\core\\session\\onnxruntime_session_options_config_keys.h`\r\n   * Look for `kOrtEpDynamicOptions`\r\n   *\r\n   * \\param[in] sess OrtSession\r\n   * \\param[in] keys Array of null terminated UTF8 encoded strings of EP dynamic option keys\r\n   * \\param[in] values Array of null terminated UTF8 encoded string of EP dynamic option values\r\n   * \\param[in] kv_len Number of elements in the keys and values arrays\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.20.\r\n   */\r\n  ORT_API2_STATUS(SetEpDynamicOptions, _Inout_ OrtSession* sess, _In_reads_(kv_len) const char* const* keys,\r\n                  _In_reads_(kv_len) const char* const* values, _In_ size_t kv_len);\r\n\r\n  /// @}\r\n\r\n  /** \\brief Release an OrtValueInfo instance if it was not added to an OrtGraph.\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_CLASS_RELEASE(ValueInfo);\r\n\r\n  /** \\brief Release an OrtNode if it was not added to an OrtGraph.\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_CLASS_RELEASE(Node);\r\n\r\n  /** \\brief Release an OrtGraph.\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_CLASS_RELEASE(Graph);\r\n\r\n  /** \\brief Release an OrtModel.\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_CLASS_RELEASE(Model);\r\n\r\n  /** \\brief Get the value name from an OrtValueInfo instance.\r\n   * \\param[in] value_info The OrtValueInfo instance.\r\n   * \\param[out] name The name of the OrtValueInfo\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(GetValueInfoName, _In_ const OrtValueInfo* value_info, _Out_ const char** name);\r\n\r\n  /** \\brief Get the type information from an OrtValueInfo instance.\r\n   * \\param[in] value_info The OrtValueInfo instance.\r\n   * \\param[out] type_info The type info of the OrtValueInfo\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(GetValueInfoTypeInfo, _In_ const OrtValueInfo* value_info, _Outptr_ const OrtTypeInfo** type_info);\r\n\r\n  /** \\brief Get the Model Editor API instance\r\n   *\r\n   * Get the Model Editor API instance to create a new model or augment an existing model.\r\n   *\r\n   * \\return Model Editor API struct\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  const OrtModelEditorApi*(ORT_API_CALL* GetModelEditorApi)(void);\r\n\r\n  /** \\brief Create an OrtValue for a Tensor that uses pre-existing memory.\r\n   *\r\n   * ORT will take ownership of the memory and free it using the provided deleter when no longer in use.\r\n   *\r\n   * \\param[in] deleter OrtAllocator instance that will be used to free the memory.\r\n   *                    Only the OrtAllocator:Info and OrtAllocator::Release functions are required.\r\n   *                    The OrtMemoryInfo returned by OrtAllocator::Info must match the location of p_data.\r\n   * \\param[in] p_data Pointer to the memory that will be used by the Tensor. ORT will take ownership of the memory.\r\n   * \\param[in] p_data_len Length of the memory in bytes.\r\n   * \\param[in] shape Dimensions of the Tensor. All values should be > 0.\r\n   * \\param[in] shape_len Number of dimensions in the shape array.\r\n   * \\param[in] type Data type of the Tensor.\r\n   * \\param[out] out Newly created ::OrtValue. Must be freed with OrtApi::ReleaseValue\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CreateTensorWithDataAndDeleterAsOrtValue, _In_ OrtAllocator* deleter,\r\n                  _In_ void* p_data, size_t p_data_len,\r\n                  _In_ const int64_t* shape, size_t shape_len,\r\n                  ONNXTensorElementDataType type,\r\n                  _Outptr_ OrtValue** out);\r\n\r\n  /** \\brief sets load cancellation flag to abort session loading process.\r\n   *\r\n   * \\param[in] options instance that was passed to the session at creation time.\r\n   * \\param[in] cancel setting this to true after model loading process was initiated will\r\n   *            attempt to cancel the loading process. If cancellation is successful, CreateSession()\r\n   *            CreateSessionFromArray() or any other session creation API that take session options as an\r\n   *            argument will return an OrtStatus indicating that session loading was canceled at user request,\r\n   *            error code ORT_MODEL_LOAD_CANCELED.\r\n   *            The APIs above would not return any valid Session instance. This is the best case effort and the result\r\n   *            is not guaranteed. The session may have already been created and initialized\r\n   *            before the cancellation request was issued.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsSetLoadCancellationFlag, _Inout_ OrtSessionOptions* options,\r\n                  _In_ bool cancel);\r\n\r\n  /** \\brief Get the Compile API instance.\r\n   *\r\n   * Get the Compile API instance to compile ONNX models. Execution providers that support compilation fuse a subgraph\r\n   * into an EPContext node that wraps a provider-specific binary representation of the subgraph.\r\n   * For more details about the EPContext design, refer to:\r\n   *  \\htmlonly\r\n   *  <a href=\"https://onnxruntime.ai/docs/execution-providers/EP-Context-Design.html\">EPContext design document.</a>\r\n   *  \\endhtmlonly\r\n   *\r\n   * \\return Compile API struct instance.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  const OrtCompileApi*(ORT_API_CALL* GetCompileApi)(void);\r\n\r\n  //\r\n  // OrtKeyValuePairs\r\n  //\r\n\r\n  /** \\brief Create an OrtKeyValuePairs instance.\r\n   *\r\n   * \\param[out] out A pointer to a newly created OrtKeyValuePairs instance.\r\n   *\r\n   * \\note Must be released by calling ReleaseKeyValuePairs.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  void(ORT_API_CALL* CreateKeyValuePairs)(_Outptr_ OrtKeyValuePairs** out);\r\n\r\n  /** \\brief Add a key-value pair to the OrtKeyValuePairs instance.\r\n   *\r\n   * If a pair with the same key already exists, it is overwritten.\r\n   *\r\n   * \\param[in] kvps OrtKeyValuePairs instance.\r\n   * \\param[in] key Key to be added.\r\n   * \\param[in] value Value to be added.\r\n   *\r\n   * \\note The `key` and `value` are copied internally.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n\r\n  void(ORT_API_CALL* AddKeyValuePair)(_In_ OrtKeyValuePairs* kvps, _In_ const char* key, _In_ const char* value);\r\n\r\n  /** \\brief Get the value associated with a key in the OrtKeyValuePairs instance.\r\n   *\r\n   * \\param[in] kvps OrtKeyValuePairs instance.\r\n   * \\param[in] key Key to be searched.\r\n   *\r\n   * \\return The value associated with the key, or nullptr if the key does not exist.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  const char*(ORT_API_CALL* GetKeyValue)(_In_ const OrtKeyValuePairs* kvps, _In_ const char* key);\r\n\r\n  /** \\brief Get all the key-value pairs from the OrtKeyValuePairs instance.\r\n   *\r\n   * \\param[in] kvps OrtKeyValuePairs instance.\r\n   * \\param[out] keys Array of keys from `kvps`.\r\n   * \\param[out] values Array of values from `kvps`.\r\n   * \\param[out] num_entries Number of entries in `keys` and `values`.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  void(ORT_API_CALL* GetKeyValuePairs)(_In_ const OrtKeyValuePairs* kvps,\r\n                                       _Outptr_ const char* const** keys, _Outptr_ const char* const** values,\r\n                                       _Out_ size_t* num_entries);\r\n\r\n  /** \\brief Remove a key-value pair from the OrtKeyValuePairs instance.\r\n   *\r\n   * \\param[in] kvps OrtKeyValuePairs instance.\r\n   * \\param[in] key Key to be removed. No error if not found.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  void(ORT_API_CALL* RemoveKeyValuePair)(_In_ OrtKeyValuePairs* kvps, _In_ const char* key);\r\n\r\n  /** \\brief Release an OrtKeyValuePairs instance.\r\n   *\r\n   * \\param[in] input OrtKeyValuePairs instance to be released.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_CLASS_RELEASE(KeyValuePairs);\r\n\r\n  /** \\brief Register an execution provider library with ORT.\r\n   *\r\n   * The library must export 'CreateEpFactories' and 'ReleaseEpFactory' functions.\r\n   * See OrtEpApi for more details.\r\n   *\r\n   * \\param[in] env The OrtEnv instance to register the library in.\r\n   * \\param[in] registration_name The name to register the execution provider library under.\r\n   * \\param[in] path The path to the execution provider library.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(RegisterExecutionProviderLibrary, _In_ OrtEnv* env, _In_ const char* registration_name,\r\n                  _In_ const ORTCHAR_T* path);\r\n\r\n  /** \\brief Unregister an execution provider library with ORT.\r\n   *\r\n   * ORT will call ReleaseEpFactory for all factories created by the library, and unload the library.\r\n   *\r\n   * You <b>MUST</b> ensure there are no Session instances using execution providers created by the library\r\n   * before calling this function.\r\n   *\r\n   * \\param[in] env The OrtEnv instance to unregister the library from.\r\n   * \\param[in] registration_name The name the execution provider library was registered under.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(UnregisterExecutionProviderLibrary, _In_ OrtEnv* env, _In_ const char* registration_name);\r\n\r\n  /** \\brief Get the list of available OrtEpDevice instances.\r\n   *\r\n   * Each OrtEpDevice instance contains details of the execution provider and the device it will use.\r\n   *\r\n   * \\param[in] env The OrtEnv instance to query.\r\n   * \\param[out] ep_devices The OrtEpDevice instances that the execution provider will use.\r\n   * \\param[out] num_ep_devices The number of OrtEpDevice instances returned.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(GetEpDevices, _In_ const OrtEnv* env,\r\n                  _Outptr_ const OrtEpDevice* const** ep_devices, _Out_ size_t* num_ep_devices);\r\n\r\n  /** \\brief Append the execution provider that is responsible for the selected OrtEpDevice instances\r\n   *         to the session options.\r\n   *\r\n   * \\param[in] session_options Session options to add execution provider to.\r\n   * \\param[in] env Environment that execution providers were registered with.\r\n   * \\param[in] ep_devices One or more OrtEpDevice instances to create an execution provider for.\r\n   *                       Obtain from GetEpDevices. All OrtEpDevice instances must be from the same execution\r\n   *                       provider. It is only necessary to provide multiple OrtEpDevices if you want to use the\r\n   *                       same execution provider for multiple devices.\r\n   *                       e.g. the EP is capable of running on GPU and NPU.\r\n   * \\param[in] num_ep_devices Number of OrtEpDevice instances.\r\n   * \\param[in] ep_option_keys Optional keys to configure the execution provider.\r\n   * \\param[in] ep_option_vals Optional values to configure the execution provider.\r\n   * \\param[in] num_ep_options Number of execution provide options to add.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsAppendExecutionProvider_V2, _In_ OrtSessionOptions* session_options,\r\n                  _In_ OrtEnv* env,\r\n                  _In_reads_(num_ep_devices) const OrtEpDevice* const* ep_devices, _In_ size_t num_ep_devices,\r\n                  _In_reads_(num_op_options) const char* const* ep_option_keys,\r\n                  _In_reads_(num_op_options) const char* const* ep_option_vals,\r\n                  size_t num_ep_options);\r\n\r\n  /** \\brief Set the execution provider selection policy for the session.\r\n   *\r\n   * Allows users to specify a device selection policy for automatic execution provider (EP) selection.\r\n   * If custom selection is required please use SessionOptionsSetEpSelectionPolicyDelegate instead.\r\n   *\r\n   * \\param[in] session_options The OrtSessionOptions instance.\r\n   * \\param[in] policy The device selection policy to use (see OrtExecutionProviderDevicePolicy).\r\n   *\r\n   * \\since Version 1.22\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsSetEpSelectionPolicy, _In_ OrtSessionOptions* session_options,\r\n                  _In_ OrtExecutionProviderDevicePolicy policy);\r\n\r\n  /** \\brief Set the execution provider selection policy delegate for the session.\r\n   *\r\n   * Allows users to provide a custom device selection policy for automatic execution provider (EP) selection.\r\n   *\r\n   * \\param[in] session_options The OrtSessionOptions instance.\r\n   * \\param[in] delegate Delegate callback for custom selection.\r\n   * \\param[in] delegate_state Optional state that will be passed to the delegate callback. nullptr if not required.\r\n   *\r\n   * \\since Version 1.22\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsSetEpSelectionPolicyDelegate, _In_ OrtSessionOptions* session_options,\r\n                  _In_ EpSelectionDelegate delegate,\r\n                  _In_opt_ void* delegate_state);\r\n\r\n  /** \\brief Get the hardware device type.\r\n   *\r\n   * \\param[in] device The OrtHardwareDevice instance to query.\r\n   * \\return The hardware device type.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  OrtHardwareDeviceType(ORT_API_CALL* HardwareDevice_Type)(_In_ const OrtHardwareDevice* device);\r\n\r\n  /** \\brief Get the hardware device's vendor identifier.\r\n   *\r\n   * \\param[in] device The OrtHardwareDevice instance to query.\r\n   * \\return The hardware device vendor identifier.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  uint32_t(ORT_API_CALL* HardwareDevice_VendorId)(_In_ const OrtHardwareDevice* device);\r\n\r\n  /** \\brief Get the hardware device's vendor name.\r\n   *\r\n   * \\param[in] device The OrtHardwareDevice instance to query.\r\n   * \\return The hardware device's vendor name.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  const char*(ORT_API_CALL* HardwareDevice_Vendor)(_In_ const OrtHardwareDevice* device);\r\n\r\n  /** \\brief Get the hardware device's unique identifier.\r\n   *\r\n   * \\param[in] device The OrtHardwareDevice instance to query.\r\n   * \\return The device id.\r\n   *\r\n   * \\note This is not a unique identifier. It identifies the hardware type when combined with vendor id.\r\n   * \\since Version 1.22.\r\n   */\r\n  uint32_t(ORT_API_CALL* HardwareDevice_DeviceId)(_In_ const OrtHardwareDevice* device);\r\n\r\n  /** \\brief Get hardware device metadata.\r\n   *\r\n   * \\param[in] device The OrtHardwareDevice instance to query.\r\n   * \\return An OrtKeyValuePairs instance containing the metadata for the device.\r\n   *         Note: ORT owns the instance so the user must not call ReleaseKeyValuePairs with it.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  const OrtKeyValuePairs*(ORT_API_CALL* HardwareDevice_Metadata)(_In_ const OrtHardwareDevice* device);\r\n\r\n  /** \\brief Get the execution provider name.\r\n   *\r\n   * \\param[in] ep_device The OrtEpDevice instance to query.\r\n   * \\return The execution provider name.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  const char*(ORT_API_CALL* EpDevice_EpName)(_In_ const OrtEpDevice* ep_device);\r\n\r\n  /** \\brief Get the execution provider's vendor name.\r\n   *\r\n   * \\param[in] ep_device The OrtEpDevice instance to query.\r\n   * \\return The execution provider's vendor name.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  const char*(ORT_API_CALL* EpDevice_EpVendor)(_In_ const OrtEpDevice* ep_device);\r\n\r\n  /** \\brief Get the metadata for the OrtEpDevice.\r\n   *\r\n   * \\param[in] ep_device The OrtEpDevice instance to query.\r\n   * \\return An OrtKeyValuePairs instance containing the metadata for the device.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  const OrtKeyValuePairs*(ORT_API_CALL* EpDevice_EpMetadata)(_In_ const OrtEpDevice* ep_device);\r\n\r\n  /** \\brief Get the execution provider options for the OrtEpDevice.\r\n   *\r\n   * \\param[in] ep_device The OrtEpDevice instance to query.\r\n   * \\return An OrtKeyValuePairs instance containing the execution provider options for the device.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  const OrtKeyValuePairs*(ORT_API_CALL* EpDevice_EpOptions)(_In_ const OrtEpDevice* ep_device);\r\n\r\n  /** \\brief Get the OrtHardwareDevice instance for the OrtEpDevice.\r\n   *\r\n   * \\param[in] ep_device The OrtEpDevice instance to query.\r\n   * \\return The OrtHardwareDevice instance for the device.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  const OrtHardwareDevice*(ORT_API_CALL* EpDevice_Device)(_In_ const OrtEpDevice* ep_device);\r\n\r\n  /** \\brief Get the OrtEpApi instance for implementing an execution provider.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  const OrtEpApi*(ORT_API_CALL* GetEpApi)(void);\r\n\r\n  /** \\brief Compute total size in bytes of the tensor data contained in an OrtValue.\r\n   *\r\n   * Returns the total number of bytes used to store the tensor data. For numeric tensors,\r\n   * this is sizeof(element_type) * total_element_count. OrtValues that are not tensors or\r\n   * that are tensors that contain strings will cause an error to be returned.\r\n   *\r\n   * \\param[in] ort_value OrtValue instance containing a tensor\r\n   * \\param[out] size The total size of the tensor data in bytes\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API2_STATUS(GetTensorSizeInBytes, _In_ const OrtValue* ort_value, _Out_ size_t* size);\r\n\r\n  /** \\brief Calls OrtAllocator::GetStats function\r\n   *\r\n   * Return a pointer to the OrtKeyValuePairs structure that contains the statistics of the allocator\r\n   * and the user should call OrtApi::ReleaseKeyValuePairs.\r\n   *\r\n   * NOTE: If the allocator does not implement this function, the OrtKeyValuePairs instance will be empty.\r\n   *\r\n   * \\param[in] ort_allocator The allocator to get stats from\r\n   * \\param[out] out A pointer to the OrtKeyValuePairs instance that contains the stats\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(AllocatorGetStats, _In_ const OrtAllocator* ort_allocator, _Outptr_ OrtKeyValuePairs** out);\r\n\r\n  /** \\brief Create an ::OrtMemoryInfo\r\n   *\r\n   * \\param[in] name Arbitrary name.\r\n   * \\param[in] device_type Device type.\r\n   * \\param[in] vendor_id PCI Vendor ID. Use 0 for a generic allocator (e.g. WebGPU).\r\n   * \\param[in] device_id Device ID if there are multiple devices of the same type. e.g. 2 GPU devices.\r\n   * \\param[in] mem_type Memory type. Use OrtDeviceMemoryType_DEFAULT for device memory, and\r\n   *                     OrtDeviceMemoryType_HOST_ACCESSIBLE (if applicable) for memory used to transfer between the\r\n   *                     device and the CPU. Use the device_type and device_id of the GPU/NPU that the memory is also\r\n   *                     accessible to.\r\n   * \\param[in] alignment Alignment of the memory if required. Pass 0 for default alignment.\r\n   * \\param[in] allocator_type Allocator type. If OrtAllocatorType::OrtArenaAllocator, the ORT arena will be used.\r\n   *                           Caveat: Support for OrtArenaAllocator is currently limited to usage of internal ORT\r\n   *                           allocators via CreateAllocator/CreateAndRegisterAllocator/CreateAndRegisterAllocatorV2.\r\n   * \\param[out] out Newly created ::OrtMemoryInfo. Must be freed with OrtApi::ReleaseMemoryInfo\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API2_STATUS(CreateMemoryInfo_V2, _In_ const char* name, _In_ enum OrtMemoryInfoDeviceType device_type,\r\n                  _In_ uint32_t vendor_id, _In_ int32_t device_id, _In_ enum OrtDeviceMemoryType mem_type,\r\n                  _In_ size_t alignment, enum OrtAllocatorType allocator_type,\r\n                  _Outptr_ OrtMemoryInfo** out);\r\n\r\n  /** \\brief Get the device memory type from ::OrtMemoryInfo\r\n   *\r\n   * \\param[in] ptr The OrtMemoryInfo instance to query.\r\n   * \\return The device memory type.\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API_T(OrtDeviceMemoryType, MemoryInfoGetDeviceMemType, _In_ const OrtMemoryInfo* ptr);\r\n\r\n  /** \\brief Get the vendor id from ::OrtMemoryInfo\r\n   *\r\n   * \\param[in] ptr The OrtMemoryInfo instance to query.\r\n   * \\return The vendor id.\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API_T(uint32_t, MemoryInfoGetVendorId, _In_ const OrtMemoryInfo* ptr);\r\n\r\n  /// \\name OrtValueInfo\r\n  /// @{\r\n\r\n  /** \\brief Get the OrtNode that produces the value represented by the given OrtValueInfo.\r\n   * Optionally returns the associated output index.\r\n   *\r\n   * \\param[in] value_info The OrtValueInfo instance.\r\n   * \\param[out] producer_node Output parameter set to the OrtNode that produces the OrtValueInfo.\r\n   * \\param[out] producer_output_index Optional output parameter set to the OrtNode instance's output index\r\n   *                                   that produces the value. Ignored if set to NULL.\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ValueInfo_GetValueProducer, _In_ const OrtValueInfo* value_info,\r\n                  _Outptr_ const OrtNode** producer_node, _Out_opt_ size_t* producer_output_index);\r\n\r\n  /** \\brief Get the number of consumers of a value as a node input.\r\n   *\r\n   * Only nodes are considered \"consumers\" by this function. To check if an OrtValueInfo is a graph output,\r\n   * call ValueInfo_IsGraphOutput().\r\n   *\r\n   * A single OrtNode may use a single value for more than one input (e.g., Mul(x, x)), so the returned\r\n   * `num_consumers` may be larger than the number of unique OrtNode instances that consume the value.\r\n   *\r\n   * \\param[in] value_info The OrtValueInfo instance.\r\n   * \\param[out] num_consumers Output parameter set to the number of consumers of the value.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ValueInfo_GetValueNumConsumers, _In_ const OrtValueInfo* value_info, _Out_ size_t* num_consumers);\r\n\r\n  /** \\brief Returns information (OrtNode and input index) for all consumer nodes that use the value as an input.\r\n   *\r\n   * Only nodes are considered \"consumers\" by this function.\r\n   *\r\n   * Caller provides 2 pre-allocated arrays that will be filled with the OrtNode and input index values.\r\n   * Use ValueInfo_GetValueNumConsumers() to get the number of consumers of the value.\r\n   *\r\n   * An OrtNode instance may appear multiple times if it uses the given value more than once.\r\n   * Example: For a node MulNode(x, x) that consumes the value 'x' twice, the following is returned:\r\n   *   - nodes: [MulNode, MulNode]\r\n   *   - input_indices: [0, 1]\r\n   *\r\n   * \\param[in] value_info The OrtValueInfo instance.\r\n   * \\param[out] nodes Pre-allocated array of size `num_consumers` that is filled with OrtNode instances.\r\n   * \\param[out] input_indices Pre-allocated array of `num_consumers` elements that is filled\r\n   *                           with input indices. Index is set to -1 for an \"implicit\" input to a consumer node\r\n   *                           that contains a subgraph (e.g., If, Loop) with nodes that use the value internally.\r\n   * \\param[in] num_consumers The size of the `consumer_nodes` and `consumer_input_indices` arrays.\r\n   *                          Typical usage sets this to the value of ValueInfo_GetValueNumConsumers().\r\n   *                          An error status is returned if `num_consumers` is less than the number of actual\r\n   *                          consumers.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ValueInfo_GetValueConsumers, _In_ const OrtValueInfo* value_info,\r\n                  _Out_writes_all_(num_consumers) const OrtNode** nodes,\r\n                  _Out_writes_all_(num_consumers) int64_t* input_indices,\r\n                  _In_ size_t num_consumers);\r\n\r\n  /** \\brief Get the underlying initializer value, as an OrtValue, from the given OrtValueInfo.\r\n   *\r\n   * Sets the output parameter to NULL if the given OrtValueInfo does not represent an initializer.\r\n   * Does not return an error status in this case.\r\n   *\r\n   * Supports initializers defined in an outer scope (i.e., a parent graph).\r\n   *\r\n   * Supports initializers stored in an external file. For external initializers, ORT memory maps\r\n   * the initializer data on the first call to this function. If caller needs custom memory mapping,\r\n   * use ValueInfo_GetExternalInitializerInfo to get the location of the initializer data.\r\n   *\r\n   * \\param[in] value_info The OrtValueInfo instance.\r\n   * \\param[out] initializer_value Output parameter set to the initializer value or NULL. Do not cache the OrtValue\r\n   *                               as it is released when the owning OrtGraph is released.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ValueInfo_GetInitializerValue, _In_ const OrtValueInfo* value_info,\r\n                  _Outptr_ const OrtValue** initializer_value);\r\n\r\n  /** \\brief Get information about an external initializer (e.g., filepath, file offset, byte size).\r\n   *\r\n   * Sets the output parameter `info` to NULL if the given OrtValueInfo does not represent an initializer\r\n   * with external data. In this case, a NULL status (non-error) is returned.\r\n   *\r\n   * \\param[in] value_info The OrtValueInfo instance.\r\n   * \\param[out] info Output parameter set to an OrtExternalInitializerInfo instance that can be used to query\r\n   *                  file path, file offset, etc. ORT sets this to NULL if the OrtValueInfo does not represent\r\n   *                  an external initializer.\r\n   *                  Must release with ReleaseExternalInitializerInfo.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ValueInfo_GetExternalInitializerInfo, _In_ const OrtValueInfo* value_info,\r\n                  _Outptr_result_maybenull_ OrtExternalInitializerInfo** info);\r\n\r\n  /** \\brief Returns a boolean indicating if the given value is a required graph input.\r\n   *\r\n   * For ONNX IR version < 4, all graph inputs without a matching initializer are required.\r\n   *\r\n   * For ONNX IR version >=4, a graph input with a matching initializer is an optional graph input\r\n   * with the initializer serving as the default value.\r\n   *\r\n   * \\param[in] value_info The OrtValueInfo instance representing the graph value.\r\n   * \\param[out] is_required_graph_input Output parameter set to true if the graph value is a required graph input.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ValueInfo_IsRequiredGraphInput, _In_ const OrtValueInfo* value_info,\r\n                  _Out_ bool* is_required_graph_input);\r\n\r\n  /** \\brief Returns a boolean indicating if the given value is an optional graph input.\r\n   *\r\n   * Optional graph inputs were introduced in ONNX IR version 4. For ONNX IR version >=4, a graph input with a\r\n   * matching initializer is an optional graph input with the initializer serving as the default value.\r\n   * The matching initializer is also known as a non-constant initializer.\r\n   *\r\n   * \\param[in] value_info The OrtValueInfo instance representing the graph value.\r\n   * \\param[out] is_optional_graph_input Output parameter set to true if the graph value is an optional graph input.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ValueInfo_IsOptionalGraphInput, _In_ const OrtValueInfo* value_info,\r\n                  _Out_ bool* is_optional_graph_input);\r\n\r\n  /** \\brief Returns a boolean indicating if the given value is a graph output.\r\n   *\r\n   * \\param[in] value_info The OrtValueInfo instance representing the graph value.\r\n   * \\param[out] is_graph_output Output parameter set to true if the graph value is a graph output.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ValueInfo_IsGraphOutput, _In_ const OrtValueInfo* value_info, _Out_ bool* is_graph_output);\r\n\r\n  /** \\brief Returns a boolean indicating if the given value is a constant initializer.\r\n   *\r\n   * For ONNX IR version < 4, all initializers are constant.\r\n   *\r\n   * For ONNX IR version >=4, an initializer that serves as the default value for a matching graph input is not a\r\n   * constant initializer.\r\n   *\r\n   * \\param[in] value_info The OrtValueInfo instance representing the graph value.\r\n   * \\param[out] is_constant_initializer Output parameter set to true if the graph value is a constant initializer.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ValueInfo_IsConstantInitializer, _In_ const OrtValueInfo* value_info,\r\n                  _Out_ bool* is_constant_initializer);\r\n\r\n  /** \\brief Returns a boolean indicating if the given value is defined in an outer scope.\r\n   *\r\n   * Certain operator types (e.g., If and Loop) contain nested subgraphs. This function enables\r\n   * determining whether a value is defined in a parent node's graph.\r\n   *\r\n   * \\param[in] value_info The OrtValueInfo instance representing the graph value.\r\n   * \\param[out] is_from_outer_scope Output parameter set to true if the value is defined in an outer\r\n   *                                 scope (i.e., a parent graph).\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ValueInfo_IsFromOuterScope, _In_ const OrtValueInfo* value_info,\r\n                  _Out_ bool* is_from_outer_scope);\r\n\r\n  /// @}\r\n\r\n  /// \\name OrtGraph\r\n  /// @{\r\n\r\n  /** \\brief Returns a graph's name.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] graph_name Output parameter set to the graph's name.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetName, _In_ const OrtGraph* graph, _Outptr_ const char** graph_name);\r\n\r\n  /** \\brief Get the filepath to the model from which an OrtGraph is constructed.\r\n   *\r\n   * \\note The model's filepath is empty if the filepath is unknown, such as when the model is loaded from bytes\r\n   * via CreateSessionFromArray.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] model_path Output parameter set to the model's null-terminated filepath.\r\n   *                        Set to an empty path string if unknown.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetModelPath, _In_ const OrtGraph* graph, _Outptr_ const ORTCHAR_T** model_path);\r\n\r\n  /** \\brief Returns the ONNX IR version.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] onnx_ir_version Output parameter set to the ONNX IR version.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetOnnxIRVersion, _In_ const OrtGraph* graph, _Out_ int64_t* onnx_ir_version);\r\n\r\n  /** \\brief Returns the number of operator sets that the graph's model uses.\r\n   *\r\n   * \\note An operator set is uniquely identified by the (domain, opset_version) pair. All models must have at\r\n   * least one entry that specifies which entry of the ONNX operator set is used. The ONNX domain is represented by\r\n   * an empty string.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] num_operator_sets Output parameter set to the number of operator sets that the graph's model uses.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetNumOperatorSets, _In_ const OrtGraph* graph, _Out_ size_t* num_operator_sets);\r\n\r\n  /** \\brief Returns the operator sets that the graph's model uses.\r\n   *\r\n   * \\note An operator set is uniquely identified by the (domain, opset_version) pair. All models must have at\r\n   * least one entry that specifies which entry of the ONNX operator set is used. The ONNX domain is represented by\r\n   * an empty string.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] domains Pre-allocated array of `num_operator_sets` elements that is filled with\r\n   *                     null-terminated domain names.\r\n   * \\param[out] opset_versions Pre-allocated array of `num_operator_sets` elements that is filled with\r\n   *                            the opset version of the corresponding domain in the `domains` array.\r\n   * \\param[in] num_operator_sets The size of the `domains` and `opset_versions` arrays.\r\n   *                              Typical usage sets this to the result of Graph_GetNumOperatorSets().\r\n   *                              An error status is returned if `num_operator_sets` is less than the actual number\r\n   *                              of operator sets.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetOperatorSets, _In_ const OrtGraph* graph,\r\n                  _Out_writes_(num_operator_sets) const char** domains,\r\n                  _Out_writes_(num_operator_sets) int64_t* opset_versions, _In_ size_t num_operator_sets);\r\n\r\n  /** \\brief Returns the number of graph inputs.\r\n   *\r\n   * \\note The count includes initializers that are included in the list of graph inputs.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] num_inputs Output parameter set to the number of graph inputs.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetNumInputs, _In_ const OrtGraph* graph, _Out_ size_t* num_inputs);\r\n\r\n  /** \\brief Returns the graph's inputs as OrtValueInfo instances.\r\n   *\r\n   * \\note The result includes initializers that are included in the list of graph inputs.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] inputs Pre-allocated array of `num_inputs` elements that is filled with the graph's inputs.\r\n   * \\param[in] num_inputs The size of the `inputs` array.\r\n   *                       Typical usage sets this to the result of Graph_GetNumInputs(). An error status is\r\n   *                       returned if `num_inputs` is less than the number of graph inputs.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetInputs, _In_ const OrtGraph* graph,\r\n                  _Out_writes_(num_inputs) const OrtValueInfo** inputs, _In_ size_t num_inputs);\r\n\r\n  /** \\brief Returns the number of graph outputs.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] num_outputs Output parameter set to the number of graph outputs.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetNumOutputs, _In_ const OrtGraph* graph, _Out_ size_t* num_outputs);\r\n\r\n  /** \\brief Returns the graph's outputs as OrtValueInfo instances.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] outputs Pre-allocated array of `num_outputs` elements that is filled with the graph's outputs.\r\n   * \\param[in] num_outputs The size of the `outputs` array.\r\n   *                        Typical usage sets this to the result of Graph_GetNumOutputs(). An error status is\r\n   *                        returned if `num_outputs` is less than the number of graph outputs.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetOutputs, _In_ const OrtGraph* graph,\r\n                  _Out_writes_(num_outputs) const OrtValueInfo** outputs, _In_ size_t num_outputs);\r\n\r\n  /** \\brief Returns the number of graph initializers.\r\n   *\r\n   * Counts constant and non-constant initializers.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] num_initializers Output parameter set to the number of graph initializers.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetNumInitializers, _In_ const OrtGraph* graph, _Out_ size_t* num_initializers);\r\n\r\n  /** \\brief Returns the graph's initializers as OrtValueInfo instances.\r\n   *\r\n   * Includes constant and non-constant initializers.\r\n   *\r\n   * For ONNX IR version < 4, all initializers are constant.\r\n   *\r\n   * For ONNX IR version >= 4, an initializer with a name that matches a graph input is considered a\r\n   * non-constant initializer.\r\n   *\r\n   * Call ValueInfo_GetInitializerValue to get the initializer's data.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] initializers Pre-allocated array of `num_outputs` elements that is filled with the initializers.\r\n   * \\param[in] num_initializers The size of the `initializers` array. Typical usage sets this to the\r\n   *                             result of Graph_GetNumInitializers(). An error status is returned if\r\n   *                            `num_initializers` is less than the number of graph initializers.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetInitializers, _In_ const OrtGraph* graph,\r\n                  _Out_writes_(num_initializers) const OrtValueInfo** initializers,\r\n                  _In_ size_t num_initializers);\r\n\r\n  /** \\brief Returns the number of graph nodes.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] num_nodes Output parameter set to the number of graph nodes.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetNumNodes, _In_ const OrtGraph* graph, _Out_ size_t* num_nodes);\r\n\r\n  /** \\brief Returns the graph's nodes as OrtNode instances.\r\n   *\r\n   * The nodes are sorted using a stable topological ordering. Callers are responsible for maintaining their\r\n   * own node ordering if a different order is required.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] nodes Pre-allocated array of `num_nodes` elements that is filled with the graph's nodes.\r\n   * \\param[in] num_nodes The size of the `nodes` array. Typical usage sets this to the\r\n   *                      result of Graph_GetNumNodes(). An error status is returned if\r\n   *                      `num_nodes` is less than the number of graph nodes.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetNodes, _In_ const OrtGraph* graph,\r\n                  _Out_writes_(num_nodes) const OrtNode** nodes, _In_ size_t num_nodes);\r\n\r\n  /** \\brief Get the parent node for the given graph, if any exists.\r\n   *\r\n   * Certain operator types (e.g., If and Loop) contain nested subgraphs. This function enables\r\n   * access to the parent node (e.g., the If and Loop node) from a nested subgraph.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] node Output parameter that is set to the graph's parent node.\r\n   *                  Set to NULL if a parent node does not exist (e.g., for a top-level graph).\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetParentNode, _In_ const OrtGraph* graph, _Outptr_result_maybenull_ const OrtNode** node);\r\n\r\n  /** \\brief Returns an OrtGraph that contains a subset of nodes in the source OrtGraph.\r\n   *\r\n   * \\note The lifetime of \"dst_graph\" is tied to that of \"src_graph\", as they both internally reference\r\n   * the same underlying graph.\r\n   *\r\n   * \\param[in] src_graph The source OrtGraph instance.\r\n   * \\param[in] nodes A subset of the nodes/OrtNodes in 'graph'.\r\n   * \\param[in] num_nodes Number of nodes.\r\n   * \\param[out] dst_graph An OrtGraph created from a given set of nodes. Must be released by calling ReleaseGraph.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetGraphView, _In_ const OrtGraph* src_graph, _In_ const OrtNode** nodes,\r\n                  _In_ size_t num_nodes, _Outptr_ OrtGraph** dst_graph);\r\n\r\n  /// @}\r\n\r\n  /// \\name OrtNode\r\n  /// @{\r\n\r\n  /** \\brief Returns a node's identifier.\r\n   *\r\n   * The node's identifier is only unique in the node's parent graph. Different nested subgraphs\r\n   * (e.g., subgraphs contained by If and Loop nodes) may reuse identifiers.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] node_id Output parameter set to the node's identifier.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetId, _In_ const OrtNode* node, _Out_ size_t* node_id);\r\n\r\n  /** \\brief Returns a node's name. Can be an empty string.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] node_name Output parameter set to the node's name.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetName, _In_ const OrtNode* node, _Outptr_ const char** node_name);\r\n\r\n  /** \\brief Returns a node's operator type (e.g., \"Conv\").\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] operator_type Output parameter set to the name of the node's operator type.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetOperatorType, _In_ const OrtNode* node, _Outptr_ const char** operator_type);\r\n\r\n  /** \\brief Returns a node's domain name.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] domain_name Output parameter set to the node's domain name.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetDomain, _In_ const OrtNode* node, _Outptr_ const char** domain_name);\r\n\r\n  /** \\brief Get the opset version in which the given node's operator type was first defined.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] since_version The opset version in which the node's operator type was first defined.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetSinceVersion, _In_ const OrtNode* node, _Out_ int* since_version);\r\n\r\n  /** \\brief Returns the number of node inputs.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] num_inputs Output parameter set to the number of node inputs.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetNumInputs, _In_ const OrtNode* node, _Out_ size_t* num_inputs);\r\n\r\n  /** \\brief Returns the node's inputs as OrtValueInfo instances.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] inputs Pre-allocated array of `num_inputs` elements that is filled with the node's inputs.\r\n   * \\param[in] num_inputs The size of the `inputs` array.\r\n   *                       Typical usage sets this to the result of Node_GetNumInputs(). An error status is\r\n   *                       returned if `num_inputs` is less than the number of node inputs.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetInputs, _In_ const OrtNode* node,\r\n                  _Out_writes_(num_inputs) const OrtValueInfo** inputs, _In_ size_t num_inputs);\r\n\r\n  /** \\brief Returns the number of node outputs.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] num_outputs Output parameter set to the number of node outputs.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetNumOutputs, _In_ const OrtNode* node, _Out_ size_t* num_outputs);\r\n\r\n  /** \\brief Returns the node's outputs as OrtValueInfo instances.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] outputs Pre-allocated array of `num_outputs` elements that is filled with the node's outputs.\r\n   * \\param[in] num_outputs The size of the `outputs` array.\r\n   *                        Typical usage sets this to the result of Node_GetNumOutputs(). An error status is\r\n   *                        returned if `num_outputs` is less than the number of node outputs.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetOutputs, _In_ const OrtNode* node,\r\n                  _Out_writes_(num_outputs) const OrtValueInfo** outputs, _In_ size_t num_outputs);\r\n\r\n  /** \\brief Returns the number of node implicit inputs.\r\n   *\r\n   * Certain operator types (e.g., If and Loop) contain nested subgraphs. The internal nodes within the nested subgraphs\r\n   * may use values from the outer scope. Those \"outer scope\" values are considered implicit inputs to the node that\r\n   * contains the subgraphs (e.g., the If or Loop node).\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] num_implicit_inputs Output parameter set to the number of node implicit inputs.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetNumImplicitInputs, _In_ const OrtNode* node, _Out_ size_t* num_implicit_inputs);\r\n\r\n  /** \\brief Get the implicit inputs, as OrtValueInfo instances, that are used within the given node's subgraphs.\r\n   *\r\n   * \\note Only certain operator types (e.g., If and Loop) contain nested subgraphs.\r\n   * The internal nodes within the nested subgraphs may use values from the outer scope. Those \"outer scope\" values\r\n   * are considered implicit inputs to the node that contains the subgraphs (e.g., the If or Loop node).\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] implicit_inputs Pre-allocated array of `num_implicit_inputs` elements that is filled the node's\r\n   *                             implicit inputs.\r\n   * \\param[in] num_implicit_inputs The size of the `implicit_inputs` array. Typical usage sets this to the result\r\n   *                                of Node_GetNumImplicitInputs(). An error status is returned if\r\n   *                                `num_implicit_inputs` is less than the number of node implicit inputs.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetImplicitInputs, _In_ const OrtNode* node,\r\n                  _Out_writes_(num_implicit_inputs) const OrtValueInfo** implicit_inputs,\r\n                  _In_ size_t num_implicit_inputs);\r\n\r\n  /** \\brief Returns the number of node attributes.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] num_attributes Output parameter set to the number of node attributes.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetNumAttributes, _In_ const OrtNode* node, _Out_ size_t* num_attributes);\r\n\r\n  /** \\brief Returns a node's attributes as OrtOpAttr instances.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] attributes Pre-allocated array of `num_attributes` elements that is filled with the node's attributes.\r\n   * \\param[in] num_attributes The size of the `num_attributes` array.\r\n   *                           Typical usage sets this to the result of Node_GetNumAttributes(). An error status is\r\n   *                           returned if `num_attributes` is less than the number of node attributes.\r\n   *\r\n   * \\note ONNX Runtime automatically sets optional (unset) attributes to their default values if the default value\r\n   * is a constant expression that does not depend on other tensor/model characteristics. Conv's 'kernel_shape'\r\n   * attribute is an example of an optional attribute that does not have a constant default value. This function\r\n   * does not provide any unset optional attributes without a constant default value.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetAttributes, _In_ const OrtNode* node,\r\n                  _Out_writes_(num_attributes) const OrtOpAttr** attributes, _In_ size_t num_attributes);\r\n\r\n  /** \\brief Gets the OrtNode's attribute as OrtOpAttr by name.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[in] attribute_name The name of the attribute\r\n   * \\param[out] attribute Output parameter set to the OrtOpAttr instance if an attribute by the given name exists.\r\n   *                       For an unset optional attribute, `attribute` is set to NULL and a non-error status is\r\n   *                       returned. For an invalid attribute name, `attribute` is set to NULL and an error status with\r\n   *                       code ORT_NOT_FOUND is returned.\r\n   *\r\n   * \\note ONNX Runtime automatically sets optional (unset) attributes to their default values if the default value\r\n   * is a constant expression that does not depend on other tensor/model characteristics. Conv's 'kernel_shape'\r\n   * attribute is an example of an optional attribute that does not have a constant default value. This function\r\n   * does not provide any unset optional attributes without a constant default value.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetAttributeByName, _In_ const OrtNode* node, _In_ const char* attribute_name,\r\n                  _Outptr_result_maybenull_ const OrtOpAttr** attribute);\r\n\r\n  /** \\brief Get the OrtNode's 'TENSOR' attribute as an OrtValue.\r\n   *\r\n   * \\param[in] attribute The OrtOpAttr instance.\r\n   * \\param[out] attr_tensor If successful, contains the 'TENSOR' attribute as a newly created OrtValue.\r\n                             Must be freed with OrtApi::ReleaseValue.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(OpAttr_GetTensorAttributeAsOrtValue, _In_ const OrtOpAttr* attribute,\r\n                  _Outptr_result_maybenull_ OrtValue** attr_tensor);\r\n\r\n  /** \\brief Get the attribute type as OrtOpAttrType from an OrtOpAttr.\r\n   *\r\n   * \\param[in] attribute The OrtOpAttr instance.\r\n   * \\param[out] type Output the attribute type as OrtOpAttrType.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(OpAttr_GetType, _In_ const OrtOpAttr* attribute, _Out_ OrtOpAttrType* type);\r\n\r\n  /** \\brief Get the attribute name from an OrtOpAttr.\r\n   *\r\n   * \\param[in] attribute The OrtOpAttr instance.\r\n   * \\param[out] name Output parameter set to the attribute's name. The name is a null-terminated string.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(OpAttr_GetName, _In_ const OrtOpAttr* attribute, _Outptr_ const char** name);\r\n\r\n  /** \\brief Returns the number of subgraphs contained by the given node.\r\n   *\r\n   * \\note Only certain operator types (e.g., If and Loop) contain nested subgraphs.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] num_subgraphs Output parameter set to the number of node subgraphs.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetNumSubgraphs, _In_ const OrtNode* node, _Out_ size_t* num_subgraphs);\r\n\r\n  /** \\brief Get the subgraphs, as OrtGraph instances, contained by the given node.\r\n   *\r\n   * \\note Only certain operator types (e.g., If and Loop) contain nested subgraphs. ONNX nodes store subgraphs in\r\n   * their attributes, however, this function must be used to obtain subgraphs from an OrtNode.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] subgraphs Pre-allocated array of `num_subgraphs` elements that is filled with the node's subgraphs.\r\n   * \\param[in] num_subgraphs The size of the `num_subgraphs` array.\r\n   *                          Typical usage sets this to the result of Node_GetNumSubgraphs(). An error status is\r\n   *                          returned if `num_subgraphs` is less than the number of node subgraphs.\r\n   * \\param[out] attribute_names Optional pre-allocated array of `num_subgraphs` elements that is filled with the\r\n   *                             attribute names that correspond to the subgraphs. Ignored if set to NULL.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetSubgraphs, _In_ const OrtNode* node,\r\n                  _Out_writes_(num_subgraphs) const OrtGraph** subgraphs, _In_ size_t num_subgraphs,\r\n                  _Out_writes_opt_(num_subgraphs) const char** attribute_names);\r\n\r\n  /** \\brief Get the node's parent OrtGraph instance.\r\n   *\r\n   * Can return NULL if the OrtNode was created without an owning graph.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] graph Output parameter set to the node's OrtGraph. Can be set to NULL\r\n   *                   if the node is not currently contained by a graph.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetGraph, _In_ const OrtNode* node, _Outptr_result_maybenull_ const OrtGraph** graph);\r\n\r\n  /** \\brief Returns the execution provider name that this node is assigned to run on.\r\n   *         Returns NULL if the node has not been assigned to any execution provider yet.\r\n   *         For plugin execution providers, the name is the one returned by OrtEp::GetName.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] out Output execution provider type and can be NULL if node has not been assigned.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetEpName, _In_ const OrtNode* node, _Outptr_result_maybenull_ const char** out);\r\n\r\n  /// @}\r\n\r\n  /// \\name OrtExternalInitializerInfo\r\n  /// @{\r\n\r\n  /** \\brief Release an OrtExternalInitializerInfo instance.\r\n   *\r\n   * \\param[in] input OrtExternalInitializerInfo instance to be released.\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_CLASS_RELEASE(ExternalInitializerInfo);\r\n\r\n  /** \\brief Get the relative path to the file that stores the initializer's data.\r\n   *\r\n   * \\note The path is relative to the filesystem directory where the ONNX model was stored.\r\n   * Caller can use Graph_GetModelPath to get the model's full path and construct the absolute path to the\r\n   * external initializer file if necessary.\r\n   *\r\n   * \\param[in] info The OrtExternalInitializerInfo instance.\r\n   * \\return The relative path to the file that stores the initializer's data. Do NOT free this pointer.\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API_T(const ORTCHAR_T*, ExternalInitializerInfo_GetFilePath, _In_ const OrtExternalInitializerInfo* info);\r\n\r\n  /** \\brief Get the byte offset within the file where the initializer's data is stored.\r\n   *\r\n   * \\param[in] info The OrtExternalInitializerInfo instance.\r\n   * \\return The byte offset where the initializer's data is stored within the file.\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API_T(int64_t, ExternalInitializerInfo_GetFileOffset, _In_ const OrtExternalInitializerInfo* info);\r\n\r\n  /** \\brief Get the size in bytes of the initializer's data within the file.\r\n   *\r\n   * \\param[in] info The OrtExternalInitializerInfo instance.\r\n   * \\return The size in bytes of the initializer's data within the file.\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API_T(size_t, ExternalInitializerInfo_GetByteSize, _In_ const OrtExternalInitializerInfo* info);\r\n\r\n  /// @}\r\n\r\n  /// \\name OrtRunOptions\r\n  /// @{\r\n\r\n  /** \\brief Get a run configuration entry.\r\n   *\r\n   * If a run configuration entry with key `config_key` doesn't exist, `config_value` will be set to NULL.\r\n   *\r\n   * `config_key`s are defined in onnxruntime_run_options_config_keys.h.\r\n   *\r\n   * \\param[in] options The OrtRunOptions instance.\r\n   * \\param[in] config_key The configuration entry key. A null-terminated string.\r\n   * \\return The configuration entry value. Either a null-terminated string if the entry was found. nullptr otherwise.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API_T(const char*, GetRunConfigEntry, _In_ const OrtRunOptions* options,\r\n            _In_z_ const char* config_key);\r\n\r\n  /// @}\r\n\r\n  /** \\brief Get the OrtMemoryInfo for the device.\r\n   *\r\n   * \\param[in] ep_device The OrtEpDevice instance to query.\r\n   * \\param[in] memory_type The memory type to return.\r\n   * \\return A pointer to the OrtMemoryInfo for the device. This may be nullptr if not set.\r\n   *         If memory_type is OrtDeviceMemoryType_DEFAULT and nullptr is returned the EP uses CPU memory.\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API_T(const OrtMemoryInfo*, EpDevice_MemoryInfo, _In_ const OrtEpDevice* ep_device,\r\n            _In_ OrtDeviceMemoryType memory_type);\r\n\r\n  /** \\brief Create/replace a shared allocator for the OrtEpDevice in the OrtEnv.\r\n   *\r\n   * OrtEpDevice maps to the EP factory, and the factory provides the allocator implementation.\r\n   *\r\n   * Both OrtDeviceMemoryType_DEFAULT and OrtDeviceMemoryType_HOST_ACCESSIBLE are optional for an EP to provide.\r\n   * It is EP implementation dependent as to what is available.\r\n   *\r\n   * If a shared allocator already exists for the OrtEpDevice and OrtDeviceMemoryType, it is replaced. This allows\r\n   * changing the shared allocator configuration from the default. e.g. adding an arena.\r\n   *\r\n   * \\param[in] env The OrtEnv instance to create the shared allocator in.\r\n   * \\param[in] ep_device The OrtEpDevice instance to create the shared allocator for.\r\n   * \\param[in] mem_type The memory type to use for the shared allocator.\r\n   * \\param[in] allocator_type The type of allocator to create. Only OrtDeviceAllocator is valid currently.\r\n   * \\param[in] allocator_options Optional key-value pairs to configure the allocator. If arena based, see\r\n   *                              include/onnxruntime/core/framework/allocator.h for the keys and values that can be\r\n   *                              used.\r\n   * \\param[out] allocator A pointer to the created shared allocator. Owned by the OrtEnv instance.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API2_STATUS(CreateSharedAllocator, _In_ OrtEnv* env, _In_ const OrtEpDevice* ep_device,\r\n                  _In_ OrtDeviceMemoryType mem_type, _In_ OrtAllocatorType allocator_type,\r\n                  _In_opt_ const OrtKeyValuePairs* allocator_options,\r\n                  _Outptr_opt_ OrtAllocator** allocator);\r\n\r\n  /** \\brief Get a shared allocator from the OrtEnv.\r\n   *\r\n   * By default there is a shared allocator created for all OrtEpDevice instances, so if you get the OrtMemoryInfo\r\n   * from the OrtEpDevice using EpDevice_MemoryInfo a shared allocator is guaranteed to exist.\r\n   *\r\n   * This will also match and return custom allocators added with RegisterAllocator.\r\n   *\r\n   * It is not an error to not find a matching allocator.\r\n   *\r\n   * \\param[in] env The OrtEnv instance to get the shared allocator from.\r\n   * \\param[in] mem_info The OrtMemoryInfo instance to get the shared allocator for.\r\n   * \\param[out] allocator A pointer to the shared allocator, or nullptr if no shared allocator exists for\r\n   *                       the given memory info.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API2_STATUS(GetSharedAllocator, _In_ OrtEnv* env, _In_ const OrtMemoryInfo* mem_info,\r\n                  _Outptr_result_maybenull_ OrtAllocator** allocator);\r\n\r\n  /** \\brief Release a shared allocator from the OrtEnv for the OrtEpDevice and memory type.\r\n   *\r\n   * This will release the shared allocator for the given OrtEpDevice and memory type.\r\n   * If no shared allocator exists, this is a no-op.\r\n   *\r\n   * \\param[in] env The OrtEnv instance to release the shared allocator from.\r\n   * \\param[in] ep_device The OrtEpDevice instance to release the shared allocator for.\r\n   * \\param[in] mem_type The memory type of the shared allocator to release.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API2_STATUS(ReleaseSharedAllocator, _In_ OrtEnv* env, _In_ const OrtEpDevice* ep_device,\r\n                  _In_ OrtDeviceMemoryType mem_type);\r\n\r\n  /** \\brief Get a const pointer to the raw data inside a tensor\r\n   *\r\n   * Used to read the internal tensor data directly.\r\n   * \\note The returned pointer is valid until the OrtValue is destroyed.\r\n   *\r\n   * \\param[in] value A tensor type (string tensors are not supported)\r\n   * \\param[out] out Filled in with a pointer to the internal storage\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API2_STATUS(GetTensorData, _In_ const OrtValue* value, _Outptr_ const void** out);\r\n\r\n  /** \\brief Get Session configuration entries.\r\n   *\r\n   * \\param[in] options The session options.\r\n   * \\param[out] out A pointer to a newly created OrtKeyValuePairs instance.\r\n   *\r\n   *  An OrtKeyValuePairs instance containing all session configuration entries.\r\n   *  Note: the user should call OrtApi::ReleaseKeyValuePairs.\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(GetSessionOptionsConfigEntries, _In_ const OrtSessionOptions* options, _Outptr_ OrtKeyValuePairs** out);\r\n\r\n  /** \\brief Get the OrtMemoryInfo for each input of the session.\r\n   *\r\n   * The memory info can be used to determine where the input tensors are required.\r\n   *\r\n   * The session must be fully initialized before calling this function as the input locations are not known until\r\n   * this has occurred.\r\n   *\r\n   * \\param[in] session The OrtSession instance.\r\n   * \\param[out] inputs_memory_info Pre-allocated array of size `num_inputs` that will be filled with the\r\n   *                                OrtMemoryInfo* value for each input.\r\n   *                                The order is the same as returned by SessionGetInputName.\r\n   * \\param[in] num_inputs The number of inputs in the session. Must match SessionGetInputCount.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API2_STATUS(SessionGetMemoryInfoForInputs, _In_ const OrtSession* session,\r\n                  _Out_writes_(num_inputs) const OrtMemoryInfo** inputs_memory_info,\r\n                  _In_ size_t num_inputs);\r\n\r\n  /** \\brief Get the OrtMemoryInfo for each output of the session.\r\n   *\r\n   * The memory info can be used to determine the device the output tensors are produced on.\r\n   * The user can pre-allocate an OrtValue using this information or use IOBinding to keep the data on the device.\r\n   * ORT will copy the output to CPU otherwise.\r\n   *\r\n   * The session must be fully initialized before calling this function as the output locations are not known until\r\n   * this has occurred.\r\n   *\r\n   * \\param[in] session The OrtSession instance.\r\n   * \\param[out] outputs_memory_info Pre-allocated array of size `num_outputs` that will be filled with\r\n   *                                 OrtMemoryInfo* values for each output.\r\n   *                                 The order is the same as returned by SessionGetOutputName.\r\n   * \\param[in] num_outputs The number of outputs in the session. Must match SessionGetOutputCount.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API2_STATUS(SessionGetMemoryInfoForOutputs, _In_ const OrtSession* session,\r\n                  _Out_writes_(num_outputs) const OrtMemoryInfo** outputs_memory_info,\r\n                  _In_ size_t num_outputs);\r\n\r\n  /** \\brief Get the OrtEpDevice (if available) for each input of the session.\r\n   *\r\n   * An OrtEpDevice will be available if auto EP selection is enabled by calling\r\n   * SessionOptionsSetEpSelectionPolicy or SessionOptionsSetEpSelectionPolicyDelegate,\r\n   * or if the OrtEpDevice was manually added to the session using SessionOptionsAppendExecutionProvider_V2.\r\n   *\r\n   * If an OrtEpDevice is not available for the input a nullptr is returned.\r\n   *\r\n   * The returned OrtEpDevice can be used to create an OrtSyncStream via CreateSyncStreamForEpDevice to asynchronously\r\n   * provide input to the inference session Run.\r\n   *\r\n   * The session must be fully initialized before calling this function as the assigned EPs are not known until\r\n   * this has occurred.\r\n   *\r\n   * \\param[in] session The OrtSession instance.\r\n   * \\param[out] inputs_ep_devices Pre-allocated array of size `num_inputs` that will be filled with\r\n   *                               OrtEpDevice* values for each input.\r\n   *                               The order is the same as returned by SessionGetInputName.\r\n   * \\param[in] num_inputs The number of inputs in the session. Must match SessionGetInputCount.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API2_STATUS(SessionGetEpDeviceForInputs, _In_ const OrtSession* session,\r\n                  _Out_writes_(num_inputs) const OrtEpDevice** inputs_ep_devices,\r\n                  _In_ size_t num_inputs);\r\n\r\n  /** \\brief Create an OrtSyncStream for the given OrtEpDevice.\r\n   *\r\n   * The OrtSyncStream can be used to enable asynchronous operations.\r\n   * e.g. async usage of CopyTensors to provide input to an OrtSession Run call.\r\n   *\r\n   * An error code of ORT_NOT_IMPLEMENTED will be returned if the EP does not support OrtSyncStream.\r\n   *\r\n   * \\param[in] ep_device The OrtEpDevice instance to create the sync stream for.\r\n   * \\param[in] stream_options Options for OrtSyncStream creation. May be nullptr.\r\n   * \\param[out] stream Output parameter set to the created OrtSyncStream instance.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API2_STATUS(CreateSyncStreamForEpDevice, _In_ const OrtEpDevice* ep_device,\r\n                  _In_opt_ const OrtKeyValuePairs* stream_options,\r\n                  _Outptr_ OrtSyncStream** stream);\r\n\r\n  /** \\brief Get the native handle of the sync stream.\r\n   *\r\n   * This returns the native handle for the stream. e.g. cudaStream_t for CUDA streams.\r\n   *\r\n   * \\param[in] stream The OrtSyncStream instance to get the handle from.\r\n   *\r\n   * \\returns The native handle of the stream.\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API_T(void*, SyncStream_GetHandle, _In_ OrtSyncStream* stream);\r\n\r\n  ORT_CLASS_RELEASE(SyncStream);\r\n\r\n  /** \\brief Copy OrtValue instances containing Tensors between devices.\r\n   *\r\n   * The overall copy must be between a single source device and a single destination device. i.e.\r\n   *   - all src_tensors must have matching OrtMemoryInfo,\r\n   *   - all dst_tensors must have matching OrtMemoryInfo.\r\n   *\r\n   * OrtValue instances can be created by:\r\n   *   - Use GetSharedAllocator to get the shared allocator for the OrtMemoryInfo if you need to allocate memory\r\n   *     on the device.\r\n   *   - Use CreateTensorAsOrtValue, CreateTensorWithDataAsOrtValue or CreateTensorWithDataAndDeleterAsOrtValue\r\n   *     to create an OrtValue containing a tensor depending on whether you have existing data or not, and whether\r\n   *     you want ORT to free the existing data once it is done with the OrtValue.\r\n   *\r\n   * \\param[in] env The OrtEnv instance to use. The data transfer implementation is provided by an execution provider\r\n   *                that is registered in this OrtEnv.\r\n   * \\param[in] src_tensors Array of OrtValue instances containing the source tensors to copy.\r\n   * \\param[in] dst_tensors Array of OrtValue instances to copy the source tensors to.\r\n   * \\param[in] stream Optional OrtSyncStream that can be used to perform the copy asynchronously. May be nullptr.\r\n   * \\param[in] num_tensors The number of tensors to copy. The size of `src_tensors` and `dst_tensors` must match.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API2_STATUS(CopyTensors, _In_ const OrtEnv* env,\r\n                  _In_reads_(num_tensors) const OrtValue* const* src_tensors,\r\n                  _In_reads_(num_tensors) OrtValue* const* dst_tensors,\r\n                  _In_opt_ OrtSyncStream* stream,\r\n                  _In_ size_t num_tensors);\r\n\r\n  /** \\brief Get ::OrtModelMetadata from an ::OrtGraph\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] out Newly created ::OrtModelMetadata. Must be freed using OrtApi::ReleaseModelMetadata.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetModelMetadata, _In_ const OrtGraph* graph, _Outptr_ OrtModelMetadata** out);\r\n\r\n  /** \\brief Validate a compiled model's compatibility information for one or more EP devices.\r\n   *\r\n   * \\param[in] ep_devices The EP devices to validate against (e.g., from GetEpDevices).\r\n   *                        All devices must belong to the same execution provider.\r\n   * \\param[in] num_ep_devices The number of EP devices provided.\r\n   * \\param[in] compatibility_info The compatibility info string produced when the model was compiled.\r\n   * \\param[out] out_status The resulting compatibility status for the EP devices.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(GetModelCompatibilityForEpDevices,\r\n                  _In_reads_(num_ep_devices) const OrtEpDevice* const* ep_devices,\r\n                  _In_ size_t num_ep_devices,\r\n                  _In_ const char* compatibility_info,\r\n                  _Out_ OrtCompiledModelCompatibility* out_status);\r\n\r\n  /// \\name OrtExternalInitializerInfo\r\n  /// @{\r\n\r\n  /** \\brief Creates an OrtExternalInitializerInfo instance.\r\n   *\r\n   * \\param[in] filepath The relative path to the file that stores the initializer's data. ORT copies this path string.\r\n   * \\param[in] file_offset The byte offset where the initializer's data is stored within the file.\r\n   * \\param[in] byte_size The size in bytes of the initializer's data within the file.\r\n   * \\param[out] out Output parameter set to the new OrtExternalInitializerInfo instance.\r\n   *                 Must be released by calling ReleaseExternalInitializerInfo().\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(CreateExternalInitializerInfo, _In_ const ORTCHAR_T* filepath, _In_ int64_t file_offset,\r\n                  _In_ size_t byte_size, _Outptr_ OrtExternalInitializerInfo** out);\r\n\r\n  /// @}\r\n  /** \\brief Fetch whether the tensor has shape information.\r\n   * \\param[in] info The OrtTensorTypeAndShapeInfo instance.\r\n   * \\return true if the tensor has shape information, false otherwise.\r\n   *\r\n   * \\since Version 1.24\r\n   */\r\n  ORT_API_T(bool, TensorTypeAndShape_HasShape, _In_ const OrtTensorTypeAndShapeInfo* info);\r\n};\r\n\r\n/*\r\n * Steps to use a custom op:\r\n *   1 Create an OrtCustomOpDomain with the domain name used by the custom ops\r\n *   2 Create an OrtCustomOp structure for each op and add them to the domain\r\n *   3 Call OrtAddCustomOpDomain to add the custom domain of ops to the session options\r\n */\r\n\r\n// Specifies some characteristics of inputs/outputs of custom ops:\r\n// Specify if the inputs/outputs are one of:\r\n// 1) Non-optional (input/output must be present in the node)\r\n// 2) Optional (input/output may be absent in the node)\r\n// 3) Variadic: A variadic input or output specifies N (i.e., the minimum arity) or more operands.\r\n//              Only the last input or output of a custom op may be marked as variadic.\r\n//              The homogeneity of the variadic input or output determines whether all operands must be of the same\r\n//              tensor element type.\r\ntypedef enum OrtCustomOpInputOutputCharacteristic {\r\n  INPUT_OUTPUT_REQUIRED = 0,\r\n  INPUT_OUTPUT_OPTIONAL,\r\n  INPUT_OUTPUT_VARIADIC,\r\n} OrtCustomOpInputOutputCharacteristic;\r\n\r\n/*\r\n * The OrtCustomOp structure defines a custom op's schema and its kernel callbacks. The callbacks are filled in by\r\n * the implementor of the custom op.\r\n */\r\nstruct OrtCustomOp {\r\n  uint32_t version;  // Must be initialized to ORT_API_VERSION\r\n\r\n  // This callback creates the kernel, which is a user defined\r\n  // parameter that is passed to the Kernel* callbacks below. It is\r\n  // recommended to use CreateKernelV2 which allows for a safe error\r\n  // propagation by returning an OrtStatusPtr.\r\n  void*(ORT_API_CALL* CreateKernel)(_In_ const struct OrtCustomOp* op, _In_ const OrtApi* api,\r\n                                    _In_ const OrtKernelInfo* info);\r\n\r\n  // Returns the name of the op\r\n  const char*(ORT_API_CALL* GetName)(_In_ const struct OrtCustomOp* op);\r\n\r\n  // Returns the type of the execution provider, return nullptr to use CPU execution provider\r\n  const char*(ORT_API_CALL* GetExecutionProviderType)(_In_ const struct OrtCustomOp* op);\r\n\r\n  // Returns the count and types of the input & output tensors\r\n  ONNXTensorElementDataType(ORT_API_CALL* GetInputType)(_In_ const struct OrtCustomOp* op, _In_ size_t index);\r\n  size_t(ORT_API_CALL* GetInputTypeCount)(_In_ const struct OrtCustomOp* op);\r\n  ONNXTensorElementDataType(ORT_API_CALL* GetOutputType)(_In_ const struct OrtCustomOp* op, _In_ size_t index);\r\n  size_t(ORT_API_CALL* GetOutputTypeCount)(_In_ const struct OrtCustomOp* op);\r\n\r\n  // Perform a computation step.  It is recommended to use\r\n  // KernelComputeV2 which allows for a safe error propagation by\r\n  // returning an OrtStatusPtr.\r\n  void(ORT_API_CALL* KernelCompute)(_In_ void* op_kernel, _In_ OrtKernelContext* context);\r\n  void(ORT_API_CALL* KernelDestroy)(_In_ void* op_kernel);\r\n\r\n  // Returns the characteristics of the input & output tensors\r\n  OrtCustomOpInputOutputCharacteristic(ORT_API_CALL* GetInputCharacteristic)(_In_ const struct OrtCustomOp* op, _In_ size_t index);\r\n  OrtCustomOpInputOutputCharacteristic(ORT_API_CALL* GetOutputCharacteristic)(_In_ const struct OrtCustomOp* op, _In_ size_t index);\r\n\r\n  // Returns the memory type of the input tensors. This API allows the custom op\r\n  // to place the inputs on specific devices. By default, it returns\r\n  // OrtMemTypeDefault, which means the input is placed on the default device for\r\n  // the execution provider. If the inputs need to be with different memory types,\r\n  // this function can be overridden to return the specific memory types.\r\n  OrtMemType(ORT_API_CALL* GetInputMemoryType)(_In_ const struct OrtCustomOp* op, _In_ size_t index);\r\n\r\n  // Returns the minimum number of input arguments expected for the variadic input.\r\n  // Applicable only for custom ops that have a variadic input.\r\n  int(ORT_API_CALL* GetVariadicInputMinArity)(_In_ const struct OrtCustomOp* op);\r\n\r\n  // Returns true (non-zero) if all arguments of a variadic input have to be of the same type (homogeneous),\r\n  // and false (zero) otherwise.\r\n  // Applicable only for custom ops that have a variadic input.\r\n  int(ORT_API_CALL* GetVariadicInputHomogeneity)(_In_ const struct OrtCustomOp* op);\r\n\r\n  // Returns the minimum number of output values expected for the variadic output.\r\n  // Applicable only for custom ops that have a variadic output.\r\n  int(ORT_API_CALL* GetVariadicOutputMinArity)(_In_ const struct OrtCustomOp* op);\r\n\r\n  // Returns true (non-zero) if all outputs values of a variadic output have to be of the same type (homogeneous),\r\n  // and false (zero) otherwise.\r\n  // Applicable only for custom ops that have a variadic output.\r\n  int(ORT_API_CALL* GetVariadicOutputHomogeneity)(_In_ const struct OrtCustomOp* op);\r\n\r\n  // Create the kernel state which is passed to each compute call.\r\n  OrtStatusPtr(ORT_API_CALL* CreateKernelV2)(_In_ const struct OrtCustomOp* op, _In_ const OrtApi* api,\r\n                                             _In_ const OrtKernelInfo* info,\r\n                                             _Out_ void** kernel);\r\n\r\n  // Perform the computation step.\r\n  OrtStatusPtr(ORT_API_CALL* KernelComputeV2)(_In_ void* op_kernel, _In_ OrtKernelContext* context);\r\n\r\n  OrtStatusPtr(ORT_API_CALL* InferOutputShapeFn)(_In_ const struct OrtCustomOp* op, _In_ OrtShapeInferContext*);\r\n\r\n  // Get start range\r\n  int(ORT_API_CALL* GetStartVersion)(_In_ const struct OrtCustomOp* op);\r\n  int(ORT_API_CALL* GetEndVersion)(_In_ const struct OrtCustomOp* op);\r\n\r\n  // Get the inplace_map that defines which output can reuse which input\r\n  // Callers will provide 2 raw int* and pass in their address, this function will fill these 2 arrays\r\n  // when return, output (*output_index)[i] may reuse the input (*input_index[i]).\r\n  // The return value is the size of these 2 arrays.\r\n  // Callers are responsible to delete these 2 arrays after use by calling OrtCustomOp::ReleaseMayInplace().\r\n  size_t(ORT_API_CALL* GetMayInplace)(_Out_ int** input_index, _Out_ int** output_index);\r\n\r\n  // Release the pointer input_index and output_index allocated from GetMayInplace() function.\r\n  // If GetMayInplace() is defined, this function MUST be defined as well.\r\n  void(ORT_API_CALL* ReleaseMayInplace)(_Frees_ptr_opt_ int* input_index, _Frees_ptr_opt_ int* output_index);\r\n\r\n  // Same as GetMayInplace() and ReleaseMayInplace()\r\n  size_t(ORT_API_CALL* GetAliasMap)(_Out_ int** input_index, _Out_ int** output_index);\r\n  void(ORT_API_CALL* ReleaseAliasMap)(_Frees_ptr_opt_ int* input_index, _Frees_ptr_opt_ int* output_index);\r\n};\r\n\r\n/**\r\n * ORT Model Editor API\r\n */\r\n\r\n/**\r\n * \\brief The OrtModelEditorApi struct provides functions to create or edit an ONNX model.\r\n *\r\n * See onnxruntime/test/shared_lib/test_model_editor_api.cc for example usage.\r\n *\r\n * \\since Version 1.22.\r\n */\r\nstruct OrtModelEditorApi {\r\n  // Model building/editing requires a full build. We return nullptr from GetModelEditorApi if this is a minimal\r\n  // build, so it doesn't matter if there are no function pointers in this struct as a user will never get an\r\n  // OrtModelEditorApi instance. We do however need a dummy field to avoid empty struct warning.\r\n#if defined(ORT_MINIMAL_BUILD)\r\n  const bool not_defined_in_this_build;\r\n#else\r\n  /** \\brief Create an OrtTypeInfo instance for a Tensor.\r\n   *\r\n   * Create an OrtTypeInfo instance for a Tensor to use as graph inputs/outputs with the Model Editor API.\r\n   *\r\n   * User can release `tensor_info` after creating the OrtTypeInfo.\r\n   *\r\n   * \\param[in] tensor_info Tensor type and shape information.\r\n   * \\param[out] type_info TypeInfo instance for the tensor.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CreateTensorTypeInfo, _In_ const OrtTensorTypeAndShapeInfo* tensor_info,\r\n                  _Outptr_ OrtTypeInfo** type_info);\r\n\r\n  /** \\brief Create an OrtTypeInfo instance for a SparseTensor.\r\n   *\r\n   * Create an OrtTypeInfo instance for a SparseTensor to use as graph inputs/outputs with the Model Editor API.\r\n   *\r\n   * User can release `tensor_info` after creating the OrtTypeInfo.\r\n   *\r\n   * \\param[in] tensor_info SparseTensor type and shape information.\r\n   * \\param[out] type_info TypeInfo instance for the tensor.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CreateSparseTensorTypeInfo, _In_ const OrtTensorTypeAndShapeInfo* tensor_info,\r\n                  _Outptr_ OrtTypeInfo** type_info);\r\n\r\n  /** \\brief Create an OrtTypeInfo instance for a Map.\r\n   *\r\n   * Create an OrtTypeInfo instance for a Map to use as graph inputs/outputs with the Model Editor API.\r\n   *\r\n   * User can release `map_value_type` after creating the OrtTypeInfo.\r\n   *\r\n   * \\param[in] map_key_type Key type for the map.\r\n   * \\param[in] map_value_type Value type for the map.\r\n   * \\param[out] type_info TypeInfo instance for the map.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CreateMapTypeInfo, ONNXTensorElementDataType map_key_type, _In_ const OrtTypeInfo* map_value_type,\r\n                  _Outptr_ OrtTypeInfo** type_info);\r\n\r\n  /** \\brief Create an OrtTypeInfo instance for a Sequence.\r\n   *\r\n   * Create an OrtTypeInfo instance for a Sequence to use as graph inputs/outputs with the Model Editor API.\r\n   *\r\n   * User can release `sequence_type` after creating the OrtTypeInfo.\r\n   *\r\n   * \\param[in] sequence_type Sequence type and shape information.\r\n   * \\param[out] type_info TypeInfo instance for the sequence.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CreateSequenceTypeInfo, _In_ const OrtTypeInfo* sequence_type, _Outptr_ OrtTypeInfo** type_info);\r\n\r\n  /** \\brief Create an OrtTypeInfo instance for an Optional.\r\n   *\r\n   * Create an OrtTypeInfo instance for an Optional to use as graph inputs/outputs with the Model Editor API.\r\n   *\r\n   * User can release `contained_type` after creating the OrtTypeInfo.\r\n   *\r\n   * \\param[in] contained_type Tensor type and shape information.\r\n   * \\param[out] type_info TypeInfo instance for the tensor.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CreateOptionalTypeInfo, _In_ const OrtTypeInfo* contained_type, _Outptr_ OrtTypeInfo** type_info);\r\n\r\n  /** \\brief Create an OrtValueInfo for use as an OrtGraph input or output.\r\n   *\r\n   * \\param[in] name The name of the input or output.\r\n   * \\param[in] type_info The type information for the input or output. The provided value is copied.\r\n   * \\param[out] value_info The OrtValueInfo instance.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CreateValueInfo, _In_ const char* name, _In_ const OrtTypeInfo* type_info,\r\n                  _Outptr_ OrtValueInfo** value_info);\r\n\r\n  /** \\brief Create an OrtNode to add to an OrtGraph.\r\n   *\r\n   * Create an OrtNode.\r\n   *\r\n   * Create attributes with CreateOpAttr. OrtOpAttr instances are copied.\r\n   *\r\n   * \\param[in] operator_name The name of the operator.\r\n   * \\param[in] domain_name The domain of the operator. Use an empty string for ONNX operators.\r\n   * \\param[in] node_name The name of the node.\r\n   * \\param[in] input_names The names of the inputs.\r\n   * \\param[in] input_names_len The number of input names.\r\n   * \\param[in] output_names The names of the outputs.\r\n   * \\param[in] output_names_len The number of output names.\r\n   * \\param[in] attributes The optional attributes of the node.\r\n   * \\param[in] attribs_len The number of attributes. May be zero.\r\n   * \\param[out] node The OrtNode instance.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CreateNode, _In_ const char* operator_name, _In_ const char* domain_name, _In_ const char* node_name,\r\n                  _In_reads_(input_names_len) const char* const* input_names, size_t input_names_len,\r\n                  _In_reads_(output_names_len) const char* const* output_names, size_t output_names_len,\r\n                  _In_reads_(attribs_len) _In_opt_ OrtOpAttr** attributes, _In_ size_t attribs_len,\r\n                  _Outptr_ OrtNode** node);\r\n\r\n  /** \\brief Create an OrtGraph\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CreateGraph, _Outptr_ OrtGraph** graph);\r\n\r\n  /** \\brief Set the inputs for the OrtGraph.\r\n   *\r\n   * Set the graph inputs. This will replace any existing inputs with the new values.\r\n   * The OrtGraph takes ownership of the OrtValueInfo instances and you should NOT call ReleaseOrtValueInfo.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance to update.\r\n   * \\param[in] inputs The input OrtValueInfo instances.\r\n   * \\param[in] inputs_len The number of input OrtValueInfo instances.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(SetGraphInputs, _Inout_ OrtGraph* graph,\r\n                  _In_reads_(inputs_len) _In_ OrtValueInfo** inputs, _In_ size_t inputs_len);\r\n\r\n  /** \\brief Set the outputs for the OrtGraph.\r\n   *\r\n   * Set the graph outputs. This will replace any existing outputs with the new values.\r\n   * The OrtGraph takes ownership of the OrtValueInfo instances provided and you should NOT call ReleaseOrtValueInfo.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance to update.\r\n   * \\param[in] outputs The output OrtValueInfo instances.\r\n   * \\param[in] outputs_len The number of output OrtValueInfo instances.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(SetGraphOutputs, _Inout_ OrtGraph* graph,\r\n                  _In_reads_(outputs_len) _In_ OrtValueInfo** outputs, _In_ size_t outputs_len);\r\n\r\n  /** \\brief Add an initializer to the OrtGraph\r\n   *\r\n   * ORT will take ownership of the OrtValue and you should NOT call ReleaseOrtValue.\r\n   *\r\n   * Two options:\r\n   *\r\n   * Allocated memory:\r\n   *    Use CreateTensorAsOrtValue (allocates memory) and populate the tensor with the data.\r\n   *    Set `data_is_external` to false.\r\n   *\r\n   * Pre-existing memory:\r\n   *    Use CreateTensorWithDataAsOrtValue or CreateTensorWithDataAndDeleterAsOrtValue to create an OrtValue\r\n   *    with a tensor that contains a pointer to the existing data.\r\n   *    Set `data_is_external` to true.\r\n   *\r\n   *    The pointer must remain valid for the duration of the inference session.\r\n   *    If using CreateTensorWithDataAsOrtValue you are responsible for freeing the memory after the inference session\r\n   *    is released.\r\n   *    If using CreateTensorWithDataAndDeleterAsOrtValue, ORT will free the memory using the provided deleter as\r\n   *    soon as the OrtValue is no longer in use.\r\n   *\r\n   *    NOTE: A tensor containing pre-existing memory MUST have 128 bytes of data or more.\r\n   *          For smaller tensors use CreateTensorAsOrtValue.\r\n   *\r\n   *          ONNX shape inferencing does not support external data. An initializer involved in shape inferencing is\r\n   *          typically small (a single value or limited by the rank of a tensor) and uses less than 128 bytes of\r\n   *          memory, so this limit acts as a simple catch-all rule to avoid issues.\r\n   *          e.g. Reshape's `shape`, Clip's `min` and `max`, various ops `axes`.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance to update.\r\n   * \\param[in] name The value name for the initializer.\r\n   * \\param[in] tensor The OrtValue instance containing the tensor data.\r\n   * \\param[in] data_is_external Set to true if the data is external and should not be copied.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(AddInitializerToGraph, _Inout_ OrtGraph* graph, _In_ const char* name, _In_ OrtValue* tensor,\r\n                  bool data_is_external);\r\n\r\n  /** \\brief Add an OrtNode to an OrtGraph\r\n   *\r\n   * Add the node to the graph. The OrtGraph will take ownership of OrtNode and you should NOT call ReleaseOrtNode.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance to update.\r\n   * \\param[in] node The OrtNode instance to add to the graph.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(AddNodeToGraph, _Inout_ OrtGraph* graph, _In_ OrtNode* node);\r\n\r\n  /** \\brief Create an OrtModel.\r\n   *\r\n   * Create an OrtModel.\r\n   *\r\n   * This can be used to build a new model, or to augment an existing model.\r\n   *\r\n   * \\param[in] domain_names The domain names for the model.\r\n   *                         If augmenting an existing model add additional domains if needed.\r\n   * \\param[in] opset_versions The opset versions for the model.\r\n   *                           If augmenting an existing model add additional opset versions if needed.\r\n   * \\param[in] opset_entries_len The number of domain_names and opset_versions entries.\r\n   *                              Domain and opset entries should be 1:1\r\n   * \\param[out] model The OrtModel instance.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CreateModel,\r\n                  _In_reads_(opset_entries_len) const char* const* domain_names,\r\n                  _In_reads_(opset_entries_len) const int* opset_versions,\r\n                  size_t opset_entries_len,\r\n                  _Outptr_ OrtModel** model);\r\n\r\n  /** \\brief Add an OrtGraph to an OrtModel.\r\n   *\r\n   * Add the graph to a model. This should be called once when creating a new model.\r\n   *\r\n   * The OrtModel takes ownership of the OrtGraph and you should NOT call ReleaseOrtGraph.\r\n   *\r\n   * \\param[in] model The OrtModel instance to update.\r\n   * \\param[in] graph The OrtGraph instance to add to the model.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(AddGraphToModel, _Inout_ OrtModel* model, _In_ OrtGraph* graph);\r\n\r\n  /** \\brief Create an OrtSession using the OrtModel.\r\n   *\r\n   * Create an inference session using the OrtModel instance.\r\n   * The OrtModel should have been populated with an OrtGraph containing nodes and initializers, and SetGraphInputs\r\n   * and SetGraphOutputs must have been called.\r\n   * This will validate the model, run optimizers, and prepare the session for inferencing.\r\n   *\r\n   * ReleaseOrtModel must be called to free the OrtModel after session creation.\r\n   *\r\n   * \\param[in] env The OrtEnv instance.\r\n   * \\param[in] model The OrtModel instance.\r\n   * \\param[in] options The OrtSessionOptions instance.\r\n   * \\param[out] out The OrtSession instance.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CreateSessionFromModel, _In_ const OrtEnv* env, _In_ const OrtModel* model,\r\n                  _In_ const OrtSessionOptions* options, _Outptr_ OrtSession** out);\r\n\r\n  /** \\brief Create an OrtSession to augment an existing model.\r\n   *\r\n   * Create an OrtSession with an existing model that will be augmented with additional nodes and initializers.\r\n   * Nodes can be added before or after the existing nodes in the model. ONNX Runtime will connect the nodes when the\r\n   * model is finalized.\r\n   *\r\n   * To add nodes and initializers to the existing model, first create an OrtModel using CreateModel.\r\n   * Add nodes and initializers to the OrtModel using AddNodeToGraph and AddInitializerToGraph.\r\n   * Graph inputs/outputs should be updated with SetGraphInputs and SetGraphOutputs as needed to reflect changes made\r\n   * by the new nodes. The list of graph inputs/outputs should be for the overall model and not just the new nodes.\r\n   *\r\n   * Add the new information from the OrtModel to the original model using ApplyModelToSession, and prepare the\r\n   * session for inferencing by calling FinalizeModelEditorSession.\r\n   *\r\n   * \\param{in} env The OrtEnv instance.\r\n   * \\param{in} model_path The path to the existing ONNX model to augment.\r\n   * \\param{in} options The OrtSessionOptions instance.\r\n   * \\param{out} out The created OrtSession instance.\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CreateModelEditorSession, _In_ const OrtEnv* env, _In_ const ORTCHAR_T* model_path,\r\n                  _In_ const OrtSessionOptions* options,\r\n                  _Outptr_ OrtSession** out);\r\n\r\n  /** \\brief Create an OrtSession to augment an existing model.\r\n   *\r\n   * Create an OrtSession with an existing model that will be augmented with additional nodes and initializers.\r\n   * Nodes can be added before or after the existing nodes in the model. ONNX Runtime will connect the nodes when the\r\n   * model is finalized.\r\n   *\r\n   * To add nodes and initializers to the existing model, first create an OrtModel using CreateModel.\r\n   * Add nodes and initializers to the OrtModel using AddNodeToGraph and AddInitializerToGraph.\r\n   * Graph inputs/outputs should be updated with SetGraphInputs and SetGraphOutputs as needed to reflect changes made\r\n   * by the new nodes. The list of graph inputs/outputs should be for the overall model and not just the new nodes.\r\n   *\r\n   * Add the new information from the OrtModel to the original model using ApplyModelToSession, and prepare the\r\n   * session for inferencing by calling FinalizeModelEditorSession.\r\n   *\r\n   * \\param{in} env The OrtEnv instance.\r\n   * \\param{in} model_data The model data for the existing model to augment.\r\n   * \\param{in} model_data_length The length of the model data.\r\n   * \\param{in} options The OrtSessionOptions instance.\r\n   * \\param{out} out The created OrtSession instance.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CreateModelEditorSessionFromArray, _In_ const OrtEnv* env,\r\n                  _In_ const void* model_data, size_t model_data_length,\r\n                  _In_ const OrtSessionOptions* options,\r\n                  _Outptr_ OrtSession** out);\r\n\r\n  /** \\brief Query the session for the opset version of a domain.\r\n   *\r\n   * When using the Model Editor API to augment a model, any new nodes must conform to the opset version of the\r\n   * original model. To do that the user must be able to discover that opset version.\r\n   * Returns an error if the domain is not used in the model.\r\n   *\r\n   * \\param[in] session OrtSession to query\r\n   * \\param[in] domain Domain to query. The ONNX domain is an empty string.\r\n   * \\param[out] opset The opset version of the domain.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(SessionGetOpsetForDomain, _In_ const OrtSession* session, _In_ const char* domain, _Out_ int* opset);\r\n\r\n  /** \\brief Apply changes to augment the ONNX model in a session created using CreateModelEditorSession[FromArray]\r\n   *\r\n   * Adds new nodes and updates graph inputs/outputs using `model` to augment the original ONNX model in the session.\r\n   * All changes will be validated.\r\n   * Call FinalizeModelEditorSession to prepare the session for inferencing.\r\n   *\r\n   * Existing input/outputs will only be updated if the OrtGraph inputs/outputs are set in the OrtModel.\r\n   *   i.e. you don't need to call SetGraphInputs/SetGraphOutputs if they are unchanged.\r\n   *\r\n   * ReleaseOrtModel must be called to free the OrtModel after it is applied to the session.\r\n   *\r\n   * \\param[in] session OrtSession to update. Session must have been created using CreateModelEditorSession[FromArray].\r\n   * \\param[in] model OrtModel containing new nodes, new initializers, and updated graph input and/or output info.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(ApplyModelToModelEditorSession, _Inout_ OrtSession* session, _In_ OrtModel* model);\r\n\r\n  /** \\brief Finalize the Model Editor session that was created using CreateModelEditorSession[FromArray].\r\n   *\r\n   * Finalize the Model Editor session that augmented an ONNX model by adding new nodes.\r\n   * This will run optimizers and prepare the session for inferencing.\r\n   *\r\n   * \\param[in] session OrtSession to finalize. Session must have been created using CreateModelEditorSession[FromArray].\r\n   * \\param[in] options OrtSessionOptions to use for the session.\r\n   * \\param[in] prepacked_weights_container Optional OrtPrepackedWeightsContainer to use for the session.\r\n                Set to nullptr if not used.\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(FinalizeModelEditorSession, _Inout_ OrtSession* session, _In_ const OrtSessionOptions* options,\r\n                  _In_opt_ OrtPrepackedWeightsContainer* prepacked_weights_container);\r\n#endif  // !defined(ORT_MINIMAL_BUILD)\r\n};\r\n\r\n/**\r\n * ORT Compile API\r\n */\r\n\r\n/** \\brief Flags representing options to enable when compiling a model.\r\n */\r\ntypedef enum OrtCompileApiFlags {\r\n  // Default. Do not enable any additional compilation options.\r\n  OrtCompileApiFlags_NONE = 0,\r\n\r\n  // Force compilation to return an error (ORT_FAIL) if no nodes were compiled.\r\n  // Otherwise, a model with basic optimizations (ORT_ENABLE_BASIC) is still generated by default.\r\n  OrtCompileApiFlags_ERROR_IF_NO_NODES_COMPILED = 1 << 0,\r\n\r\n  // Force compilation to return an error (ORT_FAIL) if a file with the same filename as the output model exists.\r\n  // Otherwise, compilation will automatically overwrite the output file if it exists.\r\n  OrtCompileApiFlags_ERROR_IF_OUTPUT_FILE_EXISTS = 1 << 1,\r\n} OrtCompileApiFlags;\r\n\r\n/**\r\n * \\brief The OrtCompileApi struct provides functions to compile ONNX models.\r\n *\r\n * Execution providers that support compilation fuse a subgraph into an EPContext node that wraps a provider-specific\r\n * binary representation of the subgraph.\r\n * For more details about the EPContext design, refer to:\r\n *  \\htmlonly\r\n *  <a href=\"https://onnxruntime.ai/docs/execution-providers/EP-Context-Design.html\">EPContext design document.</a>\r\n *  \\endhtmlonly\r\n *\r\n * Example (error handling not shown):\r\n *   OrtStatus* status = NULL;\r\n *   OrtCompileApi* compile_api = ort_api->GetCompileApi();\r\n *   OrtModelCompilationOptions* compile_options = NULL;\r\n *\r\n *   status = compile_api->CreateModelCompilationOptionsFromSessionOptions(env, session_options, &compile_options);\r\n *   status = compile_api->ModelCompilationOptions_SetInputModelPath(compile_options, ORT_TSTR(\"model.onnx\"));\r\n *   status = compile_api->ModelCompilationOptions_SetOutputModelPath(compile_options, ORT_TSTR(\"model.compiled.onnx\"));\r\n *   status = compile_api->CompileModel(env, compile_options);\r\n *   compile_api->ReleaseModelCompilationOptions(compile_options);\r\n *\r\n * \\since Version 1.22.\r\n */\r\nstruct OrtCompileApi {\r\n  /// \\name OrtModelCompilationOptions\r\n  /// @{\r\n  ORT_CLASS_RELEASE(ModelCompilationOptions);\r\n\r\n  /** \\brief Creates an OrtModelCompilationOptions object from an existing OrtSessionOptions object.\r\n   *\r\n   * An OrtModelCompilationOptions object contains the settings used to generate a compiled ONNX model.\r\n   * The OrtSessionOptions object has the execution providers with which the model will be compiled.\r\n   *\r\n   * ReleaseOrtModelCompilationsOptions must be called to free the OrtModelCompilationOptions after calling\r\n   * CompileModel.\r\n   *\r\n   * \\note By default, the GraphOptimizationLevel is set to ORT_DISABLE_ALL. Use\r\n   * ModelCompilationOptions_SetGraphOptimizationLevel to enable graph optimizations.\r\n   *\r\n   * \\param[in] env OrtEnv object.\r\n   * \\param[in] session_options The OrtSessionOptions instance from which to create the OrtModelCompilationOptions.\r\n   * \\param[out] out The created OrtModelCompilationOptions instance.\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CreateModelCompilationOptionsFromSessionOptions, _In_ const OrtEnv* env,\r\n                  _In_ const OrtSessionOptions* session_options, _Outptr_ OrtModelCompilationOptions** out);\r\n\r\n  /** \\brief Sets the file path to the input ONNX model to compile.\r\n   *\r\n   * The input model's location (e.g., file path or memory buffer) must be set with either\r\n   * ModelCompilationOptions_SetInputModelPath or ModelCompilationOptions_SetInputModelFromBuffer.\r\n   *\r\n   * \\param[in] model_compile_options The OrtModelCompilationOptions instance.\r\n   * \\param[in] input_model_path Null terminated string of the path (wchar on Windows, char otherwise).\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(ModelCompilationOptions_SetInputModelPath, _In_ OrtModelCompilationOptions* model_compile_options,\r\n                  _In_ const ORTCHAR_T* input_model_path);\r\n\r\n  /** \\brief Sets the buffer that stores the bytes of the loaded ONNX model to compile.\r\n   *\r\n   * The input model's location (e.g., file path or memory buffer) must be set with either\r\n   * ModelCompilationOptions_SetInputModelPath or ModelCompilationOptions_SetInputModelFromBuffer.\r\n   *\r\n   * \\param[in] model_compile_options The OrtModelCompilationOptions instance.\r\n   * \\param[in] input_model_data Buffer containing the loaded ONNX model bytes.\r\n   * \\param[in] input_model_data_size The number of bytes in the `input_model_data` buffer.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(ModelCompilationOptions_SetInputModelFromBuffer,\r\n                  _In_ OrtModelCompilationOptions* model_compile_options,\r\n                  _In_ const void* input_model_data,\r\n                  size_t input_model_data_size);\r\n\r\n  /** \\brief Sets the file path for the output ONNX model generated by CompileModel.\r\n   *\r\n   * The output model's location (e.g., file path or memory buffer) can be set with either\r\n   * ModelCompilationOptions_SetOutputModelPath or ModelCompilationOptions_SetOutputModelBuffer.\r\n   *\r\n   * If the output model's location is not set, ONNX Runtime will generate an output file with a path based on\r\n   * the input model's file path. Examples:\r\n   *   /Path/my_model.onnx -> /Path/my_model_ctx.onnx\r\n   *   /Path/my_model -> /Path/my_model_ctx.onnx\r\n   *\r\n   * \\param[in] model_compile_options The OrtModelCompilationOptions instance.\r\n   * \\param[in] output_model_path Null terminated string of the path (wchar on Windows, char otherwise).\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(ModelCompilationOptions_SetOutputModelPath, _In_ OrtModelCompilationOptions* model_compile_options,\r\n                  _In_ const ORTCHAR_T* output_model_path);\r\n\r\n  /** \\brief Optionally sets the file that should store external initializers for the compiled ONNX model.\r\n   * If not set, initializers are stored within the model.\r\n   *\r\n   * Only initializers for nodes that were not compiled are stored in the external initializers file.\r\n   * Compiled nodes contain their initializer data within the `ep_cache_context` attribute of EPContext nodes.\r\n   * Refer to ModelCompilationOptions_SetEpContextEmbedMode.\r\n   *\r\n   * \\param[in] model_compile_options The OrtModelCompilationOptions instance.\r\n   * \\param[in] external_initializers_file_path Null terminated string of the path to the file.\r\n   * \\param[in] external_initializers_size_threshold Initializers larger than this threshold are stored in the file.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(ModelCompilationOptions_SetOutputModelExternalInitializersFile,\r\n                  _In_ OrtModelCompilationOptions* model_compile_options,\r\n                  _In_ const ORTCHAR_T* external_initializers_file_path,\r\n                  size_t external_initializers_size_threshold);\r\n\r\n  /** \\brief Configures model compilation to store the output compiled ONNX model in a buffer.\r\n   *\r\n   * The caller passes an OrtAllocator that ONNX Runtime uses to allocate memory for the buffer.\r\n   *\r\n   * The output model's location (e.g., file path or memory buffer) can be set with either\r\n   * ModelCompilationOptions_SetOutputModelPath or ModelCompilationOptions_SetOutputModelBuffer.\r\n   *\r\n   * If the output model's location is not set, ONNX Runtime will generate an output file with a path based on\r\n   * the input model's file path. Examples:\r\n   *   /Path/my_model.onnx -> /Path/my_model_ctx.onnx\r\n   *   /Path/my_model -> /Path/my_model_ctx.onnx\r\n   *\r\n   * \\param[in] model_compile_options The OrtModelCompilationOptions instance.\r\n   * \\param[in] allocator The allocator used to allocate the buffer for the compiled model.\r\n   * \\param[out] output_model_buffer_ptr Pointer to the buffer that stores the compiled model.\r\n   * \\param[out] output_model_buffer_size_ptr Pointer set to the size of output model in bytes.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(ModelCompilationOptions_SetOutputModelBuffer,\r\n                  _In_ OrtModelCompilationOptions* model_compile_options,\r\n                  _Inout_ OrtAllocator* allocator,\r\n                  _Outptr_ void** output_model_buffer_ptr,\r\n                  _Out_ size_t* output_model_buffer_size_ptr);\r\n\r\n  /** \\brief Enables or disables the embedding of EPContext binary data into the `ep_cache_context` attribute\r\n   * of EPContext nodes. Defaults to false.\r\n   *\r\n   * If enabled, the `ep_cache_context` attribute of EPContext nodes will store the context binary data, which may\r\n   * include weights for compiled subgraphs.\r\n   *\r\n   * If disabled, the `ep_cache_context` attribute of EPContext nodes will contain the path to the file containing the\r\n   * context binary data. The path is set by the execution provider creating the EPContext node.\r\n   *\r\n   * More details relate to EPContext design refers to:\r\n   *  \\htmlonly\r\n   *  <a href=\"https://onnxruntime.ai/docs/execution-providers/EP-Context-Design.html\">EPContext design document.</a>\r\n   *  \\endhtmlonly\r\n   *\r\n   * \\param[in] model_compile_options The OrtModelCompilationOptions instance.\r\n   * \\param[in] embed_ep_context_in_model True to embed EPContext binary data into the EPContext node\r\n   *                                      `ep_cache_context` attributes.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(ModelCompilationOptions_SetEpContextEmbedMode, _In_ OrtModelCompilationOptions* model_compile_options,\r\n                  bool embed_ep_context_in_model);\r\n\r\n  /** \\brief Compiles an input ONNX model with the given compilation options.\r\n   *\r\n   * \\param[in] env OrtEnv object.\r\n   * \\param[in] model_options The compilation options that defines compilation options for a model.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CompileModel, _In_ const OrtEnv* env, _In_ const OrtModelCompilationOptions* model_options);\r\n\r\n  /** \\brief Sets flags from OrtCompileApiFlags that represent one or more boolean options to enable.\r\n   *\r\n   * \\param[in] model_compile_options The OrtModelCompilationOptions instance.\r\n   * \\param[in] flags bitwise OR of flags in OrtCompileApiFlags to enable.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ModelCompilationOptions_SetFlags, _In_ OrtModelCompilationOptions* model_compile_options,\r\n                  uint32_t flags);\r\n\r\n  /** Sets information related to EP context binary file.\r\n   *\r\n   * EP uses this information to decide the location and context binary file name.\r\n   * Used while compiling model with input and output in memory buffer\r\n   *\r\n   * \\param[in] model_compile_options The OrtModelCompilationOptions instance.\r\n   * \\param[in] output_directory Null terminated string of the path (wchar on Windows, char otherwise).\r\n   * \\param[in] model_name Null terminated string of the model name (wchar on Windows, char otherwise).\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ModelCompilationOptions_SetEpContextBinaryInformation,\r\n                  _In_ OrtModelCompilationOptions* model_compile_options,\r\n                  _In_ const ORTCHAR_T* output_directory,\r\n                  _In_ const ORTCHAR_T* model_name);\r\n\r\n  /** Set the graph optimization level.\r\n   *\r\n   * \\param[in] model_compile_options The OrtModelCompilationOptions instance.\r\n   * \\param[in] graph_optimization_level The graph optimization level.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ModelCompilationOptions_SetGraphOptimizationLevel,\r\n                  _In_ OrtModelCompilationOptions* model_compile_options,\r\n                  _In_ GraphOptimizationLevel graph_optimization_level);\r\n\r\n  /** \\brief Sets a OrtWriteBufferFunc function that is called by ORT to write out the output model's serialized\r\n   * ONNX bytes.\r\n   *\r\n   * The provided write function may be called repeatedly until then entire output model has been written out. Each call\r\n   * to the write function is expected to consume the entire input buffer.\r\n   *\r\n   * The output model's destination (e.g., file path, memory buffer, or stream) can be set with any of the functions\r\n   * that begin with ModelCompilationOptions_SetOutputModel____.\r\n   *\r\n   * \\param[in] model_compile_options The OrtModelCompilationOptions instance.\r\n   * \\param[in] write_func The OrtWriteBufferFunc function called by ORT when writing out the model.\r\n   * \\param[in] state Opaque state passed as the first argument to OrtWriteBufferFunc. Can be NULL.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ModelCompilationOptions_SetOutputModelWriteFunc,\r\n                  _In_ OrtModelCompilationOptions* model_compile_options,\r\n                  _In_ OrtWriteBufferFunc write_func, _In_ void* state);\r\n\r\n  /** \\brief Sets a OrtGetInitializerLocationFunc function that is called by ORT for every initializer in the generated\r\n   * model. Allows implementer to specify whether initializers should be stored within the model or externally.\r\n   *\r\n   * \\param[in] model_compile_options The OrtModelCompilationOptions instance.\r\n   * \\param[in] get_initializer_location_func The OrtGetInitializerLocationFunc function called by ORT when\r\n   *                                          to determine the location of the initializer.\r\n   * \\param[in] state Opaque state passed as the first argument to OrtGetInitializerLocationFunc. Can be NULL.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ModelCompilationOptions_SetOutputModelGetInitializerLocationFunc,\r\n                  _In_ OrtModelCompilationOptions* model_compile_options,\r\n                  _In_ OrtGetInitializerLocationFunc get_initializer_location_func, _In_ void* state);\r\n};\r\n\r\n/*\r\n * This is the old way to add the CUDA provider to the session, please use SessionOptionsAppendExecutionProvider_CUDA above to access the latest functionality\r\n * This function always exists, but will only succeed if Onnxruntime was built with CUDA support and the CUDA provider shared library exists\r\n *\r\n * \\param device_id CUDA device id, starts from zero.\r\n */\r\nORT_API_STATUS(OrtSessionOptionsAppendExecutionProvider_CUDA, _In_ OrtSessionOptions* options, int device_id);\r\n\r\n/*\r\n * This is the old way to add the ROCm provider to the session, please use\r\n * SessionOptionsAppendExecutionProvider_ROCM above to access the latest functionality\r\n * This function always exists, but will only succeed if Onnxruntime was built with\r\n * HIP support and the ROCm provider shared library exists\r\n *\r\n * \\param device_id HIP device id, starts from zero.\r\n */\r\nORT_API_STATUS(OrtSessionOptionsAppendExecutionProvider_ROCM, _In_ OrtSessionOptions* options, int device_id);\r\n\r\n/*\r\n * This is the old way to add the MIGraphX provider to the session, please use\r\n * SessionOptionsAppendExecutionProvider_MIGraphX above to access the latest functionality\r\n * This function always exists, but will only succeed if Onnxruntime was built with\r\n * HIP support and the MIGraphX provider shared library exists\r\n *\r\n * \\param device_id HIP device id, starts from zero.\r\n */\r\nORT_API_STATUS(OrtSessionOptionsAppendExecutionProvider_MIGraphX, _In_ OrtSessionOptions* options, int device_id);\r\n\r\n/*\r\n * This is the old way to add the oneDNN provider to the session, please use\r\n * SessionOptionsAppendExecutionProvider_oneDNN above to access the latest functionality\r\n * This function always exists, but will only succeed if Onnxruntime was built with\r\n * oneDNN support and the oneDNN provider shared library exists\r\n *\r\n * \\param use_arena zero: false. non-zero: true.\r\n */\r\nORT_API_STATUS(OrtSessionOptionsAppendExecutionProvider_Dnnl, _In_ OrtSessionOptions* options, int use_arena);\r\n\r\n/*\r\n * This is the old way to add the TensorRT provider to the session, please use SessionOptionsAppendExecutionProvider_TensorRT_V2 above to access the latest functionality\r\n * This function always exists, but will only succeed if Onnxruntime was built with TensorRT support and the TensorRT provider shared library exists\r\n *\r\n * \\param device_id CUDA device id, starts from zero.\r\n */\r\nORT_API_STATUS(OrtSessionOptionsAppendExecutionProvider_Tensorrt, _In_ OrtSessionOptions* options, int device_id);\r\n\r\n#ifdef __cplusplus\r\n}\r\n#endif\r\n/// @}\r\n\r\n#include \"onnxruntime_ep_c_api.h\"\r\n","replacement":"ï»¿// Copyright (c) Microsoft Corporation. All rights reserved.\r\n// Licensed under the MIT License.\r\n\r\n// See docs\\c_cxx\\README.md on generating the Doxygen documentation from this file\r\n\r\n/** \\mainpage ONNX Runtime\r\n *\r\n * ONNX Runtime is a high-performance inference and training graph execution engine for deep learning models.\r\n *\r\n * ONNX Runtime's C, C++ APIs offer an easy to use interface to onboard and execute onnx models.\r\n * - \\subpage c_cpp_api \"Core C, C++ APIs\"\r\n * - \\subpage training_c_cpp_api \"Training C, C++ APIs for on-device training\"\r\n *\r\n * \\page c_cpp_api Core C, C++ APIs\r\n * <h1>C</h1>\r\n *\r\n * ::OrtApi - Click here to go to the structure with all C API functions.\r\n *\r\n * <h1>C++</h1>\r\n *\r\n * ::Ort - Click here to go to the namespace holding all of the C++ wrapper classes\r\n *\r\n * It is a set of header only wrapper classes around the C API. The goal is to turn the C style return value error codes into C++ exceptions, and to\r\n * automate memory management through standard C++ RAII principles.\r\n *\r\n * \\addtogroup Global\r\n * ONNX Runtime C API\r\n * @{\r\n */\r\n\r\n#pragma once\r\n#include <stdbool.h>\r\n#include <stdint.h>\r\n#include <stdlib.h>\r\n#include <string.h>\r\n\r\n/** \\brief The API version defined in this header\r\n *\r\n * This value is used by some API functions to behave as this version of the header expects.\r\n */\r\n#define ORT_API_VERSION 24\r\n\r\n#ifdef __cplusplus\r\nextern \"C\" {\r\n#endif\r\n\r\n//! @}\r\n// SAL2 Definitions\r\n#ifndef _MSC_VER\r\n#define _In_\r\n#define _In_z_\r\n#define _In_opt_\r\n#define _In_opt_z_\r\n#define _Out_\r\n#define _Out_opt_\r\n#define _Outptr_\r\n#define _Outptr_opt_\r\n#define _Inout_\r\n#define _Inout_opt_\r\n#define _Frees_ptr_opt_\r\n#define _Ret_maybenull_\r\n#define _Ret_notnull_\r\n#define _Check_return_\r\n#define _Outptr_result_maybenull_\r\n#define _Outptr_result_maybenull_z_\r\n#define _In_reads_(X)\r\n#define _In_reads_opt_\r\n#define _Inout_updates_(X)\r\n#define _Out_writes_(X)\r\n#define _Out_writes_opt_(X)\r\n#define _Inout_updates_all_(X)\r\n#define _Out_writes_bytes_all_(X)\r\n#define _Out_writes_all_(X)\r\n#define _Success_(X)\r\n#define _Outptr_result_buffer_maybenull_(X)\r\n#define ORT_ALL_ARGS_NONNULL __attribute__((nonnull))\r\n#else\r\n#include <specstrings.h>\r\n#define ORT_ALL_ARGS_NONNULL\r\n#endif\r\n\r\n#ifdef _WIN32\r\n// Define ORT_DLL_IMPORT if your program is dynamically linked to Ort.\r\n// dllexport is not used, we use a .def file.\r\n#ifdef ORT_DLL_IMPORT\r\n#define ORT_EXPORT __declspec(dllimport)\r\n#else\r\n#define ORT_EXPORT\r\n#endif\r\n#define ORT_API_CALL __stdcall\r\n#define ORT_MUST_USE_RESULT\r\n#define ORTCHAR_T wchar_t\r\n#else\r\n// To make symbols visible on macOS/iOS\r\n#ifdef __APPLE__\r\n#define ORT_EXPORT __attribute__((visibility(\"default\")))\r\n#else\r\n#define ORT_EXPORT\r\n#endif\r\n#define ORT_API_CALL\r\n#define ORT_MUST_USE_RESULT __attribute__((warn_unused_result))\r\n#define ORTCHAR_T char\r\n#endif\r\n\r\n/// ORTCHAR_T, ORT_TSTR are reserved specifically for path handling.\r\n/// All other strings are UTF-8 encoded, use char and std::string\r\n#ifndef ORT_TSTR\r\n#ifdef _WIN32\r\n#define ORT_TSTR(X) L##X\r\n// When X is a macro, L##X is not defined. In this case, we need to use ORT_TSTR_ON_MACRO.\r\n#define ORT_TSTR_ON_MACRO(X) L\"\" X\r\n#else\r\n#define ORT_TSTR(X) X\r\n#define ORT_TSTR_ON_MACRO(X) X\r\n#endif\r\n#endif\r\n\r\n// On Windows, ORT_FILE is a wchar_t version of the __FILE__ macro.\r\n// Otherwise, ORT_FILE is equivalent to __FILE__.\r\n#ifndef ORT_FILE\r\n#define ORT_FILE_INTERNAL(x) ORT_TSTR(x)\r\n#define ORT_FILE ORT_FILE_INTERNAL(__FILE__)\r\n#endif\r\n\r\n// Any pointer marked with _In_ or _Out_, cannot be NULL.\r\n\r\n// Windows users should use unicode paths when possible to bypass the MAX_PATH limitation\r\n// Every pointer marked with _In_ or _Out_, cannot be NULL. Caller should ensure that.\r\n// for ReleaseXXX(...) functions, they can accept NULL pointer.\r\n\r\n#ifdef __cplusplus\r\n// For any compiler with C++11 support, MSVC 2015 and greater, or Clang version supporting noexcept.\r\n// Such complex condition is needed because compilers set __cplusplus value differently.\r\n#ifndef __has_feature\r\n#define __has_feature(x) 0\r\n#endif\r\n#if ((__cplusplus >= 201103L) || (_MSC_VER >= 1900) || (defined(__has_feature) && __has_feature(cxx_noexcept)))\r\n#define NO_EXCEPTION noexcept\r\n#else\r\n#define NO_EXCEPTION throw()\r\n#endif\r\n#else\r\n#define NO_EXCEPTION\r\n#endif\r\n\r\n// __VA_ARGS__ on Windows and Linux are different\r\n#define ORT_API(RETURN_TYPE, NAME, ...) RETURN_TYPE ORT_API_CALL NAME(__VA_ARGS__) NO_EXCEPTION\r\n\r\n#define ORT_API_T(RETURN_TYPE, NAME, ...) \\\r\n  RETURN_TYPE(ORT_API_CALL* NAME)         \\\r\n  (__VA_ARGS__) NO_EXCEPTION\r\n\r\n#define ORT_API_STATUS(NAME, ...)                                                                   \\\r\n  _Success_(return == 0) _Check_return_ _Ret_maybenull_ OrtStatusPtr ORT_API_CALL NAME(__VA_ARGS__) \\\r\n  NO_EXCEPTION ORT_MUST_USE_RESULT\r\n\r\n// XXX: Unfortunately, SAL annotations are known to not work with function pointers\r\n#define ORT_API2_STATUS(NAME, ...) \\\r\n  _Check_return_ _Ret_maybenull_ OrtStatusPtr(ORT_API_CALL* NAME)(__VA_ARGS__) NO_EXCEPTION ORT_MUST_USE_RESULT\r\n\r\n// Used in *.cc files. Almost as same as ORT_API_STATUS, except without ORT_MUST_USE_RESULT and ORT_EXPORT\r\n#define ORT_API_STATUS_IMPL(NAME, ...) \\\r\n  _Success_(return == 0) _Check_return_ _Ret_maybenull_ OrtStatusPtr ORT_API_CALL NAME(__VA_ARGS__) NO_EXCEPTION\r\n\r\n#define ORT_CLASS_RELEASE(X) void(ORT_API_CALL * Release##X)(_Frees_ptr_opt_ Ort##X * input)\r\n\r\n#ifdef __DOXYGEN__\r\n#undef ORT_API_STATUS\r\n#define ORT_API_STATUS(NAME, ...) OrtStatus* NAME(__VA_ARGS__)\r\n#undef ORT_API2_STATUS\r\n#define ORT_API2_STATUS(NAME, ...) OrtStatus* NAME(__VA_ARGS__)\r\n#undef ORT_CLASS_RELEASE\r\n#define ORT_CLASS_RELEASE(X) void Release##X(Ort##X* input)\r\n#undef NO_EXCEPTION\r\n#define NO_EXCEPTION\r\n#endif\r\n/** \\addtogroup Global\r\n * ONNX Runtime C API\r\n * @{\r\n */\r\n\r\n/** Copied from TensorProto::DataType\r\n * Currently, Ort doesn't support complex64, complex128\r\n */\r\ntypedef enum ONNXTensorElementDataType {\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_UNDEFINED,\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT,   // maps to c type float\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_UINT8,   // maps to c type uint8_t\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_INT8,    // maps to c type int8_t\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_UINT16,  // maps to c type uint16_t\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_INT16,   // maps to c type int16_t\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_INT32,   // maps to c type int32_t\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_INT64,   // maps to c type int64_t\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_STRING,  // maps to c++ type std::string\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_BOOL,\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16,\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_DOUBLE,      // maps to c type double\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_UINT32,      // maps to c type uint32_t\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_UINT64,      // maps to c type uint64_t\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_COMPLEX64,   // complex with float32 real and imaginary components\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_COMPLEX128,  // complex with float64 real and imaginary components\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_BFLOAT16,    // Non-IEEE floating-point format based on IEEE754 single-precision\r\n  // float 8 types were introduced in onnx 1.14, see https://onnx.ai/onnx/technical/float8.html\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT8E4M3FN,    // Non-IEEE floating-point format based on IEEE754 single-precision\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT8E4M3FNUZ,  // Non-IEEE floating-point format based on IEEE754 single-precision\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT8E5M2,      // Non-IEEE floating-point format based on IEEE754 single-precision\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT8E5M2FNUZ,  // Non-IEEE floating-point format based on IEEE754 single-precision\r\n  // Int4 types were introduced in ONNX 1.16. See https://onnx.ai/onnx/technical/int4.html\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_UINT4,  // maps to a pair of packed uint4 values (size == 1 byte)\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_INT4,   // maps to a pair of packed int4 values (size == 1 byte)\r\n  // Float4 types were introduced in ONNX 1.18. See https://onnx.ai/onnx/technical/float4.html\r\n  ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT4E2M1,  // maps to a pair of packed float4 values (size == 1 byte)\r\n} ONNXTensorElementDataType;\r\n\r\n// Synced with onnx TypeProto oneof\r\ntypedef enum ONNXType {\r\n  ONNX_TYPE_UNKNOWN,\r\n  ONNX_TYPE_TENSOR,\r\n  ONNX_TYPE_SEQUENCE,\r\n  ONNX_TYPE_MAP,\r\n  ONNX_TYPE_OPAQUE,\r\n  ONNX_TYPE_SPARSETENSOR,\r\n  ONNX_TYPE_OPTIONAL\r\n} ONNXType;\r\n\r\n// These types are synced with internal\r\n// SparseFormatFlags\r\ntypedef enum OrtSparseFormat {\r\n  ORT_SPARSE_UNDEFINED = 0,\r\n  ORT_SPARSE_COO = 0x1,\r\n  ORT_SPARSE_CSRC = 0x2,\r\n  ORT_SPARSE_BLOCK_SPARSE = 0x4\r\n} OrtSparseFormat;\r\n\r\n// Enum allows to query sparse tensor indices\r\nenum OrtSparseIndicesFormat {\r\n  ORT_SPARSE_COO_INDICES,\r\n  ORT_SPARSE_CSR_INNER_INDICES,\r\n  ORT_SPARSE_CSR_OUTER_INDICES,\r\n  ORT_SPARSE_BLOCK_SPARSE_INDICES\r\n};\r\n\r\n/** \\brief Logging severity levels\r\n *\r\n * In typical API usage, specifying a logging severity level specifies the minimum severity of log messages to show.\r\n */\r\ntypedef enum OrtLoggingLevel {\r\n  ORT_LOGGING_LEVEL_VERBOSE,  ///< Verbose informational messages (least severe).\r\n  ORT_LOGGING_LEVEL_INFO,     ///< Informational messages.\r\n  ORT_LOGGING_LEVEL_WARNING,  ///< Warning messages.\r\n  ORT_LOGGING_LEVEL_ERROR,    ///< Error messages.\r\n  ORT_LOGGING_LEVEL_FATAL,    ///< Fatal error messages (most severe).\r\n} OrtLoggingLevel;\r\n\r\ntypedef enum OrtErrorCode {\r\n  ORT_OK,\r\n  ORT_FAIL,\r\n  ORT_INVALID_ARGUMENT,\r\n  ORT_NO_SUCHFILE,\r\n  ORT_NO_MODEL,\r\n  ORT_ENGINE_ERROR,\r\n  ORT_RUNTIME_EXCEPTION,\r\n  ORT_INVALID_PROTOBUF,\r\n  ORT_MODEL_LOADED,\r\n  ORT_NOT_IMPLEMENTED,\r\n  ORT_INVALID_GRAPH,\r\n  ORT_EP_FAIL,\r\n  ORT_MODEL_LOAD_CANCELED,\r\n  ORT_MODEL_REQUIRES_COMPILATION,\r\n  ORT_NOT_FOUND,\r\n} OrtErrorCode;\r\n\r\ntypedef enum OrtOpAttrType {\r\n  ORT_OP_ATTR_UNDEFINED = 0,\r\n  ORT_OP_ATTR_INT,\r\n  ORT_OP_ATTR_INTS,\r\n  ORT_OP_ATTR_FLOAT,\r\n  ORT_OP_ATTR_FLOATS,\r\n  ORT_OP_ATTR_STRING,\r\n  ORT_OP_ATTR_STRINGS,\r\n  ORT_OP_ATTR_GRAPH,\r\n  ORT_OP_ATTR_TENSOR,\r\n} OrtOpAttrType;\r\n\r\n//! @}\r\n#define ORT_RUNTIME_CLASS(X) \\\r\n  struct Ort##X;             \\\r\n  typedef struct Ort##X Ort##X\r\n\r\n/** \\addtogroup Global\r\n * ONNX Runtime C API\r\n * @{\r\n */\r\n// The actual types defined have an Ort prefix\r\nORT_RUNTIME_CLASS(Env);\r\nORT_RUNTIME_CLASS(Status);  // nullptr for Status* indicates success\r\nORT_RUNTIME_CLASS(MemoryInfo);\r\nORT_RUNTIME_CLASS(IoBinding);\r\nORT_RUNTIME_CLASS(Session);  // Don't call ReleaseSession from Dllmain (because session owns a thread pool)\r\nORT_RUNTIME_CLASS(Value);\r\nORT_RUNTIME_CLASS(RunOptions);\r\nORT_RUNTIME_CLASS(TypeInfo);\r\nORT_RUNTIME_CLASS(TensorTypeAndShapeInfo);\r\nORT_RUNTIME_CLASS(MapTypeInfo);\r\nORT_RUNTIME_CLASS(SequenceTypeInfo);\r\nORT_RUNTIME_CLASS(OptionalTypeInfo);\r\nORT_RUNTIME_CLASS(SessionOptions);\r\nORT_RUNTIME_CLASS(CustomOpDomain);\r\nORT_RUNTIME_CLASS(ModelMetadata);\r\nORT_RUNTIME_CLASS(ThreadPoolParams);\r\nORT_RUNTIME_CLASS(ThreadingOptions);\r\nORT_RUNTIME_CLASS(ArenaCfg);\r\nORT_RUNTIME_CLASS(PrepackedWeightsContainer);\r\nORT_RUNTIME_CLASS(TensorRTProviderOptionsV2);\r\nORT_RUNTIME_CLASS(NvTensorRtRtxProviderOptions);\r\nORT_RUNTIME_CLASS(CUDAProviderOptionsV2);\r\nORT_RUNTIME_CLASS(CANNProviderOptions);\r\nORT_RUNTIME_CLASS(DnnlProviderOptions);\r\nORT_RUNTIME_CLASS(Op);\r\nORT_RUNTIME_CLASS(OpAttr);\r\nORT_RUNTIME_CLASS(Logger);\r\nORT_RUNTIME_CLASS(ShapeInferContext);\r\nORT_RUNTIME_CLASS(LoraAdapter);\r\nORT_RUNTIME_CLASS(ValueInfo);\r\nORT_RUNTIME_CLASS(Node);\r\nORT_RUNTIME_CLASS(Graph);\r\nORT_RUNTIME_CLASS(Model);\r\nORT_RUNTIME_CLASS(ModelCompilationOptions);\r\nORT_RUNTIME_CLASS(HardwareDevice);\r\nORT_RUNTIME_CLASS(EpDevice);\r\nORT_RUNTIME_CLASS(KeyValuePairs);\r\nORT_RUNTIME_CLASS(SyncStream);  // Opaque class to create an onnxruntime::Stream.\r\nORT_RUNTIME_CLASS(ExternalInitializerInfo);\r\n\r\n#ifdef _MSC_VER\r\ntypedef _Return_type_success_(return == 0) OrtStatus* OrtStatusPtr;\r\n#else\r\ntypedef OrtStatus* OrtStatusPtr;\r\n#endif\r\n\r\n/** \\brief Memory allocation interface\r\n *\r\n * Structure of function pointers that defines a memory allocator. This can be created and filled in by the user for custom allocators.\r\n *\r\n * When an allocator is passed to any function, be sure that the allocator object is not destroyed until the last allocated object using it is freed.\r\n */\r\ntypedef struct OrtAllocator {\r\n  uint32_t version;  ///< Must be initialized to ORT_API_VERSION\r\n\r\n  /// Returns a pointer to an allocated block of `size` bytes\r\n  void*(ORT_API_CALL* Alloc)(struct OrtAllocator* this_, size_t size);\r\n\r\n  /// Free a block of memory previously allocated with OrtAllocator::Alloc\r\n  void(ORT_API_CALL* Free)(struct OrtAllocator* this_, void* p);\r\n\r\n  /// Return a pointer to an ::OrtMemoryInfo that describes this allocator\r\n  const struct OrtMemoryInfo*(ORT_API_CALL* Info)(const struct OrtAllocator* this_);\r\n  /**\r\n   * @brief Optional allocation function to use for memory allocations made during session initialization.\r\n   * Use this function if you want to separate allocations made by ORT during Run() calls from\r\n   * those made during session initialization. This allows for separate memory management strategies for these\r\n   * allocations.\r\n   *\r\n   * \\return pointer to an allocated block of `size` bytes. nullptr if size was 0 or allocation failed.\r\n   *\r\n   * \\since 1.18\r\n   */\r\n  void*(ORT_API_CALL* Reserve)(struct OrtAllocator* this_, size_t size);\r\n\r\n  /**\r\n   * @brief Function used to get the statistics of the allocator.\r\n   *\r\n   * Return a pointer to the OrtKeyValuePairs structure that contains the statistics of the allocator.\r\n   * The user should call OrtApi::ReleaseKeyValuePairs when done.\r\n   *\r\n   * Current known keys are:\r\n   * - Limit: Bytes limit of the allocator. -1 if no limit is set.\r\n   * - InUse: Number of bytes in use.\r\n   * - TotalAllocated: The total number of allocated bytes by the allocator.\r\n   * - MaxInUse: The maximum bytes in use.\r\n   * - NumAllocs: Number of allocations.\r\n   * - NumReserves: Number of reserves. (Number of calls to Reserve() in arena-based allocators)\r\n   * - NumArenaExtensions: Number of arena extensions (Relevant only for arena based allocators)\r\n   * - NumArenaShrinkages: Number of arena shrinkages (Relevant only for arena based allocators)\r\n   * - MaxAllocSize: The max single allocation seen.\r\n   *\r\n   * The allocator is free to add other entries as appropriate.\r\n   *\r\n   * \\note Implementation of this function is optional and GetStats may be set to a nullptr.\r\n   *       If the OrtAllocator is wrapping an internal ORT allocator that does not implement GetStats\r\n   *       the returned OrtKeyValuePairs instance will be empty.\r\n   *\r\n   * \\since 1.23\r\n   */\r\n  ORT_API2_STATUS(GetStats, _In_ const struct OrtAllocator* this_, _Outptr_ OrtKeyValuePairs** out);\r\n\r\n  /** \\brief Allocate using a stream.\r\n   *\r\n   * If the allocator is stream aware this performs allocation using a stream.\r\n   *\r\n   * Alloc will be used if this is nullptr.\r\n   *\r\n   * \\param[in] this_ OrtAllocator instance\r\n   * \\param[in] size Size of the allocation in bytes. nullptr if size was 0 or allocation failed.\r\n   * \\param[in] stream The stream to allocate on.\r\n   *\r\n   * \\return pointer to an allocated block of `size` bytes\r\n   *\r\n   * \\note Implementation of this function is optional and AllocOnStream may be set to a nullptr.\r\n   * \\since 1.23\r\n   */\r\n  void*(ORT_API_CALL* AllocOnStream)(struct OrtAllocator* this_, size_t size, OrtSyncStream* stream);\r\n} OrtAllocator;\r\n\r\ntypedef void(ORT_API_CALL* OrtLoggingFunction)(\r\n    void* param, OrtLoggingLevel severity, const char* category, const char* logid, const char* code_location,\r\n    const char* message);\r\n\r\n/** \\brief Graph optimization level\r\n *\r\n * Refer to https://www.onnxruntime.ai/docs/performance/graph-optimizations.html#graph-optimization-levels\r\n * for an in-depth understanding of the Graph Optimization Levels.\r\n */\r\ntypedef enum GraphOptimizationLevel {\r\n  ORT_DISABLE_ALL = 0,\r\n  ORT_ENABLE_BASIC = 1,\r\n  ORT_ENABLE_EXTENDED = 2,\r\n  ORT_ENABLE_LAYOUT = 3,\r\n  ORT_ENABLE_ALL = 99\r\n} GraphOptimizationLevel;\r\n\r\ntypedef enum ExecutionMode {\r\n  ORT_SEQUENTIAL = 0,\r\n  ORT_PARALLEL = 1,\r\n} ExecutionMode;\r\n\r\n/** \\brief Language projection identifiers\r\n * /see OrtApi::SetLanguageProjection\r\n */\r\ntypedef enum OrtLanguageProjection {\r\n  ORT_PROJECTION_C = 0,\r\n  ORT_PROJECTION_CPLUSPLUS = 1,\r\n  ORT_PROJECTION_CSHARP = 2,\r\n  ORT_PROJECTION_PYTHON = 3,\r\n  ORT_PROJECTION_JAVA = 4,\r\n  ORT_PROJECTION_WINML = 5,\r\n  ORT_PROJECTION_NODEJS = 6,\r\n} OrtLanguageProjection;\r\n\r\nstruct OrtKernelInfo;\r\ntypedef struct OrtKernelInfo OrtKernelInfo;\r\nstruct OrtKernelContext;\r\ntypedef struct OrtKernelContext OrtKernelContext;\r\nstruct OrtCustomOp;\r\ntypedef struct OrtCustomOp OrtCustomOp;\r\n\r\ntypedef enum OrtAllocatorType {\r\n  OrtInvalidAllocator = -1,\r\n  OrtDeviceAllocator = 0,\r\n  OrtArenaAllocator = 1,\r\n  OrtReadOnlyAllocator = 2,\r\n} OrtAllocatorType;\r\n\r\n/** \\brief Memory types for allocated memory, execution provider specific types should be extended in each provider.\r\n */\r\n// Whenever this struct is updated, please also update the MakeKey function in onnxruntime / core / framework / execution_provider.cc\r\ntypedef enum OrtMemType {\r\n  /// Any CPU memory used by non-CPU execution provider\r\n  OrtMemTypeCPUInput = -2,\r\n  /// CPU accessible memory outputted by non-CPU execution provider, i.e. HOST_ACCESSIBLE\r\n  OrtMemTypeCPUOutput = -1,\r\n  /// CPU accessible memory allocated by non-CPU execution provider, i.e. HOST_ACCESSIBLE\r\n  OrtMemTypeCPU = OrtMemTypeCPUOutput,\r\n  /// The default allocator for execution provider\r\n  OrtMemTypeDefault = 0,\r\n} OrtMemType;\r\n\r\n/** \\brief This matches OrtDevice::MemoryType values */\r\ntypedef enum OrtDeviceMemoryType {\r\n  OrtDeviceMemoryType_DEFAULT = 0,          ///< Device memory\r\n  OrtDeviceMemoryType_HOST_ACCESSIBLE = 5,  ///< Shared/pinned memory for transferring between CPU and the device\r\n} OrtDeviceMemoryType;\r\n\r\n/** \\brief This mimics OrtDevice type constants so they can be returned in the API\r\n */\r\ntypedef enum OrtMemoryInfoDeviceType {\r\n  OrtMemoryInfoDeviceType_CPU = 0,\r\n  OrtMemoryInfoDeviceType_GPU = 1,\r\n  OrtMemoryInfoDeviceType_FPGA = 2,\r\n  OrtMemoryInfoDeviceType_NPU = 3,\r\n} OrtMemoryInfoDeviceType;\r\n\r\ntypedef enum OrtHardwareDeviceType {\r\n  OrtHardwareDeviceType_CPU,\r\n  OrtHardwareDeviceType_GPU,\r\n  OrtHardwareDeviceType_NPU\r\n} OrtHardwareDeviceType;\r\n\r\n/** \\brief These are the default EP selection policies used by ORT when doing automatic EP selection.\r\n */\r\ntypedef enum OrtExecutionProviderDevicePolicy {\r\n  OrtExecutionProviderDevicePolicy_DEFAULT,\r\n  OrtExecutionProviderDevicePolicy_PREFER_CPU,\r\n  OrtExecutionProviderDevicePolicy_PREFER_NPU,\r\n  OrtExecutionProviderDevicePolicy_PREFER_GPU,\r\n  OrtExecutionProviderDevicePolicy_MAX_PERFORMANCE,\r\n  OrtExecutionProviderDevicePolicy_MAX_EFFICIENCY,\r\n  OrtExecutionProviderDevicePolicy_MIN_OVERALL_POWER,\r\n} OrtExecutionProviderDevicePolicy;\r\n\r\n/** \\brief Delegate to allow providing custom OrtEpDevice selection logic\r\n *\r\n * This delegate is called by the EP selection code to allow the user to provide custom device selection logic.\r\n * The user can use this to select OrtEpDevice instances from the list of available devices.\r\n *\r\n * \\param ep_devices The list of available devices.\r\n * \\param num_devices The number of available devices.\r\n * \\param model_metadata The model metadata.\r\n * \\param runtime_metadata The runtime metadata. May be nullptr.\r\n * \\param selected Pre-allocated array to populate with selected OrtEpDevice pointers from ep_devices.\r\n * \\param max_selected The maximum number of devices that can be selected in the pre-allocated array.\r\n                       Currently the maximum is 8.\r\n * \\param num_selected The number of selected devices.\r\n * \\param state Opaque pointer. Required to use the delegate from other languages like C# and python.\r\n *\r\n * \\return OrtStatus* Selection status. Return nullptr on success.\r\n *                    Use CreateStatus to provide error info. Use ORT_FAIL as the error code.\r\n *                    ORT will release the OrtStatus* if not null.\r\n */\r\ntypedef OrtStatus*(ORT_API_CALL* EpSelectionDelegate)(_In_ const OrtEpDevice** ep_devices,\r\n                                                      _In_ size_t num_devices,\r\n                                                      _In_ const OrtKeyValuePairs* model_metadata,\r\n                                                      _In_opt_ const OrtKeyValuePairs* runtime_metadata,\r\n                                                      _Inout_ const OrtEpDevice** selected,\r\n                                                      _In_ size_t max_selected,\r\n                                                      _Out_ size_t* num_selected,\r\n                                                      _In_ void* state);\r\n\r\n/** \\brief Function called by ORT to write a buffer to a custom destination (e.g., file, stream, etc.).\r\n *\r\n * \\param state Opaque pointer holding the user's state.\r\n * \\param buffer The buffer to write.\r\n * \\param buffer_num_bytes The size of the buffer in bytes.\r\n *\r\n * \\return OrtStatus* Write status. Return nullptr on success.\r\n *                    Use CreateStatus to provide error info. Use ORT_FAIL as the error code.\r\n *                    ORT will release the OrtStatus* if not null.\r\n */\r\ntypedef OrtStatus*(ORT_API_CALL* OrtWriteBufferFunc)(_In_ void* state,\r\n                                                     _In_ const void* buffer,\r\n                                                     _In_ size_t buffer_num_bytes);\r\n\r\n/** \\brief Function called by ORT to allow user to specify how an initializer should be saved, that is, either\r\n * written to an external file or stored within the model. ORT calls this function for every initializer when\r\n * generating a model.\r\n *\r\n * If the function implementation sets the `new_external_info` output parameter to NULL, ORT stores the initializer data\r\n * within the generated model.\r\n *\r\n * Otherwise, if the function implementation sets `new_external_info` to a valid OrtExternalInitializerInfo instance,\r\n * ORT assumes that this function stores the initializer data in a file. In this case, ORT configures the model's\r\n * initializer to point to the location specified by the `new_external_info` output parameter.\r\n *\r\n * \\param[in] state Opaque pointer holding the user's state.\r\n * \\param[in] initializer_name The initializer's name as a null-terminated string.\r\n * \\param[in] initializer_value OrtValue containing the initializer's data, type, and shape.\r\n * \\param[in] external_info If the initializer is originally stored in an external file, `external_info` contains\r\n *                          the file path, file offset, and the data's byte size within the file. Otherwise,\r\n *                          `external_info` is NULL if the initializer is not originally stored in a file.\r\n * \\param[out] new_external_info Output parameter set to a new OrtExternalInitializerInfo instance indicating the\r\n *                               location where the function implementation stored the initializer data.\r\n *                               The function implementation must use `OrtApi::CreateExternalInitializerInfo()` to\r\n *                               create the instance.\r\n *                               If the function implementation sets `new_external_info` to NULL,\r\n *                               ORT stores the initializers within the model.\r\n *\r\n * \\note ORT takes ownership of the `new_external_info` output parameter.\r\n *\r\n * \\return OrtStatus* Write status. Return nullptr on success.\r\n *                    Use CreateStatus to provide error info. Use ORT_FAIL as the error code.\r\n *                    ORT will release the OrtStatus* if not null.\r\n */\r\ntypedef OrtStatus*(ORT_API_CALL* OrtGetInitializerLocationFunc)(\r\n    _In_ void* state,\r\n    _In_ const char* initializer_name,\r\n    _In_ const OrtValue* initializer_value,\r\n    _In_opt_ const OrtExternalInitializerInfo* external_info,\r\n    _Outptr_result_maybenull_ OrtExternalInitializerInfo** new_external_info);\r\n\r\n/** \\brief Algorithm to use for cuDNN Convolution Op\r\n */\r\ntypedef enum OrtCudnnConvAlgoSearch {\r\n  OrtCudnnConvAlgoSearchExhaustive,  // expensive exhaustive benchmarking using cudnnFindConvolutionForwardAlgorithmEx\r\n  OrtCudnnConvAlgoSearchHeuristic,   // lightweight heuristic based search using cudnnGetConvolutionForwardAlgorithm_v7\r\n  OrtCudnnConvAlgoSearchDefault,     // default algorithm using CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM\r\n} OrtCudnnConvAlgoSearch;\r\n\r\n/** \\brief CUDA Provider Options\r\n *\r\n * \\see OrtApi::SessionOptionsAppendExecutionProvider_CUDA\r\n */\r\ntypedef struct OrtCUDAProviderOptions {\r\n#ifdef __cplusplus\r\n  OrtCUDAProviderOptions()\r\n      : device_id{},\r\n        cudnn_conv_algo_search{OrtCudnnConvAlgoSearchExhaustive},\r\n        gpu_mem_limit{SIZE_MAX},\r\n        arena_extend_strategy{},\r\n        do_copy_in_default_stream{1},\r\n        has_user_compute_stream{},\r\n        user_compute_stream{},\r\n        default_memory_arena_cfg{},\r\n        tunable_op_enable{false},\r\n        tunable_op_tuning_enable{false},\r\n        tunable_op_max_tuning_duration_ms{} {}\r\n#endif\r\n\r\n  /** \\brief CUDA device Id\r\n   *   Defaults to 0.\r\n   */\r\n  int device_id;\r\n\r\n  /** \\brief CUDA Convolution algorithm search configuration.\r\n   *   See enum OrtCudnnConvAlgoSearch for more details.\r\n   *   Defaults to OrtCudnnConvAlgoSearchExhaustive.\r\n   */\r\n  OrtCudnnConvAlgoSearch cudnn_conv_algo_search;\r\n\r\n  /** \\brief CUDA memory limit (To use all possible memory pass in maximum size_t)\r\n   *   Defaults to SIZE_MAX.\r\n   *   \\note If a ::OrtArenaCfg has been applied, it will override this field\r\n   */\r\n  size_t gpu_mem_limit;\r\n\r\n  /** \\brief Strategy used to grow the memory arena\r\n   *   0 = kNextPowerOfTwo<br>\r\n   *   1 = kSameAsRequested<br>\r\n   *   Defaults to 0.\r\n   *   \\note If a ::OrtArenaCfg has been applied, it will override this field\r\n   */\r\n  int arena_extend_strategy;\r\n\r\n  /** \\brief Flag indicating if copying needs to take place on the same stream as the compute stream in the CUDA EP\r\n   *   0 = Use separate streams for copying and compute.\r\n   *   1 = Use the same stream for copying and compute.\r\n   *   Defaults to 1.\r\n   *   WARNING: Setting this to 0 may result in data races for some models.\r\n   *   Please see issue #4829 for more details.\r\n   */\r\n  int do_copy_in_default_stream;\r\n\r\n  /** \\brief Flag indicating if there is a user provided compute stream\r\n   *   Defaults to 0.\r\n   */\r\n  int has_user_compute_stream;\r\n\r\n  /** \\brief User provided compute stream.\r\n   *   If provided, please set `has_user_compute_stream` to 1.\r\n   */\r\n  void* user_compute_stream;\r\n\r\n  /** \\brief CUDA memory arena configuration parameters\r\n   */\r\n  OrtArenaCfg* default_memory_arena_cfg;\r\n\r\n  /** \\brief Enable TunableOp for using.\r\n   *   Set it to 1/0 to enable/disable TunableOp. Otherwise, it is disabled by default.\r\n   *   This option can be overridden by environment variable ORT_CUDA_TUNABLE_OP_ENABLE.\r\n   */\r\n  int tunable_op_enable;\r\n\r\n  /** \\brief Enable TunableOp for tuning.\r\n   *   Set it to 1/0 to enable/disable TunableOp tuning. Otherwise, it is disabled by default.\r\n   *   This option can be overridden by environment variable ORT_CUDA_TUNABLE_OP_TUNING_ENABLE.\r\n   */\r\n  int tunable_op_tuning_enable;\r\n\r\n  /** \\brief Max tuning duration time limit for each instance of TunableOp.\r\n   *   Defaults to 0 to disable the limit.\r\n   */\r\n  int tunable_op_max_tuning_duration_ms;\r\n\r\n} OrtCUDAProviderOptions;\r\n\r\n/** \\brief ROCM Provider Options\r\n *\r\n * \\see OrtApi::SessionOptionsAppendExecutionProvider_ROCM\r\n */\r\ntypedef struct OrtROCMProviderOptions {\r\n#ifdef __cplusplus\r\n  OrtROCMProviderOptions()\r\n      : device_id{},\r\n        miopen_conv_exhaustive_search{0},\r\n        gpu_mem_limit{SIZE_MAX},\r\n        arena_extend_strategy{},\r\n        do_copy_in_default_stream{1},\r\n        has_user_compute_stream{},\r\n        user_compute_stream{},\r\n        default_memory_arena_cfg{},\r\n        enable_hip_graph{false},\r\n        tunable_op_enable{false},\r\n        tunable_op_tuning_enable{false},\r\n        tunable_op_max_tuning_duration_ms{} {}\r\n#endif\r\n\r\n  /** \\brief ROCM device Id\r\n   *   Defaults to 0.\r\n   */\r\n  int device_id;\r\n\r\n  /** \\brief ROCM MIOpen Convolution algorithm exhaustive search option.\r\n   *   Defaults to 0 (false).\r\n   */\r\n  int miopen_conv_exhaustive_search;\r\n\r\n  /** \\brief ROCM memory limit (To use all possible memory pass in maximum size_t)\r\n   *   Defaults to SIZE_MAX.\r\n   *   \\note If a ::OrtArenaCfg has been applied, it will override this field\r\n   */\r\n  size_t gpu_mem_limit;\r\n\r\n  /** \\brief Strategy used to grow the memory arena\r\n   *   0 = kNextPowerOfTwo<br>\r\n   *   1 = kSameAsRequested<br>\r\n   *   Defaults to 0.\r\n   *   \\note If a ::OrtArenaCfg has been applied, it will override this field\r\n   */\r\n  int arena_extend_strategy;\r\n\r\n  /** \\brief Flag indicating if copying needs to take place on the same stream as the compute stream in the ROCM EP\r\n   *   0 = Use separate streams for copying and compute.\r\n   *   1 = Use the same stream for copying and compute.\r\n   *   Defaults to 1.\r\n   *   WARNING: Setting this to 0 may result in data races for some models.\r\n   *   Please see issue #4829 for more details.\r\n   */\r\n  int do_copy_in_default_stream;\r\n\r\n  /** \\brief Flag indicating if there is a user provided compute stream\r\n   *   Defaults to 0.\r\n   */\r\n  int has_user_compute_stream;\r\n\r\n  /** \\brief User provided compute stream.\r\n   *   If provided, please set `has_user_compute_stream` to 1.\r\n   */\r\n  void* user_compute_stream;\r\n\r\n  /** \\brief ROCM memory arena configuration parameters\r\n   */\r\n  OrtArenaCfg* default_memory_arena_cfg;\r\n\r\n  int enable_hip_graph;\r\n\r\n  /** \\brief Enable TunableOp for using.\r\n   *   Set it to 1/0 to enable/disable TunableOp. Otherwise, it is disabled by default.\r\n   *   This option can be overridden by environment variable ORT_ROCM_TUNABLE_OP_ENABLE.\r\n   */\r\n  int tunable_op_enable;\r\n\r\n  /** \\brief Enable TunableOp for tuning.\r\n   *   Set it to 1/0 to enable/disable TunableOp tuning. Otherwise, it is disabled by default.\r\n   *   This option can be overridden by environment variable ORT_ROCM_TUNABLE_OP_TUNING_ENABLE.\r\n   */\r\n  int tunable_op_tuning_enable;\r\n\r\n  /** \\brief Max tuning duration time limit for each instance of TunableOp.\r\n   *   Defaults to 0 to disable the limit.\r\n   */\r\n  int tunable_op_max_tuning_duration_ms;\r\n\r\n} OrtROCMProviderOptions;\r\n\r\n/** \\brief TensorRT Provider Options\r\n *\r\n * \\see OrtApi::SessionOptionsAppendExecutionProvider_TensorRT\r\n */\r\ntypedef struct OrtTensorRTProviderOptions {\r\n  int device_id;                                ///< CUDA device id (0 = default device)\r\n  int has_user_compute_stream;                  // indicator of user specified CUDA compute stream.\r\n  void* user_compute_stream;                    // user specified CUDA compute stream.\r\n  int trt_max_partition_iterations;             // maximum iterations for TensorRT parser to get capability\r\n  int trt_min_subgraph_size;                    // minimum size of TensorRT subgraphs\r\n  size_t trt_max_workspace_size;                // maximum workspace size for TensorRT.\r\n  int trt_fp16_enable;                          // enable TensorRT FP16 precision. Default 0 = false, nonzero = true\r\n  int trt_int8_enable;                          // enable TensorRT INT8 precision. Default 0 = false, nonzero = true\r\n  const char* trt_int8_calibration_table_name;  // TensorRT INT8 calibration table name.\r\n  int trt_int8_use_native_calibration_table;    // use native TensorRT generated calibration table. Default 0 = false, nonzero = true\r\n  int trt_dla_enable;                           // enable DLA. Default 0 = false, nonzero = true\r\n  int trt_dla_core;                             // DLA core number. Default 0\r\n  int trt_dump_subgraphs;                       // dump TRT subgraph. Default 0 = false, nonzero = true\r\n  int trt_engine_cache_enable;                  // enable engine caching. Default 0 = false, nonzero = true\r\n  const char* trt_engine_cache_path;            // specify engine cache path\r\n  int trt_engine_decryption_enable;             // enable engine decryption. Default 0 = false, nonzero = true\r\n  const char* trt_engine_decryption_lib_path;   // specify engine decryption library path\r\n  int trt_force_sequential_engine_build;        // force building TensorRT engine sequentially. Default 0 = false, nonzero = true\r\n  // This is the legacy struct and don't add new fields here.\r\n  // For new field that can be represented by string, please add it in include/onnxruntime/core/providers/tensorrt/tensorrt_provider_options.h\r\n  // For non-string field, need to create a new separate api to handle it.\r\n} OrtTensorRTProviderOptions;\r\n\r\n/** \\brief MIGraphX Provider Options\r\n *\r\n * \\see OrtApi::SessionOptionsAppendExecutionProvider_MIGraphX\r\n */\r\ntypedef struct OrtMIGraphXProviderOptions {\r\n  int device_id;                                     // hip device id.\r\n  int migraphx_fp16_enable;                          // MIGraphX FP16 precision. Default 0 = false, nonzero = true\r\n  int migraphx_fp8_enable;                           // MIGraphX FP8 precision. Default 0 = false, nonzero = true\r\n  int migraphx_int8_enable;                          // MIGraphX INT8 precision. Default 0 = false, nonzero = true\r\n  int migraphx_use_native_calibration_table;         // MIGraphx INT8 cal table. Default 0 = false, nonzero = true\r\n  const char* migraphx_int8_calibration_table_name;  // MIGraphx INT8 calibration table name\r\n  int migraphx_save_compiled_model;                  // migraphx save compiled model. Default 0 = false, nonzero = true\r\n  const char* migraphx_save_model_path;              // migraphx model path name\r\n  int migraphx_load_compiled_model;                  // migraphx int8 cal table. Default 0 = false, nonzero = true\r\n  const char* migraphx_load_model_path;              // migraphx model path name\r\n  bool migraphx_exhaustive_tune;                     // MIGraphX tuned compile. Default = false, nonzero = true\r\n\r\n  /** \\brief MIGraphX memory limit (To use all possible memory pass in maximum size_t)\r\n   *   Defaults to SIZE_MAX.\r\n   *   \\note If a ::OrtArenaCfg has been applied, it will override this field\r\n   */\r\n  size_t migraphx_mem_limit;\r\n\r\n  /** \\brief Strategy used to grow the memory arena\r\n   *   0 = kNextPowerOfTwo<br>\r\n   *   1 = kSameAsRequested<br>\r\n   *   Defaults to 0.\r\n   *   \\note If a ::OrtArenaCfg has been applied, it will override this field\r\n   */\r\n  int migraphx_arena_extend_strategy;\r\n\r\n  // This is the legacy struct and don't add new fields here.\r\n} OrtMIGraphXProviderOptions;\r\n\r\n/** \\brief OpenVINO Provider Options\r\n *  \\brief This Struct is frozen since ORT 1.13.0. Its maintained part of Legacy API for compatibility.\r\n *  \\brief For latest OpenVINO Provider Options update to the ProviderOptions map.\r\n *  \\brief Latest OpenVINO Provider Options are listed in the\r\n *  \\htmlonly\r\n *  <a href=\"https://onnxruntime.ai/docs/execution-providers/OpenVINO-ExecutionProvider.html#summary-of-options\">onnxruntime document.</a>\r\n *  \\endhtmlonly\r\n * \\see OrtApi::SessionOptionsAppendExecutionProvider()\r\n */\r\ntypedef struct OrtOpenVINOProviderOptions {\r\n#ifdef __cplusplus\r\n  OrtOpenVINOProviderOptions() : device_type{},\r\n                                 enable_npu_fast_compile{},\r\n                                 device_id{},\r\n                                 num_of_threads{},\r\n                                 cache_dir{},\r\n                                 context{},\r\n                                 enable_opencl_throttling{},\r\n                                 enable_dynamic_shapes{} {}\r\n#endif\r\n  /** \\brief Device type string\r\n   *\r\n   * Valid settings are one of: \"CPU_FP32\", \"CPU_FP16\", \"GPU_FP32\", \"GPU_FP16\"\r\n   */\r\n  const char* device_type;\r\n  unsigned char enable_npu_fast_compile;  ///< 0 = disabled, nonzero = enabled\r\n  const char* device_id;\r\n  size_t num_of_threads;  ///< 0 = Use default number of threads\r\n  const char* cache_dir;  // path is set to empty by default\r\n  void* context;\r\n  unsigned char enable_opencl_throttling;  ///< 0 = disabled, nonzero = enabled\r\n  unsigned char enable_dynamic_shapes;     ///< 0 = disabled, nonzero = enabled\r\n} OrtOpenVINOProviderOptions;\r\n\r\nstruct OrtApi;\r\ntypedef struct OrtApi OrtApi;\r\n\r\nstruct OrtTrainingApi;\r\ntypedef struct OrtTrainingApi OrtTrainingApi;\r\n\r\nstruct OrtModelEditorApi;\r\ntypedef struct OrtModelEditorApi OrtModelEditorApi;\r\n\r\nstruct OrtCompileApi;\r\ntypedef struct OrtCompileApi OrtCompileApi;\r\n\r\nstruct OrtEpApi;\r\ntypedef struct OrtEpApi OrtEpApi;\r\n\r\n/** \\brief The helper interface to get the right version of OrtApi\r\n *\r\n * Get a pointer to this structure through ::OrtGetApiBase\r\n */\r\nstruct OrtApiBase {\r\n  /** \\brief Get a pointer to the requested version of the ::OrtApi\r\n   *\r\n   * \\param[in] version Must be ::ORT_API_VERSION\r\n   * \\return The ::OrtApi for the version requested, nullptr will be returned if this version is unsupported, for example when using a runtime\r\n   *   older than the version created with this header file.\r\n   *\r\n   * One can call GetVersionString() to get the version of the Onnxruntime library for logging\r\n   * and error reporting purposes.\r\n   */\r\n  const OrtApi*(ORT_API_CALL* GetApi)(uint32_t version)NO_EXCEPTION;\r\n\r\n  /** \\brief Returns a null terminated string of the version of the Onnxruntime library (eg: \"1.8.1\")\r\n   *\r\n   *  \\return UTF-8 encoded version string. Do not deallocate the returned buffer.\r\n   */\r\n  const char*(ORT_API_CALL* GetVersionString)(void)NO_EXCEPTION;\r\n};\r\n\r\ntypedef struct OrtApiBase OrtApiBase;\r\n\r\n/** \\brief The Onnxruntime library's entry point to access the C API\r\n *\r\n * Call this to get the a pointer to an ::OrtApiBase\r\n */\r\nORT_EXPORT const OrtApiBase* ORT_API_CALL OrtGetApiBase(void) NO_EXCEPTION;\r\n\r\n/** \\brief Thread work loop function\r\n *\r\n * Onnxruntime will provide the working loop on custom thread creation\r\n * Argument is an onnxruntime built-in type which will be provided when thread pool calls OrtCustomCreateThreadFn\r\n */\r\ntypedef void (*OrtThreadWorkerFn)(void* ort_worker_fn_param);\r\n\r\ntypedef const struct OrtCustomHandleType {\r\n  char __place_holder;\r\n}* OrtCustomThreadHandle;\r\n\r\n/** \\brief Ort custom thread creation function\r\n *\r\n * The function should return a thread handle to be used in onnxruntime thread pools\r\n * Onnxruntime will throw exception on return value of nullptr or 0, indicating that the function failed to create a thread\r\n */\r\ntypedef OrtCustomThreadHandle (*OrtCustomCreateThreadFn)(void* ort_custom_thread_creation_options, OrtThreadWorkerFn ort_thread_worker_fn, void* ort_worker_fn_param);\r\n\r\n/** \\brief Custom thread join function\r\n *\r\n * Onnxruntime thread pool destructor will call the function to join a custom thread.\r\n * Argument ort_custom_thread_handle is the value returned by OrtCustomCreateThreadFn\r\n */\r\ntypedef void (*OrtCustomJoinThreadFn)(OrtCustomThreadHandle ort_custom_thread_handle);\r\n\r\ntypedef OrtStatus*(ORT_API_CALL* RegisterCustomOpsFn)(OrtSessionOptions* options, const OrtApiBase* api);\r\n\r\n/** \\brief Callback function for RunAsync\r\n *\r\n * \\param[in] user_data User specific data that passed back to the callback\r\n * \\param[out] outputs On succeed, outputs host inference results, on error, the value will be nullptr\r\n * \\param[out] num_outputs Number of outputs, on error, the value will be zero\r\n * \\param[out] status On error, status will provide details\r\n */\r\ntypedef void (*RunAsyncCallbackFn)(void* user_data, OrtValue** outputs, size_t num_outputs, OrtStatusPtr status);\r\n\r\n/** \\brief The C API\r\n *\r\n * All C API functions are defined inside this structure as pointers to functions.\r\n * Call OrtApiBase::GetApi to get a pointer to it\r\n *\r\n * \\nosubgrouping\r\n */\r\n/*\r\n * Public enum for compiled model compatibility across EPs.\r\n */\r\ntypedef enum OrtCompiledModelCompatibility {\r\n  OrtCompiledModelCompatibility_EP_NOT_APPLICABLE = 0,\r\n  OrtCompiledModelCompatibility_EP_SUPPORTED_OPTIMAL,\r\n  OrtCompiledModelCompatibility_EP_SUPPORTED_PREFER_RECOMPILATION,\r\n  OrtCompiledModelCompatibility_EP_UNSUPPORTED,\r\n} OrtCompiledModelCompatibility;\r\n\r\nstruct OrtApi {\r\n  /// \\name OrtStatus\r\n  /// @{\r\n\r\n  /**\r\n   * \\brief Create an OrtStatus from a null terminated string\r\n   *\r\n   * \\param[in] code\r\n   * \\param[in] msg A null-terminated string. Its contents will be copied.\r\n   * \\return A new OrtStatus object, must be destroyed with OrtApi::ReleaseStatus\r\n   */\r\n  OrtStatus*(ORT_API_CALL* CreateStatus)(OrtErrorCode code, _In_ const char* msg)NO_EXCEPTION ORT_ALL_ARGS_NONNULL;\r\n\r\n  /** \\brief Get OrtErrorCode from OrtStatus\r\n   *\r\n   * \\param[in] status\r\n   * \\return OrtErrorCode that \\p status was created with\r\n   */\r\n  OrtErrorCode(ORT_API_CALL* GetErrorCode)(_In_ const OrtStatus* status) NO_EXCEPTION ORT_ALL_ARGS_NONNULL;\r\n\r\n  /** \\brief Get error string from OrtStatus\r\n   *\r\n   * \\param[in] status\r\n   * \\return The error message inside the `status`. Do not free the returned value.\r\n   */\r\n  const char*(ORT_API_CALL* GetErrorMessage)(_In_ const OrtStatus* status)NO_EXCEPTION ORT_ALL_ARGS_NONNULL;\r\n\r\n  /// @}\r\n  /// \\name OrtEnv\r\n  /// @{\r\n\r\n  /** \\brief Create an OrtEnv\r\n   *\r\n   * \\note Invoking this function will return the same instance of the environment as that returned by a previous call\r\n   * to another env creation function; all arguments to this function will be ignored.\r\n   * \\param[in] log_severity_level The log severity level.\r\n   * \\param[in] logid The log identifier.\r\n   * \\param[out] out Returned newly created OrtEnv. Must be freed with OrtApi::ReleaseEnv\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateEnv, OrtLoggingLevel log_severity_level, _In_ const char* logid, _Outptr_ OrtEnv** out);\r\n\r\n  /** \\brief Create an OrtEnv\r\n   *\r\n   * \\note Invoking this function will return the same instance of the environment as that returned by a previous call\r\n   * to another env creation function; all arguments to this function will be ignored. If you want to provide your\r\n   * own logging function, consider setting it using the SetUserLoggingFunction API instead.\r\n   * \\param[in] logging_function A pointer to a logging function.\r\n   * \\param[in] logger_param A pointer to arbitrary data passed as the ::OrtLoggingFunction `param` parameter to\r\n   *                         `logging_function`. This parameter is optional.\r\n   * \\param[in] log_severity_level The log severity level.\r\n   * \\param[in] logid The log identifier.\r\n   * \\param[out] out Returned newly created OrtEnv. Must be freed with OrtApi::ReleaseEnv\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateEnvWithCustomLogger, _In_ OrtLoggingFunction logging_function, _In_opt_ void* logger_param,\r\n                  _In_ OrtLoggingLevel log_severity_level, _In_ const char* logid, _Outptr_ OrtEnv** out);\r\n\r\n  /** \\brief Enable Telemetry\r\n   *\r\n   * \\note Telemetry events are on by default since they are lightweight\r\n   * \\param[in] env\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(EnableTelemetryEvents, _In_ const OrtEnv* env);\r\n  /** \\brief Disable Telemetry\r\n   *\r\n   * \\see OrtApi::EnableTelemetryEvents\r\n   * \\param[in] env\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(DisableTelemetryEvents, _In_ const OrtEnv* env);\r\n\r\n  /// @}\r\n  /// \\name OrtSession\r\n  /// @{\r\n\r\n  /** \\brief Create an OrtSession from a model file\r\n   *\r\n   * \\param[in] env\r\n   * \\param[in] model_path\r\n   * \\param[in] options\r\n   * \\param[out] out Returned newly created OrtSession. Must be freed with OrtApi::ReleaseSession\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  // TODO: document the path separator convention? '/' vs '\\'\r\n  // TODO: should specify the access characteristics of model_path. Is this read only during the\r\n  // execution of CreateSession, or does the OrtSession retain a handle to the file/directory\r\n  // and continue to access throughout the OrtSession lifetime?\r\n  //  What sort of access is needed to model_path : read or read/write?\r\n  ORT_API2_STATUS(CreateSession, _In_ const OrtEnv* env, _In_ const ORTCHAR_T* model_path,\r\n                  _In_ const OrtSessionOptions* options, _Outptr_ OrtSession** out);\r\n\r\n  /** \\brief Create an OrtSession from memory\r\n   *\r\n   * \\param[in] env\r\n   * \\param[in] model_data\r\n   * \\param[in] model_data_length\r\n   * \\param[in] options\r\n   * \\param[out] out Returned newly created OrtSession. Must be freed with OrtApi::ReleaseSession\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateSessionFromArray, _In_ const OrtEnv* env,\r\n                  _In_ const void* model_data, size_t model_data_length,\r\n                  _In_ const OrtSessionOptions* options, _Outptr_ OrtSession** out);\r\n\r\n  /** \\brief Run the model in an ::OrtSession\r\n   *\r\n   * Will not return until the model run has completed. Multiple threads might be used to run the model based on\r\n   * the options in the ::OrtSession and settings used when creating the ::OrtEnv\r\n   *\r\n   * \\param[in] session\r\n   * \\param[in] run_options If nullptr, will use a default ::OrtRunOptions\r\n   * \\param[in] input_names Array of null terminated UTF8 encoded strings of the input names\r\n   * \\param[in] inputs Array of ::OrtValue%s of the input values\r\n   * \\param[in] input_len Number of elements in the input_names and inputs arrays\r\n   * \\param[in] output_names Array of null terminated UTF8 encoded strings of the output names\r\n   * \\param[in] output_names_len Number of elements in the output_names and outputs array\r\n   * \\param[out] outputs Array of ::OrtValue%s that the outputs are stored in. This can also be\r\n   *     an array of nullptr values, in this case ::OrtValue objects will be allocated and pointers\r\n   *     to them will be set into the `outputs` array.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(Run, _Inout_ OrtSession* session, _In_opt_ const OrtRunOptions* run_options,\r\n                  _In_reads_(input_len) const char* const* input_names,\r\n                  _In_reads_(input_len) const OrtValue* const* inputs, size_t input_len,\r\n                  _In_reads_(output_names_len) const char* const* output_names, size_t output_names_len,\r\n                  _Inout_updates_all_(output_names_len) OrtValue** outputs);\r\n\r\n  /// @}\r\n  /// \\name OrtSessionOptions\r\n  /// @{\r\n\r\n  /** \\brief Create an ::OrtSessionOptions object\r\n   *\r\n   * To use additional providers, you must build ORT with the extra providers enabled. Then call one of these\r\n   * functions to enable them in the session:<br>\r\n   *   OrtSessionOptionsAppendExecutionProvider_CPU<br>\r\n   *   OrtSessionOptionsAppendExecutionProvider_CUDA<br>\r\n   *   OrtSessionOptionsAppendExecutionProvider_(remaining providers...)<br>\r\n   * The order they are called indicates the preference order as well. In other words call this method\r\n   * on your most preferred execution provider first followed by the less preferred ones.\r\n   * If none are called Ort will use its internal CPU execution provider.\r\n   *\r\n   * \\param[out] options The newly created OrtSessionOptions. Must be freed with OrtApi::ReleaseSessionOptions\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateSessionOptions, _Outptr_ OrtSessionOptions** options);\r\n\r\n  /** \\brief Set filepath to save optimized model after graph level transformations\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] optimized_model_filepath\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetOptimizedModelFilePath, _Inout_ OrtSessionOptions* options,\r\n                  _In_ const ORTCHAR_T* optimized_model_filepath);\r\n\r\n  /** \\brief Create a copy of an existing ::OrtSessionOptions\r\n   *\r\n   * \\param[in] in_options OrtSessionOptions to copy\r\n   * \\param[out] out_options Returned newly created ::OrtSessionOptions. Must be freed with OrtApi::ReleaseSessionOptions\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CloneSessionOptions, _In_ const OrtSessionOptions* in_options,\r\n                  _Outptr_ OrtSessionOptions** out_options);\r\n\r\n  /** \\brief Set execution mode\r\n   *\r\n   * Controls whether you want to execute operators in your graph sequentially or in parallel. Usually when the model\r\n   *  has many branches, setting this option to ExecutionMode.ORT_PARALLEL will give you better performance.\r\n   *  See [docs/ONNX_Runtime_Perf_Tuning.md] for more details.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] execution_mode\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetSessionExecutionMode, _Inout_ OrtSessionOptions* options, ExecutionMode execution_mode);\r\n\r\n  /** \\brief Enable profiling for a session\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] profile_file_prefix\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(EnableProfiling, _Inout_ OrtSessionOptions* options, _In_ const ORTCHAR_T* profile_file_prefix);\r\n\r\n  /** \\brief Disable profiling for a session\r\n   *\r\n   * \\param[in] options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(DisableProfiling, _Inout_ OrtSessionOptions* options);\r\n\r\n  /** \\brief Enable the memory pattern optimization\r\n   *\r\n   * The idea is if the input shapes are the same, we could trace the internal memory allocation\r\n   * and generate a memory pattern for future request. So next time we could just do one allocation\r\n   * with a big chunk for all the internal memory allocation.\r\n   * \\note Memory pattern optimization is only available when Sequential Execution mode is enabled (see OrtApi::SetSessionExecutionMode)\r\n   *\r\n   * \\see OrtApi::DisableMemPattern\r\n   *\r\n   * \\param[in] options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(EnableMemPattern, _Inout_ OrtSessionOptions* options);\r\n\r\n  /** \\brief Disable the memory pattern optimization\r\n   *\r\n   * \\see OrtApi::EnableMemPattern\r\n   *\r\n   * \\param[in] options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(DisableMemPattern, _Inout_ OrtSessionOptions* options);\r\n\r\n  /** \\brief Enable the memory arena on CPU\r\n   *\r\n   * Arena may pre-allocate memory for future usage.\r\n   *\r\n   * \\param[in] options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(EnableCpuMemArena, _Inout_ OrtSessionOptions* options);\r\n\r\n  /** \\brief Disable the memory arena on CPU\r\n   *\r\n   * \\param[in] options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(DisableCpuMemArena, _Inout_ OrtSessionOptions* options);\r\n\r\n  /** \\brief Set session log id\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] logid The log identifier.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetSessionLogId, _Inout_ OrtSessionOptions* options, const char* logid);\r\n\r\n  /** \\brief Set session log verbosity level\r\n   *\r\n   * Applies to session load, initialization, etc\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] session_log_verbosity_level \\snippet{doc} snippets.dox Log Verbosity Level\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetSessionLogVerbosityLevel, _Inout_ OrtSessionOptions* options, int session_log_verbosity_level);\r\n\r\n  /** \\brief Set session log severity level\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] session_log_severity_level The log severity level (refer to ::OrtLoggingLevel for possible values).\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetSessionLogSeverityLevel, _Inout_ OrtSessionOptions* options, int session_log_severity_level);\r\n\r\n  /** \\brief Set the optimization level to apply when loading a graph\r\n   *\r\n   * Please see https://onnxruntime.ai/docs/performance/model-optimizations/graph-optimizations.html for an in-depth explanation\r\n   * \\param[in,out] options The session options object\r\n   * \\param[in] graph_optimization_level The optimization level\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetSessionGraphOptimizationLevel, _Inout_ OrtSessionOptions* options,\r\n                  GraphOptimizationLevel graph_optimization_level);\r\n\r\n  /** \\brief Sets the number of threads used to parallelize the execution within nodes\r\n   *\r\n   * When running a single node operation, ex. add, this sets the maximum number of threads to use.\r\n   *\r\n   * \\note If built with OpenMP, this has no effect on the number of threads used. In this case\r\n   *       use the OpenMP env variables to configure the number of intra op num threads.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] intra_op_num_threads Number of threads to use<br>\r\n   *   A value of 0 will use the default number of threads<br>\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetIntraOpNumThreads, _Inout_ OrtSessionOptions* options, int intra_op_num_threads);\r\n\r\n  /** \\brief Sets the number of threads used to parallelize the execution of the graph\r\n   *\r\n   * If nodes can be run in parallel, this sets the maximum number of threads to use to run them in parallel.\r\n   *\r\n   * \\note If sequential execution is enabled this value is ignored, it acts as if it was set to 1.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] inter_op_num_threads Number of threads to use<br>\r\n   *   A value of 0 will use the default number of threads<br>\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetInterOpNumThreads, _Inout_ OrtSessionOptions* options, int inter_op_num_threads);\r\n\r\n  /// @}\r\n  /// \\name OrtCustomOpDomain\r\n  /// @{\r\n\r\n  /** \\brief Create a custom op domain\r\n   *\r\n   * \\param[in] domain\r\n   * \\param[out] out Newly created domain. Must be freed with OrtApi::ReleaseCustomOpDomain\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateCustomOpDomain, _In_ const char* domain, _Outptr_ OrtCustomOpDomain** out);\r\n\r\n  /** \\brief Add a custom op to a custom op domain\r\n   *\r\n   * \\note The OrtCustomOp* pointer must remain valid until the ::OrtCustomOpDomain using it is released\r\n   *\r\n   * \\param[in] custom_op_domain\r\n   * \\param[in] op\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CustomOpDomain_Add, _Inout_ OrtCustomOpDomain* custom_op_domain, _In_ const OrtCustomOp* op);\r\n\r\n  /// @}\r\n  /// \\name OrtSessionOptions\r\n  /// @{\r\n\r\n  /** \\brief Add custom op domain to a session options\r\n   *\r\n   * \\note The OrtCustomOpDomain* must not be deleted until all sessions using it are released\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] custom_op_domain\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(AddCustomOpDomain, _Inout_ OrtSessionOptions* options, _In_ OrtCustomOpDomain* custom_op_domain);\r\n\r\n  /** \\deprecated Use OrtApi::RegisterCustomOpsLibrary_V2.\r\n   *\r\n   * Registers custom ops from a shared library.\r\n   *\r\n   * Loads a shared library (dll on windows, so on linux, etc) named 'library_path' and looks for this entry point:\r\n   *\t\tOrtStatus* RegisterCustomOps(OrtSessionOptions * options, const OrtApiBase* api);\r\n   * It then passes in the provided session options to this function along with the api base.\r\n   * The handle to the loaded library is returned in library_handle. It can be freed by the caller after all sessions using the passed in\r\n   * session options are destroyed, or if an error occurs and it is non null.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] library_path\r\n   * \\param[out] library_handle OS specific handle to the loaded library (Use FreeLibrary on Windows, dlclose on Linux, etc.. to unload)\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(RegisterCustomOpsLibrary, _Inout_ OrtSessionOptions* options, _In_ const char* library_path, _Outptr_ void** library_handle);\r\n\r\n  /// @}\r\n  /// \\name OrtSession\r\n  /// @{\r\n\r\n  /** \\brief Get input count for a session\r\n   *\r\n   * This number must also match the number of inputs passed to OrtApi::Run\r\n   *\r\n   * \\see OrtApi::SessionGetInputTypeInfo, OrtApi::SessionGetInputName, OrtApi::Session\r\n   *\r\n   * \\param[in] session\r\n   * \\param[out] out Number of inputs\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionGetInputCount, _In_ const OrtSession* session, _Out_ size_t* out);\r\n\r\n  /** \\brief Get output count for a session\r\n   *\r\n   * This number must also match the number of outputs returned by OrtApi::Run\r\n   *\r\n   * \\see OrtApi::SessionGetOutputTypeInfo, OrtApi::SessionGetOutputName, OrtApi::Session\r\n   *\r\n   * \\param[in] session\r\n   * \\param[out] out Number of outputs\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionGetOutputCount, _In_ const OrtSession* session, _Out_ size_t* out);\r\n\r\n  /** \\brief Get overridable initializer count\r\n   *\r\n   * \\see OrtApi::SessionGetOverridableInitializerTypeInfo, OrtApi::SessionGetOverridableInitializerName\r\n   *\r\n   * \\param[in] session\r\n   * \\param[in] out\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionGetOverridableInitializerCount, _In_ const OrtSession* session, _Out_ size_t* out);\r\n\r\n  /** \\brief Get input type information\r\n   *\r\n   * \\param[in] session\r\n   * \\param[in] index Must be between 0 (inclusive) and what OrtApi::SessionGetInputCount returns (exclusive)\r\n   * \\param[out] type_info Must be freed with OrtApi::ReleaseTypeInfo\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionGetInputTypeInfo, _In_ const OrtSession* session, size_t index, _Outptr_ OrtTypeInfo** type_info);\r\n\r\n  /** \\brief Get output type information\r\n   *\r\n   * \\param[in] session\r\n   * \\param[in] index Must be between 0 (inclusive) and what OrtApi::SessionGetOutputCount returns (exclusive)\r\n   * \\param[out] type_info Must be freed with OrtApi::ReleaseTypeInfo\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionGetOutputTypeInfo, _In_ const OrtSession* session, size_t index, _Outptr_ OrtTypeInfo** type_info);\r\n\r\n  /** \\brief Get overridable initializer type information\r\n   *\r\n   * \\param[in] session\r\n   * \\param[in] index Must be between 0 (inclusive) and what OrtApi::SessionGetOverridableInitializerCount returns (exclusive)\r\n   * \\param[out] type_info Must be freed with OrtApi::ReleaseTypeInfo\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionGetOverridableInitializerTypeInfo, _In_ const OrtSession* session, size_t index, _Outptr_ OrtTypeInfo** type_info);\r\n\r\n  /** \\brief Get input name\r\n   *\r\n   * \\param[in] session\r\n   * \\param[in] index Must be between 0 (inclusive) and what OrtApi::SessionGetInputCount returns (exclusive)\r\n   * \\param[in] allocator\r\n   * \\param[out] value Set to a null terminated UTF-8 encoded string allocated using `allocator`. Must be freed using `allocator`.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionGetInputName, _In_ const OrtSession* session, size_t index, _Inout_ OrtAllocator* allocator, _Outptr_ char** value);\r\n\r\n  /** \\brief Get output name\r\n   *\r\n   * \\param[in] session\r\n   * \\param[in] index Must be between 0 (inclusive) and what OrtApi::SessionGetOutputCount returns (exclusive)\r\n   * \\param[in] allocator\r\n   * \\param[out] value Set to a null terminated UTF-8 encoded string allocated using `allocator`. Must be freed using `allocator`.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionGetOutputName, _In_ const OrtSession* session, size_t index, _Inout_ OrtAllocator* allocator, _Outptr_ char** value);\r\n\r\n  /** \\brief Get overridable initializer name\r\n   *\r\n   * \\param[in] session\r\n   * \\param[in] index Must be between 0 (inclusive) and what OrtApi::SessionGetOverridableInitializerCount returns (exclusive)\r\n   * \\param[in] allocator\r\n   * \\param[out] value Set to a null terminated UTF-8 encoded string allocated using `allocator`. Must be freed using `allocator`.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionGetOverridableInitializerName, _In_ const OrtSession* session, size_t index,\r\n                  _Inout_ OrtAllocator* allocator, _Outptr_ char** value);\r\n\r\n  /// @}\r\n  /// \\name OrtRunOptions\r\n  /// @{\r\n\r\n  /** \\brief Create an OrtRunOptions\r\n   *\r\n   * \\param[out] out Returned newly created ::OrtRunOptions. Must be freed with OrtApi::ReleaseRunOptions\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateRunOptions, _Outptr_ OrtRunOptions** out);\r\n\r\n  /** \\brief Set per-run log verbosity level\r\n   *\r\n   * \\see OrtApi::RunOptionsGetRunLogVerbosityLevel\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] log_verbosity_level \\snippet{doc} snippets.dox Log Verbosity Level\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(RunOptionsSetRunLogVerbosityLevel, _Inout_ OrtRunOptions* options, int log_verbosity_level);\r\n\r\n  /** \\brief Set per-run log severity level\r\n   *\r\n   * \\see OrtApi::RunOptionsGetRunLogSeverityLevel\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] log_severity_level The log severity level (refer to ::OrtLoggingLevel for possible values).\r\n   */\r\n  ORT_API2_STATUS(RunOptionsSetRunLogSeverityLevel, _Inout_ OrtRunOptions* options, int log_severity_level);\r\n\r\n  /** \\brief Set per-run tag\r\n   *\r\n   * This is used in a per-run log identifier.\r\n   *\r\n   * \\see OrtApi::RunOptionsGetRunTag\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] run_tag The run tag.\r\n   */\r\n  ORT_API2_STATUS(RunOptionsSetRunTag, _Inout_ OrtRunOptions* options, _In_ const char* run_tag);\r\n\r\n  /** \\brief Get per-run log verbosity level\r\n   *\r\n   * \\see OrtApi::RunOptionsSetRunLogVerbosityLevel\r\n   *\r\n   * \\param[in] options\r\n   * \\param[out] log_verbosity_level \\snippet{doc} snippets.dox Log Verbosity Level\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(RunOptionsGetRunLogVerbosityLevel, _In_ const OrtRunOptions* options,\r\n                  _Out_ int* log_verbosity_level);\r\n\r\n  /** \\brief Get per-run log severity level\r\n   *\r\n   * \\see OrtApi::RunOptionsSetRunLogSeverityLevel\r\n   *\r\n   * \\param[in] options\r\n   * \\param[out] log_severity_level The log severity level (refer to ::OrtLoggingLevel for possible values).\r\n   */\r\n  ORT_API2_STATUS(RunOptionsGetRunLogSeverityLevel, _In_ const OrtRunOptions* options, _Out_ int* log_severity_level);\r\n\r\n  /** \\brief Get per-run tag\r\n   *\r\n   * This is used in a per-run log identifier.\r\n   *\r\n   * \\see OrtApi::RunOptionsSetRunTag\r\n   *\r\n   * \\param[in] options\r\n   * \\param[out] run_tag The run tag.\r\n   *                     Do not free this value, it is owned by `options`. It will be invalidated if the run tag\r\n   *                     changes (i.e., with OrtApi::RunOptionsSetRunTag) or `options` is freed.\r\n   */\r\n  ORT_API2_STATUS(RunOptionsGetRunTag, _In_ const OrtRunOptions* options, _Out_ const char** run_tag);\r\n\r\n  /** \\brief Set terminate flag\r\n   *\r\n   * If a currently executing session needs to be force terminated, this can be called from another thread to force it to fail with an error.\r\n   *\r\n   * \\param[in] options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(RunOptionsSetTerminate, _Inout_ OrtRunOptions* options);\r\n\r\n  /** \\brief Clears the terminate flag\r\n   *\r\n   * Used so the OrtRunOptions instance can be used in a new OrtApi::Run call without it instantly terminating\r\n   *\r\n   * \\param[in] options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(RunOptionsUnsetTerminate, _Inout_ OrtRunOptions* options);\r\n\r\n  /// @}\r\n  /// \\name OrtValue\r\n  /// @{\r\n\r\n  /** \\brief Create a tensor\r\n   *\r\n   * Create a tensor using a supplied ::OrtAllocator\r\n   *\r\n   * \\param[in] allocator\r\n   * \\param[in] shape Pointer to the tensor shape dimensions.\r\n   * \\param[in] shape_len The number of tensor shape dimensions.\r\n   * \\param[in] type\r\n   * \\param[out] out Returns newly created ::OrtValue. Must be freed with OrtApi::ReleaseValue\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateTensorAsOrtValue, _Inout_ OrtAllocator* allocator, _In_ const int64_t* shape, size_t shape_len,\r\n                  ONNXTensorElementDataType type, _Outptr_ OrtValue** out);\r\n\r\n  /** \\brief Create a tensor backed by a user supplied buffer\r\n   *\r\n   * Create a tensor with user's buffer. You can fill the buffer either before calling this function or after.\r\n   * p_data is owned by caller. ReleaseValue won't release p_data.\r\n   *\r\n   * If you wish to transfer ownership of p_data to ORT use CreateTensorWithDataAndDeleterAsOrtValue.\r\n   *\r\n   * \\param[in] info Memory description of where the p_data buffer resides (CPU vs GPU etc).\r\n   * \\param[in] p_data Pointer to the data buffer.\r\n   * \\param[in] p_data_len The number of bytes in the data buffer.\r\n   * \\param[in] shape Pointer to the tensor shape dimensions.\r\n   * \\param[in] shape_len The number of tensor shape dimensions.\r\n   * \\param[in] type The data type.\r\n   * \\param[out] out Returns newly created ::OrtValue. Must be freed with OrtApi::ReleaseValue\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateTensorWithDataAsOrtValue, _In_ const OrtMemoryInfo* info, _Inout_ void* p_data,\r\n                  size_t p_data_len, _In_ const int64_t* shape, size_t shape_len, ONNXTensorElementDataType type,\r\n                  _Outptr_ OrtValue** out);\r\n\r\n  /** \\brief Return if an ::OrtValue is a tensor type\r\n   *\r\n   * \\param[in] value A tensor type (string tensors are not supported)\r\n   * \\param[out] out Set to 1 iff ::OrtValue is a tensor, 0 otherwise\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(IsTensor, _In_ const OrtValue* value, _Out_ int* out);\r\n\r\n  /** \\brief Get a pointer to the raw data inside a tensor\r\n   *\r\n   * Used to read/write/modify the internal tensor data directly.\r\n   * \\note The returned pointer is valid until the \\p value is destroyed.\r\n   *\r\n   * \\param[in] value A tensor type (string tensors are not supported)\r\n   * \\param[out] out Filled in with a pointer to the internal storage\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetTensorMutableData, _In_ OrtValue* value, _Outptr_ void** out);\r\n\r\n  /** \\brief Set all strings at once in a string tensor\r\n   *\r\n   * \\param[in,out] value A tensor of type ONNX_TENSOR_ELEMENT_DATA_TYPE_STRING\r\n   * \\param[in] s An array of strings. Each string in this array must be null terminated.\r\n   * \\param[in] s_len Count of strings in s (Must match the size of \\p value's tensor shape)\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(FillStringTensor, _Inout_ OrtValue* value, _In_ const char* const* s, size_t s_len);\r\n\r\n  /** \\brief Get total byte length for all strings in a string tensor\r\n   *\r\n   * Typically used with OrtApi::GetStringTensorContent\r\n   *\r\n   * \\param[in] value A tensor of type ONNX_TENSOR_ELEMENT_DATA_TYPE_STRING\r\n   * \\param[out] len Total byte length of all strings (does not include trailing nulls)\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetStringTensorDataLength, _In_ const OrtValue* value, _Out_ size_t* len);\r\n\r\n  /** \\brief Get all strings from a string tensor\r\n   *\r\n   * An example of the results:<br>\r\n   * Given \\p value is a string tensor with the strings { \"This\" \"is\" \"a\" \"test\" }<br>\r\n   * \\p s must have a size of 11 bytes<br>\r\n   * \\p offsets must have 4 elements<br>\r\n   * After the call, these values will be filled in:<br>\r\n   * \\p s will contain \"Thisisatest\"<br>\r\n   * \\p offsets will contain { 0, 4, 6, 7 }<br>\r\n   * The length of the last string is just s_len - offsets[last]\r\n   *\r\n   * \\param[in] value A tensor of type ONNX_TENSOR_ELEMENT_DATA_TYPE_STRING\r\n   * \\param[in] s Buffer to sequentially write all tensor strings to. Each string is NOT null-terminated.\r\n   * \\param[in] s_len Number of bytes of buffer pointed to by \\p s (Get it from OrtApi::GetStringTensorDataLength)\r\n   * \\param[out] offsets Array of start offsets into the strings written to \\p s\r\n   * \\param[in] offsets_len Number of elements in offsets\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetStringTensorContent, _In_ const OrtValue* value, _Out_writes_bytes_all_(s_len) void* s,\r\n                  size_t s_len, _Out_writes_all_(offsets_len) size_t* offsets, size_t offsets_len);\r\n\r\n  /// @}\r\n  /// \\name OrtTypeInfo\r\n  /// @{\r\n\r\n  /** \\brief Get ::OrtTensorTypeAndShapeInfo from an ::OrtTypeInfo\r\n   *\r\n   * \\param[in] type_info\r\n   * \\param[out] out Do not free this value, it will be valid until type_info is freed.\r\n   *             If type_info does not represent tensor, this value will be set to nullptr.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CastTypeInfoToTensorInfo, _In_ const OrtTypeInfo* type_info,\r\n                  _Outptr_result_maybenull_ const OrtTensorTypeAndShapeInfo** out);\r\n\r\n  /** \\brief Get ::ONNXType from ::OrtTypeInfo\r\n   *\r\n   * \\param[in] type_info\r\n   * \\param[out] out\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetOnnxTypeFromTypeInfo, _In_ const OrtTypeInfo* type_info, _Out_ enum ONNXType* out);\r\n\r\n  /// @}\r\n  /// \\name OrtTensorTypeAndShapeInfo\r\n  /// @{\r\n\r\n  /** \\brief Create an ::OrtTensorTypeAndShapeInfo object\r\n   *\r\n   * \\param[out] out Returns newly created ::OrtTensorTypeAndShapeInfo. Must be freed with OrtApi::ReleaseTensorTypeAndShapeInfo\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateTensorTypeAndShapeInfo, _Outptr_ OrtTensorTypeAndShapeInfo** out);\r\n\r\n  /** \\brief Set element type in ::OrtTensorTypeAndShapeInfo\r\n   *\r\n   * \\param[in] info\r\n   * \\param[in] type\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetTensorElementType, _Inout_ OrtTensorTypeAndShapeInfo* info, enum ONNXTensorElementDataType type);\r\n\r\n  /** \\brief Set shape information in ::OrtTensorTypeAndShapeInfo\r\n   *\r\n   * \\param[in] info\r\n   * \\param[in] dim_values Array with `dim_count` elements. Can contain negative values.\r\n   * \\param[in] dim_count Number of elements in `dim_values`\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetDimensions, OrtTensorTypeAndShapeInfo* info, _In_ const int64_t* dim_values, size_t dim_count);\r\n\r\n  /** \\brief Get element type in ::OrtTensorTypeAndShapeInfo\r\n   *\r\n   * \\see OrtApi::SetTensorElementType\r\n   *\r\n   * \\param[in] info\r\n   * \\param[out] out\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetTensorElementType, _In_ const OrtTensorTypeAndShapeInfo* info,\r\n                  _Out_ enum ONNXTensorElementDataType* out);\r\n\r\n  /** \\brief Get dimension count in ::OrtTensorTypeAndShapeInfo\r\n   *\r\n   * \\see OrtApi::GetDimensions\r\n   *\r\n   * \\param[in] info\r\n   * \\param[out] out\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetDimensionsCount, _In_ const OrtTensorTypeAndShapeInfo* info, _Out_ size_t* out);\r\n\r\n  /** \\brief Get dimensions in ::OrtTensorTypeAndShapeInfo\r\n   *\r\n   * \\param[in] info\r\n   * \\param[out] dim_values Array with `dim_values_length` elements. On return, filled with the dimensions stored in the ::OrtTensorTypeAndShapeInfo\r\n   * \\param[in] dim_values_length Number of elements in `dim_values`. Use OrtApi::GetDimensionsCount to get this value\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetDimensions, _In_ const OrtTensorTypeAndShapeInfo* info, _Out_ int64_t* dim_values,\r\n                  size_t dim_values_length);\r\n\r\n  /** \\brief Get symbolic dimension names in ::OrtTensorTypeAndShapeInfo\r\n   *\r\n   * \\param[in] info\r\n   * \\param[in] dim_params Array with `dim_params_length` elements. On return filled with pointers to null terminated strings of the dimension names\r\n   * \\param[in] dim_params_length Number of elements in `dim_params`. Use OrtApi::GetDimensionsCount to get this value\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetSymbolicDimensions, _In_ const OrtTensorTypeAndShapeInfo* info,\r\n                  _Out_writes_all_(dim_params_length) const char* dim_params[], size_t dim_params_length);\r\n\r\n  /** \\brief Get total number of elements in a tensor shape from an ::OrtTensorTypeAndShapeInfo\r\n   *\r\n   * Return the number of elements specified by the tensor shape (all dimensions multiplied by each other).\r\n   * For 0 dimensions, 1 is returned. If any dimension is less than 0, the result is always -1.\r\n   *\r\n   * Examples:<br>\r\n   * [] = 1<br>\r\n   * [1,3,4] = 12<br>\r\n   * [2,0,4] = 0<br>\r\n   * [-1,3,4] = -1<br>\r\n   *\r\n   * \\param[in] info\r\n   * \\param[out] out Number of elements\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetTensorShapeElementCount, _In_ const OrtTensorTypeAndShapeInfo* info, _Out_ size_t* out);\r\n\r\n  /// @}\r\n  /// \\name OrtValue\r\n  /// @{\r\n\r\n  /** \\brief Get type and shape information from a tensor ::OrtValue\r\n   *\r\n   * \\param[in] value Must be a tensor (not a map/sequence/etc) or will return failure\r\n   * \\param[out] out Newly created ::OrtTensorTypeAndShapeInfo. Must be freed with OrtApi::ReleaseTensorTypeAndShapeInfo\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetTensorTypeAndShape, _In_ const OrtValue* value, _Outptr_ OrtTensorTypeAndShapeInfo** out);\r\n\r\n  /** \\brief Get type information of an OrtValue\r\n   *\r\n   * \\param[in] value\r\n   * \\param[out] out Newly created ::OrtTypeInfo. Must be freed with OrtApi::ReleaseTypeInfo\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetTypeInfo, _In_ const OrtValue* value, _Outptr_result_maybenull_ OrtTypeInfo** out);\r\n\r\n  /** \\brief Get ONNXType of an ::OrtValue\r\n   *\r\n   * \\param[in] value\r\n   * \\param[out] out\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetValueType, _In_ const OrtValue* value, _Out_ enum ONNXType* out);\r\n\r\n  /// @}\r\n  /// \\name OrtMemoryInfo\r\n  /// @{\r\n\r\n  /** \\brief Create an ::OrtMemoryInfo\r\n   *\r\n   * \\param[in] name\r\n   * \\param[in] type\r\n   * \\param[in] id\r\n   * \\param[in] mem_type\r\n   * \\param[out] out Newly created ::OrtMemoryInfo. Must be freed with OrtAPi::ReleaseMemoryInfo\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateMemoryInfo, _In_ const char* name, enum OrtAllocatorType type, int id,\r\n                  enum OrtMemType mem_type, _Outptr_ OrtMemoryInfo** out);\r\n\r\n  /** \\brief Create an ::OrtMemoryInfo for CPU memory\r\n   *\r\n   * Special case version of OrtApi::CreateMemoryInfo for CPU based memory. Same as using OrtApi::CreateMemoryInfo with name = \"Cpu\" and id = 0.\r\n   *\r\n   * \\param[in] type\r\n   * \\param[in] mem_type\r\n   * \\param[out] out\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateCpuMemoryInfo, enum OrtAllocatorType type, enum OrtMemType mem_type,\r\n                  _Outptr_ OrtMemoryInfo** out);\r\n\r\n  /** \\brief Compare ::OrtMemoryInfo objects for equality\r\n   *\r\n   * Compares all settings of each ::OrtMemoryInfo for equality\r\n   *\r\n   * \\param[in] info1\r\n   * \\param[in] info2\r\n   * \\param[out] out Set to 0 if equal, -1 if not equal\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CompareMemoryInfo, _In_ const OrtMemoryInfo* info1, _In_ const OrtMemoryInfo* info2, _Out_ int* out);\r\n\r\n  /** \\brief Get name from ::OrtMemoryInfo\r\n   *\r\n   * \\param[in] ptr\r\n   * \\param[out] out Writes null terminated string to this pointer. Do NOT free the returned pointer. It is valid for the lifetime of the ::OrtMemoryInfo\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(MemoryInfoGetName, _In_ const OrtMemoryInfo* ptr, _Out_ const char** out);\r\n\r\n  /** \\brief Get the device id from ::OrtMemoryInfo\r\n   */\r\n  ORT_API2_STATUS(MemoryInfoGetId, _In_ const OrtMemoryInfo* ptr, _Out_ int* out);\r\n\r\n  /** \\brief Get the ::OrtMemType from ::OrtMemoryInfo\r\n   */\r\n  ORT_API2_STATUS(MemoryInfoGetMemType, _In_ const OrtMemoryInfo* ptr, _Out_ OrtMemType* out);\r\n\r\n  /** \\brief Get the ::OrtAllocatorType from ::OrtMemoryInfo\r\n   */\r\n  ORT_API2_STATUS(MemoryInfoGetType, _In_ const OrtMemoryInfo* ptr, _Out_ OrtAllocatorType* out);\r\n\r\n  /// @}\r\n  /// \\name OrtAllocator\r\n  /// @{\r\n\r\n  /// \\brief Calls OrtAllocator::Alloc function\r\n  ORT_API2_STATUS(AllocatorAlloc, _Inout_ OrtAllocator* ort_allocator, size_t size, _Outptr_ void** out);\r\n  /// \\brief Calls OrtAllocator::Free function\r\n  ORT_API2_STATUS(AllocatorFree, _Inout_ OrtAllocator* ort_allocator, void* p);\r\n  /// \\brief Calls OrtAllocator::Info function\r\n  ORT_API2_STATUS(AllocatorGetInfo, _In_ const OrtAllocator* ort_allocator, _Outptr_ const struct OrtMemoryInfo** out);\r\n\r\n  /** \\brief Get the default allocator\r\n   *\r\n   * The default allocator is a CPU based, non-arena. Always returns the same pointer to the same default allocator.\r\n   *\r\n   * \\param[out] out Returned value should NOT be freed\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetAllocatorWithDefaultOptions, _Outptr_ OrtAllocator** out);\r\n\r\n  /// @}\r\n  /// \\name OrtSessionOptions\r\n  /// @{\r\n\r\n  /** \\brief Override session symbolic dimensions\r\n   *\r\n   * Override symbolic dimensions (by specific denotation strings) with actual values if known at session initialization time to enable\r\n   * optimizations that can take advantage of fixed values (such as memory planning, etc)\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] dim_denotation\r\n   * \\param[in] dim_value\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(AddFreeDimensionOverride, _Inout_ OrtSessionOptions* options, _In_ const char* dim_denotation,\r\n                  _In_ int64_t dim_value);\r\n\r\n  /// @}\r\n  /// \\name OrtValue\r\n  /// @{\r\n\r\n  /* Internal information (not seen in Doxygen)\r\n   *\r\n   * APIs to support non-tensor types - map and sequence.\r\n   * Currently only the following types are supported\r\n   * Note: the following types should be kept in sync with data_types.h\r\n   * Map types\r\n   * =========\r\n   * std::map<std::string, std::string>\r\n   * std::map<std::string, int64_t>\r\n   * std::map<std::string, float>\r\n   * std::map<std::string, double>\r\n   * std::map<int64_t, std::string>\r\n   * std::map<int64_t, int64_t>\r\n   * std::map<int64_t, float>\r\n   * std::map<int64_t, double>\r\n   *\r\n   * Sequence types\r\n   * ==============\r\n   * std::vector<std::string>\r\n   * std::vector<int64_t>\r\n   * std::vector<float>\r\n   * std::vector<double>\r\n   * std::vector<std::map<std::string, float>>\r\n   * std::vector<std::map<int64_t, float>\r\n   */\r\n\r\n  /** \\brief Get non tensor data from an ::OrtValue\r\n   *\r\n   * If `value` is of type ONNX_TYPE_MAP, you need to retrieve the keys and values\r\n   * separately. Use index=0 to retrieve keys and index=1 to retrieve values.\r\n   * If `value` is of type ONNX_TYPE_SEQUENCE, use index to retrieve the index'th element\r\n   * of the sequence.\r\n   *\r\n   * \\param[in] value\r\n   * \\param[in] index See above for usage based on `value` type\r\n   * \\param[in] allocator Allocator used to allocate ::OrtValue\r\n   * \\param[out] out Created ::OrtValue that holds the element requested. Must be freed with OrtApi::ReleaseValue\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetValue, _In_ const OrtValue* value, int index, _Inout_ OrtAllocator* allocator,\r\n                  _Outptr_ OrtValue** out);\r\n\r\n  /** \\brief Get non tensor value count from an ::OrtValue\r\n   *\r\n   * If `value` is of type ONNX_TYPE_MAP 2 will always be returned. For ONNX_TYPE_SEQUENCE\r\n   * the number of elements in the sequence will be returned\r\n   *\r\n   * \\param[in] value\r\n   * \\param[out] out\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetValueCount, _In_ const OrtValue* value, _Out_ size_t* out);\r\n\r\n  /** \\brief Create a map or sequence ::OrtValue\r\n   *\r\n   * To construct a map (ONNX_TYPE_MAP), use num_values = 2 and `in` should be an array of 2 ::OrtValue%s\r\n   * representing keys and values.<br>\r\n   *\r\n   * To construct a sequence (ONNX_TYPE_SEQUENCE), use num_values = N where N is the number of the elements in the\r\n   * sequence. 'in' should be an array of N ::OrtValue%s.\r\n   *\r\n   * \\param[in] in See above for details\r\n   * \\param[in] num_values\r\n   * \\param[in] value_type Must be either ONNX_TYPE_MAP or ONNX_TYPE_SEQUENCE\r\n   * \\param[out] out Newly created ::OrtValue. Must be freed with OrtApi::ReleaseValue\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateValue, _In_reads_(num_values) const OrtValue* const* in, size_t num_values,\r\n                  enum ONNXType value_type, _Outptr_ OrtValue** out);\r\n\r\n  /** \\brief Create an opaque (custom user defined type) ::OrtValue\r\n   *\r\n   * Constructs an ::OrtValue that contains a value of non-standard type created for\r\n   * experiments or while awaiting standardization. ::OrtValue in this case would contain\r\n   * an internal representation of the Opaque type. Opaque types are distinguished from\r\n   * each other by two strings 1) domain and 2) type name. The combination of the two\r\n   * must be unique, so the type representation is properly identified internally. The combination\r\n   * must be properly registered from within ORT at both compile/run time or by another API.\r\n   *\r\n   * To construct the ::OrtValue pass domain and type names, also a pointer to a data container\r\n   * the type of which must be known to both ORT and the client program. That data container may or may\r\n   * not match the internal representation of the Opaque type. The sizeof(data_container) is passed for\r\n   * verification purposes.\r\n   *\r\n   * \\param[in] domain_name Null terminated string of the domain name\r\n   * \\param[in] type_name Null terminated string of the type name\r\n   * \\param[in] data_container User pointer Data to populate ::OrtValue\r\n   * \\param[in] data_container_size Size in bytes of what `data_container` points to\r\n   * \\param[out] out Newly created ::OrtValue. Must be freed with OrtApi::ReleaseValue\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateOpaqueValue, _In_z_ const char* domain_name, _In_z_ const char* type_name,\r\n                  _In_ const void* data_container, size_t data_container_size, _Outptr_ OrtValue** out);\r\n\r\n  /** \\brief Get internal data from an opaque (custom user defined type) ::OrtValue\r\n   *\r\n   * Copies internal data from an opaque value into a user provided buffer\r\n   *\r\n   * \\see OrtApi::CreateOpaqueValue\r\n   *\r\n   * \\param[in] domain_name Null terminated string of the domain name\r\n   * \\param[in] type_name Null terminated string of the type name\r\n   * \\param[in] in The opaque ::OrtValue\r\n   * \\param[out] data_container Buffer to copy data into\r\n   * \\param[out] data_container_size Size in bytes of the buffer pointed to by data_container. Must match the size of the internal buffer.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetOpaqueValue, _In_ const char* domain_name, _In_ const char* type_name, _In_ const OrtValue* in,\r\n                  _Out_ void* data_container, size_t data_container_size);\r\n\r\n  /// @}\r\n  /// \\name OrtKernelInfo\r\n  /// Custom operator APIs.\r\n  /// @{\r\n\r\n  /** \\brief Get a float stored as an attribute in the graph node\r\n   *\r\n   * \\param[in] info ::OrtKernelInfo instance\r\n   * \\param[in] name Null terminated string of the name of the attribute\r\n   * \\param[out] out Pointer to memory where the attribute will be stored\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(KernelInfoGetAttribute_float, _In_ const OrtKernelInfo* info, _In_ const char* name,\r\n                  _Out_ float* out);\r\n\r\n  /** \\brief Fetch a 64-bit int stored as an attribute in the graph node\r\n   *\r\n   * \\param[in] info ::OrtKernelInfo instance\r\n   * \\param[in] name Null terminated string of the name of the attribute\r\n   * \\param[out] out Pointer to memory where the attribute will be stored\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(KernelInfoGetAttribute_int64, _In_ const OrtKernelInfo* info, _In_ const char* name,\r\n                  _Out_ int64_t* out);\r\n\r\n  /** \\brief Fetch a string stored as an attribute in the graph node\r\n   *\r\n   * If `out` is nullptr, the value of `size` is set to the true size of the string\r\n   * attribute, and a success status is returned.\r\n   *\r\n   * If the `size` parameter is greater than or equal to the actual string attribute's size,\r\n   * the value of `size` is set to the true size of the string attribute, the provided memory\r\n   * is filled with the attribute's contents, and a success status is returned.\r\n   *\r\n   * If the `size` parameter is less than the actual string attribute's size and `out`\r\n   * is not nullptr, the value of `size` is set to the true size of the string attribute\r\n   * and a failure status is returned.)\r\n   *\r\n   * \\param[in] info ::OrtKernelInfo instance\r\n   * \\param[in] name Null terminated string of the name of the attribute\r\n   * \\param[out] out Pointer to memory where the attribute will be stored\r\n   * \\param[in,out] size See above comments for details\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(KernelInfoGetAttribute_string, _In_ const OrtKernelInfo* info, _In_ const char* name, _Out_ char* out,\r\n                  _Inout_ size_t* size);\r\n\r\n  /// @}\r\n  /// \\name OrtKernelContext\r\n  /// Custom operator APIs.\r\n  /// @{\r\n\r\n  /** \\brief Used for custom operators, get the input count of a kernel\r\n   *\r\n   * \\see ::OrtCustomOp\r\n   */\r\n  ORT_API2_STATUS(KernelContext_GetInputCount, _In_ const OrtKernelContext* context, _Out_ size_t* out);\r\n\r\n  /** \\brief Used for custom operators, get the output count of a kernel\r\n   *\r\n   * \\see ::OrtCustomOp\r\n   */\r\n  ORT_API2_STATUS(KernelContext_GetOutputCount, _In_ const OrtKernelContext* context, _Out_ size_t* out);\r\n\r\n  /** \\brief Used for custom operators, get an input of a kernel\r\n   *\r\n   * The function attempts fetches the input of the kernel. If the input is optional\r\n   * and not present, the function returns success and out is set to nullptr.\r\n   *\r\n   * \\param[in] context ::OrtKernelContext instance\r\n   * \\param[in] index See KernelContext_GetInputCount for boundaries check.\r\n   * \\param[out] out OrtValue if the input is present otherwise is set nullptr\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(KernelContext_GetInput, _In_ const OrtKernelContext* context, _In_ size_t index,\r\n                  _Out_ const OrtValue** out);\r\n\r\n  /** \\brief Used for custom operators, get an output of a kernel\r\n   *\r\n   * The function attempts fetches the output of the kernel. If the output is optional\r\n   * and not present, the function returns success and out is set to nullptr.\r\n   *\r\n   * \\param[in] context ::OrtKernelContext instance\r\n   * \\param[in] index See KernelContext_GetOutputCount for boundaries check.\r\n   * \\param[in] dim_values output dimensions\r\n   * \\param[in] dim_count number of dimensions\r\n   * \\param[out] out a ptr to OrtValue to output otherwise set to nullptr\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(KernelContext_GetOutput, _Inout_ OrtKernelContext* context, _In_ size_t index,\r\n                  _In_ const int64_t* dim_values, size_t dim_count, _Outptr_ OrtValue** out);\r\n\r\n  /// @}\r\n  /// \\name OrtEnv\r\n  /// @{\r\n  ORT_CLASS_RELEASE(Env);\r\n  /// @}\r\n  /// \\name OrtStatus\r\n  /// @{\r\n  ORT_CLASS_RELEASE(Status);\r\n  /// @}\r\n  /// \\name OrtMemoryInfo\r\n  /// @{\r\n  ORT_CLASS_RELEASE(MemoryInfo);\r\n  /// @}\r\n  /// \\name OrtSession\r\n  /// @{\r\n  ORT_CLASS_RELEASE(Session);  // Don't call ReleaseSession from Dllmain (because session owns a thread pool)\r\n  /// @}\r\n  /// \\name OrtValue\r\n  /// @{\r\n  ORT_CLASS_RELEASE(Value);\r\n  /// @}\r\n  /// \\name OrtRunOptions\r\n  /// @{\r\n  ORT_CLASS_RELEASE(RunOptions);\r\n  /// @}\r\n  /// \\name OrtTypeInfo\r\n  /// @{\r\n  ORT_CLASS_RELEASE(TypeInfo);\r\n  /// @}\r\n  /// \\name OrtTensorTypeAndShapeInfo\r\n  /// @{\r\n  ORT_CLASS_RELEASE(TensorTypeAndShapeInfo);\r\n  /// @}\r\n  /// \\name OrtSessionOptions\r\n  /// @{\r\n  ORT_CLASS_RELEASE(SessionOptions);\r\n  /// @}\r\n  /// \\name OrtCustomOpDomain\r\n  /// @{\r\n  ORT_CLASS_RELEASE(CustomOpDomain);\r\n\r\n  /// @}\r\n  /// \\name OrtTypeInfo\r\n  /// @{\r\n\r\n  /** \\brief Get denotation from type information\r\n   *\r\n   * Augments ::OrtTypeInfo to return denotations on the type.\r\n   *\r\n   * This is used by WinML to determine if an input/output is intended to be an Image or a Tensor.\r\n   *\r\n   * \\param[in] type_info\r\n   * \\param[out] denotation Pointer to the null terminated denotation string is written to this pointer. This pointer is valid until the object is destroyed or the name is changed, do not free.\r\n   * \\param[out] len Length in bytes of the string returned in `denotation`\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetDenotationFromTypeInfo, _In_ const OrtTypeInfo* type_info, _Out_ const char** const denotation,\r\n                  _Out_ size_t* len);\r\n\r\n  /** \\brief Get detailed map information from an ::OrtTypeInfo\r\n   *\r\n   * This augments ::OrtTypeInfo to return an ::OrtMapTypeInfo when the type is a map.\r\n   * The OrtMapTypeInfo has additional information about the map's key type and value type.\r\n   *\r\n   * This is used by WinML to support model reflection APIs.\r\n   *\r\n   * \\param[out] type_info\r\n   * \\param[out] out A pointer to the ::OrtMapTypeInfo. Do not free this value. If type_info\r\n   *             does not contain a map, this value will be set to nullptr.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CastTypeInfoToMapTypeInfo, _In_ const OrtTypeInfo* type_info,\r\n                  _Outptr_result_maybenull_ const OrtMapTypeInfo** out);\r\n\r\n  /** \\brief Cast ::OrtTypeInfo to an ::OrtSequenceTypeInfo\r\n   *\r\n   * This api augments ::OrtTypeInfo to return an ::OrtSequenceTypeInfo when the type is a sequence.\r\n   * The ::OrtSequenceTypeInfo has additional information about the sequence's element type.\r\n   *\r\n   * This is used by WinML to support model reflection APIs.\r\n   *\r\n   * \\param[in] type_info\r\n   * \\param[out] out A pointer to the OrtSequenceTypeInfo. Do not free this value. If type_info\r\n   *             doesn not contain a sequence, this value will be set to nullptr.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CastTypeInfoToSequenceTypeInfo, _In_ const OrtTypeInfo* type_info,\r\n                  _Outptr_result_maybenull_ const OrtSequenceTypeInfo** out);\r\n\r\n  /// @}\r\n  /// \\name OrtMapTypeInfo\r\n  /// @{\r\n\r\n  /** \\brief Get key type from an ::OrtMapTypeInfo\r\n   *\r\n   * Key types are restricted to being scalar types.\r\n   *\r\n   * This is used by WinML to support model reflection APIs.\r\n   *\r\n   * \\param[in] map_type_info\r\n   * \\param[out] out\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetMapKeyType, _In_ const OrtMapTypeInfo* map_type_info, _Out_ enum ONNXTensorElementDataType* out);\r\n\r\n  /** \\brief Get the value type from an ::OrtMapTypeInfo\r\n   *\r\n   * \\param[in] map_type_info\r\n   * \\param[out] type_info A copy of the OrtTypeInfo for the map value type.\r\n   *                       The user must free this value with ReleaseTypeInfo.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetMapValueType, _In_ const OrtMapTypeInfo* map_type_info, _Outptr_ OrtTypeInfo** type_info);\r\n\r\n  /// @}\r\n  /// \\name OrtSequenceTypeInfo\r\n  /// @{\r\n\r\n  /** \\brief Get element type from an ::OrtSequenceTypeInfo\r\n   *\r\n   * This is used by WinML to support model reflection APIs.\r\n   *\r\n   * \\param[in] sequence_type_info\r\n   * \\param[out] type_info A copy of the OrtTypeInfo for the sequence element type.\r\n   *                       The user must free this value with ReleaseTypeInfo.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetSequenceElementType, _In_ const OrtSequenceTypeInfo* sequence_type_info,\r\n                  _Outptr_ OrtTypeInfo** type_info);\r\n\r\n  /// @}\r\n  /// \\name OrtMapTypeInfo\r\n  /// @{\r\n  ORT_CLASS_RELEASE(MapTypeInfo);\r\n  /// @}\r\n  /// \\name OrtSequenceTypeInfo\r\n  /// @{\r\n  ORT_CLASS_RELEASE(SequenceTypeInfo);\r\n\r\n  /// @}\r\n  /// \\name OrtSession\r\n  /// @{\r\n\r\n  /** \\brief End profiling and return filename of the profile data\r\n   *\r\n   * Profiling is turned on through OrtApi::EnableProfiling\r\n   *\r\n   * \\param[in] session\r\n   * \\param[in] allocator\r\n   * \\param[out] out Null terminated string of the filename, allocated using `allocator`. Must be freed using `allocator`\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionEndProfiling, _In_ OrtSession* session, _Inout_ OrtAllocator* allocator, _Outptr_ char** out);\r\n\r\n  /** \\brief Get ::OrtModelMetadata from an ::OrtSession\r\n   *\r\n   * \\param[in] session\r\n   * \\param[out] out Newly created ::OrtModelMetadata. Must be freed using OrtApi::ReleaseModelMetadata\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionGetModelMetadata, _In_ const OrtSession* session, _Outptr_ OrtModelMetadata** out);\r\n\r\n  /// @}\r\n  /// \\name OrtModelMetadata\r\n  /// @{\r\n\r\n  /** \\brief Get `producer name` from an ::OrtModelMetadata\r\n   *\r\n   * \\param[in] model_metadata\r\n   * \\param[in] allocator\r\n   * \\param[out] value Set to a null terminated string allocated using `allocator`. Must be freed using `allocator`\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(ModelMetadataGetProducerName, _In_ const OrtModelMetadata* model_metadata,\r\n                  _Inout_ OrtAllocator* allocator, _Outptr_ char** value);\r\n\r\n  /** \\brief Get `graph name` from an ::OrtModelMetadata\r\n   *\r\n   * \\param[in] model_metadata\r\n   * \\param[in] allocator\r\n   * \\param[out] value Set to a null terminated string allocated using `allocator`. Must be freed using `allocator`\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(ModelMetadataGetGraphName, _In_ const OrtModelMetadata* model_metadata,\r\n                  _Inout_ OrtAllocator* allocator, _Outptr_ char** value);\r\n\r\n  /** \\brief Get `domain` from an ::OrtModelMetadata\r\n   *\r\n   * \\param[in] model_metadata\r\n   * \\param[in] allocator\r\n   * \\param[out] value Set to a null terminated string allocated using `allocator`. Must be freed using `allocator`\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(ModelMetadataGetDomain, _In_ const OrtModelMetadata* model_metadata, _Inout_ OrtAllocator* allocator,\r\n                  _Outptr_ char** value);\r\n\r\n  /** \\brief Get `description` from an ::OrtModelMetadata\r\n   *\r\n   * \\param[in] model_metadata\r\n   * \\param[in] allocator\r\n   * \\param[out] value Set to a null terminated string allocated using `allocator`. Must be freed using `allocator`\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(ModelMetadataGetDescription, _In_ const OrtModelMetadata* model_metadata,\r\n                  _Inout_ OrtAllocator* allocator, _Outptr_ char** value);\r\n\r\n  /** \\brief Return data for a key in the custom metadata map in an ::OrtModelMetadata\r\n   *\r\n   * \\param[in] model_metadata\r\n   * \\param[in] allocator\r\n   * \\param[in] key Null terminated string\r\n   * \\param[out] value Set to a null terminated string allocated using `allocator`. Must be freed using `allocator`\r\n   * `value` will be set to nullptr if the given key is not found in the custom metadata map.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(ModelMetadataLookupCustomMetadataMap, _In_ const OrtModelMetadata* model_metadata,\r\n                  _Inout_ OrtAllocator* allocator, _In_ const char* key, _Outptr_result_maybenull_ char** value);\r\n\r\n  /** \\brief Get version number from an ::OrtModelMetadata\r\n   *\r\n   * \\param[in] model_metadata\r\n   * \\param[out] value Set to the version number\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(ModelMetadataGetVersion, _In_ const OrtModelMetadata* model_metadata, _Out_ int64_t* value);\r\n\r\n  ORT_CLASS_RELEASE(ModelMetadata);\r\n\r\n  /// @}\r\n  /// \\name OrtEnv\r\n  /// @{\r\n\r\n  /** \\brief Create an OrtEnv\r\n   *\r\n   * Create an environment with global threadpools that will be shared across sessions.\r\n   * Use this in conjunction with OrtApi::DisablePerSessionThreads or else the session will use\r\n   * its own thread pools.\r\n   *\r\n   * \\param[in] log_severity_level The log severity level.\r\n   * \\param[in] logid The log identifier.\r\n   * \\param[in] tp_options\r\n   * \\param[out] out Returned newly created OrtEnv. Must be freed with OrtApi::ReleaseEnv\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateEnvWithGlobalThreadPools, OrtLoggingLevel log_severity_level, _In_ const char* logid,\r\n                  _In_ const OrtThreadingOptions* tp_options, _Outptr_ OrtEnv** out);\r\n\r\n  /// @}\r\n  /// \\name OrtSessionOptions\r\n  /// @{\r\n\r\n  /** \\brief Use global thread pool on a session\r\n   *\r\n   * Disable using per session thread pool and use the shared global threadpool.\r\n   * This should be used in conjunction with OrtApi::CreateEnvWithGlobalThreadPools.\r\n   *\r\n   * \\param[in] options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(DisablePerSessionThreads, _Inout_ OrtSessionOptions* options);\r\n\r\n  /// @}\r\n  /// \\name OrtThreadingOptions\r\n  /// @{\r\n\r\n  /** \\brief Create an ::OrtThreadingOptions\r\n   *\r\n   * \\param[out] out Newly created ::OrtThreadingOptions. Must be freed with OrtApi::ReleaseThreadingOptions\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateThreadingOptions, _Outptr_ OrtThreadingOptions** out);\r\n\r\n  ORT_CLASS_RELEASE(ThreadingOptions);\r\n\r\n  /// @}\r\n  /// \\name OrtModelMetadata\r\n  /// @{\r\n\r\n  /**\r\n   *\r\n   * \\param[in] model_metadata\r\n   * \\param[in] allocator\r\n   * \\param[out] keys Array of null terminated strings (array count = num_keys) allocated using `allocator`.\r\n   *  The strings and the pointer array must be freed using `allocator`\r\n   *  `keys` will be set to nullptr if the custom metadata map is empty.\r\n   * \\param[out] num_keys Set to the number of elements in the `keys` array\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(ModelMetadataGetCustomMetadataMapKeys, _In_ const OrtModelMetadata* model_metadata,\r\n                  _Inout_ OrtAllocator* allocator, _Outptr_result_buffer_maybenull_(*num_keys) char*** keys, _Out_ int64_t* num_keys);\r\n\r\n  /// @}\r\n  /// \\name OrtSessionOptions\r\n  /// @{\r\n\r\n  /**\r\n   *\r\n   * Override symbolic dimensions (by specific name strings) with actual values\r\n   * if known at session initialization time to enable optimizations that can\r\n   * take advantage of fixed values (such as memory planning, etc)\r\n   *\r\n   */\r\n  ORT_API2_STATUS(AddFreeDimensionOverrideByName,\r\n                  _Inout_ OrtSessionOptions* options, _In_ const char* dim_name,\r\n                  _In_ int64_t dim_value);\r\n\r\n  /// @}\r\n  /// \\name Misc\r\n  /// @{\r\n\r\n  /** \\brief Get the names of all available providers\r\n   *\r\n   * \\note The providers in the list are not guaranteed to be usable. They may fail to load due to missing system dependencies.\r\n   *    For example, if the CUDA/cuDNN libraries are not installed, the CUDA provider will report an error when it is added to the session options.\r\n   *\r\n   * \\param[out] out_ptr Set to a pointer to an array of null terminated strings of the available providers. The entries and the\r\n   *    array itself must be freed using OrtApi::ReleaseAvailableProviders\r\n   * \\param[out] provider_length Set to the number of entries in the `out_ptr` array\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetAvailableProviders, _Outptr_ char*** out_ptr, _Out_ int* provider_length);\r\n\r\n  /** \\brief Release data from OrtApi::GetAvailableProviders. This API will never fail\r\n   * so you can rely on it in a noexcept code.\r\n   *\r\n   * \\param[in] ptr The `out_ptr` result from OrtApi::GetAvailableProviders.\r\n   * \\param[in] providers_length The `provider_length` result from OrtApi::GetAvailableProviders\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(ReleaseAvailableProviders, _In_ char** ptr,\r\n                  _In_ int providers_length);\r\n\r\n  /// @}\r\n  /// \\name OrtValue\r\n  /// @{\r\n\r\n  /** \\brief Get the length of a single string in a string tensor\r\n   *\r\n   * \\param[in] value A string tensor\r\n   * \\param[in] index Index of the string in the tensor\r\n   * \\param[out] out Set to number of bytes of the string element\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetStringTensorElementLength, _In_ const OrtValue* value, size_t index, _Out_ size_t* out);\r\n\r\n  /** \\brief Get a single string from a string tensor\r\n   *\r\n   * \\param[in] value A string tensor\r\n   * \\param[in] s_len Number of bytes in the `s` buffer. Must match the value returned by OrtApi::GetStringTensorElementLength.\r\n   * \\param[in] index Index of the string in the tensor\r\n   * \\param[out] s The string element contents in UTF-8 encoding. The string is NOT null-terminated.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetStringTensorElement, _In_ const OrtValue* value, size_t s_len, size_t index, _Out_writes_bytes_all_(s_len) void* s);\r\n\r\n  /** \\brief Set a single string in a string tensor\r\n   *\r\n   * \\param[in] value A string tensor\r\n   * \\param[in] s A null terminated UTF-8 encoded string\r\n   * \\param[in] index Index of the string in the tensor to set\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(FillStringTensorElement, _Inout_ OrtValue* value, _In_ const char* s, size_t index);\r\n\r\n  /// @}\r\n  /// \\name OrtSessionOptions\r\n  /// @{\r\n\r\n  /** \\brief Set a session configuration entry as a pair of strings\r\n   *\r\n   * If a configuration with same key exists, this will overwrite the configuration with the given config_value.\r\n   *\r\n   * The config_key and the format of config_value are defined in onnxruntime_session_options_config_keys.h\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] config_key A null terminated string representation of the config key\r\n   * \\param[in] config_value A null terminated string representation of the config value\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(AddSessionConfigEntry, _Inout_ OrtSessionOptions* options,\r\n                  _In_z_ const char* config_key, _In_z_ const char* config_value);\r\n\r\n  /// @}\r\n  /// \\name OrtAllocator\r\n  /// @{\r\n\r\n  /** \\brief Create an allocator for an ::OrtSession following an ::OrtMemoryInfo\r\n   *\r\n   * The allocator wraps the internal allocator from the OrtSession and becomes invalid when the session does.\r\n   *\r\n   * \\param[in] session\r\n   * \\param[in] mem_info valid ::OrtMemoryInfo instance\r\n   * \\param[out] out Newly created ::OrtAllocator. Must be freed with OrtApi::ReleaseAllocator\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateAllocator, _In_ const OrtSession* session, _In_ const OrtMemoryInfo* mem_info,\r\n                  _Outptr_ OrtAllocator** out);\r\n\r\n  /** \\brief Release an ::OrtAllocator obtained from OrtApi::CreateAllocator\r\n   */\r\n  ORT_CLASS_RELEASE(Allocator);\r\n\r\n  /// @}\r\n  /// \\name OrtSession\r\n  /// @{\r\n\r\n  /** \\brief Run a model using Io Bindings for the inputs & outputs\r\n   *\r\n   * \\see OrtApi::Run\r\n   *\r\n   * \\param[in] session\r\n   * \\param[in] run_options\r\n   * \\param[in] binding_ptr\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(RunWithBinding, _Inout_ OrtSession* session, _In_ const OrtRunOptions* run_options, _In_ const OrtIoBinding* binding_ptr);\r\n\r\n  /** \\brief Create an ::OrtIoBinding instance\r\n   *\r\n   * An IoBinding object allows one to bind pre-allocated ::OrtValue%s to input names.\r\n   * Thus if you want to use a raw on device buffer as input or output you can avoid\r\n   * extra copy during runtime.\r\n   *\r\n   * \\param[in] session\r\n   * \\param[out] out Newly created ::OrtIoBinding. Must be freed with OrtApi::ReleaseIoBinding\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateIoBinding, _Inout_ OrtSession* session, _Outptr_ OrtIoBinding** out);\r\n\r\n  /// @}\r\n  /// \\name OrtIoBinding\r\n  /// @{\r\n\r\n  /** \\brief Release an ::OrtIoBinding obtained from OrtApi::CreateIoBinding\r\n   */\r\n  ORT_CLASS_RELEASE(IoBinding);\r\n\r\n  /** \\brief Bind an ::OrtValue to an ::OrtIoBinding input\r\n   *\r\n   * When using OrtApi::RunWithBinding this value is used for the named input\r\n   *\r\n   * \\param[in] binding_ptr\r\n   * \\param[in] name Name for the model input\r\n   * \\param[in] val_ptr ::OrtValue of Tensor type.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(BindInput, _Inout_ OrtIoBinding* binding_ptr, _In_ const char* name, _In_ const OrtValue* val_ptr);\r\n\r\n  /** \\brief Bind an ::OrtValue to an ::OrtIoBinding output\r\n   *\r\n   * When using OrtApi::RunWithBinding this value is used for the named output\r\n   *\r\n   * \\param[in] binding_ptr\r\n   * \\param[in] name Null terminated string of the model output name\r\n   * \\param[in] val_ptr ::OrtValue of Tensor type.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(BindOutput, _Inout_ OrtIoBinding* binding_ptr, _In_ const char* name, _In_ const OrtValue* val_ptr);\r\n\r\n  /** \\brief Bind an ::OrtIoBinding output to a device\r\n   *\r\n   * Binds the ::OrtValue to a device which is specified by ::OrtMemoryInfo.\r\n   * You can either create an instance of ::OrtMemoryInfo with a device id or obtain one from the allocator that you have created/are using\r\n   * This is useful when one or more outputs have dynamic shapes and, it is hard to pre-allocate and bind a chunk of\r\n   * memory within ::OrtValue ahead of time.\r\n   *\r\n   * \\see OrtApi::RunWithBinding\r\n   *\r\n   * \\param[in] binding_ptr\r\n   * \\param[in] name Null terminated string of the device name\r\n   * \\param[in] mem_info_ptr\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(BindOutputToDevice, _Inout_ OrtIoBinding* binding_ptr, _In_ const char* name, _In_ const OrtMemoryInfo* mem_info_ptr);\r\n\r\n  /** \\brief Get the names of an ::OrtIoBinding's outputs\r\n   *\r\n   * Returns the names of the outputs in the order they were bound. This is useful after running the model\r\n   * with bound outputs because the returned names are in order in which output ::OrtValue are returned. This is useful if\r\n   * the order of outputs and their names is not known.\r\n   *\r\n   * \\param[in] binding_ptr\r\n   * \\param[in] allocator Allocator used to allocate continuous buffers for output strings and lengths.\r\n   * \\param[out] buffer Returns an array of non-null terminated UTF-8 strings. The number of strings stored is returned in the count parameter.\r\n   *   This buffer is allocated using `allocator` and must be freed using it.\r\n   * \\param[out] lengths Returns an array of `count` lengths of the strings returned in `buffer`\r\n   *   This buffer is allocated using `allocator` and must be freed using it.\r\n   * \\param[out] count Number of strings returned. If `binding_ptr` has no bound outputs, zero is returned,\r\n   *              no memory allocation is performed and buffer and lengths are set to nullptr.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetBoundOutputNames, _In_ const OrtIoBinding* binding_ptr, _In_ OrtAllocator* allocator,\r\n                  _Out_ char** buffer, _Out_writes_all_(count) size_t** lengths, _Out_ size_t* count);\r\n\r\n  /** \\brief Get the output ::OrtValue objects from an ::OrtIoBinding\r\n   *\r\n   * Returns an array of pointers to individually allocated ::OrtValue%s that contain results of a model execution with OrtApi::RunWithBinding\r\n   * The array contains the same number of ::OrtValue%s and they are in the same order as they were bound with OrtApi::BindOutput\r\n   * or OrtApi::BindOutputToDevice.\r\n   *\r\n   * The returned ::OrtValue%s must be released using OrtApi::ReleaseValue after they are no longer needed.\r\n   * The array is allocated using the specified instance of the allocator and must be freed using the same allocator after\r\n   * all the ::OrtValue%s contained therein are individually released.\r\n   *\r\n   * \\param[in] binding_ptr\r\n   * \\param[in] allocator Allocator used to allocate output array\r\n   * \\param[out] output Set to the allocated array of allocated ::OrtValue outputs. Set to nullptr if there are 0 outputs.\r\n   * \\param[out] output_count Set to number of ::OrtValue%s returned\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetBoundOutputValues, _In_ const OrtIoBinding* binding_ptr, _In_ OrtAllocator* allocator,\r\n                  _Out_writes_all_(output_count) OrtValue*** output, _Out_ size_t* output_count);\r\n\r\n  /** \\brief Clears any previously set Inputs for an ::OrtIoBinding\r\n   */\r\n  void(ORT_API_CALL* ClearBoundInputs)(_Inout_ OrtIoBinding* binding_ptr) NO_EXCEPTION ORT_ALL_ARGS_NONNULL;\r\n\r\n  /** \\brief Clears any previously set Outputs for an ::OrtIoBinding\r\n   */\r\n  void(ORT_API_CALL* ClearBoundOutputs)(_Inout_ OrtIoBinding* binding_ptr) NO_EXCEPTION ORT_ALL_ARGS_NONNULL;\r\n\r\n  /// @}\r\n  /// \\name OrtValue\r\n  /// @{\r\n\r\n  /** \\brief Direct memory access to a specified tensor element\r\n   *\r\n   * For example, given a tensor with shape of [3,224,224], a pointer to the element at location [2,150,128] can be retrieved\r\n   *\r\n   * This function only works for numeric type tensors (No strings, etc).\r\n   * This is a no-copy method whose returned pointer is valid until the passed in ::OrtValue is free'd.\r\n   *\r\n   * \\param[in] value\r\n   * \\param[in] location_values Pointer to an array of index values that specify an element's location relative to its shape\r\n   * \\param[in] location_values_count Number of elements in location_values. Must match the number of elements in the tensor's shape.\r\n   * \\param[out] out Set to a pointer to the element specified\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(TensorAt, _Inout_ OrtValue* value, const int64_t* location_values, size_t location_values_count, _Outptr_ void** out);\r\n\r\n  /// @}\r\n  /// \\name OrtEnv\r\n  /// @{\r\n\r\n  /** \\brief Create an allocator and register it with the ::OrtEnv\r\n   *\r\n   * Enables sharing the allocator between multiple sessions that use the same env instance.\r\n   * Lifetime of the created allocator will be valid for the duration of the environment.\r\n   * Returns an error if an allocator with the same ::OrtMemoryInfo is already registered.\r\n   *\r\n   * See https://onnxruntime.ai/docs/get-started/with-c.html for details.\r\n   *\r\n   * \\param[in] env ::OrtEnv instance\r\n   * \\param[in] mem_info\r\n   * \\param[in] arena_cfg Pass nullptr for defaults\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateAndRegisterAllocator, _Inout_ OrtEnv* env, _In_ const OrtMemoryInfo* mem_info,\r\n                  _In_ const OrtArenaCfg* arena_cfg);\r\n\r\n  /** \\brief Set language projection\r\n   *\r\n   * Set the language projection for collecting telemetry data when Env is created.\r\n   *\r\n   * The default is ORT_PROJECTION_C, which means it will classify the language not in the list to C also.\r\n   *\r\n   * \\param[in] ort_env\r\n   * \\param[in] projection\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetLanguageProjection, _In_ const OrtEnv* ort_env, _In_ OrtLanguageProjection projection);\r\n\r\n  /// @}\r\n  /// \\name OrtSession\r\n  /// @{\r\n\r\n  /** \\brief Return the time that profiling was started\r\n   *\r\n   * \\note The timer precision varies per platform. On Windows and MacOS, the precision will be ~100ns\r\n   *\r\n   * \\param[in] session\r\n   * \\param[out] out nanoseconds of profiling's start time\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionGetProfilingStartTimeNs, _In_ const OrtSession* session, _Outptr_ uint64_t* out);\r\n\r\n  /// @}\r\n  /// \\name OrtThreadingOptions\r\n  /// @{\r\n\r\n  /** \\brief Set global intra-op thread count\r\n   *\r\n   * This configures the global thread pool options to be used in the call to OrtApi::CreateEnvWithGlobalThreadPools\r\n   *\r\n   * \\param[in] tp_options\r\n   * \\param[in] intra_op_num_threads Number of threads, special values:<br>\r\n   *    0 = Use default thread count<br>\r\n   *    1 = The invoking thread will be used; no threads will be created in the thread pool.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetGlobalIntraOpNumThreads, _Inout_ OrtThreadingOptions* tp_options, int intra_op_num_threads);\r\n\r\n  /** \\brief Set global inter-op thread count\r\n   *\r\n   * This configures the global thread pool options to be used in the call to OrtApi::CreateEnvWithGlobalThreadPools\r\n   *\r\n   * \\param[in] tp_options\r\n   * \\param[in] inter_op_num_threads Number of threads, special values:<br>\r\n   *    0 = Use default thread count<br>\r\n   *    1 = The invoking thread will be used; no threads will be created in the thread pool.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetGlobalInterOpNumThreads, _Inout_ OrtThreadingOptions* tp_options, int inter_op_num_threads);\r\n\r\n  /** \\brief Set global spin control options\r\n   *\r\n   * This will configure the global thread pool options to be used in the call to OrtApi::CreateEnvWithGlobalThreadPools.\r\n   * Allow spinning of thread pools when their queues are empty. This will set the value for both\r\n   * inter_op and intra_op threadpools.\r\n   *\r\n   * \\param[in] tp_options\r\n   * \\param[in] allow_spinning Valid values are 0 or 1.<br>\r\n   *   0 = It won't spin (recommended if CPU usage is high)<br>\r\n   *   1 = Threadpool will spin to wait for queue to become non-empty\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetGlobalSpinControl, _Inout_ OrtThreadingOptions* tp_options, int allow_spinning);\r\n\r\n  /// @}\r\n  /// \\name OrtSessionOptions\r\n  /// @{\r\n\r\n  /** \\brief Add a pre-allocated initializer to a session\r\n   *\r\n   * If a model contains an initializer with a name that is same as the name passed to this call,\r\n   * ORT will use this initializer instance instead of deserializing one from the model file. This\r\n   * is useful when you want to share the same initializer across sessions.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] name Null terminated string of the initializer name\r\n   * \\param[in] val ::OrtValue containing the initializer. Its lifetime and the underlying initializer buffer must be\r\n   *   managed by the user (created using the OrtApi::CreateTensorWithDataAsOrtValue) and it must outlive the session object\r\n   *   to which it is added.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(AddInitializer, _Inout_ OrtSessionOptions* options, _In_z_ const char* name,\r\n                  _In_ const OrtValue* val);\r\n\r\n  /// @}\r\n  /// \\name OrtEnv\r\n  /// @{\r\n\r\n  /**\r\n   * Create a custom environment with global threadpools and logger that will be shared across sessions.\r\n   * Use this in conjunction with OrtApi::DisablePerSessionThreads or else the session will use\r\n   * its own thread pools.\r\n   *\r\n   * \\param[in] logging_function A pointer to a logging function.\r\n   * \\param[in] logger_param A pointer to arbitrary data passed as the ::OrtLoggingFunction `param` parameter to\r\n   *                         `logging_function`.\r\n   * \\param[in] log_severity_level The log severity level.\r\n   * \\param[in] logid The log identifier.\r\n   * \\param[in] tp_options\r\n   * \\param[out] out Newly created OrtEnv. Must be freed with OrtApi::ReleaseEnv\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateEnvWithCustomLoggerAndGlobalThreadPools, OrtLoggingFunction logging_function, _In_opt_ void* logger_param, OrtLoggingLevel log_severity_level,\r\n                  _In_ const char* logid, _In_ const struct OrtThreadingOptions* tp_options, _Outptr_ OrtEnv** out);\r\n\r\n  /// @}\r\n  /// \\name OrtSessionOptions\r\n  /// @{\r\n\r\n  /** \\brief Append CUDA provider to session options\r\n   *\r\n   * If CUDA is not available (due to a non CUDA enabled build, or if CUDA is not installed on the system), this function will return failure.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] cuda_options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsAppendExecutionProvider_CUDA,\r\n                  _In_ OrtSessionOptions* options, _In_ const OrtCUDAProviderOptions* cuda_options);\r\n\r\n  /** \\brief Append ROCM execution provider to the session options\r\n   *\r\n   * If ROCM is not available (due to a non ROCM enabled build, or if ROCM is not installed on the system), this function will return failure.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] rocm_options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsAppendExecutionProvider_ROCM,\r\n                  _In_ OrtSessionOptions* options, _In_ const OrtROCMProviderOptions* rocm_options);\r\n\r\n  /** \\brief Append OpenVINO execution provider to the session options\r\n   *\r\n   * If OpenVINO is not available (due to a non OpenVINO enabled build, or if OpenVINO is not installed on the system), this function will fail.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] provider_options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsAppendExecutionProvider_OpenVINO,\r\n                  _In_ OrtSessionOptions* options, _In_ const OrtOpenVINOProviderOptions* provider_options);\r\n\r\n  /// @}\r\n  /// \\name OrtThreadingOptions\r\n  /// @{\r\n\r\n  /** \\brief Set threading flush-to-zero and denormal-as-zero\r\n   *\r\n   * Sets global thread pool options to be used in the call to OrtApi::CreateEnvWithGlobalThreadPools.\r\n   * Flush-to-zero and denormal-as-zero are applied to threads in both intra and inter global thread pool.\r\n   * \\note This option is not needed if the models used have no denormals. Having no denormals is recommended as this option may hurt model accuracy.\r\n   *\r\n   * \\param[in] tp_options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetGlobalDenormalAsZero, _Inout_ OrtThreadingOptions* tp_options);\r\n\r\n  /// @}\r\n  /// \\name OrtArenaCfg\r\n  /// @{\r\n\r\n  /** \\deprecated Use OrtApi::CreateArenaCfgV2\r\n   *\r\n   * This will create the configuration of an arena that can eventually be used to define an arena based allocator's behavior\r\n   *\r\n   * \\param[in] max_mem Use 0 to allow ORT to choose the default\r\n   * \\param[in] arena_extend_strategy Use -1 to allow ORT to choose the default, 0 = kNextPowerOfTwo, 1 = kSameAsRequested\r\n   * \\param[in] initial_chunk_size_bytes Use -1 to allow ORT to choose the default\r\n   * \\param[in] max_dead_bytes_per_chunk Use -1 to allow ORT to choose the default\r\n   * \\param[in] out A pointer to an OrtArenaCfg instance\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateArenaCfg, _In_ size_t max_mem, int arena_extend_strategy, int initial_chunk_size_bytes,\r\n                  int max_dead_bytes_per_chunk, _Outptr_ OrtArenaCfg** out);\r\n\r\n  ORT_CLASS_RELEASE(ArenaCfg);\r\n\r\n  /// @}\r\n  /// \\name OrtModelMetadata\r\n  /// @{\r\n\r\n  /**\r\n   * Use this to obtain the description of the graph present in the model\r\n   * (doc_string field of the GraphProto message within the ModelProto message).\r\n   * If it doesn't exist, an empty string will be returned.\r\n   *\r\n   * \\param[in] model_metadata An instance of ::OrtModelMetadata\r\n   * \\param[in] allocator Allocator used to allocate the string that will be returned back\r\n   * \\param[out] value Set to a null terminated string allocated using `allocator`.  The caller is responsible for freeing it using `allocator`\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(ModelMetadataGetGraphDescription, _In_ const OrtModelMetadata* model_metadata,\r\n                  _Inout_ OrtAllocator* allocator, _Outptr_ char** value);\r\n\r\n  /// @}\r\n  /// \\name OrtSessionOptions\r\n  /// @{\r\n\r\n  /** \\brief Append TensorRT provider to session options\r\n   *\r\n   * If TensorRT is not available (due to a non TensorRT enabled build, or if TensorRT is not installed on the system), this function will return failure.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] tensorrt_options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsAppendExecutionProvider_TensorRT,\r\n                  _In_ OrtSessionOptions* options, _In_ const OrtTensorRTProviderOptions* tensorrt_options);\r\n\r\n  /// @}\r\n  /// \\name Misc\r\n  /// @{\r\n\r\n  /** \\brief Set current GPU device ID\r\n   *\r\n   * Set the current device id of the GPU execution provider (CUDA/tensorrt/rocm). The device id should be less\r\n   * than the total number of devices available. This is only useful when multiple-GPUs are installed and it is\r\n   * required to restrict execution to a single GPU.\r\n   *\r\n   * \\param[in] device_id\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetCurrentGpuDeviceId, _In_ int device_id);\r\n\r\n  /** \\brief Get current GPU device ID\r\n   *\r\n   * Get the current device id of the GPU execution provider (CUDA/tensorrt/rocm).\r\n   *\r\n   * \\see OrtApi::SetCurrentGpuDeviceId\r\n   *\r\n   * \\param[out] device_id\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetCurrentGpuDeviceId, _In_ int* device_id);\r\n\r\n  /// @}\r\n  /// \\name OrtKernelInfo\r\n  /// Custom operator APIs.\r\n  /// @{\r\n\r\n  /** \\brief Fetch an array of int64_t values stored as an attribute in the graph node\r\n   *\r\n   *\r\n   * If `out` is nullptr, the value of `size` is set to the true size of the attribute\r\n   * array's size, and a success status is returned.\r\n   *\r\n   * If the `size` parameter is greater than or equal to the actual attribute array's size,\r\n   * the value of `size` is set to the true size of the attribute array's size,\r\n   * the provided memory is filled with the attribute's contents,\r\n   * and a success status is returned.\r\n   *\r\n   * If the `size` parameter is less than the actual attribute array's size and `out`\r\n   * is not nullptr, the value of `size` is set to the true size of the attribute array's size\r\n   * and a failure status is returned.)\r\n   *\r\n   * \\param[in] info instance\r\n   * \\param[in] name name of the attribute to be parsed\r\n   * \\param[out] out pointer to memory where the attribute's contents are to be stored\r\n   * \\param[in, out] size actual size of attribute array\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(KernelInfoGetAttributeArray_float, _In_ const OrtKernelInfo* info, _In_ const char* name,\r\n                  _Out_ float* out, _Inout_ size_t* size);\r\n\r\n  /** \\brief Fetch an array of int64_t values stored as an attribute in the graph node\r\n   *\r\n   * If `out` is nullptr, the value of `size` is set to the true size of the attribute\r\n   * array's size, and a success status is returned.\r\n   *\r\n   * If the `size` parameter is greater than or equal to the actual attribute array's size,\r\n   * the value of `size` is set to the true size of the attribute array's size,\r\n   * the provided memory is filled with the attribute's contents,\r\n   * and a success status is returned.\r\n   *\r\n   * If the `size` parameter is less than the actual attribute array's size and `out`\r\n   * is not nullptr, the value of `size` is set to the true size of the attribute array's size\r\n   * and a failure status is returned.)\r\n   *\r\n   * \\param[in] info instance\r\n   * \\param[in] name name of the attribute to be parsed\r\n   * \\param[out] out pointer to memory where the attribute's contents are to be stored\r\n   * \\param[in, out] size actual size of attribute array\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(KernelInfoGetAttributeArray_int64, _In_ const OrtKernelInfo* info, _In_ const char* name,\r\n                  _Out_ int64_t* out, _Inout_ size_t* size);\r\n\r\n  /// @}\r\n  /// \\name OrtArenaCfg\r\n  /// @{\r\n\r\n  /** \\brief Create an ::OrtArenaCfg\r\n   *\r\n   * Create the configuration of an arena that can eventually be used to define an arena based allocator's behavior.\r\n   *\r\n   * Supported keys are (See https://onnxruntime.ai/docs/get-started/with-c.html for details on what the\r\n   * following parameters mean and how to choose these values.):\r\n   * \"max_mem\": Maximum memory that can be allocated by the arena based allocator.\r\n   *  Use 0 for ORT to pick the best value. Default is 0.\r\n   * \"arena_extend_strategy\": 0 = kNextPowerOfTwo, 1 = kSameAsRequested.\r\n   *  Use -1 to allow ORT to choose the default.\r\n   * \"initial_chunk_size_bytes\": (Possible) Size of the first allocation in the arena.\r\n   *  Only relevant if arena strategy is `kNextPowerOfTwo`. Use -1 to allow ORT to choose the default.\r\n   *  Ultimately, the first allocation size is determined by the allocation memory request.\r\n   * \"max_dead_bytes_per_chunk\": Threshold of unused memory in an allocated chunk of arena memory after\r\n   *  crossing which the current chunk is chunked into 2.\r\n   * \"initial_growth_chunk_size_bytes\": (Possible) Size of the second allocation in the arena.\r\n   *  Only relevant if arena strategy is `kNextPowerOfTwo`. Use -1 to allow ORT to choose the default.\r\n   * \"max_power_of_two_extend_bytes\": The maximum extend size if arena strategy is `kNextPowerOfTwo`.\r\n   *  It is not an allocation limit, it is only a limit for extension when requested byte is less than the limit.\r\n   *  When requested bytes is more than the limit, allocator will still return as requested.\r\n   *  Use -1 to allow ORT to choose the default 1GB for max_power_of_two_extend_bytes.\r\n   *  Ultimately, the allocation size is determined by the allocation memory request.\r\n   *  Further allocation sizes are governed by the arena extend strategy.\r\n   *\r\n   * \\param[in] arena_config_keys Keys to configure the arena\r\n   * \\param[in] arena_config_values Values to configure the arena\r\n   * \\param[in] num_keys Number of keys in `arena_config_keys` and `arena_config_values`\r\n   * \\param[out] out Newly created ::OrtArenaCfg. Must be freed with OrtApi::ReleaseArenaCfg\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateArenaCfgV2, _In_reads_(num_keys) const char* const* arena_config_keys,\r\n                  _In_reads_(num_keys) const size_t* arena_config_values, _In_ size_t num_keys,\r\n                  _Outptr_ OrtArenaCfg** out);\r\n\r\n  /// @}\r\n  /// \\name OrtRunOptions\r\n  /// @{\r\n\r\n  /** \\brief Set a single run configuration entry as a pair of strings\r\n   *\r\n   * If a configuration with same key exists, this will overwrite the configuration with the given config_value\r\n   *\r\n   * The config_key and the format of config_value are defined in onnxruntime_run_options_config_keys.h\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] config_key A null terminated string representation of the config key\r\n   * \\param[in] config_value  A null terminated string representation of the config value\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(AddRunConfigEntry, _Inout_ OrtRunOptions* options,\r\n                  _In_z_ const char* config_key, _In_z_ const char* config_value);\r\n\r\n  /// @}\r\n  /// \\name OrtPrepackedWeightsContainer\r\n  /// @{\r\n\r\n  /** \\brief Create an ::OrtPrepackedWeightsContainer\r\n   *\r\n   * This container will hold pre-packed buffers of shared initializers for sharing between sessions\r\n   * (i.e.) if there are shared initializers that can be shared between sessions, the pre-packed buffers\r\n   * of these (if any) may possibly be shared to provide memory footprint savings. Pass this container\r\n   * to sessions that you would like to share pre-packed buffers of shared initializers at session\r\n   * creation time.\r\n   *\r\n   *  \\param[out] out Newly created ::OrtPrepackedWeightsContainer. Must be freed with OrtApi::ReleasePrepackedWeightsContainer\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreatePrepackedWeightsContainer, _Outptr_ OrtPrepackedWeightsContainer** out);\r\n\r\n  /** \\brief Release OrtPrepackedWeightsContainer instance\r\n   *\r\n   * \\note instance must not be released until the sessions using it are released\r\n   */\r\n  ORT_CLASS_RELEASE(PrepackedWeightsContainer);\r\n\r\n  /// @}\r\n  /// \\name OrtSession\r\n  /// @{\r\n\r\n  /** \\brief Create session with prepacked weights container\r\n   *\r\n   * Same functionality offered by OrtApi::CreateSession except that a container that contains\r\n   * pre-packed weights' buffers is written into/read from by the created session.\r\n   * This is useful when used in conjunction with OrtApi::AddInitializer which injects\r\n   * shared initializer info into sessions. Wherever possible, the pre-packed versions of these\r\n   * shared initializers are cached in this container so that multiple sessions can just re-use\r\n   * these instead of duplicating these in memory.\r\n   *\r\n   * \\param[in] env OrtEnv instance instance\r\n   * \\param[in] model_path Null terminated string of the path (wchar on Windows, char otherwise)\r\n   * \\param[in] options\r\n   * \\param[in] prepacked_weights_container\r\n   * \\param[out] out Newly created ::OrtSession. Must be freed with OrtApi::ReleaseSession\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateSessionWithPrepackedWeightsContainer, _In_ const OrtEnv* env, _In_ const ORTCHAR_T* model_path,\r\n                  _In_ const OrtSessionOptions* options,\r\n                  _Inout_ OrtPrepackedWeightsContainer* prepacked_weights_container,\r\n                  _Outptr_ OrtSession** out);\r\n\r\n  /** \\brief Create session from memory with prepacked weights container\r\n   *\r\n   * Same functionality offered by OrtApi::CreateSessionFromArray except that a container that contains\r\n   * pre-packed weights' buffers is written into/read from by the created session.\r\n   * This is useful when used in conjunction with OrtApi::AddInitializer which injects\r\n   * shared initializer info into sessions. Wherever possible, the pre-packed versions of these\r\n   * shared initializers are cached in this container so that multiple sessions can just re-use\r\n   * these instead of duplicating these in memory.\r\n   *\r\n   * \\param[in] env\r\n   * \\param[in] model_data Array of bytes holding the model\r\n   * \\param[in] model_data_length Number of bytes in `model_data_model`\r\n   * \\param[in] options\r\n   * \\param[in] prepacked_weights_container\r\n   * \\param[out] out Newly created ::OrtSession. Must be freed with OrtApi::ReleaseSession\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateSessionFromArrayWithPrepackedWeightsContainer, _In_ const OrtEnv* env,\r\n                  _In_ const void* model_data, size_t model_data_length,\r\n                  _In_ const OrtSessionOptions* options,\r\n                  _Inout_ OrtPrepackedWeightsContainer* prepacked_weights_container,\r\n                  _Outptr_ OrtSession** out);\r\n\r\n  /// @}\r\n  /// \\name OrtSessionOptions\r\n  /// @{\r\n\r\n  /** \\brief Append TensorRT execution provider to the session options\r\n   *\r\n   * If TensorRT is not available (due to a non TensorRT enabled build), this function will return failure.\r\n   *\r\n   * This is slightly different from OrtApi::SessionOptionsAppendExecutionProvider_TensorRT, it takes an\r\n   * ::OrtTensorRTProviderOptions which is publicly defined. This takes an opaque ::OrtTensorRTProviderOptionsV2\r\n   * which must be created with OrtApi::CreateTensorRTProviderOptions.\r\n   *\r\n   * For OrtApi::SessionOptionsAppendExecutionProvider_TensorRT, the user needs to instantiate ::OrtTensorRTProviderOptions\r\n   * as well as allocate/release buffers for some members of ::OrtTensorRTProviderOptions.\r\n   * Here, OrtApi::CreateTensorRTProviderOptions and Ortapi::ReleaseTensorRTProviderOptions will do the memory management for you.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] tensorrt_options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsAppendExecutionProvider_TensorRT_V2,\r\n                  _In_ OrtSessionOptions* options, _In_ const OrtTensorRTProviderOptionsV2* tensorrt_options);\r\n\r\n  /// @}\r\n  /// \\name OrtTensorRTProviderOptionsV2\r\n  /// @{\r\n\r\n  /** \\brief Create an OrtTensorRTProviderOptionsV2\r\n   *\r\n   * \\param[out] out Newly created ::OrtTensorRTProviderOptionsV2. Must be released with OrtApi::ReleaseTensorRTProviderOptions\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateTensorRTProviderOptions, _Outptr_ OrtTensorRTProviderOptionsV2** out);\r\n\r\n  /** \\brief Set options in a TensorRT Execution Provider.\r\n   *\r\n   * Please refer to https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html#cc\r\n   * to know the available keys and values. Key should be in null terminated string format of the member of ::OrtTensorRTProviderOptionsV2\r\n   * and value should be its related range. Recreates the options and only sets the supplied values.\r\n   *\r\n   * For example, key=\"trt_max_workspace_size\" and value=\"2147483648\"\r\n   *\r\n   * \\param[in] tensorrt_options\r\n   * \\param[in] provider_options_keys Array of UTF-8 null-terminated string for provider options keys\r\n   * \\param[in] provider_options_values Array of UTF-8 null-terminated string for provider options values\r\n   * \\param[in] num_keys Number of elements in the `provider_option_keys` and `provider_options_values` arrays\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(UpdateTensorRTProviderOptions, _Inout_ OrtTensorRTProviderOptionsV2* tensorrt_options,\r\n                  _In_reads_(num_keys) const char* const* provider_options_keys,\r\n                  _In_reads_(num_keys) const char* const* provider_options_values,\r\n                  _In_ size_t num_keys);\r\n\r\n  /** \\brief Get serialized TensorRT provider options string.\r\n   *\r\n   * For example, \"trt_max_workspace_size=2147483648;trt_max_partition_iterations=10;trt_int8_enable=1;......\"\r\n   *\r\n   * \\param tensorrt_options - OrtTensorRTProviderOptionsV2 instance\r\n   * \\param allocator - a ptr to an instance of OrtAllocator obtained with OrtApi::CreateAllocator or OrtApi::GetAllocatorWithDefaultOptions\r\n   *                      the specified allocator will be used to allocate continuous buffers for output strings and lengths.\r\n   * \\param ptr - is a UTF-8 null terminated string allocated using 'allocator'. The caller is responsible for using the same allocator to free it.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetTensorRTProviderOptionsAsString, _In_ const OrtTensorRTProviderOptionsV2* tensorrt_options, _Inout_ OrtAllocator* allocator, _Outptr_ char** ptr);\r\n\r\n  /** \\brief Release an ::OrtTensorRTProviderOptionsV2\r\n   *\r\n   * \\note This is an exception in the naming convention of other Release* functions, as the name of the method does not have the V2 suffix, but the type does\r\n   */\r\n  void(ORT_API_CALL* ReleaseTensorRTProviderOptions)(_Frees_ptr_opt_ OrtTensorRTProviderOptionsV2* input);\r\n\r\n  /// @}\r\n  /// \\name OrtSessionOptions\r\n  /// @{\r\n\r\n  /** \\brief Enable custom operators\r\n   *\r\n   * See onnxruntime-extensions: https://github.com/microsoft/onnxruntime-extensions.git\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(EnableOrtCustomOps, _Inout_ OrtSessionOptions* options);\r\n\r\n  /// @}\r\n  /// \\name OrtAllocator\r\n  /// @{\r\n\r\n  /** \\brief Register a custom allocator\r\n   *\r\n   * Enables sharing between multiple sessions that use the same env instance.\r\n   * Returns an error if an allocator with the same ::OrtMemoryInfo is already registered.\r\n   *\r\n   * The behavior of this is exactly the same as OrtApi::CreateAndRegisterAllocator except\r\n   * instead of ORT creating an allocator based on provided info, in this case\r\n   * ORT uses the user-provided custom allocator.\r\n   * See https://onnxruntime.ai/docs/get-started/with-c.html for details.\r\n   *\r\n   * \\param[in] env\r\n   * \\param[in] allocator User provided allocator\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(RegisterAllocator, _Inout_ OrtEnv* env, _In_ OrtAllocator* allocator);\r\n\r\n  /** \\brief Unregister a custom allocator\r\n   *\r\n   * It is an error if you provide an ::OrtMemoryInfo not corresponding to any\r\n   * registered allocators for sharing.\r\n   *\r\n   * \\param[in] env\r\n   * \\param[in] mem_info\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(UnregisterAllocator, _Inout_ OrtEnv* env,\r\n                  _In_ const OrtMemoryInfo* mem_info);\r\n\r\n  /// @}\r\n  /// \\name OrtValue\r\n  /// @{\r\n\r\n  /** \\brief Sets *out to 1 iff an ::OrtValue is a SparseTensor, and 0 otherwise\r\n   *\r\n   * \\param[in] value existing ::OrtValue\r\n   * \\param[out] out unless an error occurs, contains 1 iff the value contains an instance\r\n   *  of sparse tensor or 0 otherwise.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(IsSparseTensor, _In_ const OrtValue* value, _Out_ int* out);\r\n\r\n  /** \\brief Create an ::OrtValue with a sparse tensor that is empty.\r\n   *\r\n   * Use FillSparseTensor<Format>() functions to populate sparse tensor with non-zero values and\r\n   * format specific indices data.\r\n   * Use ReleaseValue to destroy the sparse tensor, this will also release the buffer inside the output value\r\n   * if any was allocated.\r\n   * \\param[in,out] allocator allocator to use when performing an allocation. Allocation will be performed\r\n   *   by FillSparseTensor<Format>() APIs. The lifespan of the allocator instance must eclipse the lifespan\r\n   *   this sparse tensor instance as the same allocator will be used to free memory.\r\n   * \\param[in] dense_shape shape of the original dense tensor\r\n   * \\param[in] dense_shape_len number of shape dimensions being passed\r\n   * \\param[in] type must be one of TENSOR_ELEMENT_DATA_TYPE_xxxx\r\n   * \\param[out] out Should be freed by calling ReleaseValue\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateSparseTensorAsOrtValue, _Inout_ OrtAllocator* allocator, _In_ const int64_t* dense_shape,\r\n                  size_t dense_shape_len, ONNXTensorElementDataType type, _Outptr_ OrtValue** out);\r\n\r\n  /**\r\n   * This fills populates an empty tensor that was created using OrtApi::CreateSparseTensorAsOrtValue.\r\n   * This will allocate required memory and copy the supplied NNZ values and COO indices into that memory allocation.\r\n   * Memory allocation is performed using the allocator that was specified with OrtApi::CreateSparseTensorAsOrtValue.\r\n   *\r\n   * \\param[in,out] ort_value ::OrtValue to populate with data\r\n   * \\param[in] data_mem_info serves to identify the location of the data to be copied. If the allocator specified\r\n   *  at the creation time has memory info that is not the same as mem_info argument to this function a X-device copy will be performed.\r\n   *  String data is assumed to be on CPU and will only be copied into a CPU allocated buffer.\r\n   * \\param[in] values_shape pointer to values shape array\r\n   * \\param[in] values_shape_len length of the values_shape\r\n   * \\param[in] values pointer to an array of values. For strings, pass const char**.\r\n   * \\param[in] indices_data pointer to a location of COO indices\r\n   * \\param[in] indices_num number of COO indices\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(FillSparseTensorCoo, _Inout_ OrtValue* ort_value, _In_ const OrtMemoryInfo* data_mem_info,\r\n                  _In_ const int64_t* values_shape, size_t values_shape_len, _In_ const void* values,\r\n                  _In_ const int64_t* indices_data, size_t indices_num);\r\n\r\n  /**\r\n   * This fills populates an empty tensor that was created using OrtApi::CreateSparseTensorAsOrtValue.\r\n   * This will allocate required memory and copy the supplied NNZ values and CSR indices into that memory allocation.\r\n   * Memory allocation is performed using the allocator that was specified with OrtApi::CreateSparseTensorAsOrtValue.\r\n   *\r\n   * \\param[in,out] ort_value ::OrtValue to populate with data\r\n   * \\param[in] data_mem_info serves to identify the location of the data to be copied. If the allocator specified\r\n   *  at the creation time has memory info that is not the same as mem_info argument to this function a X-device copy will be performed.\r\n   *  String data is assumed to be on CPU and will only be copied into a CPU allocated buffer.\r\n   * \\param[in] values_shape pointer to values shape array\r\n   * \\param[in] values_shape_len length of the values_shape\r\n   * \\param[in] values - pointer to an array of values. For strings, pass const char**.\r\n   * \\param[in] inner_indices_data pointer to a location of CSR inner indices\r\n   * \\param[in] inner_indices_num number of CSR inner indices\r\n   * \\param[in] outer_indices_data pointer to a location of CSR outer indices\r\n   * \\param[in] outer_indices_num number of CSR outer indices\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(FillSparseTensorCsr, _Inout_ OrtValue* ort_value, _In_ const OrtMemoryInfo* data_mem_info,\r\n                  _In_ const int64_t* values_shape, size_t values_shape_len, _In_ const void* values,\r\n                  _In_ const int64_t* inner_indices_data, size_t inner_indices_num,\r\n                  _In_ const int64_t* outer_indices_data, size_t outer_indices_num);\r\n\r\n  /**\r\n   * This fills populates an empty tensor that was created using OrtApi::CreateSparseTensorAsOrtValue.\r\n   * This will allocate required memory and copy the supplied NNZ values and BlockSparse indices into that memory allocation.\r\n   * Memory allocation is performed using the allocator that was specified with OrtApi::CreateSparseTensorAsOrtValue.\r\n   *\r\n   * \\param[in,out] ort_value ::OrtValue to populate with data\r\n   * \\param[in] data_mem_info serves to identify the location of the data to be copied. If the allocator specified\r\n   *  at the creation time has memory info that is not the same as mem_info argument to this function a X-device copy will be performed.\r\n   *  String data is assumed to be on CPU and will only be copied into a CPU allocated buffer.\r\n   * \\param[in] values_shape\r\n   * \\param[in] values_shape_len\r\n   * \\param[in] values structure with values information\r\n   * \\param[in] indices_shape_data pointer to a location of indices shape\r\n   * \\param[in] indices_shape_len length of the block sparse indices shape\r\n   * \\param[in] indices_data pointer to a location of indices data. Shape will determine the length of the indices data.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(FillSparseTensorBlockSparse, _Inout_ OrtValue* ort_value, _In_ const OrtMemoryInfo* data_mem_info,\r\n                  _In_ const int64_t* values_shape, size_t values_shape_len, _In_ const void* values,\r\n                  _In_ const int64_t* indices_shape_data, size_t indices_shape_len,\r\n                  _In_ const int32_t* indices_data);\r\n\r\n  /**\r\n   * Create an ::OrtValue with a sparse tensor. This is the first step.\r\n   * Next, use Use<Format>Indices() functions to supply sparse tensor with\r\n   * format specific indices data and set its sparse format to a specific enum value.\r\n   * This will not perform memory allocations. It will\r\n   * use supplied user buffer which should outlive the created sparse tensor.\r\n   * Use OrtApi::ReleaseValue to destroy the sparse tensor. It would not release the supplied values buffer.\r\n   * This function can not be used to map strings from the user allocated memory. Strings must always be copied\r\n   * and have UTF-8 encoding. Therefore, use OrtApi::CreateSparseTensorAsOrtValue above and then fill it with data\r\n   * using appropriate Make*() function.\r\n   *\r\n   * \\param[in] info memory info where sparse values reside.\r\n   * \\param[in,out] p_data pointer to a user allocated buffer with values. To create a full sparse tensor with no non-zero\r\n   *   values, pass nullptr\r\n   * \\param[in] dense_shape shape of the original dense tensor\r\n   * \\param[in] dense_shape_len number of shape dimensions being passed\r\n   * \\param[in] values_shape shape of the values data. To create a fully sparse tensor with no non-zero values,\r\n   *   pass {0} shape.\r\n   * \\param[in] values_shape_len number of values shape dimensions\r\n   * \\param[in] type must be one of TENSOR_ELEMENT_DATA_TYPE_xxxx\r\n   * \\param[out] out Should be freed by calling ReleaseValue\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(CreateSparseTensorWithValuesAsOrtValue, _In_ const OrtMemoryInfo* info, _Inout_ void* p_data,\r\n                  _In_ const int64_t* dense_shape, size_t dense_shape_len,\r\n                  _In_ const int64_t* values_shape, size_t values_shape_len,\r\n                  ONNXTensorElementDataType type, _Outptr_ OrtValue** out);\r\n\r\n  /**\r\n   * This assigns Coo format indices to the SparseTensor that was created by\r\n   * OrtApi::CreateSparseTensorWithValuesAsOrtValue above. It also sets OrtSparseFormat to\r\n   * ORT_SPARSE_COO. This will not allocate any additional memory for data. The life span of\r\n   * indices_data buffer should eclipse the life span of this ::OrtValue.\r\n   *\r\n   * \\param[in,out] ort_value ::OrtValue instance constructed with OrtApi::CreateSparseTensorWithValuesAsOrtValue\r\n   * \\param[in,out] indices_data pointer to a user pre-allocated buffer or nullptr for fully sparse tensors.\r\n   * \\param[in] indices_num  number of COO indices. Should either be 0 for fully sparse tensors, be equal\r\n   *  to the number of nnz values specified to OrtApi::CreateSparseTensorWithValuesAsOrtValue for 1-D {nnz} indices or\r\n   *  be twice as number of nnz values for a  2-D indices {nnz, 2}\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(UseCooIndices, _Inout_ OrtValue* ort_value, _Inout_ int64_t* indices_data, size_t indices_num);\r\n\r\n  /**\r\n   * The assigns CSR format indices to the SparseTensor that was created by\r\n   * OrtApi::CreateSparseTensorWithValuesAsOrtValue above. It also sets OrtSparseFormat to\r\n   * ORT_SPARSE_CSRC. This will not allocate any additional memory for data. The life spans of\r\n   * inner_data and outer_data buffers should eclipse the life span of this ::OrtValue.\r\n   *\r\n   * \\param[in,out] ort_value ::OrtValue instance constructed with OrtApi::CreateSparseTensorWithValuesAsOrtValue\r\n   * \\param[in,out] inner_data pointer to a user pre-allocated buffer or nullptr for fully sparse tensors.\r\n   * \\param[in] inner_num  number of inner CSR indices. Should either be 0 for fully sparse tensors or be equal\r\n   * to the number of nnz values specified to OrtApi::CreateSparseTensorWithValuesAsOrtValue.\r\n   * \\param[in,out] outer_data pointer to user pre-allocated buffer or nullptr for fully sparse tensors.\r\n   * \\param[in] outer_num number of CSR outer indices. Should either be 0 for fully sparse tensors or\r\n   * equal to rows + 1 of the dense shape.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(UseCsrIndices, _Inout_ OrtValue* ort_value, _Inout_ int64_t* inner_data, size_t inner_num,\r\n                  _Inout_ int64_t* outer_data, size_t outer_num);\r\n\r\n  /**\r\n   * The assigns BlockSparse format indices to the SparseTensor that was created by\r\n   * OrtApi::CreateSparseTensorWithValuesAsOrtValue above. It also sets OrtSparseFormat to\r\n   * ORT_SPARSE_BLOCK_SPARSE. This will not allocate any additional memory for data. The life span of\r\n   * indices_data buffer must eclipse the lifespan of this ::OrtValue.\r\n   *\r\n   * \\param[in,out] ort_value OrtValue instance constructed with OrtApi::CreateSparseTensorWithValuesAsOrtValue\r\n   * \\param[in] indices_shape pointer to indices shape. Use {0} for fully sparse tensors\r\n   * \\param[in] indices_shape_len length of the indices shape\r\n   * \\param[in,out] indices_data pointer to user pre-allocated buffer or nullptr for fully sparse tensors.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(UseBlockSparseIndices, _Inout_ OrtValue* ort_value, const int64_t* indices_shape, size_t indices_shape_len, _Inout_ int32_t* indices_data);\r\n\r\n  /** \\brief Returns sparse tensor format enum iff a given ort value contains an instance of sparse tensor.\r\n   *\r\n   * \\param[in] ort_value ::OrtValue that contains an instance of sparse tensor\r\n   * \\param[out] out pointer to out parameter\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetSparseTensorFormat, _In_ const OrtValue* ort_value, _Out_ enum OrtSparseFormat* out);\r\n\r\n  /** \\brief Returns data type and shape of sparse tensor values (nnz) iff ::OrtValue contains a SparseTensor.\r\n   *\r\n   * \\param[in] ort_value An ::OrtValue that contains a fully constructed sparse tensor\r\n   * \\param[out] out Must be freed by OrtApi::ReleaseTensorTypeAndShapeInfo\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetSparseTensorValuesTypeAndShape, _In_ const OrtValue* ort_value, _Outptr_ OrtTensorTypeAndShapeInfo** out);\r\n\r\n  /** \\brief Returns numeric data for sparse tensor values (nnz). For string values use GetStringTensor*().\r\n   *\r\n   * \\param[in] ort_value an instance of ::OrtValue containing sparse tensor\r\n   * \\param[out] out returns a pointer to values data.  Do not attempt to free this ptr.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetSparseTensorValues, _In_ const OrtValue* ort_value, _Outptr_ const void** out);\r\n\r\n  /** \\brief Returns data type, shape for the type of indices specified by indices_format.\r\n   *\r\n   * \\param[in] ort_value ::OrtValue containing sparse tensor.\r\n   * \\param[in] indices_format One of the indices formats. It is an error to request a format that the sparse\r\n   * tensor does not contain.\r\n   * \\param[out] out an instance of ::OrtTensorTypeAndShapeInfo. Must be freed by OrtApi::ReleaseTensorTypeAndShapeInfo\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetSparseTensorIndicesTypeShape, _In_ const OrtValue* ort_value, enum OrtSparseIndicesFormat indices_format, _Outptr_ OrtTensorTypeAndShapeInfo** out);\r\n\r\n  /** \\brief Returns indices data for the type of the indices specified by indices_format\r\n   *\r\n   * \\param[in] ort_value ::OrtValue containing sparse tensor.\r\n   * \\param[in] indices_format One of the indices formats. It is an error to request a format that the sparse tensor does not contain.\r\n   * \\param[out] num_indices Pointer to where the number of indices entries is returned\r\n   * \\param[out] indices Returned pointer to the indices data. Do not free the returned pointer as it refers to internal data owned by the ::OrtValue\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetSparseTensorIndices, _In_ const OrtValue* ort_value, enum OrtSparseIndicesFormat indices_format, _Out_ size_t* num_indices, _Outptr_ const void** indices);\r\n  /// @}\r\n  /// \\name OrtSessionOptions\r\n  /// @{\r\n\r\n  /**\r\n   * \\brief Sets out to 1 iff an optional type OrtValue has an element, 0 otherwise (OrtValue is None)\r\n   * Use this API to find if the optional type OrtValue is None or not.\r\n   * If the optional type OrtValue is not None, use the OrtValue just like any other OrtValue.\r\n   * For example, if you get an OrtValue that corresponds to Optional(tensor) and\r\n   * if HasValue() returns true, use it as tensor and so on.\r\n\r\n   * \\param[in] value Input OrtValue.\r\n   * \\param[out] out indicating if the input OrtValue contains data (1) or if it is a None (0)\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(HasValue, _In_ const OrtValue* value, _Out_ int* out);\r\n\r\n  /// @}\r\n  /// \\name OrtKernelContext\r\n  /// Custom operator APIs.\r\n  /// @{\r\n\r\n  /** \\brief Used for custom operators, gets the GPU compute stream to use to launch the custom a GPU kernel\r\n   *   \\see ::OrtCustomOp\r\n   * \\param[in]  context OrtKernelContext instance\r\n   * \\param[out] out Returns pointer to a GPU compute stream that can be used to launch the custom GPU kernel.\r\n   *             If retrieving the GPU compute stream is not relevant (GPU not enabled in the build, kernel partitioned to\r\n   *             some other EP), then a nullptr is returned as the output param.\r\n   *             Do not free or mutate the returned pointer as it refers to internal data owned by the underlying session.\r\n   *             Only use it for custom kernel launching.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(KernelContext_GetGPUComputeStream, _In_ const OrtKernelContext* context, _Outptr_ void** out);\r\n\r\n  /// @}\r\n  /// \\name GetTensorMemoryInfo\r\n  /// @{\r\n  /** \\brief Returns a pointer to the ::OrtMemoryInfo of a Tensor\r\n   * \\param[in] value ::OrtValue containing tensor.\r\n   * \\param[out] mem_info ::OrtMemoryInfo of the tensor. Do NOT free the returned pointer. It is valid for the lifetime of the ::OrtValue\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetTensorMemoryInfo, _In_ const OrtValue* value, _Out_ const OrtMemoryInfo** mem_info);\r\n\r\n  /// @}\r\n  /// \\name GetExecutionProviderApi\r\n  /// @{\r\n  /** \\brief Get a pointer to the requested version of the Execution Provider specific\r\n   * API extensions to the OrtApi\r\n   * \\param[in] provider_name The name of the execution provider name. Currently only the following\r\n   * values are supported: \"DML\".\r\n   * \\param[in] version Must be ::ORT_API_VERSION.\r\n   * \\param[out] provider_api A void pointer containing a reference to the execution provider versioned api structure.\r\n   * For example, the provider_api pointer can be cast to the OrtDmlApi* when the provider_name is \"DML\".\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetExecutionProviderApi, _In_ const char* provider_name, _In_ uint32_t version, _Outptr_ const void** provider_api);\r\n\r\n  /// @}\r\n\r\n  /// \\name SessionOptions\r\n  /// @{\r\n  /** \\brief Set custom thread creation function\r\n   *\r\n   * \\param[in] options Session options\r\n   * \\param[in] ort_custom_create_thread_fn Custom thread creation function\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsSetCustomCreateThreadFn, _Inout_ OrtSessionOptions* options, _In_ OrtCustomCreateThreadFn ort_custom_create_thread_fn);\r\n\r\n  /** \\brief Set creation options for custom thread\r\n   *\r\n   * \\param[in] options Session options\r\n   * \\param[in] ort_custom_thread_creation_options Custom thread creation options (can be nullptr)\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsSetCustomThreadCreationOptions, _Inout_ OrtSessionOptions* options, _In_ void* ort_custom_thread_creation_options);\r\n\r\n  /** \\brief Set custom thread join function\r\n   *\r\n   * \\param[in] options Session options\r\n   * \\param[in] ort_custom_join_thread_fn Custom join thread function, must not be nullptr when ort_custom_create_thread_fn is set\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsSetCustomJoinThreadFn, _Inout_ OrtSessionOptions* options, _In_ OrtCustomJoinThreadFn ort_custom_join_thread_fn);\r\n  /// @}\r\n\r\n  /// \\name OrtThreadingOptions\r\n  /// @{\r\n  /** \\brief Set custom thread creation function for global thread pools\r\n   *\r\n   * \\param[inout] tp_options\r\n   * \\param[in] ort_custom_create_thread_fn Custom thread creation function\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetGlobalCustomCreateThreadFn, _Inout_ OrtThreadingOptions* tp_options, _In_ OrtCustomCreateThreadFn ort_custom_create_thread_fn);\r\n\r\n  /** \\brief Set custom thread creation options for global thread pools\r\n   *\r\n   * \\param[inout] tp_options\r\n   * \\param[in] ort_custom_thread_creation_options Custom thread creation options (can be nullptr)\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetGlobalCustomThreadCreationOptions, _Inout_ OrtThreadingOptions* tp_options, _In_ void* ort_custom_thread_creation_options);\r\n\r\n  /** \\brief Set custom thread join function for global thread pools\r\n   *\r\n   * \\param[inout] tp_options\r\n   * \\param[in] ort_custom_join_thread_fn Custom thread join function, must not be nullptr when global ort_custom_create_thread_fn is set\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SetGlobalCustomJoinThreadFn, _Inout_ OrtThreadingOptions* tp_options, _In_ OrtCustomJoinThreadFn ort_custom_join_thread_fn);\r\n  /// @}\r\n\r\n  /** \\brief Synchronize bound inputs. The call may be necessary for some providers, such as cuda,\r\n   *   in case the system that allocated bound memory operated on a different stream. However, the\r\n   *   operation is provider specific and could be a no-op.\r\n   *\r\n   * \\param[inout] binding_ptr\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SynchronizeBoundInputs, _Inout_ OrtIoBinding* binding_ptr);\r\n\r\n  /** \\brief Synchronize bound outputs. The call may be necessary for some providers, such as cuda,\r\n   *   in case the system that allocated bound memory operated on a different stream. However, the\r\n   *   operation is provider specific and could be a no-op.\r\n   *\r\n   * \\param[inout] binding_ptr\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(SynchronizeBoundOutputs, _Inout_ OrtIoBinding* binding_ptr);\r\n\r\n  /// \\name OrtSessionOptions\r\n  /// @{\r\n\r\n  /** \\brief Append CUDA execution provider to the session options\r\n   *\r\n   * If CUDA is not available (due to a non CUDA enabled build), this function will return failure.\r\n   *\r\n   * This is slightly different from OrtApi::SessionOptionsAppendExecutionProvider_CUDA, it takes an\r\n   * ::OrtCUDAProviderOptions which is publicly defined. This takes an opaque ::OrtCUDAProviderOptionsV2\r\n   * which must be created with OrtApi::CreateCUDAProviderOptions.\r\n   *\r\n   * For OrtApi::SessionOptionsAppendExecutionProvider_CUDA, the user needs to instantiate ::OrtCUDAProviderOptions\r\n   * as well as allocate/release buffers for some members of ::OrtCUDAProviderOptions.\r\n   * Here, OrtApi::CreateCUDAProviderOptions and Ortapi::ReleaseCUDAProviderOptions will do the memory management for you.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] cuda_options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.11.\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsAppendExecutionProvider_CUDA_V2,\r\n                  _In_ OrtSessionOptions* options, _In_ const OrtCUDAProviderOptionsV2* cuda_options);\r\n\r\n  /// @}\r\n  /// \\name OrtCUDAProviderOptionsV2\r\n  /// @{\r\n\r\n  /** \\brief Create an OrtCUDAProviderOptionsV2\r\n   *\r\n   * \\param[out] out Newly created ::OrtCUDAProviderOptionsV2. Must be released with OrtApi::ReleaseCudaProviderOptions\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.11.\r\n   */\r\n  ORT_API2_STATUS(CreateCUDAProviderOptions, _Outptr_ OrtCUDAProviderOptionsV2** out);\r\n\r\n  /** \\brief Set options in a CUDA Execution Provider.\r\n   *\r\n   * Please refer to https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#configuration-options\r\n   * to know the available keys and values. Key should be in null terminated string format of the member of ::OrtCUDAProviderOptionsV2\r\n   * and value should be its related range. Recreates the options and only sets the supplied values.\r\n   *\r\n   * For example, key=\"device_id\" and value=\"0\"\r\n   *\r\n   * \\param[in] cuda_options\r\n   * \\param[in] provider_options_keys Array of UTF-8 null-terminated string for provider options keys\r\n   * \\param[in] provider_options_values Array of UTF-8 null-terminated string for provider options values\r\n   * \\param[in] num_keys Number of elements in the `provider_option_keys` and `provider_options_values` arrays\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.11.\r\n   */\r\n  ORT_API2_STATUS(UpdateCUDAProviderOptions, _Inout_ OrtCUDAProviderOptionsV2* cuda_options,\r\n                  _In_reads_(num_keys) const char* const* provider_options_keys,\r\n                  _In_reads_(num_keys) const char* const* provider_options_values,\r\n                  _In_ size_t num_keys);\r\n\r\n  /**\r\n   * Get serialized CUDA provider options string.\r\n   *\r\n   * For example, \"device_id=0;arena_extend_strategy=0;......\"\r\n   *\r\n   * \\param cuda_options - OrtCUDAProviderOptionsV2 instance\r\n   * \\param allocator - a ptr to an instance of OrtAllocator obtained with CreateAllocator() or GetAllocatorWithDefaultOptions()\r\n   *                      the specified allocator will be used to allocate continuous buffers for output strings and lengths.\r\n   * \\param ptr - is a UTF-8 null terminated string allocated using 'allocator'. The caller is responsible for using the same allocator to free it.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.11.\r\n   */\r\n  ORT_API2_STATUS(GetCUDAProviderOptionsAsString, _In_ const OrtCUDAProviderOptionsV2* cuda_options, _Inout_ OrtAllocator* allocator, _Outptr_ char** ptr);\r\n\r\n  /** \\brief Release an ::OrtCUDAProviderOptionsV2\r\n   *\r\n   * \\note This is an exception in the naming convention of other Release* functions, as the name of the method does not have the V2 suffix, but the type does\r\n   *\r\n   * \\since Version 1.11.\r\n   */\r\n  void(ORT_API_CALL* ReleaseCUDAProviderOptions)(_Frees_ptr_opt_ OrtCUDAProviderOptionsV2* input);\r\n\r\n  /// @}\r\n\r\n  /** \\brief Append MIGraphX provider to session options\r\n   *\r\n   * If MIGraphX is not available (due to a non MIGraphX enabled build, or if MIGraphX is not installed on the system), this function will return failure.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] migraphx_options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.11.\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsAppendExecutionProvider_MIGraphX,\r\n                  _In_ OrtSessionOptions* options, _In_ const OrtMIGraphXProviderOptions* migraphx_options);\r\n\r\n  /** \\brief Replace initialized Tensors with external data with the data provided in initializers.\r\n   *\r\n   * The function will find the initialized TensorProtos with external data in the graph with the provided names and\r\n   * replace them with the provided tensors. The API verifies that the TensorProto being replaced\r\n   * has an external data reference and has the same name, dimensions and data type as its replacement. The replacement\r\n   * will occur before any of the optimizations take place. The data will be copied into the graph\r\n   * since TensorProto can't refer to the user provided buffers.\r\n   *\r\n   * Once the model has been loaded, the OrtValue(s) added to SessionOptions instance will be removed\r\n   * from the internal SessionOptions copy to save memory, the user provided buffers can then be deallocated\r\n   * and the SessionOptions instance that refers to them can be destroyed.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] initializer_names Array of null terminated UTF-8 encoded strings of the initializers names.\r\n   * \\param[in] initializers Array of ::OrtValue type\r\n   * \\param[in] num_initializers Number of elements in the initializer_names and initializers\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.12.\r\n   */\r\n  ORT_API2_STATUS(AddExternalInitializers, _In_ OrtSessionOptions* options,\r\n                  _In_reads_(num_initializers) const char* const* initializer_names,\r\n                  _In_reads_(num_initializers) const OrtValue* const* initializers, size_t num_initializers);\r\n\r\n  /** \\brief: Create attribute of onnxruntime operator\r\n   *\r\n   * \\param[in] name Name of the attribute\r\n   * \\param[in] data Data content of the attribute\r\n   * \\param[in] len Number of bytes stored in data for ORT_OP_ATTR_STRING.\r\n                    Number of elements if data represents an array (e.g., ORT_OP_ATTR_INTS). Otherwise, set to 1.\r\n   * \\param[in] type Data type\r\n   * \\param[out] op_attr Attribute that has been created, which must be released by OrtApi::ReleaseOpAttr\r\n   *\r\n   * \\since Version 1.12.\r\n   */\r\n  ORT_API2_STATUS(CreateOpAttr,\r\n                  _In_ const char* name,\r\n                  _In_ const void* data,\r\n                  _In_ int len,\r\n                  _In_ OrtOpAttrType type,\r\n                  _Outptr_ OrtOpAttr** op_attr);\r\n\r\n  /* \\brief: Release op attribute\r\n   *\r\n   * \\param[in] opAttr Attribute created by OrtApi::CreateOpAttr\r\n   *\r\n   * \\since Version 1.12.\r\n   */\r\n  ORT_CLASS_RELEASE(OpAttr);\r\n\r\n  /** \\brief: Create onnxruntime native operator\r\n   *\r\n   * \\param[in] info Kernel info\r\n   * \\param[in] op_name Operator name\r\n   * \\param[in] domain Operator domain\r\n   * \\param[in] version Operator opset version\r\n   * \\param[in] type_constraint_names Name of the type constraints, such as \"T\" or \"T1\"\r\n   * \\param[in] type_constraint_values Type of each constraints\r\n   * \\param[in] type_constraint_count Number of constraints\r\n   * \\param[in] attr_values Attributes used to initialize the operator\r\n   * \\param[in] attr_count Number of the attributes\r\n   * \\param[in] input_count Number of inputs\r\n   * \\param[in] output_count Number of outputs\r\n   * \\param[out] ort_op Operator that has been created\r\n   *\r\n   * \\since Version 1.12.\r\n   */\r\n  ORT_API2_STATUS(CreateOp,\r\n                  _In_ const OrtKernelInfo* info,\r\n                  _In_z_ const char* op_name,\r\n                  _In_z_ const char* domain,\r\n                  int version,\r\n                  _In_reads_(type_constraint_count) const char** type_constraint_names,\r\n                  _In_reads_(type_constraint_count) const ONNXTensorElementDataType* type_constraint_values,\r\n                  int type_constraint_count,\r\n                  _In_reads_(attr_count) const OrtOpAttr* const* attr_values,\r\n                  int attr_count,\r\n                  int input_count,\r\n                  int output_count,\r\n                  _Outptr_ OrtOp** ort_op);\r\n\r\n  /** \\brief: Invoke the operator created by OrtApi::CreateOp\r\n   * The inputs must follow the order as specified in onnx specification\r\n   *\r\n   * \\param[in] context Kernel context\r\n   * \\param[in] ort_op Operator that has been created\r\n   * \\param[in] input_values Array of inputs\r\n   * \\param[in] input_count Number of inputs\r\n   * \\param[in] output_values Array of outputs\r\n   * \\param[in] output_count Number of outputs\r\n   *\r\n   * \\since Version 1.12.\r\n   */\r\n  ORT_API2_STATUS(InvokeOp,\r\n                  _In_ const OrtKernelContext* context,\r\n                  _In_ const OrtOp* ort_op,\r\n                  _In_ const OrtValue* const* input_values,\r\n                  _In_ int input_count,\r\n                  _Inout_ OrtValue* const* output_values,\r\n                  _In_ int output_count);\r\n\r\n  /* \\brief: Release an onnxruntime operator\r\n   *\r\n   * \\param[in] Op Operator created by OrtApi::CreateOp\r\n   *\r\n   * \\since Version 1.12.\r\n   */\r\n  ORT_CLASS_RELEASE(Op);\r\n\r\n  /** \\brief: Append execution provider to the session options.\r\n   * \\param[in] options\r\n   * \\param[in] provider_name - provider to add.\r\n   * \\param[in] provider_options_keys - keys to configure the provider options\r\n   * \\param[in] provider_options_values - values to configure the provider options\r\n   * \\param[in] num_keys - number of keys passed in\r\n   *\r\n   * Currently supported provider names:\r\n   *   QNNExecutionProvider (or QNN)\r\n   *   OpenVINOExecutionProvider (or OpenVINO)\r\n   *   XnnpackExecutionProvider (or XNNPACK)\r\n   *   WebNNExecutionProvider (or WEBNN)\r\n   *   WebGpuExecutionProvider (or WebGPU)\r\n   *   AzureExecutionProvider (or AZURE)\r\n   *   JsExecutionProvider (or JS)\r\n   *   VitisAIExecutionProvider (or VitisAI)\r\n   *   CoreMLExecutionProvider (or CoreML)\r\n   *\r\n   * Note: If an execution provider has a dedicated SessionOptionsAppendExecutionProvider_<provider name> function\r\n   *       that should be used to add it.\r\n   *\r\n   * QNN supported keys:\r\n   *   \"backend_type\": Type of QNN backend. Specifies a backend path that is the associated QNN backend library file\r\n   *      name. E.g., given backend type \"htp\", on Windows, the backend path would be \"QnnHtp.dll\", and on other\r\n   *      platforms, it would be \"libQnnHtp.so\". Mutually exclusive with \"backend_path\".\r\n   *      Available options:\r\n   *      -# \"cpu\"\r\n   *      -# \"gpu\"\r\n   *      -# \"htp\": Default.\r\n   *      -# \"saver\"\r\n   *      -# \"ir\"\r\n   *   \"backend_path\": File path to QNN backend library. Mutually exclusive with \"backend_type\".\r\n   *   \"profiling_level\": QNN profiling level.\r\n   *      Available options:\r\n   *      -# \"off\": Default.\r\n   *      -# \"basic\"\r\n   *      -# \"detailed\"\r\n   *   \"profiling_file_path\": QNN profiling file path if ETW not enabled.\r\n   *   \"rpc_control_latency\": QNN RPC control latency.\r\n   *   \"vtcm_mb\": QNN VTCM size in MB. default to 0(not set).\r\n   *   \"htp_performance_mode\": QNN performance mode.\r\n   *      Available options:\r\n   *      -# \"burst\"\r\n   *      -# \"balanced\"\r\n   *      -# \"default\": Default.\r\n   *      -# \"high_performance\"\r\n   *      -# \"high_power_saver\"\r\n   *      -# \"low_balanced\"\r\n   *      -# \"extreme_power_saver\"\r\n   *      -# \"low_power_saver\"\r\n   *      -# \"power_saver\"\r\n   *      -# \"sustained_high_performance\"\r\n   *   \"dump_qnn_ir_dlc\": Use the QnnIr backend library to write .dlc files for each subgraph dispatched to QNN. When\r\n   *       enabled, inference results will be incorrect. Use only for debugging.\r\n   *      -# \"0\": Default: disabled\r\n   *      -# \"1\": enabled\r\n   *   \"dump_qnn_ir_dlc_dir\": Set the directory into which QnnIr will be configured to write QNN graphs as .dlc files.\r\n   *      Default is current working directory.\r\n   *   \"qnn_ir_backend_path\": File path to the QnnIr backend library. If \"dump_qnn_ir_dlc\" is enabled, use this path\r\n   *      instead of looking for the Ir backend in the standard location.\r\n   *   \"qnn_saver_path\": File path to the QNN Saver backend library. If specified, QNN Saver will be enabled and will\r\n   *      dump QNN API calls to disk for replay/debugging. QNN Saver produces incorrect model inference results and\r\n   *      may alter model/EP partitioning. Use only for debugging.\r\n   *   \"qnn_context_priority\": QNN context priority.\r\n   *      Available options:\r\n   *      -# \"low\"\r\n   *      -# \"normal\": Default.\r\n   *      -# \"normal_high\"\r\n   *      -# \"high\"\r\n   *   \"htp_graph_finalization_optimization_mode\": Set the optimization mode for graph finalization on the HTP backend.\r\n   *      Available options:\r\n   *      -# \"0\": Default.\r\n   *      -# \"1\": Faster preparation time, less optimal graph.\r\n   *      -# \"2\": Longer preparation time, more optimal graph.\r\n   *      -# \"3\": Longest preparation time, most likely even more optimal graph. See QNN SDK documentation for specific\r\n   *        details.\r\n   *   \"soc_model\": The SoC model number. Refer to the QNN SDK documentation for valid values.\r\n   *      Defaults to \"0\" (unknown).\r\n   *   \"htp_arch\": The minimum HTP architecture the driver will use to select compatible QNN operators.\r\n   *      Available options:\r\n   *      -# \"0\": Default (none).\r\n   *      -# \"68\"\r\n   *      -# \"69\"\r\n   *      -# \"73\"\r\n   *      -# \"75\"\r\n   *   \"device_id\": The ID of the device to use when setting 'htp_arch'. Defaults to \"0\" (for single device).\r\n   *   \"enable_htp_fp16_precision\": Used for float32 model for HTP backend.\r\n   *      Enable the float32 model to be inferenced with fp16 precision. Otherwise, it will be fp32 precision.\r\n   *      -# \"0\": With fp32 precision.\r\n   *      -# \"1\": Default. With fp16 precision.\r\n   *   \"offload_graph_io_quantization\": Offload graph input quantization and graph output dequantization to another\r\n   *      execution provider (typically CPU EP).\r\n   *      -# \"0\": Disabled. QNN EP will handle quantization and dequantization of graph I/O.\r\n   *      -# \"1\": Enabled. This is the default value.\r\n   *   \"enable_htp_spill_fill_buffer\": Enable HTP spill fill buffer setting. The flag is used while generating context\r\n   *      binary.\r\n   *      -# \"0\": Default. Disabled.\r\n   *      -# \"1\": Enabled.\r\n   *   \"enable_htp_shared_memory_allocator\": Enable the QNN HTP shared memory allocator. Requires libcdsprpc.so/dll to\r\n   *      be available.\r\n   *      -# \"0\": Default. Disabled.\r\n   *      -# \"1\": Enabled.\r\n   *   \"dump_json_qnn_graph\": Set to \"1\" to dump QNN graphs generated by QNN EP as JSON files. Each graph partition\r\n   *      assigned to QNN EP is dumped to a separate file.\r\n   *   \"json_qnn_graph_dir\": Directory in which to dump QNN JSON graphs. If not specified, QNN graphs are dumped in the\r\n   *      program's current working directory. Ignored if \"dump_json_qnn_graph\" is not set.\r\n   *   \"op_packages\": QNN UDO op_package for QNN EP, allowed format:\r\n   *Â Â    \"<op_type>:<op_package_path>:<interface>[:<target>],<op_type2>:<op_package_path2>:<interface2>[:<target>]\",\r\n   *Â Â    where op_type is the name of the operation, op_package_path is the path to the op package shared library,\r\n   *     interface is the symbol name to register the op life cycle functions, and target is the backend type. For more\r\n   *     details, refer to: https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/op_packages.html\r\n   *\r\n   * XNNPACK supported keys:\r\n   *   \"intra_op_num_threads\": number of thread-pool size to use for XNNPACK execution provider.\r\n   *      default value is 0, which means to use the session thread-pool size.\r\n   *\r\n   * \\since Version 1.12.\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsAppendExecutionProvider, _In_ OrtSessionOptions* options,\r\n                  _In_ const char* provider_name,\r\n                  _In_reads_(num_keys) const char* const* provider_options_keys,\r\n                  _In_reads_(num_keys) const char* const* provider_options_values,\r\n                  _In_ size_t num_keys);\r\n\r\n  /* \\brief: Get a copy of kernel info\r\n   *\r\n   * \\param[in] info Kernel info\r\n   * \\param[out] info_copy Copy of kernel info\r\n   *\r\n   * \\since Version 1.12.\r\n   */\r\n  ORT_API2_STATUS(CopyKernelInfo,\r\n                  _In_ const OrtKernelInfo* info,\r\n                  _Outptr_ OrtKernelInfo** info_copy);\r\n\r\n  /* \\brief: Release kernel info\r\n   *\r\n   * \\param[in] KernelInfo A copy of kernel info returned by CopyKernelInfo\r\n   *\r\n   * \\since Version 1.12.\r\n   */\r\n  ORT_CLASS_RELEASE(KernelInfo);\r\n\r\n  /// \\name Ort Training\r\n  /// @{\r\n  /** \\brief Gets the Training C Api struct\r\n   *\r\n   * Call this function to access the ::OrtTrainingApi structure that holds pointers to functions that enable\r\n   * training with onnxruntime.\r\n   * \\note A NULL pointer will be returned and no error message will be printed if the training api\r\n   * is not supported with this build. A NULL pointer will be returned and an error message will be\r\n   * printed if the provided version is unsupported, for example when using a runtime older than the\r\n   * version created with this header file.\r\n   *\r\n   * \\param[in] version Must be ::ORT_API_VERSION\r\n   * \\return The ::OrtTrainingApi struct for the version requested.\r\n   *\r\n   * \\since Version 1.13\r\n   */\r\n  const OrtTrainingApi*(ORT_API_CALL* GetTrainingApi)(uint32_t version)NO_EXCEPTION;\r\n\r\n  /// @}\r\n\r\n  /** \\brief Append CANN provider to session options\r\n   *\r\n   * If CANN is not available (due to a non CANN enabled build, or if CANN is not installed on the system), this function will return failure.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] cann_options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.13.\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsAppendExecutionProvider_CANN,\r\n                  _In_ OrtSessionOptions* options, _In_ const OrtCANNProviderOptions* cann_options);\r\n\r\n  /** \\brief Create an OrtCANNProviderOptions\r\n   *\r\n   * \\param[out] out created ::OrtCANNProviderOptions. Must be released with OrtApi::ReleaseCANNProviderOptions\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.13.\r\n   */\r\n  ORT_API2_STATUS(CreateCANNProviderOptions, _Outptr_ OrtCANNProviderOptions** out);\r\n\r\n  /** \\brief Set options in a CANN Execution Provider.\r\n   *\r\n   * \\param[in] cann_options\r\n   * \\param[in] provider_options_keys Array of UTF-8 null-terminated string for provider options keys\r\n   * \\param[in] provider_options_values Array of UTF-8 null-terminated string for provider options values\r\n   * \\param[in] num_keys Number of elements in the `provider_option_keys` and `provider_options_values` arrays\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.13.\r\n   */\r\n  ORT_API2_STATUS(UpdateCANNProviderOptions, _Inout_ OrtCANNProviderOptions* cann_options,\r\n                  _In_reads_(num_keys) const char* const* provider_options_keys,\r\n                  _In_reads_(num_keys) const char* const* provider_options_values,\r\n                  _In_ size_t num_keys);\r\n\r\n  /** \\brief Get serialized CANN provider options string.\r\n   *\r\n   * \\param[in] cann_options OrtCANNProviderOptions instance\r\n   * \\param[in] allocator a ptr to an instance of OrtAllocator obtained with CreateAllocator()\r\n   *                      or GetAllocatorWithDefaultOptions(), the specified allocator will be used to allocate\r\n   *                      continuous buffers for output strings and lengths.\r\n   * \\param[out] ptr is a UTF-8 null terminated string allocated using 'allocator'.\r\n   *                 The caller is responsible for using the same allocator to free it.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.13.\r\n   */\r\n  ORT_API2_STATUS(GetCANNProviderOptionsAsString, _In_ const OrtCANNProviderOptions* cann_options,\r\n                  _Inout_ OrtAllocator* allocator, _Outptr_ char** ptr);\r\n\r\n  /** \\brief Release an OrtCANNProviderOptions\r\n   *\r\n   * \\param[in] input The pointer of OrtCANNProviderOptions which will been deleted\r\n   *\r\n   * \\since Version 1.13.\r\n   */\r\n  void(ORT_API_CALL* ReleaseCANNProviderOptions)(_Frees_ptr_opt_ OrtCANNProviderOptions* input);\r\n\r\n  /*  \\brief Get OrtDevice type from MemoryInfo\r\n   *\r\n   *  \\since Version 1.14\r\n   */\r\n  void(ORT_API_CALL* MemoryInfoGetDeviceType)(_In_ const OrtMemoryInfo* ptr, _Out_ OrtMemoryInfoDeviceType* out);\r\n\r\n  /* \\brief Update the OrtEnv instance with custom log severity level\r\n   *\r\n   * \\param[in] ort_env The OrtEnv instance being used\r\n   * \\param[in] log_severity_level The log severity level.\r\n   *\r\n   * \\since Version 1.14.\r\n   */\r\n  ORT_API2_STATUS(UpdateEnvWithCustomLogLevel, _In_ OrtEnv* ort_env, OrtLoggingLevel log_severity_level);\r\n\r\n  /*  \\brief Set affinities for intra op threads\r\n   *\r\n   * Affinity string follows format:\r\n   * logical_processor_id,logical_processor_id;logical_processor_id,logical_processor_id\r\n   * Semicolon isolates configurations among threads, while comma split processors where ith thread expected to attach to.\r\n   * e.g. 1,2,3;4,5\r\n   * specifies affinities for two threads, with the 1st thread attach to the 1st, 2nd, and 3rd processor, and 2nd thread to the 4th and 5th.\r\n   * To ease the configuration, an \"interval\" is also allowed:\r\n   * e.g. 1-8;8-16;17-24\r\n   * orders that the 1st thread runs on first eight processors, 2nd thread runs on next eight processors, and so forth.\r\n   * Note:\r\n   * 1. Once set, the number of thread affinities must equal to intra_op_num_threads - 1,\r\n   *    ort does not set affinity on the main thread which is started and managed by the calling app;\r\n   * 2. For windows, ort will infer the group id from a logical processor id, for example, assuming there are two groups with each has 64 logical processors,\r\n   *    an id of 64 will be inferred as the last processor of the 1st group, while 65 will be interpreted as the 1st processor of the second group.\r\n   *    Hence 64-65 is an invalid configuration, because a windows thread cannot be attached to processors across group boundary.\r\n   *\r\n   *  \\since Version 1.14\r\n   */\r\n  ORT_API2_STATUS(SetGlobalIntraOpThreadAffinity, _Inout_ OrtThreadingOptions* tp_options, const char* affinity_string);\r\n\r\n  /** \\brief Register custom ops from a shared library.\r\n   *\r\n   * Loads a shared library (.dll on windows, .so on linux, etc) named 'library_name' and looks for this entry point:\r\n   *\t\tOrtStatus* RegisterCustomOps(OrtSessionOptions * options, const OrtApiBase* api);\r\n   * It then passes in the provided session options to this function along with the api base.\r\n   *\r\n   * The handle to the loaded library is automatically released by ORT when the last OrtSession that references the\r\n   * library handle is released. If no OrtSession is created, then the library handle is released when the provided\r\n   * OrtSessionOptions is released.\r\n   *\r\n   * \\param[in] options The session options.\r\n   * \\param[in] library_name The name of the shared library to load and register. Refer to OS-specific dynamic library\r\n   *                         loading utilities (e.g., LoadLibraryEx on Windows or dlopen on Linux/MacOS) for information\r\n   *                         on the format of library names and search paths.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.14\r\n   */\r\n  ORT_API2_STATUS(RegisterCustomOpsLibrary_V2, _Inout_ OrtSessionOptions* options, _In_ const ORTCHAR_T* library_name);\r\n\r\n  /** \\brief Register custom ops by calling a RegisterCustomOpsFn function.\r\n   *\r\n   * Searches for registration_func_name and if found calls it.\r\n   *\r\n   * The library containing the function must either be linked against or previously loaded by the executable.\r\n   *\r\n   * If you want ONNX Runtime to load the library and manage its lifetime, use RegisterCustomOpsLibrary_V2.\r\n   *\r\n   * RegisterCustomOpsUsingFunction can be used in scenarios where it may not be possible for ONNX Runtime to load\r\n   * the library from a path. e.g. mobile platforms where the library must be linked into the app.\r\n   *\r\n   * The registration function must have the signature of RegisterCustomOpsFn:\r\n   *    OrtStatus* (*fn)(OrtSessionOptions* options, const OrtApiBase* api);\r\n   *\r\n   * See https://onnxruntime.ai/docs/reference/operators/add-custom-op.html for details on how the registration\r\n   * function should be implemented.\r\n   *\r\n   * \\param[in] options OrtSessionOptions that is passed through as the first argument in the call to the\r\n   *                    registration function.\r\n   * \\param[in] registration_func_name Name of registration function to use.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.14\r\n   */\r\n  ORT_API2_STATUS(RegisterCustomOpsUsingFunction, _Inout_ OrtSessionOptions* options,\r\n                  _In_ const char* registration_func_name);\r\n\r\n  /// \\name OrtKernelInfo\r\n  /// Custom operator APIs.\r\n  /// @{\r\n\r\n  /** \\brief Get the number of inputs from ::OrtKernelInfo.\r\n   *\r\n   * Used in the CreateKernel callback of an OrtCustomOp to query the number of inputs\r\n   * during kernel/session creation.\r\n   *\r\n   * \\param[in] info Instance of ::OrtKernelInfo.\r\n   * \\param[out] out Pointer to variable assigned with the result on success.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.14\r\n   */\r\n  ORT_API2_STATUS(KernelInfo_GetInputCount, _In_ const OrtKernelInfo* info, _Out_ size_t* out);\r\n\r\n  /** \\brief Get the number of outputs from ::OrtKernelInfo.\r\n   *\r\n   * Used in the CreateKernel callback of an OrtCustomOp to query the number of outputs\r\n   * during kernel/session creation.\r\n   *\r\n   * \\param[in] info Instance of ::OrtKernelInfo.\r\n   * \\param[out] out Pointer to variable assigned with the result on success.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.14\r\n   */\r\n  ORT_API2_STATUS(KernelInfo_GetOutputCount, _In_ const OrtKernelInfo* info, _Out_ size_t* out);\r\n\r\n  /** \\brief Get the name of a ::OrtKernelInfo's input.\r\n   *\r\n   * Used in the CreateKernel callback of an OrtCustomOp to query an input's name\r\n   * during kernel/session creation.\r\n   *\r\n   * If `out` is nullptr, the value of `size` is set to the size of the name\r\n   * string (including null-terminator), and a success status is returned.\r\n   *\r\n   * If the `size` parameter is greater than or equal to the name string's size,\r\n   * the value of `size` is set to the true size of the string (including null-terminator),\r\n   * the provided memory is filled with the string's contents, and a success status is returned.\r\n   *\r\n   * If the `size` parameter is less than the actual string's size and `out`\r\n   * is not nullptr, the value of `size` is set to the true size of the string\r\n   * and a failure status is returned.\r\n   *\r\n   * \\param[in] info An instance of ::OrtKernelInfo.\r\n   * \\param[in] index The index of the input name to get. Returns a failure status if out-of-bounds.\r\n   * \\param[out] out Memory location into which to write the UTF-8 null-terminated string representing the input's name.\r\n   * \\param[in,out] size Pointer to the size of the `out` buffer. See above comments for details.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.14\r\n   */\r\n  ORT_API2_STATUS(KernelInfo_GetInputName, _In_ const OrtKernelInfo* info, size_t index, _Out_ char* out,\r\n                  _Inout_ size_t* size);\r\n\r\n  /** \\brief Get the name of a ::OrtKernelInfo's output.\r\n   *\r\n   * Used in the CreateKernel callback of an OrtCustomOp to query an output's name\r\n   * during kernel/session creation.\r\n   *\r\n   * If `out` is nullptr, the value of `size` is set to the size of the name\r\n   * string (including null-terminator), and a success status is returned.\r\n   *\r\n   * If the `size` parameter is greater than or equal to the name string's size,\r\n   * the value of `size` is set to the true size of the string (including null-terminator),\r\n   * the provided memory is filled with the string's contents, and a success status is returned.\r\n   *\r\n   * If the `size` parameter is less than the actual string's size and `out`\r\n   * is not nullptr, the value of `size` is set to the true size of the string\r\n   * and a failure status is returned.\r\n   *\r\n   * \\param[in] info An instance of ::OrtKernelInfo.\r\n   * \\param[in] index The index of the output name to get. Returns a failure status if out-of-bounds.\r\n   * \\param[out] out Memory location into which to write the UTF-8 null-terminated string representing the output's\r\n   *                 name.\r\n   * \\param[in,out] size Pointer to the size of the `out` buffer. See above comments for details.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.14\r\n   */\r\n  ORT_API2_STATUS(KernelInfo_GetOutputName, _In_ const OrtKernelInfo* info, size_t index, _Out_ char* out,\r\n                  _Inout_ size_t* size);\r\n\r\n  /** \\brief Get the type information for a ::OrtKernelInfo's input.\r\n   *\r\n   * Used in the CreateKernel callback of an OrtCustomOp to query the shape and type information\r\n   * of an input during kernel/session creation.\r\n   *\r\n   * \\param[in] info An instance of ::OrtKernelInfo.\r\n   * \\param[in] index Which input to get the type information for\r\n   * \\param[out] type_info Pointer set to the resulting ::OrtTypeInfo. Must be freed with OrtApi::ReleaseTypeInfo.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.14\r\n   */\r\n  ORT_API2_STATUS(KernelInfo_GetInputTypeInfo, _In_ const OrtKernelInfo* info, size_t index,\r\n                  _Outptr_ OrtTypeInfo** type_info);\r\n\r\n  /** \\brief Get the type information for a ::OrtKernelInfo's output.\r\n   *\r\n   * Used in the CreateKernel callback of an OrtCustomOp to query the shape and type information\r\n   * of an output during kernel/session creation.\r\n   *\r\n   * \\param[in] info An instance of ::OrtKernelInfo.\r\n   * \\param[in] index Which input to get the type information for\r\n   * \\param[out] type_info Pointer set to the resulting ::OrtTypeInfo. Must be freed with OrtApi::ReleaseTypeInfo.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.14\r\n   */\r\n  ORT_API2_STATUS(KernelInfo_GetOutputTypeInfo, _In_ const OrtKernelInfo* info, size_t index,\r\n                  _Outptr_ OrtTypeInfo** type_info);\r\n\r\n  /** \\brief Get a ::OrtValue tensor stored as an attribute in the graph node.\r\n   *\r\n   * Used in the CreateKernel callback of an OrtCustomOp to get a tensor attribute.\r\n   *\r\n   * \\param[in] info ::OrtKernelInfo instance.\r\n   * \\param[in] name UTF-8 null-terminated string representing the attribute's name.\r\n   * \\param[in] allocator Allocator used to allocate the internal tensor state.\r\n   * \\param[out] out Returns newly created ::OrtValue. Must be freed with OrtApi::ReleaseValue,\r\n   *                 which will also free internal tensor state allocated with the provided allocator.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(KernelInfoGetAttribute_tensor, _In_ const OrtKernelInfo* info, _In_z_ const char* name,\r\n                  _Inout_ OrtAllocator* allocator, _Outptr_ OrtValue** out);\r\n\r\n  /// @}\r\n  /// \\name OrtSessionOptions\r\n  /// Custom operator APIs\r\n  /// @{\r\n\r\n  /** \\brief Checks if the given session configuration entry exists.\r\n   *\r\n   * The config_key formats are defined in onnxruntime_session_options_config_keys.h\r\n   *\r\n   * Can be used in a custom operator library to check for session configuration entries\r\n   * that target one or more custom operators in the library. Example: The config entry\r\n   * custom_op.myop.some_key targets a custom op named \"myop\".\r\n   *\r\n   * \\param[in] options The ::OrtSessionOptions instance.\r\n   * \\param[in] config_key A null-terminated UTF-8 string representation of the configuration key.\r\n   * \\param[out] out Pointer set to 1 if the entry exists and 0 otherwise.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.14\r\n   */\r\n  ORT_API2_STATUS(HasSessionConfigEntry, _In_ const OrtSessionOptions* options,\r\n                  _In_z_ const char* config_key, _Out_ int* out);\r\n\r\n  /** \\brief Get a session configuration value.\r\n   *\r\n   * Returns a failure status if the configuration key does not exist.\r\n   * The config_key and the format of config_value are defined in onnxruntime_session_options_config_keys.h\r\n   *\r\n   * If `config_value` is nullptr, the value of `size` is set to the true size of the string\r\n   * value (including null-terminator), and a success status is returned.\r\n   *\r\n   * If the `size` parameter is greater than or equal to the actual string value's size,\r\n   * the value of `size` is set to the true size of the string value, the provided memory\r\n   * is filled with the value's contents, and a success status is returned.\r\n   *\r\n   * If the `size` parameter is less than the actual string value's size and `config_value`\r\n   * is not nullptr, the value of `size` is set to the true size of the string value\r\n   * and a failure status is returned.\r\n   *\r\n   * Can be used in a custom operator library to get session configuration entries\r\n   * that target one or more custom operators in the library. Example: The config entry\r\n   * custom_op.myop.some_key targets a custom op named \"myop\".\r\n   *\r\n   * \\param[in] options The session options.\r\n   * \\param[in] config_key A null-terminated UTF-8 string representation of the config key.\r\n   * \\param[in] config_value Pointer to memory where the null-terminated UTF-8 string value will be stored.\r\n   * \\param[in,out] size Pointer to the size of the `config_value` buffer. See above comments for details.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.14\r\n   */\r\n  ORT_API2_STATUS(GetSessionConfigEntry, _In_ const OrtSessionOptions* options,\r\n                  _In_z_ const char* config_key, _Out_ char* config_value, _Inout_ size_t* size);\r\n\r\n  /// @}\r\n\r\n  /** \\brief Append dnnl provider to session options\r\n   *\r\n   * If oneDNN is not available, this function will return failure.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] dnnl_options\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.15.\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsAppendExecutionProvider_Dnnl,\r\n                  _In_ OrtSessionOptions* options, _In_ const OrtDnnlProviderOptions* dnnl_options);\r\n\r\n  /** \\brief Create an OrtDnnlProviderOptions\r\n   *\r\n   * \\param[out] out Newly created ::OrtDnnlProviderOptions. Must be released with OrtApi::ReleaseDnnlProviderOptions\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.15.\r\n   */\r\n  ORT_API2_STATUS(CreateDnnlProviderOptions, _Outptr_ OrtDnnlProviderOptions** out);\r\n\r\n  /** \\brief Set options in a oneDNN Execution Provider.\r\n   *\r\n   * Key should be in null terminated string format of the member of ::OrtDnnlProviderOptions\r\n   * and value should be its related range.\r\n   *\r\n   * For example, key=\"use_arena\" and value=\"1\"\r\n   *\r\n   * \\param[in] dnnl_options\r\n   * \\param[in] provider_options_keys Array of UTF-8 null-terminated string for provider options keys\r\n   * \\param[in] provider_options_values Array of UTF-8 null-terminated string for provider options values\r\n   * \\param[in] num_keys Number of elements in the `provider_option_keys` and `provider_options_values` arrays\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.15.\r\n   */\r\n  ORT_API2_STATUS(UpdateDnnlProviderOptions, _Inout_ OrtDnnlProviderOptions* dnnl_options,\r\n                  _In_reads_(num_keys) const char* const* provider_options_keys,\r\n                  _In_reads_(num_keys) const char* const* provider_options_values,\r\n                  _In_ size_t num_keys);\r\n\r\n  /**\r\n   * Get serialized oneDNN provider options string.\r\n   *\r\n   * For example, \"use_arena=1;......\"\r\n   *\r\n   * \\param dnnl_options - OrtDnnlProviderOptions instance\r\n   * \\param allocator - a ptr to an instance of OrtAllocator obtained with CreateAllocator() or GetAllocatorWithDefaultOptions()\r\n   *                      the specified allocator will be used to allocate continuous buffers for output strings and lengths.\r\n   * \\param ptr - is a UTF-8 null terminated string allocated using 'allocator'. The caller is responsible for using the same allocator to free it.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.15.\r\n   */\r\n  ORT_API2_STATUS(GetDnnlProviderOptionsAsString, _In_ const OrtDnnlProviderOptions* dnnl_options, _Inout_ OrtAllocator* allocator, _Outptr_ char** ptr);\r\n\r\n  /** \\brief Release an ::OrtDnnlProviderOptions\r\n   *\r\n   * \\since Version 1.15.\r\n   */\r\n  void(ORT_API_CALL* ReleaseDnnlProviderOptions)(_Frees_ptr_opt_ OrtDnnlProviderOptions* input);\r\n\r\n  /// \\name OrtKernelInfo\r\n  /// Custom operator APIs.\r\n  /// @{\r\n\r\n  /** \\brief Get the graph node name from ::OrtKernelInfo.\r\n   *\r\n   * If `out` is nullptr, the value of `size` is set to the size of the name\r\n   * string (including null-terminator), and a success status is returned.\r\n   *\r\n   * If the `size` parameter is greater than or equal to the name string's size,\r\n   * the value of `size` is set to the true size of the string (including null-terminator),\r\n   * the provided memory is filled with the string's contents, and a success status is returned.\r\n   *\r\n   * If the `size` parameter is less than the actual string's size and `out`\r\n   * is not nullptr, the value of `size` is set to the true size of the string\r\n   * and a failure status is returned.\r\n   *\r\n   * Can be used in a custom operator's CreateKernel callback to get the name of the operator's node name in the graph.\r\n   *\r\n   * \\param[in] info An instance of ::OrtKernelInfo.\r\n   * \\param[out] out Memory location into which to write the UTF-8 null-terminated string representing the name.\r\n   * \\param[in,out] size Pointer to the size of the `out` buffer. See above comments for details.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.15\r\n   */\r\n  ORT_API2_STATUS(KernelInfo_GetNodeName, _In_ const OrtKernelInfo* info, _Out_ char* out, _Inout_ size_t* size);\r\n\r\n  /** \\brief Get the session logger from ::OrtKernelInfo.\r\n   *\r\n   * Used in the CreateKernel callback of an OrtCustomOp to get a logger that can be used to log\r\n   * messages.\r\n   *\r\n   * \\param[in] info An instance of ::OrtKernelInfo.\r\n   * \\param[out] logger Pointer set to the session's ::OrtLogger. Owned by ONNX Runtime, so do not free.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.15\r\n   */\r\n  ORT_API2_STATUS(KernelInfo_GetLogger, _In_ const OrtKernelInfo* info, _Outptr_ const OrtLogger** logger);\r\n\r\n  /// @}\r\n  /// \\name OrtKernelContext\r\n  /// Custom operator APIs.\r\n  /// @{\r\n\r\n  /** \\brief Get the runtime logger from ::OrtKernelContext.\r\n   *\r\n   * Used in the KernelCompute callback of an OrtCustomOp to get a logger that can be used to log\r\n   * messages during inference.\r\n   *\r\n   * \\param[in] context An instance of ::OrtKernelContext.\r\n   * \\param[out] logger Pointer set to the kernel context's ::OrtLogger. Owned by ONNX Runtime, so do not free.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.15\r\n   */\r\n  ORT_API2_STATUS(KernelContext_GetLogger, _In_ const OrtKernelContext* context, _Outptr_ const OrtLogger** logger);\r\n\r\n  /// @}\r\n  /// \\name OrtLogger\r\n  /// Custom operator APIs.\r\n  /// @{\r\n\r\n  /** \\brief Logs a message at the given severity level using the provided ::OrtLogger.\r\n   *\r\n   * Only messages with a severity level equal or greater than the ::OrtLogger's logging severity level\r\n   * are logged. Use OrtApi::Logger_GetLoggingSeverityLevel to get the ::OrtLogger's logging severity\r\n   * level.\r\n   *\r\n   * Can be used in custom operators to log messages with the logger retrieved via OrtApi::KernelInfo_GetLogger.\r\n   *\r\n   * \\param[in] logger The ::OrtLogger instance.\r\n   * \\param[in] log_severity_level The message's severity level.\r\n   * \\param[in] message The message to log.\r\n   * \\param[in] file_path The filepath of the file in which the message is logged. Usually the value of ORT_FILE.\r\n   * \\param[in] line_number The file line number in which the message is logged. Usually the value of __LINE__.\r\n   * \\param[in] func_name The name of the function in which the message is logged. Usually the value of __FUNCTION__.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.15\r\n   */\r\n  ORT_API2_STATUS(Logger_LogMessage, _In_ const OrtLogger* logger, OrtLoggingLevel log_severity_level,\r\n                  _In_z_ const char* message, _In_z_ const ORTCHAR_T* file_path, int line_number,\r\n                  _In_z_ const char* func_name);\r\n\r\n  /** \\brief Get the logging severity level of the ::OrtLogger.\r\n   *\r\n   * Can be used in a custom operator to get the logging severity level of the ::OrtLogger associated with\r\n   * the ::OrtKernelInfo.\r\n   *\r\n   * \\param[in] logger The ::OrtLogger instance.\r\n   * \\param[out] out Pointer to variable assigned with the logging severity level on success.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.15\r\n   */\r\n  ORT_API2_STATUS(Logger_GetLoggingSeverityLevel, _In_ const OrtLogger* logger, _Out_ OrtLoggingLevel* out);\r\n\r\n  /// @}\r\n\r\n  /** \\brief Get a ::OrtValue tensor stored as a constant initializer in the graph node.\r\n   *\r\n   * Used in the CreateKernel callback of an OrtCustomOp to get a tensor value.\r\n   *\r\n   * \\param[in] info ::OrtKernelInfo instance.\r\n   * \\param[in] index The node index.\r\n   * \\param[out] is_constant Is it a constant node input or not.\r\n   * \\param[out] out The OrtValue tensor value.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.15.\r\n   */\r\n  ORT_API2_STATUS(KernelInfoGetConstantInput_tensor, _In_ const OrtKernelInfo* info, size_t index, _Out_ int* is_constant, _Outptr_ const OrtValue** out);\r\n\r\n  /** \\brief Get Optional Type information from an ::OrtTypeInfo\r\n   *\r\n   * This augments ::OrtTypeInfo to return an ::OrtOptionalTypeInfo when the type is optional.\r\n   * The OrtOptionalTypeInfo also has a nested ::OrtTypeInfo that describes the type of the optional value.\r\n   * ::OrtOptionalTypeInfo type can only appear within model metadata to describe inputs/outputs.\r\n   * The actual OrtValues that are supplied in place of optional type inputs should contain\r\n   * specific type that is described by ::OrtOptionalTypeInfo.\r\n   *\r\n   * So the picture: ::OrtTypeInfo -> ::OrtOptionalTypeInfo -> ::OrtTypeInfo (describes the type that can be supplied\r\n   * in place of the optional type when creating the actual ::OrtValue).\r\n   *\r\n   * \\param[in] type_info\r\n   * \\param[out] out A pointer to the ::OrtOptionalTypeInfo. Do not free this value,\r\n   *                 it is owned by OrtTypeInfo instance. When the type_info does not represent\r\n   *                 optional type, nullptr is returned in out.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.15.\r\n   */\r\n  ORT_API2_STATUS(CastTypeInfoToOptionalTypeInfo, _In_ const OrtTypeInfo* type_info,\r\n                  _Outptr_result_maybenull_ const OrtOptionalTypeInfo** out);\r\n\r\n  /** \\brief Get OrtTypeInfo for the allowed contained type from an ::OrtOptionalTypeInfo.\r\n   *\r\n   * This augments ::OrtOptionalTypeInfo to return an ::OrtTypeInfo for the contained type.\r\n   * The OrtOptionalTypeInfo has a nested ::OrtTypeInfo that describes the type of the optional value.\r\n   * ::OrtOptionalTypeInfo type can only appear within model metadata to describe inputs/outputs.\r\n   * The actual OrtValues that are supplied in place of optional type inputs should contain\r\n   * specific type that is described by the returned ::OrtTypeInfo.\r\n   *\r\n   * \\param[in] optional_type_info\r\n   * \\param[out] out A copy of ::OrtTypeInfo for what the optional value could be.\r\n   *                 The user must free this value with ReleaseTypeInfo.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.15.\r\n   */\r\n  ORT_API2_STATUS(GetOptionalContainedTypeInfo, _In_ const OrtOptionalTypeInfo* optional_type_info,\r\n                  _Outptr_ OrtTypeInfo** out);\r\n\r\n  /** \\brief Set a single string in a string tensor\r\n   *  Do not zero terminate the string data.\r\n   *\r\n   * \\param[in] value A string tensor\r\n   * \\param[in] index - flat index of the element\r\n   * \\param[in] length_in_bytes length of the buffer in utf-8 bytes (without the null terminator)\r\n   * \\param[inout] buffer - address of return value\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   */\r\n  ORT_API2_STATUS(GetResizedStringTensorElementBuffer, _Inout_ OrtValue* value, _In_ size_t index, _In_ size_t length_in_bytes, _Inout_ char** buffer);\r\n\r\n  /** \\brief Get Allocator from KernelContext for a specific memoryInfo. Please use C API ReleaseAllocator to release out object\r\n   *\r\n   * \\param[in] context OrtKernelContext instance\r\n   * \\param[in] mem_info OrtMemoryInfo instance\r\n   * \\param[out] out A pointer to OrtAllocator.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.15.\r\n   */\r\n  ORT_API2_STATUS(KernelContext_GetAllocator, _In_ const OrtKernelContext* context, _In_ const OrtMemoryInfo* mem_info, _Outptr_ OrtAllocator** out);\r\n\r\n  /** \\brief Returns a null terminated string of the build info including git info and cxx flags\r\n   *\r\n   * \\return UTF-8 encoded version string. Do not deallocate the returned buffer.\r\n   *\r\n   * \\since Version 1.15.\r\n   */\r\n  const char*(ORT_API_CALL* GetBuildInfoString)(void);\r\n\r\n  /// \\name OrtROCMProviderOptions\r\n  /// @{\r\n\r\n  /** \\brief Create an OrtROCMProviderOptions\r\n   *\r\n   * \\param[out] out Newly created ::OrtROCMProviderOptions. Must be released with OrtApi::ReleaseROCMProviderOptions\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.16.\r\n   */\r\n  ORT_API2_STATUS(CreateROCMProviderOptions, _Outptr_ OrtROCMProviderOptions** out);\r\n\r\n  /** \\brief Set options in a ROCm Execution Provider.\r\n   *\r\n   * Please refer to https://onnxruntime.ai/docs/execution-providers/ROCm-ExecutionProvider.html\r\n   * to know the available keys and values. Key should be in null terminated string format of the member of\r\n   * ::OrtROCMProviderOptions and value should be its related range.\r\n   *\r\n   * For example, key=\"device_id\" and value=\"0\"\r\n   *\r\n   * \\param[in] rocm_options\r\n   * \\param[in] provider_options_keys Array of UTF-8 null-terminated string for provider options keys\r\n   * \\param[in] provider_options_values Array of UTF-8 null-terminated string for provider options values\r\n   * \\param[in] num_keys Number of elements in the `provider_option_keys` and `provider_options_values` arrays\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.16.\r\n   */\r\n  ORT_API2_STATUS(UpdateROCMProviderOptions, _Inout_ OrtROCMProviderOptions* rocm_options,\r\n                  _In_reads_(num_keys) const char* const* provider_options_keys,\r\n                  _In_reads_(num_keys) const char* const* provider_options_values,\r\n                  _In_ size_t num_keys);\r\n\r\n  /**\r\n   * Get serialized ROCm provider options string.\r\n   *\r\n   * For example, \"device_id=0;arena_extend_strategy=0;......\"\r\n   *\r\n   * \\param rocm_options - OrtROCMProviderOptions instance\r\n   * \\param allocator - a ptr to an instance of OrtAllocator obtained with CreateAllocator() or GetAllocatorWithDefaultOptions()\r\n   *                      the specified allocator will be used to allocate continuous buffers for output strings and lengths.\r\n   * \\param ptr - is a UTF-8 null terminated string allocated using 'allocator'. The caller is responsible for using the same allocator to free it.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.16.\r\n   */\r\n  ORT_API2_STATUS(GetROCMProviderOptionsAsString, _In_ const OrtROCMProviderOptions* rocm_options, _Inout_ OrtAllocator* allocator, _Outptr_ char** ptr);\r\n\r\n  /** \\brief Release an ::OrtROCMProviderOptions\r\n   *\r\n   * \\note This is an exception in the naming convention of other Release* functions, as the name of the method does not have the V2 suffix, but the type does\r\n   *\r\n   * \\since Version 1.16.\r\n   */\r\n  void(ORT_API_CALL* ReleaseROCMProviderOptions)(_Frees_ptr_opt_ OrtROCMProviderOptions* input);\r\n\r\n  /** \\brief Create an allocator with specific type and register it with the ::OrtEnv\r\n   *  This API enhance CreateAndRegisterAllocator that it can create an allocator with specific type, not just CPU allocator\r\n   *  Enables sharing the allocator between multiple sessions that use the same env instance.\r\n   *  Lifetime of the created allocator will be valid for the duration of the environment.\r\n   *  Returns an error if an allocator with the same ::OrtMemoryInfo is already registered.\r\n   *  \\param[in] env OrtEnv instance\r\n   *  \\param[in] provider_type ExecutionProvider type\r\n   *  \\param[in] mem_info OrtMemoryInfo instance\r\n   *  \\param[in] arena_cfg Arena configuration\r\n   *  \\param[in] provider_options_keys key of the provider options map\r\n   *  \\param[in] provider_options_values value of the provider options map\r\n   *  \\param[in] num_keys Length of the provider options map\r\n   */\r\n  ORT_API2_STATUS(CreateAndRegisterAllocatorV2, _Inout_ OrtEnv* env, _In_ const char* provider_type,\r\n                  _In_ const OrtMemoryInfo* mem_info, _In_ const OrtArenaCfg* arena_cfg,\r\n                  _In_reads_(num_keys) const char* const* provider_options_keys, _In_reads_(num_keys) const char* const* provider_options_values, _In_ size_t num_keys);\r\n\r\n  /** \\brief Run the model asynchronously in a thread owned by intra op thread pool\r\n   *\r\n   * \\param[in] session\r\n   * \\param[in] run_options If nullptr, will use a default ::OrtRunOptions\r\n   * \\param[in] input_names Array of null terminated UTF8 encoded strings of the input names\r\n   * \\param[in] input Array of ::OrtValue%s of the input values\r\n   * \\param[in] input_len Number of elements in the input_names and inputs arrays\r\n   * \\param[in] output_names Array of null terminated UTF8 encoded strings of the output names\r\n   * \\param[in] output_names_len Number of elements in the output_names and outputs array\r\n   * \\param[out] output OrtValue* array of size output_names_len.\r\n   *             On calling RunAsync, output[i] could either be a null or a pointer to a preallocated OrtValue.\r\n   *             Later, the output array will be passed to run_async_callback with all null(s) filled with valid\r\n   *             OrtValue pointer(s) allocated by onnxruntime.\r\n   *             NOTE: it is customer's duty to finally release the output array and each of its member,\r\n   *             regardless of whether the member (OrtValue*) is allocated by onnxruntime or preallocated by the customer.\r\n   * \\param[in] run_async_callback Callback function on model run completion\r\n   * \\param[in] user_data User data that pass back to run_async_callback\r\n   */\r\n  ORT_API2_STATUS(RunAsync, _Inout_ OrtSession* session, _In_opt_ const OrtRunOptions* run_options,\r\n                  _In_reads_(input_len) const char* const* input_names,\r\n                  _In_reads_(input_len) const OrtValue* const* input, size_t input_len,\r\n                  _In_reads_(output_names_len) const char* const* output_names, size_t output_names_len,\r\n                  _Inout_updates_all_(output_names_len) OrtValue** output,\r\n                  _In_ RunAsyncCallbackFn run_async_callback, _In_opt_ void* user_data);\r\n\r\n  /**\r\n   * Update TensorRT EP provider option where its data type is pointer, for example 'user_compute_stream'.\r\n   * If the data type of the provider option can be represented by string please use UpdateTensorRTProviderOptions.\r\n   *\r\n   * Note: It's caller's responsibility to properly manage the lifetime of the instance pointed by this pointer.\r\n   *\r\n   * \\param tensorrt_options - OrtTensorRTProviderOptionsV2 instance\r\n   * \\param key - Name of the provider option\r\n   * \\param value - A pointer to the instance that will be assigned to this provider option\r\n   *\r\n   * \\since Version 1.16.\r\n   */\r\n  ORT_API2_STATUS(UpdateTensorRTProviderOptionsWithValue, _Inout_ OrtTensorRTProviderOptionsV2* tensorrt_options, _In_ const char* key, _In_ void* value);\r\n\r\n  /**\r\n   * Get TensorRT EP provider option where its data type is pointer.\r\n   * If the data type of the provider option can be represented by string please use GetTensorRTProviderOptionsAsString.\r\n   *\r\n   * \\param tensorrt_options - OrtTensorRTProviderOptionsV2 instance\r\n   * \\param key - Name of the provider option\r\n   * \\param ptr - A pointer to the instance that is kept by the provider option\r\n   *\r\n   * \\since Version 1.16.\r\n   */\r\n  ORT_API2_STATUS(GetTensorRTProviderOptionsByName, _In_ const OrtTensorRTProviderOptionsV2* tensorrt_options, _In_ const char* key, _Outptr_ void** ptr);\r\n\r\n  /**\r\n   * Update CUDA EP provider option where its data type is pointer, for example 'user_compute_stream'.\r\n   * If the data type of the provider option can be represented by string please use UpdateCUDAProviderOptions.\r\n   *\r\n   * Note: It's caller's responsibility to properly manage the lifetime of the instance pointed by this pointer.\r\n   *\r\n   * \\param cuda_options - OrtCUDAProviderOptionsV2 instance\r\n   * \\param key - Name of the provider option\r\n   * \\param value - A pointer to the instance that will be assigned to this provider option\r\n   *\r\n   * \\since Version 1.16.\r\n   */\r\n  ORT_API2_STATUS(UpdateCUDAProviderOptionsWithValue, _Inout_ OrtCUDAProviderOptionsV2* cuda_options, _In_ const char* key, _In_ void* value);\r\n\r\n  /**\r\n   * Get CUDA EP provider option where its data type is pointer.\r\n   * If the data type of the provider option can be represented by string please use GetCUDAProviderOptionsAsString.\r\n   *\r\n   * \\param cuda_options - OrtCUDAProviderOptionsV2 instance\r\n   * \\param key - Name of the provider option\r\n   * \\param ptr - A pointer to the instance that is kept by the provider option\r\n   *\r\n   * \\since Version 1.16.\r\n   */\r\n  ORT_API2_STATUS(GetCUDAProviderOptionsByName, _In_ const OrtCUDAProviderOptionsV2* cuda_options, _In_ const char* key, _Outptr_ void** ptr);\r\n\r\n  /**\r\n   * Get a EP resource.\r\n   * E.g. a cuda stream or a cublas handle\r\n   *\r\n   * \\param context - Kernel context\r\n   * \\param resource_version - Version of the resource\r\n   * \\param resource_id - Type of resource\r\n   * \\param resource - A pointer to returned resource\r\n   *\r\n   * \\since Version 1.16.\r\n   */\r\n  ORT_API2_STATUS(KernelContext_GetResource, _In_ const OrtKernelContext* context, _In_ int resource_version,\r\n                  _In_ int resource_id, _Outptr_ void** resource);\r\n\r\n  /** \\brief Set user logging function\r\n   *\r\n   *  By default the logger created by the CreateEnv* functions is used to create the session logger as well.\r\n   *  This function allows a user to override this default session logger with a logger of their own choosing. This way\r\n   *  the user doesn't have to create a separate environment with a custom logger. This addresses the problem when\r\n   *  the user already created an env but now wants to use a different logger for a specific session (for debugging or\r\n   *  other reasons).\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] user_logging_function A pointer to a logging function.\r\n   * \\param[in] user_logging_param A pointer to arbitrary data passed as the ::OrtLoggingFunction `param` parameter to\r\n   *                         `user_logging_function`. This parameter is optional.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.17.\r\n   */\r\n  ORT_API2_STATUS(SetUserLoggingFunction, _Inout_ OrtSessionOptions* options,\r\n                  _In_ OrtLoggingFunction user_logging_function, _In_opt_ void* user_logging_param);\r\n\r\n  /**\r\n   * Get number of input from OrtShapeInferContext\r\n   *\r\n   * \\param[in] context\r\n   * \\param[out] out The number of inputs\r\n   *\r\n   * \\since Version 1.17.\r\n   */\r\n  ORT_API2_STATUS(ShapeInferContext_GetInputCount, _In_ const OrtShapeInferContext* context, _Out_ size_t* out);\r\n\r\n  /**\r\n   * Get type and shape info of an input\r\n   *\r\n   * \\param[in] context\r\n   * \\param[in] index The index of the input\r\n   * \\param[out] info Type shape info of the input\r\n   *\r\n   * \\since Version 1.17.\r\n   */\r\n  ORT_API2_STATUS(ShapeInferContext_GetInputTypeShape, _In_ const OrtShapeInferContext* context, _In_ size_t index, _Outptr_ OrtTensorTypeAndShapeInfo** info);\r\n\r\n  /**\r\n   * Get attribute from OrtShapeInferContext. Note that OrtShapeInferContext is a per-node context, one could only read attribute from current node.\r\n   *\r\n   * \\param[in] context\r\n   * \\param[in] attr_name Name of the attribute\r\n   * \\param[out] attr Handle of the attribute fetched\r\n   *\r\n   * \\since Version 1.17.\r\n   */\r\n  ORT_API2_STATUS(ShapeInferContext_GetAttribute, _In_ const OrtShapeInferContext* context, _In_ const char* attr_name, _Outptr_ const OrtOpAttr** attr);\r\n\r\n  /**\r\n   * Set type and shape info of an output\r\n   *\r\n   * \\param[in] context\r\n   * \\param[in] index The index of the output\r\n   * \\param[out] info Type shape info of the output\r\n   *\r\n   * \\since Version 1.17.\r\n   */\r\n  ORT_API2_STATUS(ShapeInferContext_SetOutputTypeShape, _In_ const OrtShapeInferContext* context, _In_ size_t index, _In_ const OrtTensorTypeAndShapeInfo* info);\r\n\r\n  /**\r\n   * Set symbolic shape to type shape info\r\n   *\r\n   * \\param[in] info Type shape info\r\n   * \\param[in] dim_params Symbolic strings\r\n   * \\param[in] dim_params_length Number of strings\r\n   *\r\n   * \\since Version 1.17.\r\n   */\r\n  ORT_API2_STATUS(SetSymbolicDimensions, _In_ OrtTensorTypeAndShapeInfo* info, _In_ const char* dim_params[], _In_ size_t dim_params_length);\r\n\r\n  /**\r\n   * Read contents of an attribute to data\r\n   *\r\n   * \\param[in] op_attr\r\n   * \\param[in] type Attribute type\r\n   * \\param[out] data Memory address to save raw content of the attribute\r\n   * \\param[in] len Number of bytes allowed to store in data\r\n   * \\param[out] out Number of bytes required to save the data when the call failed, or the real number of bytes saved to data on success\r\n   *\r\n   * \\note Does not support reading graph attributes. Refer to Node_GetSubgraphs.\r\n   *\r\n   * \\since Version 1.17.\r\n   */\r\n  ORT_API2_STATUS(ReadOpAttr, _In_ const OrtOpAttr* op_attr, _In_ OrtOpAttrType type, _Inout_ void* data, _In_ size_t len, _Out_ size_t* out);\r\n\r\n  /** \\brief Set whether to use deterministic compute.\r\n   *\r\n   * Default is false. If set to true, this will enable deterministic compute for GPU kernels where possible.\r\n   * Note that this most likely will have a performance cost.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] value\r\n   *\r\n   * \\since Version 1.17.\r\n   */\r\n  ORT_API2_STATUS(SetDeterministicCompute, _Inout_ OrtSessionOptions* options, bool value);\r\n\r\n  /**\r\n   * Run fn in parallel\r\n   *\r\n   * \\param[in] context\r\n   * \\param[in] fn Function accepting usr_data and an integer as iterator\r\n   * \\param[in] total The number of times fn is to be invoked\r\n   * \\param[in] num_batch Number of batches by which the \"total\" is to be divided in maximum. When zero, there is no limit\r\n   * \\param[in] usr_data User data to be passed back to fn\r\n   *\r\n   * \\since Version 1.17.\r\n   */\r\n  ORT_API2_STATUS(KernelContext_ParallelFor, _In_ const OrtKernelContext* context, _In_ void (*fn)(void*, size_t), _In_ size_t total, _In_ size_t num_batch, _In_ void* usr_data);\r\n\r\n  /** \\brief Append OpenVINO execution provider to the session options\r\n   *\r\n   * If OpenVINO is not available (due to a non OpenVINO enabled build, or if OpenVINO is not installed on the system), this function will fail.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] provider_options_keys\r\n   * \\param[in] provider_options_values\r\n   * \\param[in] num_keys\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.17.\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsAppendExecutionProvider_OpenVINO_V2,\r\n                  _In_ OrtSessionOptions* options,\r\n                  _In_reads_(num_keys) const char* const* provider_options_keys,\r\n                  _In_reads_(num_keys) const char* const* provider_options_values,\r\n                  _In_ size_t num_keys);\r\n\r\n  /** \\brief Append VitisAI provider to session options\r\n   *\r\n   * If VitisAI is not available (due to a non VitisAI enabled build, or if VitisAI is not installed on the system), this function will return failure.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] provider_options_keys\r\n   * \\param[in] provider_options_values\r\n   * \\param[in] num_keys\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.18.\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsAppendExecutionProvider_VitisAI,\r\n                  _In_ OrtSessionOptions* options,\r\n                  _In_reads_(num_keys) const char* const* provider_options_keys,\r\n                  _In_reads_(num_keys) const char* const* provider_options_values,\r\n                  _In_ size_t num_keys);\r\n\r\n  /** \\brief Get scratch buffer from the corresponding allocator under the specific OrtMemoryInfo object.\r\n   *         NOTE: callers are responsible to release this scratch buffer from the corresponding allocator\r\n   *  \\param[in] context OrtKernelContext instance\r\n   *  \\param[in] mem_info OrtMemoryInfo instance\r\n   *  \\param[in] count_or_bytes How many bytes is this scratch buffer\r\n   *  \\param[out] out A pointer to the scratch buffer\r\n   *\r\n   *  \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.18.\r\n   */\r\n  ORT_API2_STATUS(KernelContext_GetScratchBuffer, _In_ const OrtKernelContext* context, _In_ const OrtMemoryInfo* mem_info, _In_ size_t count_or_bytes, _Outptr_ void** out);\r\n\r\n  /** \\brief Get allocator from KernelInfo for a specific memory type. Please use C API ReleaseAllocator to release out object\r\n   *\r\n   * \\param[in] info OrtKernelInfo instance\r\n   * \\param[in] mem_type OrtMemType object\r\n   * \\param[out] out A pointer to OrtAllocator\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.18.\r\n   */\r\n  ORT_API2_STATUS(KernelInfoGetAllocator, _In_ const OrtKernelInfo* info, _In_ OrtMemType mem_type, _Outptr_ OrtAllocator** out);\r\n\r\n  /** \\brief Replace initialized Tensors with external data with the provided files in memory\r\n   *\r\n   * The function will find the initialized TensorProtos with external data in the graph with the provided\r\n   * external file names and the file content in memory. The API gets the external file name, offset, data length\r\n   * from TensorProto, and locate the tensor data from the file in memory buffer.\r\n   * It creates a Tensor to replace the existing Tensor in graph. The replacement\r\n   * will occur before any of the optimizations take place. The data will be copied into the graph\r\n   * since TensorProto can't refer to the user provided buffers.\r\n   *\r\n   * \\param[in] options\r\n   * \\param[in] external_initializer_file_names Array of null terminated UTF-8 encoded strings of the file names\r\n   *            which holds the external initializers.\r\n   * \\param[in] external_initializer_file_buffer_array Array of pointers to the buffer of the file content.\r\n   *            The buffer can be freed after session creation.\r\n   * \\param[in] external_initializer_file_lengths Array of size_t to indicate the length of file content\r\n   * \\param[in] num_external_initializer_files Number of external files\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.18.\r\n   */\r\n  ORT_API2_STATUS(AddExternalInitializersFromFilesInMemory, _In_ OrtSessionOptions* options,\r\n                  _In_reads_(num_external_initializer_files) const ORTCHAR_T* const* external_initializer_file_names,\r\n                  _In_reads_(num_external_initializer_files) char* const* external_initializer_file_buffer_array,\r\n                  _In_reads_(num_external_initializer_files) const size_t* external_initializer_file_lengths,\r\n                  size_t num_external_initializer_files);\r\n\r\n  /** \\brief Create an OrtLoraAdapter\r\n   *\r\n   * The function attempts to locate file specified by adapter_file_path, read it and create an OrtLoraAdapter\r\n   * instance. The adapter_file_path should be a valid path to a file that contains a valid Lora Adapter\r\n   * format. The function attempts to validate the format at load time. The file will always be memory mapped, unless\r\n   * the platform does not support memory mapping, in which case the file will be read into memory.\r\n   *\r\n   * \\param[in] adapter_file_path adapter file path.\r\n   * \\param[in] allocator optional pointer to a device allocator. If specified\r\n   *            data is copied to the device at some point before Run() is invoked. If nullptr, data stays on CPU.\r\n   *            The data would still be copied to device if required by the model at inference time.\r\n   * \\param[out] out A pointer to a newly created OrtLoraAdapter instance. Must be released with\r\n   *                  OrtApi::ReleaseLoraAdapter.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.20.\r\n   */\r\n  ORT_API2_STATUS(CreateLoraAdapter, const ORTCHAR_T* adapter_file_path, _In_ OrtAllocator* allocator,\r\n                  _Outptr_ OrtLoraAdapter** out);\r\n\r\n  /** \\brief Create an OrtLoraAdapter\r\n   *\r\n   * The function copies the bytes from the array and creates an OrtLoraAdapter instance.\r\n   *\r\n   *\r\n   * \\param[in] bytes pointer to a valid Lora Adapter format buffer.\r\n   * \\param[in] num_bytes length of bytes buffer.\r\n   * \\param[in] allocator optional pointer to a device allocator. If specified\r\n   *            data is copied to the device at some point before Run() is invoked. If nullptr, data stays on CPU.\r\n   *            The data would still be copied to device if required by the model at inference time.\r\n   * \\param[out] out A pointer to a newly created OrtLoraAdapter instance. Must be released with\r\n   *                  OrtApi::ReleaseLoraAdapter.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.20.\r\n   */\r\n  ORT_API2_STATUS(CreateLoraAdapterFromArray, _In_ const void* bytes, size_t num_bytes, _In_ OrtAllocator* allocator,\r\n                  _Outptr_ OrtLoraAdapter** out);\r\n\r\n  /** \\brief Release an ::OrtLoraAdapter obtained from OrtApi::CreateLoraAdapter\r\n   */\r\n  ORT_CLASS_RELEASE(LoraAdapter);\r\n\r\n  /** \\brief Add the Lora Adapter to the list of active adapters.\r\n   *\r\n   * The function adds the Lora Adapter to the list of active adapters. The Lora Adapter must be created with\r\n   * OrtApi::CreateLoraAdapter or FromArray. The Lora Adapter will be used by the session to run the model.\r\n   * The instance of the OrtRunOptions can then be used to customize the Run() calls.\r\n   * More than one OrtLoraAdapter can be active at the same time. Lora Parameters that belong to different\r\n   * Lora adapters that will be active at the same time must not overlap.\r\n   * This setting does not affect RunWithBinding.\r\n   *\r\n   * \\param[in] options OrtRunOptions instance\r\n   * \\param[in] adapter OrtLoraAdapter instance\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.20.\r\n   */\r\n  ORT_API2_STATUS(RunOptionsAddActiveLoraAdapter, _Inout_ OrtRunOptions* options, _In_ const OrtLoraAdapter* adapter);\r\n\r\n  /// @}\r\n  /// \\name OrtEpDynamicOptions\r\n  /// @{\r\n\r\n  /** \\brief Set DynamicOptions for EPs (Execution Providers)\r\n   *\r\n   * Valid options can be found in `include\\onnxruntime\\core\\session\\onnxruntime_session_options_config_keys.h`\r\n   * Look for `kOrtEpDynamicOptions`\r\n   *\r\n   * \\param[in] sess OrtSession\r\n   * \\param[in] keys Array of null terminated UTF8 encoded strings of EP dynamic option keys\r\n   * \\param[in] values Array of null terminated UTF8 encoded string of EP dynamic option values\r\n   * \\param[in] kv_len Number of elements in the keys and values arrays\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.20.\r\n   */\r\n  ORT_API2_STATUS(SetEpDynamicOptions, _Inout_ OrtSession* sess, _In_reads_(kv_len) const char* const* keys,\r\n                  _In_reads_(kv_len) const char* const* values, _In_ size_t kv_len);\r\n\r\n  /// @}\r\n\r\n  /** \\brief Release an OrtValueInfo instance if it was not added to an OrtGraph.\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_CLASS_RELEASE(ValueInfo);\r\n\r\n  /** \\brief Release an OrtNode if it was not added to an OrtGraph.\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_CLASS_RELEASE(Node);\r\n\r\n  /** \\brief Release an OrtGraph.\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_CLASS_RELEASE(Graph);\r\n\r\n  /** \\brief Release an OrtModel.\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_CLASS_RELEASE(Model);\r\n\r\n  /** \\brief Get the value name from an OrtValueInfo instance.\r\n   * \\param[in] value_info The OrtValueInfo instance.\r\n   * \\param[out] name The name of the OrtValueInfo\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(GetValueInfoName, _In_ const OrtValueInfo* value_info, _Out_ const char** name);\r\n\r\n  /** \\brief Get the type information from an OrtValueInfo instance.\r\n   * \\param[in] value_info The OrtValueInfo instance.\r\n   * \\param[out] type_info The type info of the OrtValueInfo\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(GetValueInfoTypeInfo, _In_ const OrtValueInfo* value_info, _Outptr_ const OrtTypeInfo** type_info);\r\n\r\n  /** \\brief Get the Model Editor API instance\r\n   *\r\n   * Get the Model Editor API instance to create a new model or augment an existing model.\r\n   *\r\n   * \\return Model Editor API struct\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  const OrtModelEditorApi*(ORT_API_CALL* GetModelEditorApi)(void);\r\n\r\n  /** \\brief Create an OrtValue for a Tensor that uses pre-existing memory.\r\n   *\r\n   * ORT will take ownership of the memory and free it using the provided deleter when no longer in use.\r\n   *\r\n   * \\param[in] deleter OrtAllocator instance that will be used to free the memory.\r\n   *                    Only the OrtAllocator:Info and OrtAllocator::Release functions are required.\r\n   *                    The OrtMemoryInfo returned by OrtAllocator::Info must match the location of p_data.\r\n   * \\param[in] p_data Pointer to the memory that will be used by the Tensor. ORT will take ownership of the memory.\r\n   * \\param[in] p_data_len Length of the memory in bytes.\r\n   * \\param[in] shape Dimensions of the Tensor. All values should be > 0.\r\n   * \\param[in] shape_len Number of dimensions in the shape array.\r\n   * \\param[in] type Data type of the Tensor.\r\n   * \\param[out] out Newly created ::OrtValue. Must be freed with OrtApi::ReleaseValue\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CreateTensorWithDataAndDeleterAsOrtValue, _In_ OrtAllocator* deleter,\r\n                  _In_ void* p_data, size_t p_data_len,\r\n                  _In_ const int64_t* shape, size_t shape_len,\r\n                  ONNXTensorElementDataType type,\r\n                  _Outptr_ OrtValue** out);\r\n\r\n  /** \\brief sets load cancellation flag to abort session loading process.\r\n   *\r\n   * \\param[in] options instance that was passed to the session at creation time.\r\n   * \\param[in] cancel setting this to true after model loading process was initiated will\r\n   *            attempt to cancel the loading process. If cancellation is successful, CreateSession()\r\n   *            CreateSessionFromArray() or any other session creation API that take session options as an\r\n   *            argument will return an OrtStatus indicating that session loading was canceled at user request,\r\n   *            error code ORT_MODEL_LOAD_CANCELED.\r\n   *            The APIs above would not return any valid Session instance. This is the best case effort and the result\r\n   *            is not guaranteed. The session may have already been created and initialized\r\n   *            before the cancellation request was issued.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsSetLoadCancellationFlag, _Inout_ OrtSessionOptions* options,\r\n                  _In_ bool cancel);\r\n\r\n  /** \\brief Get the Compile API instance.\r\n   *\r\n   * Get the Compile API instance to compile ONNX models. Execution providers that support compilation fuse a subgraph\r\n   * into an EPContext node that wraps a provider-specific binary representation of the subgraph.\r\n   * For more details about the EPContext design, refer to:\r\n   *  \\htmlonly\r\n   *  <a href=\"https://onnxruntime.ai/docs/execution-providers/EP-Context-Design.html\">EPContext design document.</a>\r\n   *  \\endhtmlonly\r\n   *\r\n   * \\return Compile API struct instance.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  const OrtCompileApi*(ORT_API_CALL* GetCompileApi)(void);\r\n\r\n  //\r\n  // OrtKeyValuePairs\r\n  //\r\n\r\n  /** \\brief Create an OrtKeyValuePairs instance.\r\n   *\r\n   * \\param[out] out A pointer to a newly created OrtKeyValuePairs instance.\r\n   *\r\n   * \\note Must be released by calling ReleaseKeyValuePairs.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  void(ORT_API_CALL* CreateKeyValuePairs)(_Outptr_ OrtKeyValuePairs** out);\r\n\r\n  /** \\brief Add a key-value pair to the OrtKeyValuePairs instance.\r\n   *\r\n   * If a pair with the same key already exists, it is overwritten.\r\n   *\r\n   * \\param[in] kvps OrtKeyValuePairs instance.\r\n   * \\param[in] key Key to be added.\r\n   * \\param[in] value Value to be added.\r\n   *\r\n   * \\note The `key` and `value` are copied internally.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n\r\n  void(ORT_API_CALL* AddKeyValuePair)(_In_ OrtKeyValuePairs* kvps, _In_ const char* key, _In_ const char* value);\r\n\r\n  /** \\brief Get the value associated with a key in the OrtKeyValuePairs instance.\r\n   *\r\n   * \\param[in] kvps OrtKeyValuePairs instance.\r\n   * \\param[in] key Key to be searched.\r\n   *\r\n   * \\return The value associated with the key, or nullptr if the key does not exist.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  const char*(ORT_API_CALL* GetKeyValue)(_In_ const OrtKeyValuePairs* kvps, _In_ const char* key);\r\n\r\n  /** \\brief Get all the key-value pairs from the OrtKeyValuePairs instance.\r\n   *\r\n   * \\param[in] kvps OrtKeyValuePairs instance.\r\n   * \\param[out] keys Array of keys from `kvps`.\r\n   * \\param[out] values Array of values from `kvps`.\r\n   * \\param[out] num_entries Number of entries in `keys` and `values`.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  void(ORT_API_CALL* GetKeyValuePairs)(_In_ const OrtKeyValuePairs* kvps,\r\n                                       _Outptr_ const char* const** keys, _Outptr_ const char* const** values,\r\n                                       _Out_ size_t* num_entries);\r\n\r\n  /** \\brief Remove a key-value pair from the OrtKeyValuePairs instance.\r\n   *\r\n   * \\param[in] kvps OrtKeyValuePairs instance.\r\n   * \\param[in] key Key to be removed. No error if not found.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  void(ORT_API_CALL* RemoveKeyValuePair)(_In_ OrtKeyValuePairs* kvps, _In_ const char* key);\r\n\r\n  /** \\brief Release an OrtKeyValuePairs instance.\r\n   *\r\n   * \\param[in] input OrtKeyValuePairs instance to be released.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_CLASS_RELEASE(KeyValuePairs);\r\n\r\n  /** \\brief Register an execution provider library with ORT.\r\n   *\r\n   * The library must export 'CreateEpFactories' and 'ReleaseEpFactory' functions.\r\n   * See OrtEpApi for more details.\r\n   *\r\n   * \\param[in] env The OrtEnv instance to register the library in.\r\n   * \\param[in] registration_name The name to register the execution provider library under.\r\n   * \\param[in] path The path to the execution provider library.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(RegisterExecutionProviderLibrary, _In_ OrtEnv* env, _In_ const char* registration_name,\r\n                  _In_ const ORTCHAR_T* path);\r\n\r\n  /** \\brief Unregister an execution provider library with ORT.\r\n   *\r\n   * ORT will call ReleaseEpFactory for all factories created by the library, and unload the library.\r\n   *\r\n   * You <b>MUST</b> ensure there are no Session instances using execution providers created by the library\r\n   * before calling this function.\r\n   *\r\n   * \\param[in] env The OrtEnv instance to unregister the library from.\r\n   * \\param[in] registration_name The name the execution provider library was registered under.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(UnregisterExecutionProviderLibrary, _In_ OrtEnv* env, _In_ const char* registration_name);\r\n\r\n  /** \\brief Get the list of available OrtEpDevice instances.\r\n   *\r\n   * Each OrtEpDevice instance contains details of the execution provider and the device it will use.\r\n   *\r\n   * \\param[in] env The OrtEnv instance to query.\r\n   * \\param[out] ep_devices The OrtEpDevice instances that the execution provider will use.\r\n   * \\param[out] num_ep_devices The number of OrtEpDevice instances returned.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(GetEpDevices, _In_ const OrtEnv* env,\r\n                  _Outptr_ const OrtEpDevice* const** ep_devices, _Out_ size_t* num_ep_devices);\r\n\r\n  /** \\brief Append the execution provider that is responsible for the selected OrtEpDevice instances\r\n   *         to the session options.\r\n   *\r\n   * \\param[in] session_options Session options to add execution provider to.\r\n   * \\param[in] env Environment that execution providers were registered with.\r\n   * \\param[in] ep_devices One or more OrtEpDevice instances to create an execution provider for.\r\n   *                       Obtain from GetEpDevices. All OrtEpDevice instances must be from the same execution\r\n   *                       provider. It is only necessary to provide multiple OrtEpDevices if you want to use the\r\n   *                       same execution provider for multiple devices.\r\n   *                       e.g. the EP is capable of running on GPU and NPU.\r\n   * \\param[in] num_ep_devices Number of OrtEpDevice instances.\r\n   * \\param[in] ep_option_keys Optional keys to configure the execution provider.\r\n   * \\param[in] ep_option_vals Optional values to configure the execution provider.\r\n   * \\param[in] num_ep_options Number of execution provide options to add.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsAppendExecutionProvider_V2, _In_ OrtSessionOptions* session_options,\r\n                  _In_ OrtEnv* env,\r\n                  _In_reads_(num_ep_devices) const OrtEpDevice* const* ep_devices, _In_ size_t num_ep_devices,\r\n                  _In_reads_(num_op_options) const char* const* ep_option_keys,\r\n                  _In_reads_(num_op_options) const char* const* ep_option_vals,\r\n                  size_t num_ep_options);\r\n\r\n  /** \\brief Set the execution provider selection policy for the session.\r\n   *\r\n   * Allows users to specify a device selection policy for automatic execution provider (EP) selection.\r\n   * If custom selection is required please use SessionOptionsSetEpSelectionPolicyDelegate instead.\r\n   *\r\n   * \\param[in] session_options The OrtSessionOptions instance.\r\n   * \\param[in] policy The device selection policy to use (see OrtExecutionProviderDevicePolicy).\r\n   *\r\n   * \\since Version 1.22\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsSetEpSelectionPolicy, _In_ OrtSessionOptions* session_options,\r\n                  _In_ OrtExecutionProviderDevicePolicy policy);\r\n\r\n  /** \\brief Set the execution provider selection policy delegate for the session.\r\n   *\r\n   * Allows users to provide a custom device selection policy for automatic execution provider (EP) selection.\r\n   *\r\n   * \\param[in] session_options The OrtSessionOptions instance.\r\n   * \\param[in] delegate Delegate callback for custom selection.\r\n   * \\param[in] delegate_state Optional state that will be passed to the delegate callback. nullptr if not required.\r\n   *\r\n   * \\since Version 1.22\r\n   */\r\n  ORT_API2_STATUS(SessionOptionsSetEpSelectionPolicyDelegate, _In_ OrtSessionOptions* session_options,\r\n                  _In_ EpSelectionDelegate delegate,\r\n                  _In_opt_ void* delegate_state);\r\n\r\n  /** \\brief Get the hardware device type.\r\n   *\r\n   * \\param[in] device The OrtHardwareDevice instance to query.\r\n   * \\return The hardware device type.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  OrtHardwareDeviceType(ORT_API_CALL* HardwareDevice_Type)(_In_ const OrtHardwareDevice* device);\r\n\r\n  /** \\brief Get the hardware device's vendor identifier.\r\n   *\r\n   * \\param[in] device The OrtHardwareDevice instance to query.\r\n   * \\return The hardware device vendor identifier.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  uint32_t(ORT_API_CALL* HardwareDevice_VendorId)(_In_ const OrtHardwareDevice* device);\r\n\r\n  /** \\brief Get the hardware device's vendor name.\r\n   *\r\n   * \\param[in] device The OrtHardwareDevice instance to query.\r\n   * \\return The hardware device's vendor name.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  const char*(ORT_API_CALL* HardwareDevice_Vendor)(_In_ const OrtHardwareDevice* device);\r\n\r\n  /** \\brief Get the hardware device's unique identifier.\r\n   *\r\n   * \\param[in] device The OrtHardwareDevice instance to query.\r\n   * \\return The device id.\r\n   *\r\n   * \\note This is not a unique identifier. It identifies the hardware type when combined with vendor id.\r\n   * \\since Version 1.22.\r\n   */\r\n  uint32_t(ORT_API_CALL* HardwareDevice_DeviceId)(_In_ const OrtHardwareDevice* device);\r\n\r\n  /** \\brief Get hardware device metadata.\r\n   *\r\n   * \\param[in] device The OrtHardwareDevice instance to query.\r\n   * \\return An OrtKeyValuePairs instance containing the metadata for the device.\r\n   *         Note: ORT owns the instance so the user must not call ReleaseKeyValuePairs with it.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  const OrtKeyValuePairs*(ORT_API_CALL* HardwareDevice_Metadata)(_In_ const OrtHardwareDevice* device);\r\n\r\n  /** \\brief Get the execution provider name.\r\n   *\r\n   * \\param[in] ep_device The OrtEpDevice instance to query.\r\n   * \\return The execution provider name.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  const char*(ORT_API_CALL* EpDevice_EpName)(_In_ const OrtEpDevice* ep_device);\r\n\r\n  /** \\brief Get the execution provider's vendor name.\r\n   *\r\n   * \\param[in] ep_device The OrtEpDevice instance to query.\r\n   * \\return The execution provider's vendor name.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  const char*(ORT_API_CALL* EpDevice_EpVendor)(_In_ const OrtEpDevice* ep_device);\r\n\r\n  /** \\brief Get the metadata for the OrtEpDevice.\r\n   *\r\n   * \\param[in] ep_device The OrtEpDevice instance to query.\r\n   * \\return An OrtKeyValuePairs instance containing the metadata for the device.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  const OrtKeyValuePairs*(ORT_API_CALL* EpDevice_EpMetadata)(_In_ const OrtEpDevice* ep_device);\r\n\r\n  /** \\brief Get the execution provider options for the OrtEpDevice.\r\n   *\r\n   * \\param[in] ep_device The OrtEpDevice instance to query.\r\n   * \\return An OrtKeyValuePairs instance containing the execution provider options for the device.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  const OrtKeyValuePairs*(ORT_API_CALL* EpDevice_EpOptions)(_In_ const OrtEpDevice* ep_device);\r\n\r\n  /** \\brief Get the OrtHardwareDevice instance for the OrtEpDevice.\r\n   *\r\n   * \\param[in] ep_device The OrtEpDevice instance to query.\r\n   * \\return The OrtHardwareDevice instance for the device.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  const OrtHardwareDevice*(ORT_API_CALL* EpDevice_Device)(_In_ const OrtEpDevice* ep_device);\r\n\r\n  /** \\brief Get the OrtEpApi instance for implementing an execution provider.\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  const OrtEpApi*(ORT_API_CALL* GetEpApi)(void);\r\n\r\n  /** \\brief Compute total size in bytes of the tensor data contained in an OrtValue.\r\n   *\r\n   * Returns the total number of bytes used to store the tensor data. For numeric tensors,\r\n   * this is sizeof(element_type) * total_element_count. OrtValues that are not tensors or\r\n   * that are tensors that contain strings will cause an error to be returned.\r\n   *\r\n   * \\param[in] ort_value OrtValue instance containing a tensor\r\n   * \\param[out] size The total size of the tensor data in bytes\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API2_STATUS(GetTensorSizeInBytes, _In_ const OrtValue* ort_value, _Out_ size_t* size);\r\n\r\n  /** \\brief Calls OrtAllocator::GetStats function\r\n   *\r\n   * Return a pointer to the OrtKeyValuePairs structure that contains the statistics of the allocator\r\n   * and the user should call OrtApi::ReleaseKeyValuePairs.\r\n   *\r\n   * NOTE: If the allocator does not implement this function, the OrtKeyValuePairs instance will be empty.\r\n   *\r\n   * \\param[in] ort_allocator The allocator to get stats from\r\n   * \\param[out] out A pointer to the OrtKeyValuePairs instance that contains the stats\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(AllocatorGetStats, _In_ const OrtAllocator* ort_allocator, _Outptr_ OrtKeyValuePairs** out);\r\n\r\n  /** \\brief Create an ::OrtMemoryInfo\r\n   *\r\n   * \\param[in] name Arbitrary name.\r\n   * \\param[in] device_type Device type.\r\n   * \\param[in] vendor_id PCI Vendor ID. Use 0 for a generic allocator (e.g. WebGPU).\r\n   * \\param[in] device_id Device ID if there are multiple devices of the same type. e.g. 2 GPU devices.\r\n   * \\param[in] mem_type Memory type. Use OrtDeviceMemoryType_DEFAULT for device memory, and\r\n   *                     OrtDeviceMemoryType_HOST_ACCESSIBLE (if applicable) for memory used to transfer between the\r\n   *                     device and the CPU. Use the device_type and device_id of the GPU/NPU that the memory is also\r\n   *                     accessible to.\r\n   * \\param[in] alignment Alignment of the memory if required. Pass 0 for default alignment.\r\n   * \\param[in] allocator_type Allocator type. If OrtAllocatorType::OrtArenaAllocator, the ORT arena will be used.\r\n   *                           Caveat: Support for OrtArenaAllocator is currently limited to usage of internal ORT\r\n   *                           allocators via CreateAllocator/CreateAndRegisterAllocator/CreateAndRegisterAllocatorV2.\r\n   * \\param[out] out Newly created ::OrtMemoryInfo. Must be freed with OrtApi::ReleaseMemoryInfo\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API2_STATUS(CreateMemoryInfo_V2, _In_ const char* name, _In_ enum OrtMemoryInfoDeviceType device_type,\r\n                  _In_ uint32_t vendor_id, _In_ int32_t device_id, _In_ enum OrtDeviceMemoryType mem_type,\r\n                  _In_ size_t alignment, enum OrtAllocatorType allocator_type,\r\n                  _Outptr_ OrtMemoryInfo** out);\r\n\r\n  /** \\brief Get the device memory type from ::OrtMemoryInfo\r\n   *\r\n   * \\param[in] ptr The OrtMemoryInfo instance to query.\r\n   * \\return The device memory type.\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API_T(OrtDeviceMemoryType, MemoryInfoGetDeviceMemType, _In_ const OrtMemoryInfo* ptr);\r\n\r\n  /** \\brief Get the vendor id from ::OrtMemoryInfo\r\n   *\r\n   * \\param[in] ptr The OrtMemoryInfo instance to query.\r\n   * \\return The vendor id.\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API_T(uint32_t, MemoryInfoGetVendorId, _In_ const OrtMemoryInfo* ptr);\r\n\r\n  /// \\name OrtValueInfo\r\n  /// @{\r\n\r\n  /** \\brief Get the OrtNode that produces the value represented by the given OrtValueInfo.\r\n   * Optionally returns the associated output index.\r\n   *\r\n   * \\param[in] value_info The OrtValueInfo instance.\r\n   * \\param[out] producer_node Output parameter set to the OrtNode that produces the OrtValueInfo.\r\n   * \\param[out] producer_output_index Optional output parameter set to the OrtNode instance's output index\r\n   *                                   that produces the value. Ignored if set to NULL.\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ValueInfo_GetValueProducer, _In_ const OrtValueInfo* value_info,\r\n                  _Outptr_ const OrtNode** producer_node, _Out_opt_ size_t* producer_output_index);\r\n\r\n  /** \\brief Get the number of consumers of a value as a node input.\r\n   *\r\n   * Only nodes are considered \"consumers\" by this function. To check if an OrtValueInfo is a graph output,\r\n   * call ValueInfo_IsGraphOutput().\r\n   *\r\n   * A single OrtNode may use a single value for more than one input (e.g., Mul(x, x)), so the returned\r\n   * `num_consumers` may be larger than the number of unique OrtNode instances that consume the value.\r\n   *\r\n   * \\param[in] value_info The OrtValueInfo instance.\r\n   * \\param[out] num_consumers Output parameter set to the number of consumers of the value.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ValueInfo_GetValueNumConsumers, _In_ const OrtValueInfo* value_info, _Out_ size_t* num_consumers);\r\n\r\n  /** \\brief Returns information (OrtNode and input index) for all consumer nodes that use the value as an input.\r\n   *\r\n   * Only nodes are considered \"consumers\" by this function.\r\n   *\r\n   * Caller provides 2 pre-allocated arrays that will be filled with the OrtNode and input index values.\r\n   * Use ValueInfo_GetValueNumConsumers() to get the number of consumers of the value.\r\n   *\r\n   * An OrtNode instance may appear multiple times if it uses the given value more than once.\r\n   * Example: For a node MulNode(x, x) that consumes the value 'x' twice, the following is returned:\r\n   *   - nodes: [MulNode, MulNode]\r\n   *   - input_indices: [0, 1]\r\n   *\r\n   * \\param[in] value_info The OrtValueInfo instance.\r\n   * \\param[out] nodes Pre-allocated array of size `num_consumers` that is filled with OrtNode instances.\r\n   * \\param[out] input_indices Pre-allocated array of `num_consumers` elements that is filled\r\n   *                           with input indices. Index is set to -1 for an \"implicit\" input to a consumer node\r\n   *                           that contains a subgraph (e.g., If, Loop) with nodes that use the value internally.\r\n   * \\param[in] num_consumers The size of the `consumer_nodes` and `consumer_input_indices` arrays.\r\n   *                          Typical usage sets this to the value of ValueInfo_GetValueNumConsumers().\r\n   *                          An error status is returned if `num_consumers` is less than the number of actual\r\n   *                          consumers.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ValueInfo_GetValueConsumers, _In_ const OrtValueInfo* value_info,\r\n                  _Out_writes_all_(num_consumers) const OrtNode** nodes,\r\n                  _Out_writes_all_(num_consumers) int64_t* input_indices,\r\n                  _In_ size_t num_consumers);\r\n\r\n  /** \\brief Get the underlying initializer value, as an OrtValue, from the given OrtValueInfo.\r\n   *\r\n   * Sets the output parameter to NULL if the given OrtValueInfo does not represent an initializer.\r\n   * Does not return an error status in this case.\r\n   *\r\n   * Supports initializers defined in an outer scope (i.e., a parent graph).\r\n   *\r\n   * Supports initializers stored in an external file. For external initializers, ORT memory maps\r\n   * the initializer data on the first call to this function. If caller needs custom memory mapping,\r\n   * use ValueInfo_GetExternalInitializerInfo to get the location of the initializer data.\r\n   *\r\n   * \\param[in] value_info The OrtValueInfo instance.\r\n   * \\param[out] initializer_value Output parameter set to the initializer value or NULL. Do not cache the OrtValue\r\n   *                               as it is released when the owning OrtGraph is released.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ValueInfo_GetInitializerValue, _In_ const OrtValueInfo* value_info,\r\n                  _Outptr_ const OrtValue** initializer_value);\r\n\r\n  /** \\brief Get information about an external initializer (e.g., filepath, file offset, byte size).\r\n   *\r\n   * Sets the output parameter `info` to NULL if the given OrtValueInfo does not represent an initializer\r\n   * with external data. In this case, a NULL status (non-error) is returned.\r\n   *\r\n   * \\param[in] value_info The OrtValueInfo instance.\r\n   * \\param[out] info Output parameter set to an OrtExternalInitializerInfo instance that can be used to query\r\n   *                  file path, file offset, etc. ORT sets this to NULL if the OrtValueInfo does not represent\r\n   *                  an external initializer.\r\n   *                  Must release with ReleaseExternalInitializerInfo.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ValueInfo_GetExternalInitializerInfo, _In_ const OrtValueInfo* value_info,\r\n                  _Outptr_result_maybenull_ OrtExternalInitializerInfo** info);\r\n\r\n  /** \\brief Returns a boolean indicating if the given value is a required graph input.\r\n   *\r\n   * For ONNX IR version < 4, all graph inputs without a matching initializer are required.\r\n   *\r\n   * For ONNX IR version >=4, a graph input with a matching initializer is an optional graph input\r\n   * with the initializer serving as the default value.\r\n   *\r\n   * \\param[in] value_info The OrtValueInfo instance representing the graph value.\r\n   * \\param[out] is_required_graph_input Output parameter set to true if the graph value is a required graph input.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ValueInfo_IsRequiredGraphInput, _In_ const OrtValueInfo* value_info,\r\n                  _Out_ bool* is_required_graph_input);\r\n\r\n  /** \\brief Returns a boolean indicating if the given value is an optional graph input.\r\n   *\r\n   * Optional graph inputs were introduced in ONNX IR version 4. For ONNX IR version >=4, a graph input with a\r\n   * matching initializer is an optional graph input with the initializer serving as the default value.\r\n   * The matching initializer is also known as a non-constant initializer.\r\n   *\r\n   * \\param[in] value_info The OrtValueInfo instance representing the graph value.\r\n   * \\param[out] is_optional_graph_input Output parameter set to true if the graph value is an optional graph input.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ValueInfo_IsOptionalGraphInput, _In_ const OrtValueInfo* value_info,\r\n                  _Out_ bool* is_optional_graph_input);\r\n\r\n  /** \\brief Returns a boolean indicating if the given value is a graph output.\r\n   *\r\n   * \\param[in] value_info The OrtValueInfo instance representing the graph value.\r\n   * \\param[out] is_graph_output Output parameter set to true if the graph value is a graph output.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ValueInfo_IsGraphOutput, _In_ const OrtValueInfo* value_info, _Out_ bool* is_graph_output);\r\n\r\n  /** \\brief Returns a boolean indicating if the given value is a constant initializer.\r\n   *\r\n   * For ONNX IR version < 4, all initializers are constant.\r\n   *\r\n   * For ONNX IR version >=4, an initializer that serves as the default value for a matching graph input is not a\r\n   * constant initializer.\r\n   *\r\n   * \\param[in] value_info The OrtValueInfo instance representing the graph value.\r\n   * \\param[out] is_constant_initializer Output parameter set to true if the graph value is a constant initializer.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ValueInfo_IsConstantInitializer, _In_ const OrtValueInfo* value_info,\r\n                  _Out_ bool* is_constant_initializer);\r\n\r\n  /** \\brief Returns a boolean indicating if the given value is defined in an outer scope.\r\n   *\r\n   * Certain operator types (e.g., If and Loop) contain nested subgraphs. This function enables\r\n   * determining whether a value is defined in a parent node's graph.\r\n   *\r\n   * \\param[in] value_info The OrtValueInfo instance representing the graph value.\r\n   * \\param[out] is_from_outer_scope Output parameter set to true if the value is defined in an outer\r\n   *                                 scope (i.e., a parent graph).\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ValueInfo_IsFromOuterScope, _In_ const OrtValueInfo* value_info,\r\n                  _Out_ bool* is_from_outer_scope);\r\n\r\n  /// @}\r\n\r\n  /// \\name OrtGraph\r\n  /// @{\r\n\r\n  /** \\brief Returns a graph's name.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] graph_name Output parameter set to the graph's name.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetName, _In_ const OrtGraph* graph, _Outptr_ const char** graph_name);\r\n\r\n  /** \\brief Get the filepath to the model from which an OrtGraph is constructed.\r\n   *\r\n   * \\note The model's filepath is empty if the filepath is unknown, such as when the model is loaded from bytes\r\n   * via CreateSessionFromArray.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] model_path Output parameter set to the model's null-terminated filepath.\r\n   *                        Set to an empty path string if unknown.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetModelPath, _In_ const OrtGraph* graph, _Outptr_ const ORTCHAR_T** model_path);\r\n\r\n  /** \\brief Returns the ONNX IR version.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] onnx_ir_version Output parameter set to the ONNX IR version.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetOnnxIRVersion, _In_ const OrtGraph* graph, _Out_ int64_t* onnx_ir_version);\r\n\r\n  /** \\brief Returns the number of operator sets that the graph's model uses.\r\n   *\r\n   * \\note An operator set is uniquely identified by the (domain, opset_version) pair. All models must have at\r\n   * least one entry that specifies which entry of the ONNX operator set is used. The ONNX domain is represented by\r\n   * an empty string.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] num_operator_sets Output parameter set to the number of operator sets that the graph's model uses.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetNumOperatorSets, _In_ const OrtGraph* graph, _Out_ size_t* num_operator_sets);\r\n\r\n  /** \\brief Returns the operator sets that the graph's model uses.\r\n   *\r\n   * \\note An operator set is uniquely identified by the (domain, opset_version) pair. All models must have at\r\n   * least one entry that specifies which entry of the ONNX operator set is used. The ONNX domain is represented by\r\n   * an empty string.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] domains Pre-allocated array of `num_operator_sets` elements that is filled with\r\n   *                     null-terminated domain names.\r\n   * \\param[out] opset_versions Pre-allocated array of `num_operator_sets` elements that is filled with\r\n   *                            the opset version of the corresponding domain in the `domains` array.\r\n   * \\param[in] num_operator_sets The size of the `domains` and `opset_versions` arrays.\r\n   *                              Typical usage sets this to the result of Graph_GetNumOperatorSets().\r\n   *                              An error status is returned if `num_operator_sets` is less than the actual number\r\n   *                              of operator sets.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetOperatorSets, _In_ const OrtGraph* graph,\r\n                  _Out_writes_(num_operator_sets) const char** domains,\r\n                  _Out_writes_(num_operator_sets) int64_t* opset_versions, _In_ size_t num_operator_sets);\r\n\r\n  /** \\brief Returns the number of graph inputs.\r\n   *\r\n   * \\note The count includes initializers that are included in the list of graph inputs.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] num_inputs Output parameter set to the number of graph inputs.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetNumInputs, _In_ const OrtGraph* graph, _Out_ size_t* num_inputs);\r\n\r\n  /** \\brief Returns the graph's inputs as OrtValueInfo instances.\r\n   *\r\n   * \\note The result includes initializers that are included in the list of graph inputs.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] inputs Pre-allocated array of `num_inputs` elements that is filled with the graph's inputs.\r\n   * \\param[in] num_inputs The size of the `inputs` array.\r\n   *                       Typical usage sets this to the result of Graph_GetNumInputs(). An error status is\r\n   *                       returned if `num_inputs` is less than the number of graph inputs.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetInputs, _In_ const OrtGraph* graph,\r\n                  _Out_writes_(num_inputs) const OrtValueInfo** inputs, _In_ size_t num_inputs);\r\n\r\n  /** \\brief Returns the number of graph outputs.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] num_outputs Output parameter set to the number of graph outputs.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetNumOutputs, _In_ const OrtGraph* graph, _Out_ size_t* num_outputs);\r\n\r\n  /** \\brief Returns the graph's outputs as OrtValueInfo instances.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] outputs Pre-allocated array of `num_outputs` elements that is filled with the graph's outputs.\r\n   * \\param[in] num_outputs The size of the `outputs` array.\r\n   *                        Typical usage sets this to the result of Graph_GetNumOutputs(). An error status is\r\n   *                        returned if `num_outputs` is less than the number of graph outputs.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetOutputs, _In_ const OrtGraph* graph,\r\n                  _Out_writes_(num_outputs) const OrtValueInfo** outputs, _In_ size_t num_outputs);\r\n\r\n  /** \\brief Returns the number of graph initializers.\r\n   *\r\n   * Counts constant and non-constant initializers.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] num_initializers Output parameter set to the number of graph initializers.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetNumInitializers, _In_ const OrtGraph* graph, _Out_ size_t* num_initializers);\r\n\r\n  /** \\brief Returns the graph's initializers as OrtValueInfo instances.\r\n   *\r\n   * Includes constant and non-constant initializers.\r\n   *\r\n   * For ONNX IR version < 4, all initializers are constant.\r\n   *\r\n   * For ONNX IR version >= 4, an initializer with a name that matches a graph input is considered a\r\n   * non-constant initializer.\r\n   *\r\n   * Call ValueInfo_GetInitializerValue to get the initializer's data.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] initializers Pre-allocated array of `num_outputs` elements that is filled with the initializers.\r\n   * \\param[in] num_initializers The size of the `initializers` array. Typical usage sets this to the\r\n   *                             result of Graph_GetNumInitializers(). An error status is returned if\r\n   *                            `num_initializers` is less than the number of graph initializers.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetInitializers, _In_ const OrtGraph* graph,\r\n                  _Out_writes_(num_initializers) const OrtValueInfo** initializers,\r\n                  _In_ size_t num_initializers);\r\n\r\n  /** \\brief Returns the number of graph nodes.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] num_nodes Output parameter set to the number of graph nodes.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetNumNodes, _In_ const OrtGraph* graph, _Out_ size_t* num_nodes);\r\n\r\n  /** \\brief Returns the graph's nodes as OrtNode instances.\r\n   *\r\n   * The nodes are sorted using a stable topological ordering. Callers are responsible for maintaining their\r\n   * own node ordering if a different order is required.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] nodes Pre-allocated array of `num_nodes` elements that is filled with the graph's nodes.\r\n   * \\param[in] num_nodes The size of the `nodes` array. Typical usage sets this to the\r\n   *                      result of Graph_GetNumNodes(). An error status is returned if\r\n   *                      `num_nodes` is less than the number of graph nodes.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetNodes, _In_ const OrtGraph* graph,\r\n                  _Out_writes_(num_nodes) const OrtNode** nodes, _In_ size_t num_nodes);\r\n\r\n  /** \\brief Get the parent node for the given graph, if any exists.\r\n   *\r\n   * Certain operator types (e.g., If and Loop) contain nested subgraphs. This function enables\r\n   * access to the parent node (e.g., the If and Loop node) from a nested subgraph.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] node Output parameter that is set to the graph's parent node.\r\n   *                  Set to NULL if a parent node does not exist (e.g., for a top-level graph).\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetParentNode, _In_ const OrtGraph* graph, _Outptr_result_maybenull_ const OrtNode** node);\r\n\r\n  /** \\brief Returns an OrtGraph that contains a subset of nodes in the source OrtGraph.\r\n   *\r\n   * \\note The lifetime of \"dst_graph\" is tied to that of \"src_graph\", as they both internally reference\r\n   * the same underlying graph.\r\n   *\r\n   * \\param[in] src_graph The source OrtGraph instance.\r\n   * \\param[in] nodes A subset of the nodes/OrtNodes in 'graph'.\r\n   * \\param[in] num_nodes Number of nodes.\r\n   * \\param[out] dst_graph An OrtGraph created from a given set of nodes. Must be released by calling ReleaseGraph.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetGraphView, _In_ const OrtGraph* src_graph, _In_ const OrtNode** nodes,\r\n                  _In_ size_t num_nodes, _Outptr_ OrtGraph** dst_graph);\r\n\r\n  /// @}\r\n\r\n  /// \\name OrtNode\r\n  /// @{\r\n\r\n  /** \\brief Returns a node's identifier.\r\n   *\r\n   * The node's identifier is only unique in the node's parent graph. Different nested subgraphs\r\n   * (e.g., subgraphs contained by If and Loop nodes) may reuse identifiers.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] node_id Output parameter set to the node's identifier.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetId, _In_ const OrtNode* node, _Out_ size_t* node_id);\r\n\r\n  /** \\brief Returns a node's name. Can be an empty string.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] node_name Output parameter set to the node's name.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetName, _In_ const OrtNode* node, _Outptr_ const char** node_name);\r\n\r\n  /** \\brief Returns a node's operator type (e.g., \"Conv\").\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] operator_type Output parameter set to the name of the node's operator type.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetOperatorType, _In_ const OrtNode* node, _Outptr_ const char** operator_type);\r\n\r\n  /** \\brief Returns a node's domain name.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] domain_name Output parameter set to the node's domain name.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetDomain, _In_ const OrtNode* node, _Outptr_ const char** domain_name);\r\n\r\n  /** \\brief Get the opset version in which the given node's operator type was first defined.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] since_version The opset version in which the node's operator type was first defined.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetSinceVersion, _In_ const OrtNode* node, _Out_ int* since_version);\r\n\r\n  /** \\brief Returns the number of node inputs.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] num_inputs Output parameter set to the number of node inputs.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetNumInputs, _In_ const OrtNode* node, _Out_ size_t* num_inputs);\r\n\r\n  /** \\brief Returns the node's inputs as OrtValueInfo instances.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] inputs Pre-allocated array of `num_inputs` elements that is filled with the node's inputs.\r\n   * \\param[in] num_inputs The size of the `inputs` array.\r\n   *                       Typical usage sets this to the result of Node_GetNumInputs(). An error status is\r\n   *                       returned if `num_inputs` is less than the number of node inputs.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetInputs, _In_ const OrtNode* node,\r\n                  _Out_writes_(num_inputs) const OrtValueInfo** inputs, _In_ size_t num_inputs);\r\n\r\n  /** \\brief Returns the number of node outputs.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] num_outputs Output parameter set to the number of node outputs.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetNumOutputs, _In_ const OrtNode* node, _Out_ size_t* num_outputs);\r\n\r\n  /** \\brief Returns the node's outputs as OrtValueInfo instances.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] outputs Pre-allocated array of `num_outputs` elements that is filled with the node's outputs.\r\n   * \\param[in] num_outputs The size of the `outputs` array.\r\n   *                        Typical usage sets this to the result of Node_GetNumOutputs(). An error status is\r\n   *                        returned if `num_outputs` is less than the number of node outputs.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetOutputs, _In_ const OrtNode* node,\r\n                  _Out_writes_(num_outputs) const OrtValueInfo** outputs, _In_ size_t num_outputs);\r\n\r\n  /** \\brief Returns the number of node implicit inputs.\r\n   *\r\n   * Certain operator types (e.g., If and Loop) contain nested subgraphs. The internal nodes within the nested subgraphs\r\n   * may use values from the outer scope. Those \"outer scope\" values are considered implicit inputs to the node that\r\n   * contains the subgraphs (e.g., the If or Loop node).\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] num_implicit_inputs Output parameter set to the number of node implicit inputs.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetNumImplicitInputs, _In_ const OrtNode* node, _Out_ size_t* num_implicit_inputs);\r\n\r\n  /** \\brief Get the implicit inputs, as OrtValueInfo instances, that are used within the given node's subgraphs.\r\n   *\r\n   * \\note Only certain operator types (e.g., If and Loop) contain nested subgraphs.\r\n   * The internal nodes within the nested subgraphs may use values from the outer scope. Those \"outer scope\" values\r\n   * are considered implicit inputs to the node that contains the subgraphs (e.g., the If or Loop node).\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] implicit_inputs Pre-allocated array of `num_implicit_inputs` elements that is filled the node's\r\n   *                             implicit inputs.\r\n   * \\param[in] num_implicit_inputs The size of the `implicit_inputs` array. Typical usage sets this to the result\r\n   *                                of Node_GetNumImplicitInputs(). An error status is returned if\r\n   *                                `num_implicit_inputs` is less than the number of node implicit inputs.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetImplicitInputs, _In_ const OrtNode* node,\r\n                  _Out_writes_(num_implicit_inputs) const OrtValueInfo** implicit_inputs,\r\n                  _In_ size_t num_implicit_inputs);\r\n\r\n  /** \\brief Returns the number of node attributes.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] num_attributes Output parameter set to the number of node attributes.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetNumAttributes, _In_ const OrtNode* node, _Out_ size_t* num_attributes);\r\n\r\n  /** \\brief Returns a node's attributes as OrtOpAttr instances.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] attributes Pre-allocated array of `num_attributes` elements that is filled with the node's attributes.\r\n   * \\param[in] num_attributes The size of the `num_attributes` array.\r\n   *                           Typical usage sets this to the result of Node_GetNumAttributes(). An error status is\r\n   *                           returned if `num_attributes` is less than the number of node attributes.\r\n   *\r\n   * \\note ONNX Runtime automatically sets optional (unset) attributes to their default values if the default value\r\n   * is a constant expression that does not depend on other tensor/model characteristics. Conv's 'kernel_shape'\r\n   * attribute is an example of an optional attribute that does not have a constant default value. This function\r\n   * does not provide any unset optional attributes without a constant default value.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetAttributes, _In_ const OrtNode* node,\r\n                  _Out_writes_(num_attributes) const OrtOpAttr** attributes, _In_ size_t num_attributes);\r\n\r\n  /** \\brief Gets the OrtNode's attribute as OrtOpAttr by name.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[in] attribute_name The name of the attribute\r\n   * \\param[out] attribute Output parameter set to the OrtOpAttr instance if an attribute by the given name exists.\r\n   *                       For an unset optional attribute, `attribute` is set to NULL and a non-error status is\r\n   *                       returned. For an invalid attribute name, `attribute` is set to NULL and an error status with\r\n   *                       code ORT_NOT_FOUND is returned.\r\n   *\r\n   * \\note ONNX Runtime automatically sets optional (unset) attributes to their default values if the default value\r\n   * is a constant expression that does not depend on other tensor/model characteristics. Conv's 'kernel_shape'\r\n   * attribute is an example of an optional attribute that does not have a constant default value. This function\r\n   * does not provide any unset optional attributes without a constant default value.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetAttributeByName, _In_ const OrtNode* node, _In_ const char* attribute_name,\r\n                  _Outptr_result_maybenull_ const OrtOpAttr** attribute);\r\n\r\n  /** \\brief Get the OrtNode's 'TENSOR' attribute as an OrtValue.\r\n   *\r\n   * \\param[in] attribute The OrtOpAttr instance.\r\n   * \\param[out] attr_tensor If successful, contains the 'TENSOR' attribute as a newly created OrtValue.\r\n                             Must be freed with OrtApi::ReleaseValue.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(OpAttr_GetTensorAttributeAsOrtValue, _In_ const OrtOpAttr* attribute,\r\n                  _Outptr_result_maybenull_ OrtValue** attr_tensor);\r\n\r\n  /** \\brief Get the attribute type as OrtOpAttrType from an OrtOpAttr.\r\n   *\r\n   * \\param[in] attribute The OrtOpAttr instance.\r\n   * \\param[out] type Output the attribute type as OrtOpAttrType.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(OpAttr_GetType, _In_ const OrtOpAttr* attribute, _Out_ OrtOpAttrType* type);\r\n\r\n  /** \\brief Get the attribute name from an OrtOpAttr.\r\n   *\r\n   * \\param[in] attribute The OrtOpAttr instance.\r\n   * \\param[out] name Output parameter set to the attribute's name. The name is a null-terminated string.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(OpAttr_GetName, _In_ const OrtOpAttr* attribute, _Outptr_ const char** name);\r\n\r\n  /** \\brief Returns the number of subgraphs contained by the given node.\r\n   *\r\n   * \\note Only certain operator types (e.g., If and Loop) contain nested subgraphs.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] num_subgraphs Output parameter set to the number of node subgraphs.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetNumSubgraphs, _In_ const OrtNode* node, _Out_ size_t* num_subgraphs);\r\n\r\n  /** \\brief Get the subgraphs, as OrtGraph instances, contained by the given node.\r\n   *\r\n   * \\note Only certain operator types (e.g., If and Loop) contain nested subgraphs. ONNX nodes store subgraphs in\r\n   * their attributes, however, this function must be used to obtain subgraphs from an OrtNode.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] subgraphs Pre-allocated array of `num_subgraphs` elements that is filled with the node's subgraphs.\r\n   * \\param[in] num_subgraphs The size of the `num_subgraphs` array.\r\n   *                          Typical usage sets this to the result of Node_GetNumSubgraphs(). An error status is\r\n   *                          returned if `num_subgraphs` is less than the number of node subgraphs.\r\n   * \\param[out] attribute_names Optional pre-allocated array of `num_subgraphs` elements that is filled with the\r\n   *                             attribute names that correspond to the subgraphs. Ignored if set to NULL.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetSubgraphs, _In_ const OrtNode* node,\r\n                  _Out_writes_(num_subgraphs) const OrtGraph** subgraphs, _In_ size_t num_subgraphs,\r\n                  _Out_writes_opt_(num_subgraphs) const char** attribute_names);\r\n\r\n  /** \\brief Get the node's parent OrtGraph instance.\r\n   *\r\n   * Can return NULL if the OrtNode was created without an owning graph.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] graph Output parameter set to the node's OrtGraph. Can be set to NULL\r\n   *                   if the node is not currently contained by a graph.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetGraph, _In_ const OrtNode* node, _Outptr_result_maybenull_ const OrtGraph** graph);\r\n\r\n  /** \\brief Returns the execution provider name that this node is assigned to run on.\r\n   *         Returns NULL if the node has not been assigned to any execution provider yet.\r\n   *         For plugin execution providers, the name is the one returned by OrtEp::GetName.\r\n   *\r\n   * \\param[in] node The OrtNode instance.\r\n   * \\param[out] out Output execution provider type and can be NULL if node has not been assigned.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Node_GetEpName, _In_ const OrtNode* node, _Outptr_result_maybenull_ const char** out);\r\n\r\n  /// @}\r\n\r\n  /// \\name OrtExternalInitializerInfo\r\n  /// @{\r\n\r\n  /** \\brief Release an OrtExternalInitializerInfo instance.\r\n   *\r\n   * \\param[in] input OrtExternalInitializerInfo instance to be released.\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_CLASS_RELEASE(ExternalInitializerInfo);\r\n\r\n  /** \\brief Get the relative path to the file that stores the initializer's data.\r\n   *\r\n   * \\note The path is relative to the filesystem directory where the ONNX model was stored.\r\n   * Caller can use Graph_GetModelPath to get the model's full path and construct the absolute path to the\r\n   * external initializer file if necessary.\r\n   *\r\n   * \\param[in] info The OrtExternalInitializerInfo instance.\r\n   * \\return The relative path to the file that stores the initializer's data. Do NOT free this pointer.\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API_T(const ORTCHAR_T*, ExternalInitializerInfo_GetFilePath, _In_ const OrtExternalInitializerInfo* info);\r\n\r\n  /** \\brief Get the byte offset within the file where the initializer's data is stored.\r\n   *\r\n   * \\param[in] info The OrtExternalInitializerInfo instance.\r\n   * \\return The byte offset where the initializer's data is stored within the file.\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API_T(int64_t, ExternalInitializerInfo_GetFileOffset, _In_ const OrtExternalInitializerInfo* info);\r\n\r\n  /** \\brief Get the size in bytes of the initializer's data within the file.\r\n   *\r\n   * \\param[in] info The OrtExternalInitializerInfo instance.\r\n   * \\return The size in bytes of the initializer's data within the file.\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API_T(size_t, ExternalInitializerInfo_GetByteSize, _In_ const OrtExternalInitializerInfo* info);\r\n\r\n  /// @}\r\n\r\n  /// \\name OrtRunOptions\r\n  /// @{\r\n\r\n  /** \\brief Get a run configuration entry.\r\n   *\r\n   * If a run configuration entry with key `config_key` doesn't exist, `config_value` will be set to NULL.\r\n   *\r\n   * `config_key`s are defined in onnxruntime_run_options_config_keys.h.\r\n   *\r\n   * \\param[in] options The OrtRunOptions instance.\r\n   * \\param[in] config_key The configuration entry key. A null-terminated string.\r\n   * \\return The configuration entry value. Either a null-terminated string if the entry was found. nullptr otherwise.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API_T(const char*, GetRunConfigEntry, _In_ const OrtRunOptions* options,\r\n            _In_z_ const char* config_key);\r\n\r\n  /// @}\r\n\r\n  /** \\brief Get the OrtMemoryInfo for the device.\r\n   *\r\n   * \\param[in] ep_device The OrtEpDevice instance to query.\r\n   * \\param[in] memory_type The memory type to return.\r\n   * \\return A pointer to the OrtMemoryInfo for the device. This may be nullptr if not set.\r\n   *         If memory_type is OrtDeviceMemoryType_DEFAULT and nullptr is returned the EP uses CPU memory.\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API_T(const OrtMemoryInfo*, EpDevice_MemoryInfo, _In_ const OrtEpDevice* ep_device,\r\n            _In_ OrtDeviceMemoryType memory_type);\r\n\r\n  /** \\brief Create/replace a shared allocator for the OrtEpDevice in the OrtEnv.\r\n   *\r\n   * OrtEpDevice maps to the EP factory, and the factory provides the allocator implementation.\r\n   *\r\n   * Both OrtDeviceMemoryType_DEFAULT and OrtDeviceMemoryType_HOST_ACCESSIBLE are optional for an EP to provide.\r\n   * It is EP implementation dependent as to what is available.\r\n   *\r\n   * If a shared allocator already exists for the OrtEpDevice and OrtDeviceMemoryType, it is replaced. This allows\r\n   * changing the shared allocator configuration from the default. e.g. adding an arena.\r\n   *\r\n   * \\param[in] env The OrtEnv instance to create the shared allocator in.\r\n   * \\param[in] ep_device The OrtEpDevice instance to create the shared allocator for.\r\n   * \\param[in] mem_type The memory type to use for the shared allocator.\r\n   * \\param[in] allocator_type The type of allocator to create. Only OrtDeviceAllocator is valid currently.\r\n   * \\param[in] allocator_options Optional key-value pairs to configure the allocator. If arena based, see\r\n   *                              include/onnxruntime/core/framework/allocator.h for the keys and values that can be\r\n   *                              used.\r\n   * \\param[out] allocator A pointer to the created shared allocator. Owned by the OrtEnv instance.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API2_STATUS(CreateSharedAllocator, _In_ OrtEnv* env, _In_ const OrtEpDevice* ep_device,\r\n                  _In_ OrtDeviceMemoryType mem_type, _In_ OrtAllocatorType allocator_type,\r\n                  _In_opt_ const OrtKeyValuePairs* allocator_options,\r\n                  _Outptr_opt_ OrtAllocator** allocator);\r\n\r\n  /** \\brief Get a shared allocator from the OrtEnv.\r\n   *\r\n   * By default there is a shared allocator created for all OrtEpDevice instances, so if you get the OrtMemoryInfo\r\n   * from the OrtEpDevice using EpDevice_MemoryInfo a shared allocator is guaranteed to exist.\r\n   *\r\n   * This will also match and return custom allocators added with RegisterAllocator.\r\n   *\r\n   * It is not an error to not find a matching allocator.\r\n   *\r\n   * \\param[in] env The OrtEnv instance to get the shared allocator from.\r\n   * \\param[in] mem_info The OrtMemoryInfo instance to get the shared allocator for.\r\n   * \\param[out] allocator A pointer to the shared allocator, or nullptr if no shared allocator exists for\r\n   *                       the given memory info.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API2_STATUS(GetSharedAllocator, _In_ OrtEnv* env, _In_ const OrtMemoryInfo* mem_info,\r\n                  _Outptr_result_maybenull_ OrtAllocator** allocator);\r\n\r\n  /** \\brief Release a shared allocator from the OrtEnv for the OrtEpDevice and memory type.\r\n   *\r\n   * This will release the shared allocator for the given OrtEpDevice and memory type.\r\n   * If no shared allocator exists, this is a no-op.\r\n   *\r\n   * \\param[in] env The OrtEnv instance to release the shared allocator from.\r\n   * \\param[in] ep_device The OrtEpDevice instance to release the shared allocator for.\r\n   * \\param[in] mem_type The memory type of the shared allocator to release.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API2_STATUS(ReleaseSharedAllocator, _In_ OrtEnv* env, _In_ const OrtEpDevice* ep_device,\r\n                  _In_ OrtDeviceMemoryType mem_type);\r\n\r\n  /** \\brief Get a const pointer to the raw data inside a tensor\r\n   *\r\n   * Used to read the internal tensor data directly.\r\n   * \\note The returned pointer is valid until the OrtValue is destroyed.\r\n   *\r\n   * \\param[in] value A tensor type (string tensors are not supported)\r\n   * \\param[out] out Filled in with a pointer to the internal storage\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API2_STATUS(GetTensorData, _In_ const OrtValue* value, _Outptr_ const void** out);\r\n\r\n  /** \\brief Get Session configuration entries.\r\n   *\r\n   * \\param[in] options The session options.\r\n   * \\param[out] out A pointer to a newly created OrtKeyValuePairs instance.\r\n   *\r\n   *  An OrtKeyValuePairs instance containing all session configuration entries.\r\n   *  Note: the user should call OrtApi::ReleaseKeyValuePairs.\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(GetSessionOptionsConfigEntries, _In_ const OrtSessionOptions* options, _Outptr_ OrtKeyValuePairs** out);\r\n\r\n  /** \\brief Get the OrtMemoryInfo for each input of the session.\r\n   *\r\n   * The memory info can be used to determine where the input tensors are required.\r\n   *\r\n   * The session must be fully initialized before calling this function as the input locations are not known until\r\n   * this has occurred.\r\n   *\r\n   * \\param[in] session The OrtSession instance.\r\n   * \\param[out] inputs_memory_info Pre-allocated array of size `num_inputs` that will be filled with the\r\n   *                                OrtMemoryInfo* value for each input.\r\n   *                                The order is the same as returned by SessionGetInputName.\r\n   * \\param[in] num_inputs The number of inputs in the session. Must match SessionGetInputCount.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API2_STATUS(SessionGetMemoryInfoForInputs, _In_ const OrtSession* session,\r\n                  _Out_writes_(num_inputs) const OrtMemoryInfo** inputs_memory_info,\r\n                  _In_ size_t num_inputs);\r\n\r\n  /** \\brief Get the OrtMemoryInfo for each output of the session.\r\n   *\r\n   * The memory info can be used to determine the device the output tensors are produced on.\r\n   * The user can pre-allocate an OrtValue using this information or use IOBinding to keep the data on the device.\r\n   * ORT will copy the output to CPU otherwise.\r\n   *\r\n   * The session must be fully initialized before calling this function as the output locations are not known until\r\n   * this has occurred.\r\n   *\r\n   * \\param[in] session The OrtSession instance.\r\n   * \\param[out] outputs_memory_info Pre-allocated array of size `num_outputs` that will be filled with\r\n   *                                 OrtMemoryInfo* values for each output.\r\n   *                                 The order is the same as returned by SessionGetOutputName.\r\n   * \\param[in] num_outputs The number of outputs in the session. Must match SessionGetOutputCount.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API2_STATUS(SessionGetMemoryInfoForOutputs, _In_ const OrtSession* session,\r\n                  _Out_writes_(num_outputs) const OrtMemoryInfo** outputs_memory_info,\r\n                  _In_ size_t num_outputs);\r\n\r\n  /** \\brief Get the OrtEpDevice (if available) for each input of the session.\r\n   *\r\n   * An OrtEpDevice will be available if auto EP selection is enabled by calling\r\n   * SessionOptionsSetEpSelectionPolicy or SessionOptionsSetEpSelectionPolicyDelegate,\r\n   * or if the OrtEpDevice was manually added to the session using SessionOptionsAppendExecutionProvider_V2.\r\n   *\r\n   * If an OrtEpDevice is not available for the input a nullptr is returned.\r\n   *\r\n   * The returned OrtEpDevice can be used to create an OrtSyncStream via CreateSyncStreamForEpDevice to asynchronously\r\n   * provide input to the inference session Run.\r\n   *\r\n   * The session must be fully initialized before calling this function as the assigned EPs are not known until\r\n   * this has occurred.\r\n   *\r\n   * \\param[in] session The OrtSession instance.\r\n   * \\param[out] inputs_ep_devices Pre-allocated array of size `num_inputs` that will be filled with\r\n   *                               OrtEpDevice* values for each input.\r\n   *                               The order is the same as returned by SessionGetInputName.\r\n   * \\param[in] num_inputs The number of inputs in the session. Must match SessionGetInputCount.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API2_STATUS(SessionGetEpDeviceForInputs, _In_ const OrtSession* session,\r\n                  _Out_writes_(num_inputs) const OrtEpDevice** inputs_ep_devices,\r\n                  _In_ size_t num_inputs);\r\n\r\n  /** \\brief Create an OrtSyncStream for the given OrtEpDevice.\r\n   *\r\n   * The OrtSyncStream can be used to enable asynchronous operations.\r\n   * e.g. async usage of CopyTensors to provide input to an OrtSession Run call.\r\n   *\r\n   * An error code of ORT_NOT_IMPLEMENTED will be returned if the EP does not support OrtSyncStream.\r\n   *\r\n   * \\param[in] ep_device The OrtEpDevice instance to create the sync stream for.\r\n   * \\param[in] stream_options Options for OrtSyncStream creation. May be nullptr.\r\n   * \\param[out] stream Output parameter set to the created OrtSyncStream instance.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API2_STATUS(CreateSyncStreamForEpDevice, _In_ const OrtEpDevice* ep_device,\r\n                  _In_opt_ const OrtKeyValuePairs* stream_options,\r\n                  _Outptr_ OrtSyncStream** stream);\r\n\r\n  /** \\brief Get the native handle of the sync stream.\r\n   *\r\n   * This returns the native handle for the stream. e.g. cudaStream_t for CUDA streams.\r\n   *\r\n   * \\param[in] stream The OrtSyncStream instance to get the handle from.\r\n   *\r\n   * \\returns The native handle of the stream.\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API_T(void*, SyncStream_GetHandle, _In_ OrtSyncStream* stream);\r\n\r\n  ORT_CLASS_RELEASE(SyncStream);\r\n\r\n  /** \\brief Copy OrtValue instances containing Tensors between devices.\r\n   *\r\n   * The overall copy must be between a single source device and a single destination device. i.e.\r\n   *   - all src_tensors must have matching OrtMemoryInfo,\r\n   *   - all dst_tensors must have matching OrtMemoryInfo.\r\n   *\r\n   * OrtValue instances can be created by:\r\n   *   - Use GetSharedAllocator to get the shared allocator for the OrtMemoryInfo if you need to allocate memory\r\n   *     on the device.\r\n   *   - Use CreateTensorAsOrtValue, CreateTensorWithDataAsOrtValue or CreateTensorWithDataAndDeleterAsOrtValue\r\n   *     to create an OrtValue containing a tensor depending on whether you have existing data or not, and whether\r\n   *     you want ORT to free the existing data once it is done with the OrtValue.\r\n   *\r\n   * \\param[in] env The OrtEnv instance to use. The data transfer implementation is provided by an execution provider\r\n   *                that is registered in this OrtEnv.\r\n   * \\param[in] src_tensors Array of OrtValue instances containing the source tensors to copy.\r\n   * \\param[in] dst_tensors Array of OrtValue instances to copy the source tensors to.\r\n   * \\param[in] stream Optional OrtSyncStream that can be used to perform the copy asynchronously. May be nullptr.\r\n   * \\param[in] num_tensors The number of tensors to copy. The size of `src_tensors` and `dst_tensors` must match.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23\r\n   */\r\n  ORT_API2_STATUS(CopyTensors, _In_ const OrtEnv* env,\r\n                  _In_reads_(num_tensors) const OrtValue* const* src_tensors,\r\n                  _In_reads_(num_tensors) OrtValue* const* dst_tensors,\r\n                  _In_opt_ OrtSyncStream* stream,\r\n                  _In_ size_t num_tensors);\r\n\r\n  /** \\brief Get ::OrtModelMetadata from an ::OrtGraph\r\n   *\r\n   * \\param[in] graph The OrtGraph instance.\r\n   * \\param[out] out Newly created ::OrtModelMetadata. Must be freed using OrtApi::ReleaseModelMetadata.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(Graph_GetModelMetadata, _In_ const OrtGraph* graph, _Outptr_ OrtModelMetadata** out);\r\n\r\n  /** \\brief Validate a compiled model's compatibility information for one or more EP devices.\r\n   *\r\n   * \\param[in] ep_devices The EP devices to validate against (e.g., from GetEpDevices).\r\n   *                        All devices must belong to the same execution provider.\r\n   * \\param[in] num_ep_devices The number of EP devices provided.\r\n   * \\param[in] compatibility_info The compatibility info string produced when the model was compiled.\r\n   * \\param[out] out_status The resulting compatibility status for the EP devices.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(GetModelCompatibilityForEpDevices,\r\n                  _In_reads_(num_ep_devices) const OrtEpDevice* const* ep_devices,\r\n                  _In_ size_t num_ep_devices,\r\n                  _In_ const char* compatibility_info,\r\n                  _Out_ OrtCompiledModelCompatibility* out_status);\r\n\r\n  /// \\name OrtExternalInitializerInfo\r\n  /// @{\r\n\r\n  /** \\brief Creates an OrtExternalInitializerInfo instance.\r\n   *\r\n   * \\param[in] filepath The relative path to the file that stores the initializer's data. ORT copies this path string.\r\n   * \\param[in] file_offset The byte offset where the initializer's data is stored within the file.\r\n   * \\param[in] byte_size The size in bytes of the initializer's data within the file.\r\n   * \\param[out] out Output parameter set to the new OrtExternalInitializerInfo instance.\r\n   *                 Must be released by calling ReleaseExternalInitializerInfo().\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(CreateExternalInitializerInfo, _In_ const ORTCHAR_T* filepath, _In_ int64_t file_offset,\r\n                  _In_ size_t byte_size, _Outptr_ OrtExternalInitializerInfo** out);\r\n\r\n  /// @}\r\n  /** \\brief Fetch whether the tensor has shape information.\r\n   * \\param[in] info The OrtTensorTypeAndShapeInfo instance.\r\n   * \\return true if the tensor has shape information, false otherwise.\r\n   *\r\n   * \\since Version 1.24\r\n   */\r\n  ORT_API_T(bool, TensorTypeAndShape_HasShape, _In_ const OrtTensorTypeAndShapeInfo* info);\r\n};\r\n\r\n/*\r\n * Steps to use a custom op:\r\n *   1 Create an OrtCustomOpDomain with the domain name used by the custom ops\r\n *   2 Create an OrtCustomOp structure for each op and add them to the domain\r\n *   3 Call OrtAddCustomOpDomain to add the custom domain of ops to the session options\r\n */\r\n\r\n// Specifies some characteristics of inputs/outputs of custom ops:\r\n// Specify if the inputs/outputs are one of:\r\n// 1) Non-optional (input/output must be present in the node)\r\n// 2) Optional (input/output may be absent in the node)\r\n// 3) Variadic: A variadic input or output specifies N (i.e., the minimum arity) or more operands.\r\n//              Only the last input or output of a custom op may be marked as variadic.\r\n//              The homogeneity of the variadic input or output determines whether all operands must be of the same\r\n//              tensor element type.\r\ntypedef enum OrtCustomOpInputOutputCharacteristic {\r\n  INPUT_OUTPUT_REQUIRED = 0,\r\n  INPUT_OUTPUT_OPTIONAL,\r\n  INPUT_OUTPUT_VARIADIC,\r\n} OrtCustomOpInputOutputCharacteristic;\r\n\r\n/*\r\n * The OrtCustomOp structure defines a custom op's schema and its kernel callbacks. The callbacks are filled in by\r\n * the implementor of the custom op.\r\n */\r\nstruct OrtCustomOp {\r\n  uint32_t version;  // Must be initialized to ORT_API_VERSION\r\n\r\n  // This callback creates the kernel, which is a user defined\r\n  // parameter that is passed to the Kernel* callbacks below. It is\r\n  // recommended to use CreateKernelV2 which allows for a safe error\r\n  // propagation by returning an OrtStatusPtr.\r\n  void*(ORT_API_CALL* CreateKernel)(_In_ const struct OrtCustomOp* op, _In_ const OrtApi* api,\r\n                                    _In_ const OrtKernelInfo* info);\r\n\r\n  // Returns the name of the op\r\n  const char*(ORT_API_CALL* GetName)(_In_ const struct OrtCustomOp* op);\r\n\r\n  // Returns the type of the execution provider, return nullptr to use CPU execution provider\r\n  const char*(ORT_API_CALL* GetExecutionProviderType)(_In_ const struct OrtCustomOp* op);\r\n\r\n  // Returns the count and types of the input & output tensors\r\n  ONNXTensorElementDataType(ORT_API_CALL* GetInputType)(_In_ const struct OrtCustomOp* op, _In_ size_t index);\r\n  size_t(ORT_API_CALL* GetInputTypeCount)(_In_ const struct OrtCustomOp* op);\r\n  ONNXTensorElementDataType(ORT_API_CALL* GetOutputType)(_In_ const struct OrtCustomOp* op, _In_ size_t index);\r\n  size_t(ORT_API_CALL* GetOutputTypeCount)(_In_ const struct OrtCustomOp* op);\r\n\r\n  // Perform a computation step.  It is recommended to use\r\n  // KernelComputeV2 which allows for a safe error propagation by\r\n  // returning an OrtStatusPtr.\r\n  void(ORT_API_CALL* KernelCompute)(_In_ void* op_kernel, _In_ OrtKernelContext* context);\r\n  void(ORT_API_CALL* KernelDestroy)(_In_ void* op_kernel);\r\n\r\n  // Returns the characteristics of the input & output tensors\r\n  OrtCustomOpInputOutputCharacteristic(ORT_API_CALL* GetInputCharacteristic)(_In_ const struct OrtCustomOp* op, _In_ size_t index);\r\n  OrtCustomOpInputOutputCharacteristic(ORT_API_CALL* GetOutputCharacteristic)(_In_ const struct OrtCustomOp* op, _In_ size_t index);\r\n\r\n  // Returns the memory type of the input tensors. This API allows the custom op\r\n  // to place the inputs on specific devices. By default, it returns\r\n  // OrtMemTypeDefault, which means the input is placed on the default device for\r\n  // the execution provider. If the inputs need to be with different memory types,\r\n  // this function can be overridden to return the specific memory types.\r\n  OrtMemType(ORT_API_CALL* GetInputMemoryType)(_In_ const struct OrtCustomOp* op, _In_ size_t index);\r\n\r\n  // Returns the minimum number of input arguments expected for the variadic input.\r\n  // Applicable only for custom ops that have a variadic input.\r\n  int(ORT_API_CALL* GetVariadicInputMinArity)(_In_ const struct OrtCustomOp* op);\r\n\r\n  // Returns true (non-zero) if all arguments of a variadic input have to be of the same type (homogeneous),\r\n  // and false (zero) otherwise.\r\n  // Applicable only for custom ops that have a variadic input.\r\n  int(ORT_API_CALL* GetVariadicInputHomogeneity)(_In_ const struct OrtCustomOp* op);\r\n\r\n  // Returns the minimum number of output values expected for the variadic output.\r\n  // Applicable only for custom ops that have a variadic output.\r\n  int(ORT_API_CALL* GetVariadicOutputMinArity)(_In_ const struct OrtCustomOp* op);\r\n\r\n  // Returns true (non-zero) if all outputs values of a variadic output have to be of the same type (homogeneous),\r\n  // and false (zero) otherwise.\r\n  // Applicable only for custom ops that have a variadic output.\r\n  int(ORT_API_CALL* GetVariadicOutputHomogeneity)(_In_ const struct OrtCustomOp* op);\r\n\r\n  // Create the kernel state which is passed to each compute call.\r\n  OrtStatusPtr(ORT_API_CALL* CreateKernelV2)(_In_ const struct OrtCustomOp* op, _In_ const OrtApi* api,\r\n                                             _In_ const OrtKernelInfo* info,\r\n                                             _Out_ void** kernel);\r\n\r\n  // Perform the computation step.\r\n  OrtStatusPtr(ORT_API_CALL* KernelComputeV2)(_In_ void* op_kernel, _In_ OrtKernelContext* context);\r\n\r\n  OrtStatusPtr(ORT_API_CALL* InferOutputShapeFn)(_In_ const struct OrtCustomOp* op, _In_ OrtShapeInferContext*);\r\n\r\n  // Get start range\r\n  int(ORT_API_CALL* GetStartVersion)(_In_ const struct OrtCustomOp* op);\r\n  int(ORT_API_CALL* GetEndVersion)(_In_ const struct OrtCustomOp* op);\r\n\r\n  // Get the inplace_map that defines which output can reuse which input\r\n  // Callers will provide 2 raw int* and pass in their address, this function will fill these 2 arrays\r\n  // when return, output (*output_index)[i] may reuse the input (*input_index[i]).\r\n  // The return value is the size of these 2 arrays.\r\n  // Callers are responsible to delete these 2 arrays after use by calling OrtCustomOp::ReleaseMayInplace().\r\n  size_t(ORT_API_CALL* GetMayInplace)(_Out_ int** input_index, _Out_ int** output_index);\r\n\r\n  // Release the pointer input_index and output_index allocated from GetMayInplace() function.\r\n  // If GetMayInplace() is defined, this function MUST be defined as well.\r\n  void(ORT_API_CALL* ReleaseMayInplace)(_Frees_ptr_opt_ int* input_index, _Frees_ptr_opt_ int* output_index);\r\n\r\n  // Same as GetMayInplace() and ReleaseMayInplace()\r\n  size_t(ORT_API_CALL* GetAliasMap)(_Out_ int** input_index, _Out_ int** output_index);\r\n  void(ORT_API_CALL* ReleaseAliasMap)(_Frees_ptr_opt_ int* input_index, _Frees_ptr_opt_ int* output_index);\r\n};\r\n\r\n/**\r\n * ORT Model Editor API\r\n */\r\n\r\n/**\r\n * \\brief The OrtModelEditorApi struct provides functions to create or edit an ONNX model.\r\n *\r\n * See onnxruntime/test/shared_lib/test_model_editor_api.cc for example usage.\r\n *\r\n * \\since Version 1.22.\r\n */\r\nstruct OrtModelEditorApi {\r\n  // Model building/editing requires a full build. We return nullptr from GetModelEditorApi if this is a minimal\r\n  // build, so it doesn't matter if there are no function pointers in this struct as a user will never get an\r\n  // OrtModelEditorApi instance. We do however need a dummy field to avoid empty struct warning.\r\n#if defined(ORT_MINIMAL_BUILD)\r\n  const bool not_defined_in_this_build;\r\n#else\r\n  /** \\brief Create an OrtTypeInfo instance for a Tensor.\r\n   *\r\n   * Create an OrtTypeInfo instance for a Tensor to use as graph inputs/outputs with the Model Editor API.\r\n   *\r\n   * User can release `tensor_info` after creating the OrtTypeInfo.\r\n   *\r\n   * \\param[in] tensor_info Tensor type and shape information.\r\n   * \\param[out] type_info TypeInfo instance for the tensor.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CreateTensorTypeInfo, _In_ const OrtTensorTypeAndShapeInfo* tensor_info,\r\n                  _Outptr_ OrtTypeInfo** type_info);\r\n\r\n  /** \\brief Create an OrtTypeInfo instance for a SparseTensor.\r\n   *\r\n   * Create an OrtTypeInfo instance for a SparseTensor to use as graph inputs/outputs with the Model Editor API.\r\n   *\r\n   * User can release `tensor_info` after creating the OrtTypeInfo.\r\n   *\r\n   * \\param[in] tensor_info SparseTensor type and shape information.\r\n   * \\param[out] type_info TypeInfo instance for the tensor.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CreateSparseTensorTypeInfo, _In_ const OrtTensorTypeAndShapeInfo* tensor_info,\r\n                  _Outptr_ OrtTypeInfo** type_info);\r\n\r\n  /** \\brief Create an OrtTypeInfo instance for a Map.\r\n   *\r\n   * Create an OrtTypeInfo instance for a Map to use as graph inputs/outputs with the Model Editor API.\r\n   *\r\n   * User can release `map_value_type` after creating the OrtTypeInfo.\r\n   *\r\n   * \\param[in] map_key_type Key type for the map.\r\n   * \\param[in] map_value_type Value type for the map.\r\n   * \\param[out] type_info TypeInfo instance for the map.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CreateMapTypeInfo, ONNXTensorElementDataType map_key_type, _In_ const OrtTypeInfo* map_value_type,\r\n                  _Outptr_ OrtTypeInfo** type_info);\r\n\r\n  /** \\brief Create an OrtTypeInfo instance for a Sequence.\r\n   *\r\n   * Create an OrtTypeInfo instance for a Sequence to use as graph inputs/outputs with the Model Editor API.\r\n   *\r\n   * User can release `sequence_type` after creating the OrtTypeInfo.\r\n   *\r\n   * \\param[in] sequence_type Sequence type and shape information.\r\n   * \\param[out] type_info TypeInfo instance for the sequence.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CreateSequenceTypeInfo, _In_ const OrtTypeInfo* sequence_type, _Outptr_ OrtTypeInfo** type_info);\r\n\r\n  /** \\brief Create an OrtTypeInfo instance for an Optional.\r\n   *\r\n   * Create an OrtTypeInfo instance for an Optional to use as graph inputs/outputs with the Model Editor API.\r\n   *\r\n   * User can release `contained_type` after creating the OrtTypeInfo.\r\n   *\r\n   * \\param[in] contained_type Tensor type and shape information.\r\n   * \\param[out] type_info TypeInfo instance for the tensor.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CreateOptionalTypeInfo, _In_ const OrtTypeInfo* contained_type, _Outptr_ OrtTypeInfo** type_info);\r\n\r\n  /** \\brief Create an OrtValueInfo for use as an OrtGraph input or output.\r\n   *\r\n   * \\param[in] name The name of the input or output.\r\n   * \\param[in] type_info The type information for the input or output. The provided value is copied.\r\n   * \\param[out] value_info The OrtValueInfo instance.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CreateValueInfo, _In_ const char* name, _In_ const OrtTypeInfo* type_info,\r\n                  _Outptr_ OrtValueInfo** value_info);\r\n\r\n  /** \\brief Create an OrtNode to add to an OrtGraph.\r\n   *\r\n   * Create an OrtNode.\r\n   *\r\n   * Create attributes with CreateOpAttr. OrtOpAttr instances are copied.\r\n   *\r\n   * \\param[in] operator_name The name of the operator.\r\n   * \\param[in] domain_name The domain of the operator. Use an empty string for ONNX operators.\r\n   * \\param[in] node_name The name of the node.\r\n   * \\param[in] input_names The names of the inputs.\r\n   * \\param[in] input_names_len The number of input names.\r\n   * \\param[in] output_names The names of the outputs.\r\n   * \\param[in] output_names_len The number of output names.\r\n   * \\param[in] attributes The optional attributes of the node.\r\n   * \\param[in] attribs_len The number of attributes. May be zero.\r\n   * \\param[out] node The OrtNode instance.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CreateNode, _In_ const char* operator_name, _In_ const char* domain_name, _In_ const char* node_name,\r\n                  _In_reads_(input_names_len) const char* const* input_names, size_t input_names_len,\r\n                  _In_reads_(output_names_len) const char* const* output_names, size_t output_names_len,\r\n                  _In_reads_(attribs_len) _In_opt_ OrtOpAttr** attributes, _In_ size_t attribs_len,\r\n                  _Outptr_ OrtNode** node);\r\n\r\n  /** \\brief Create an OrtGraph\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CreateGraph, _Outptr_ OrtGraph** graph);\r\n\r\n  /** \\brief Set the inputs for the OrtGraph.\r\n   *\r\n   * Set the graph inputs. This will replace any existing inputs with the new values.\r\n   * The OrtGraph takes ownership of the OrtValueInfo instances and you should NOT call ReleaseOrtValueInfo.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance to update.\r\n   * \\param[in] inputs The input OrtValueInfo instances.\r\n   * \\param[in] inputs_len The number of input OrtValueInfo instances.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(SetGraphInputs, _Inout_ OrtGraph* graph,\r\n                  _In_reads_(inputs_len) _In_ OrtValueInfo** inputs, _In_ size_t inputs_len);\r\n\r\n  /** \\brief Set the outputs for the OrtGraph.\r\n   *\r\n   * Set the graph outputs. This will replace any existing outputs with the new values.\r\n   * The OrtGraph takes ownership of the OrtValueInfo instances provided and you should NOT call ReleaseOrtValueInfo.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance to update.\r\n   * \\param[in] outputs The output OrtValueInfo instances.\r\n   * \\param[in] outputs_len The number of output OrtValueInfo instances.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(SetGraphOutputs, _Inout_ OrtGraph* graph,\r\n                  _In_reads_(outputs_len) _In_ OrtValueInfo** outputs, _In_ size_t outputs_len);\r\n\r\n  /** \\brief Add an initializer to the OrtGraph\r\n   *\r\n   * ORT will take ownership of the OrtValue and you should NOT call ReleaseOrtValue.\r\n   *\r\n   * Two options:\r\n   *\r\n   * Allocated memory:\r\n   *    Use CreateTensorAsOrtValue (allocates memory) and populate the tensor with the data.\r\n   *    Set `data_is_external` to false.\r\n   *\r\n   * Pre-existing memory:\r\n   *    Use CreateTensorWithDataAsOrtValue or CreateTensorWithDataAndDeleterAsOrtValue to create an OrtValue\r\n   *    with a tensor that contains a pointer to the existing data.\r\n   *    Set `data_is_external` to true.\r\n   *\r\n   *    The pointer must remain valid for the duration of the inference session.\r\n   *    If using CreateTensorWithDataAsOrtValue you are responsible for freeing the memory after the inference session\r\n   *    is released.\r\n   *    If using CreateTensorWithDataAndDeleterAsOrtValue, ORT will free the memory using the provided deleter as\r\n   *    soon as the OrtValue is no longer in use.\r\n   *\r\n   *    NOTE: A tensor containing pre-existing memory MUST have 128 bytes of data or more.\r\n   *          For smaller tensors use CreateTensorAsOrtValue.\r\n   *\r\n   *          ONNX shape inferencing does not support external data. An initializer involved in shape inferencing is\r\n   *          typically small (a single value or limited by the rank of a tensor) and uses less than 128 bytes of\r\n   *          memory, so this limit acts as a simple catch-all rule to avoid issues.\r\n   *          e.g. Reshape's `shape`, Clip's `min` and `max`, various ops `axes`.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance to update.\r\n   * \\param[in] name The value name for the initializer.\r\n   * \\param[in] tensor The OrtValue instance containing the tensor data.\r\n   * \\param[in] data_is_external Set to true if the data is external and should not be copied.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(AddInitializerToGraph, _Inout_ OrtGraph* graph, _In_ const char* name, _In_ OrtValue* tensor,\r\n                  bool data_is_external);\r\n\r\n  /** \\brief Add an OrtNode to an OrtGraph\r\n   *\r\n   * Add the node to the graph. The OrtGraph will take ownership of OrtNode and you should NOT call ReleaseOrtNode.\r\n   *\r\n   * \\param[in] graph The OrtGraph instance to update.\r\n   * \\param[in] node The OrtNode instance to add to the graph.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(AddNodeToGraph, _Inout_ OrtGraph* graph, _In_ OrtNode* node);\r\n\r\n  /** \\brief Create an OrtModel.\r\n   *\r\n   * Create an OrtModel.\r\n   *\r\n   * This can be used to build a new model, or to augment an existing model.\r\n   *\r\n   * \\param[in] domain_names The domain names for the model.\r\n   *                         If augmenting an existing model add additional domains if needed.\r\n   * \\param[in] opset_versions The opset versions for the model.\r\n   *                           If augmenting an existing model add additional opset versions if needed.\r\n   * \\param[in] opset_entries_len The number of domain_names and opset_versions entries.\r\n   *                              Domain and opset entries should be 1:1\r\n   * \\param[out] model The OrtModel instance.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CreateModel,\r\n                  _In_reads_(opset_entries_len) const char* const* domain_names,\r\n                  _In_reads_(opset_entries_len) const int* opset_versions,\r\n                  size_t opset_entries_len,\r\n                  _Outptr_ OrtModel** model);\r\n\r\n  /** \\brief Add an OrtGraph to an OrtModel.\r\n   *\r\n   * Add the graph to a model. This should be called once when creating a new model.\r\n   *\r\n   * The OrtModel takes ownership of the OrtGraph and you should NOT call ReleaseOrtGraph.\r\n   *\r\n   * \\param[in] model The OrtModel instance to update.\r\n   * \\param[in] graph The OrtGraph instance to add to the model.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(AddGraphToModel, _Inout_ OrtModel* model, _In_ OrtGraph* graph);\r\n\r\n  /** \\brief Create an OrtSession using the OrtModel.\r\n   *\r\n   * Create an inference session using the OrtModel instance.\r\n   * The OrtModel should have been populated with an OrtGraph containing nodes and initializers, and SetGraphInputs\r\n   * and SetGraphOutputs must have been called.\r\n   * This will validate the model, run optimizers, and prepare the session for inferencing.\r\n   *\r\n   * ReleaseOrtModel must be called to free the OrtModel after session creation.\r\n   *\r\n   * \\param[in] env The OrtEnv instance.\r\n   * \\param[in] model The OrtModel instance.\r\n   * \\param[in] options The OrtSessionOptions instance.\r\n   * \\param[out] out The OrtSession instance.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CreateSessionFromModel, _In_ const OrtEnv* env, _In_ const OrtModel* model,\r\n                  _In_ const OrtSessionOptions* options, _Outptr_ OrtSession** out);\r\n\r\n  /** \\brief Create an OrtSession to augment an existing model.\r\n   *\r\n   * Create an OrtSession with an existing model that will be augmented with additional nodes and initializers.\r\n   * Nodes can be added before or after the existing nodes in the model. ONNX Runtime will connect the nodes when the\r\n   * model is finalized.\r\n   *\r\n   * To add nodes and initializers to the existing model, first create an OrtModel using CreateModel.\r\n   * Add nodes and initializers to the OrtModel using AddNodeToGraph and AddInitializerToGraph.\r\n   * Graph inputs/outputs should be updated with SetGraphInputs and SetGraphOutputs as needed to reflect changes made\r\n   * by the new nodes. The list of graph inputs/outputs should be for the overall model and not just the new nodes.\r\n   *\r\n   * Add the new information from the OrtModel to the original model using ApplyModelToSession, and prepare the\r\n   * session for inferencing by calling FinalizeModelEditorSession.\r\n   *\r\n   * \\param{in} env The OrtEnv instance.\r\n   * \\param{in} model_path The path to the existing ONNX model to augment.\r\n   * \\param{in} options The OrtSessionOptions instance.\r\n   * \\param{out} out The created OrtSession instance.\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CreateModelEditorSession, _In_ const OrtEnv* env, _In_ const ORTCHAR_T* model_path,\r\n                  _In_ const OrtSessionOptions* options,\r\n                  _Outptr_ OrtSession** out);\r\n\r\n  /** \\brief Create an OrtSession to augment an existing model.\r\n   *\r\n   * Create an OrtSession with an existing model that will be augmented with additional nodes and initializers.\r\n   * Nodes can be added before or after the existing nodes in the model. ONNX Runtime will connect the nodes when the\r\n   * model is finalized.\r\n   *\r\n   * To add nodes and initializers to the existing model, first create an OrtModel using CreateModel.\r\n   * Add nodes and initializers to the OrtModel using AddNodeToGraph and AddInitializerToGraph.\r\n   * Graph inputs/outputs should be updated with SetGraphInputs and SetGraphOutputs as needed to reflect changes made\r\n   * by the new nodes. The list of graph inputs/outputs should be for the overall model and not just the new nodes.\r\n   *\r\n   * Add the new information from the OrtModel to the original model using ApplyModelToSession, and prepare the\r\n   * session for inferencing by calling FinalizeModelEditorSession.\r\n   *\r\n   * \\param{in} env The OrtEnv instance.\r\n   * \\param{in} model_data The model data for the existing model to augment.\r\n   * \\param{in} model_data_length The length of the model data.\r\n   * \\param{in} options The OrtSessionOptions instance.\r\n   * \\param{out} out The created OrtSession instance.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CreateModelEditorSessionFromArray, _In_ const OrtEnv* env,\r\n                  _In_ const void* model_data, size_t model_data_length,\r\n                  _In_ const OrtSessionOptions* options,\r\n                  _Outptr_ OrtSession** out);\r\n\r\n  /** \\brief Query the session for the opset version of a domain.\r\n   *\r\n   * When using the Model Editor API to augment a model, any new nodes must conform to the opset version of the\r\n   * original model. To do that the user must be able to discover that opset version.\r\n   * Returns an error if the domain is not used in the model.\r\n   *\r\n   * \\param[in] session OrtSession to query\r\n   * \\param[in] domain Domain to query. The ONNX domain is an empty string.\r\n   * \\param[out] opset The opset version of the domain.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(SessionGetOpsetForDomain, _In_ const OrtSession* session, _In_ const char* domain, _Out_ int* opset);\r\n\r\n  /** \\brief Apply changes to augment the ONNX model in a session created using CreateModelEditorSession[FromArray]\r\n   *\r\n   * Adds new nodes and updates graph inputs/outputs using `model` to augment the original ONNX model in the session.\r\n   * All changes will be validated.\r\n   * Call FinalizeModelEditorSession to prepare the session for inferencing.\r\n   *\r\n   * Existing input/outputs will only be updated if the OrtGraph inputs/outputs are set in the OrtModel.\r\n   *   i.e. you don't need to call SetGraphInputs/SetGraphOutputs if they are unchanged.\r\n   *\r\n   * ReleaseOrtModel must be called to free the OrtModel after it is applied to the session.\r\n   *\r\n   * \\param[in] session OrtSession to update. Session must have been created using CreateModelEditorSession[FromArray].\r\n   * \\param[in] model OrtModel containing new nodes, new initializers, and updated graph input and/or output info.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(ApplyModelToModelEditorSession, _Inout_ OrtSession* session, _In_ OrtModel* model);\r\n\r\n  /** \\brief Finalize the Model Editor session that was created using CreateModelEditorSession[FromArray].\r\n   *\r\n   * Finalize the Model Editor session that augmented an ONNX model by adding new nodes.\r\n   * This will run optimizers and prepare the session for inferencing.\r\n   *\r\n   * \\param[in] session OrtSession to finalize. Session must have been created using CreateModelEditorSession[FromArray].\r\n   * \\param[in] options OrtSessionOptions to use for the session.\r\n   * \\param[in] prepacked_weights_container Optional OrtPrepackedWeightsContainer to use for the session.\r\n                Set to nullptr if not used.\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(FinalizeModelEditorSession, _Inout_ OrtSession* session, _In_ const OrtSessionOptions* options,\r\n                  _In_opt_ OrtPrepackedWeightsContainer* prepacked_weights_container);\r\n#endif  // !defined(ORT_MINIMAL_BUILD)\r\n};\r\n\r\n/**\r\n * ORT Compile API\r\n */\r\n\r\n/** \\brief Flags representing options to enable when compiling a model.\r\n */\r\ntypedef enum OrtCompileApiFlags {\r\n  // Default. Do not enable any additional compilation options.\r\n  OrtCompileApiFlags_NONE = 0,\r\n\r\n  // Force compilation to return an error (ORT_FAIL) if no nodes were compiled.\r\n  // Otherwise, a model with basic optimizations (ORT_ENABLE_BASIC) is still generated by default.\r\n  OrtCompileApiFlags_ERROR_IF_NO_NODES_COMPILED = 1 << 0,\r\n\r\n  // Force compilation to return an error (ORT_FAIL) if a file with the same filename as the output model exists.\r\n  // Otherwise, compilation will automatically overwrite the output file if it exists.\r\n  OrtCompileApiFlags_ERROR_IF_OUTPUT_FILE_EXISTS = 1 << 1,\r\n} OrtCompileApiFlags;\r\n\r\n/**\r\n * \\brief The OrtCompileApi struct provides functions to compile ONNX models.\r\n *\r\n * Execution providers that support compilation fuse a subgraph into an EPContext node that wraps a provider-specific\r\n * binary representation of the subgraph.\r\n * For more details about the EPContext design, refer to:\r\n *  \\htmlonly\r\n *  <a href=\"https://onnxruntime.ai/docs/execution-providers/EP-Context-Design.html\">EPContext design document.</a>\r\n *  \\endhtmlonly\r\n *\r\n * Example (error handling not shown):\r\n *   OrtStatus* status = NULL;\r\n *   OrtCompileApi* compile_api = ort_api->GetCompileApi();\r\n *   OrtModelCompilationOptions* compile_options = NULL;\r\n *\r\n *   status = compile_api->CreateModelCompilationOptionsFromSessionOptions(env, session_options, &compile_options);\r\n *   status = compile_api->ModelCompilationOptions_SetInputModelPath(compile_options, ORT_TSTR(\"model.onnx\"));\r\n *   status = compile_api->ModelCompilationOptions_SetOutputModelPath(compile_options, ORT_TSTR(\"model.compiled.onnx\"));\r\n *   status = compile_api->CompileModel(env, compile_options);\r\n *   compile_api->ReleaseModelCompilationOptions(compile_options);\r\n *\r\n * \\since Version 1.22.\r\n */\r\nstruct OrtCompileApi {\r\n  /// \\name OrtModelCompilationOptions\r\n  /// @{\r\n  ORT_CLASS_RELEASE(ModelCompilationOptions);\r\n\r\n  /** \\brief Creates an OrtModelCompilationOptions object from an existing OrtSessionOptions object.\r\n   *\r\n   * An OrtModelCompilationOptions object contains the settings used to generate a compiled ONNX model.\r\n   * The OrtSessionOptions object has the execution providers with which the model will be compiled.\r\n   *\r\n   * ReleaseOrtModelCompilationsOptions must be called to free the OrtModelCompilationOptions after calling\r\n   * CompileModel.\r\n   *\r\n   * \\note By default, the GraphOptimizationLevel is set to ORT_DISABLE_ALL. Use\r\n   * ModelCompilationOptions_SetGraphOptimizationLevel to enable graph optimizations.\r\n   *\r\n   * \\param[in] env OrtEnv object.\r\n   * \\param[in] session_options The OrtSessionOptions instance from which to create the OrtModelCompilationOptions.\r\n   * \\param[out] out The created OrtModelCompilationOptions instance.\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CreateModelCompilationOptionsFromSessionOptions, _In_ const OrtEnv* env,\r\n                  _In_ const OrtSessionOptions* session_options, _Outptr_ OrtModelCompilationOptions** out);\r\n\r\n  /** \\brief Sets the file path to the input ONNX model to compile.\r\n   *\r\n   * The input model's location (e.g., file path or memory buffer) must be set with either\r\n   * ModelCompilationOptions_SetInputModelPath or ModelCompilationOptions_SetInputModelFromBuffer.\r\n   *\r\n   * \\param[in] model_compile_options The OrtModelCompilationOptions instance.\r\n   * \\param[in] input_model_path Null terminated string of the path (wchar on Windows, char otherwise).\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(ModelCompilationOptions_SetInputModelPath, _In_ OrtModelCompilationOptions* model_compile_options,\r\n                  _In_ const ORTCHAR_T* input_model_path);\r\n\r\n  /** \\brief Sets the buffer that stores the bytes of the loaded ONNX model to compile.\r\n   *\r\n   * The input model's location (e.g., file path or memory buffer) must be set with either\r\n   * ModelCompilationOptions_SetInputModelPath or ModelCompilationOptions_SetInputModelFromBuffer.\r\n   *\r\n   * \\param[in] model_compile_options The OrtModelCompilationOptions instance.\r\n   * \\param[in] input_model_data Buffer containing the loaded ONNX model bytes.\r\n   * \\param[in] input_model_data_size The number of bytes in the `input_model_data` buffer.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(ModelCompilationOptions_SetInputModelFromBuffer,\r\n                  _In_ OrtModelCompilationOptions* model_compile_options,\r\n                  _In_ const void* input_model_data,\r\n                  size_t input_model_data_size);\r\n\r\n  /** \\brief Sets the file path for the output ONNX model generated by CompileModel.\r\n   *\r\n   * The output model's location (e.g., file path or memory buffer) can be set with either\r\n   * ModelCompilationOptions_SetOutputModelPath or ModelCompilationOptions_SetOutputModelBuffer.\r\n   *\r\n   * If the output model's location is not set, ONNX Runtime will generate an output file with a path based on\r\n   * the input model's file path. Examples:\r\n   *   /Path/my_model.onnx -> /Path/my_model_ctx.onnx\r\n   *   /Path/my_model -> /Path/my_model_ctx.onnx\r\n   *\r\n   * \\param[in] model_compile_options The OrtModelCompilationOptions instance.\r\n   * \\param[in] output_model_path Null terminated string of the path (wchar on Windows, char otherwise).\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(ModelCompilationOptions_SetOutputModelPath, _In_ OrtModelCompilationOptions* model_compile_options,\r\n                  _In_ const ORTCHAR_T* output_model_path);\r\n\r\n  /** \\brief Optionally sets the file that should store external initializers for the compiled ONNX model.\r\n   * If not set, initializers are stored within the model.\r\n   *\r\n   * Only initializers for nodes that were not compiled are stored in the external initializers file.\r\n   * Compiled nodes contain their initializer data within the `ep_cache_context` attribute of EPContext nodes.\r\n   * Refer to ModelCompilationOptions_SetEpContextEmbedMode.\r\n   *\r\n   * \\param[in] model_compile_options The OrtModelCompilationOptions instance.\r\n   * \\param[in] external_initializers_file_path Null terminated string of the path to the file.\r\n   * \\param[in] external_initializers_size_threshold Initializers larger than this threshold are stored in the file.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(ModelCompilationOptions_SetOutputModelExternalInitializersFile,\r\n                  _In_ OrtModelCompilationOptions* model_compile_options,\r\n                  _In_ const ORTCHAR_T* external_initializers_file_path,\r\n                  size_t external_initializers_size_threshold);\r\n\r\n  /** \\brief Configures model compilation to store the output compiled ONNX model in a buffer.\r\n   *\r\n   * The caller passes an OrtAllocator that ONNX Runtime uses to allocate memory for the buffer.\r\n   *\r\n   * The output model's location (e.g., file path or memory buffer) can be set with either\r\n   * ModelCompilationOptions_SetOutputModelPath or ModelCompilationOptions_SetOutputModelBuffer.\r\n   *\r\n   * If the output model's location is not set, ONNX Runtime will generate an output file with a path based on\r\n   * the input model's file path. Examples:\r\n   *   /Path/my_model.onnx -> /Path/my_model_ctx.onnx\r\n   *   /Path/my_model -> /Path/my_model_ctx.onnx\r\n   *\r\n   * \\param[in] model_compile_options The OrtModelCompilationOptions instance.\r\n   * \\param[in] allocator The allocator used to allocate the buffer for the compiled model.\r\n   * \\param[out] output_model_buffer_ptr Pointer to the buffer that stores the compiled model.\r\n   * \\param[out] output_model_buffer_size_ptr Pointer set to the size of output model in bytes.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(ModelCompilationOptions_SetOutputModelBuffer,\r\n                  _In_ OrtModelCompilationOptions* model_compile_options,\r\n                  _Inout_ OrtAllocator* allocator,\r\n                  _Outptr_ void** output_model_buffer_ptr,\r\n                  _Out_ size_t* output_model_buffer_size_ptr);\r\n\r\n  /** \\brief Enables or disables the embedding of EPContext binary data into the `ep_cache_context` attribute\r\n   * of EPContext nodes. Defaults to false.\r\n   *\r\n   * If enabled, the `ep_cache_context` attribute of EPContext nodes will store the context binary data, which may\r\n   * include weights for compiled subgraphs.\r\n   *\r\n   * If disabled, the `ep_cache_context` attribute of EPContext nodes will contain the path to the file containing the\r\n   * context binary data. The path is set by the execution provider creating the EPContext node.\r\n   *\r\n   * More details relate to EPContext design refers to:\r\n   *  \\htmlonly\r\n   *  <a href=\"https://onnxruntime.ai/docs/execution-providers/EP-Context-Design.html\">EPContext design document.</a>\r\n   *  \\endhtmlonly\r\n   *\r\n   * \\param[in] model_compile_options The OrtModelCompilationOptions instance.\r\n   * \\param[in] embed_ep_context_in_model True to embed EPContext binary data into the EPContext node\r\n   *                                      `ep_cache_context` attributes.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(ModelCompilationOptions_SetEpContextEmbedMode, _In_ OrtModelCompilationOptions* model_compile_options,\r\n                  bool embed_ep_context_in_model);\r\n\r\n  /** \\brief Compiles an input ONNX model with the given compilation options.\r\n   *\r\n   * \\param[in] env OrtEnv object.\r\n   * \\param[in] model_options The compilation options that defines compilation options for a model.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.22.\r\n   */\r\n  ORT_API2_STATUS(CompileModel, _In_ const OrtEnv* env, _In_ const OrtModelCompilationOptions* model_options);\r\n\r\n  /** \\brief Sets flags from OrtCompileApiFlags that represent one or more boolean options to enable.\r\n   *\r\n   * \\param[in] model_compile_options The OrtModelCompilationOptions instance.\r\n   * \\param[in] flags bitwise OR of flags in OrtCompileApiFlags to enable.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ModelCompilationOptions_SetFlags, _In_ OrtModelCompilationOptions* model_compile_options,\r\n                  uint32_t flags);\r\n\r\n  /** Sets information related to EP context binary file.\r\n   *\r\n   * EP uses this information to decide the location and context binary file name.\r\n   * Used while compiling model with input and output in memory buffer\r\n   *\r\n   * \\param[in] model_compile_options The OrtModelCompilationOptions instance.\r\n   * \\param[in] output_directory Null terminated string of the path (wchar on Windows, char otherwise).\r\n   * \\param[in] model_name Null terminated string of the model name (wchar on Windows, char otherwise).\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ModelCompilationOptions_SetEpContextBinaryInformation,\r\n                  _In_ OrtModelCompilationOptions* model_compile_options,\r\n                  _In_ const ORTCHAR_T* output_directory,\r\n                  _In_ const ORTCHAR_T* model_name);\r\n\r\n  /** Set the graph optimization level.\r\n   *\r\n   * \\param[in] model_compile_options The OrtModelCompilationOptions instance.\r\n   * \\param[in] graph_optimization_level The graph optimization level.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ModelCompilationOptions_SetGraphOptimizationLevel,\r\n                  _In_ OrtModelCompilationOptions* model_compile_options,\r\n                  _In_ GraphOptimizationLevel graph_optimization_level);\r\n\r\n  /** \\brief Sets a OrtWriteBufferFunc function that is called by ORT to write out the output model's serialized\r\n   * ONNX bytes.\r\n   *\r\n   * The provided write function may be called repeatedly until then entire output model has been written out. Each call\r\n   * to the write function is expected to consume the entire input buffer.\r\n   *\r\n   * The output model's destination (e.g., file path, memory buffer, or stream) can be set with any of the functions\r\n   * that begin with ModelCompilationOptions_SetOutputModel____.\r\n   *\r\n   * \\param[in] model_compile_options The OrtModelCompilationOptions instance.\r\n   * \\param[in] write_func The OrtWriteBufferFunc function called by ORT when writing out the model.\r\n   * \\param[in] state Opaque state passed as the first argument to OrtWriteBufferFunc. Can be NULL.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ModelCompilationOptions_SetOutputModelWriteFunc,\r\n                  _In_ OrtModelCompilationOptions* model_compile_options,\r\n                  _In_ OrtWriteBufferFunc write_func, _In_ void* state);\r\n\r\n  /** \\brief Sets a OrtGetInitializerLocationFunc function that is called by ORT for every initializer in the generated\r\n   * model. Allows implementer to specify whether initializers should be stored within the model or externally.\r\n   *\r\n   * \\param[in] model_compile_options The OrtModelCompilationOptions instance.\r\n   * \\param[in] get_initializer_location_func The OrtGetInitializerLocationFunc function called by ORT when\r\n   *                                          to determine the location of the initializer.\r\n   * \\param[in] state Opaque state passed as the first argument to OrtGetInitializerLocationFunc. Can be NULL.\r\n   *\r\n   * \\snippet{doc} snippets.dox OrtStatus Return Value\r\n   *\r\n   * \\since Version 1.23.\r\n   */\r\n  ORT_API2_STATUS(ModelCompilationOptions_SetOutputModelGetInitializerLocationFunc,\r\n                  _In_ OrtModelCompilationOptions* model_compile_options,\r\n                  _In_ OrtGetInitializerLocationFunc get_initializer_location_func, _In_ void* state);\r\n};\r\n\r\n/*\r\n * This is the old way to add the CUDA provider to the session, please use SessionOptionsAppendExecutionProvider_CUDA above to access the latest functionality\r\n * This function always exists, but will only succeed if Onnxruntime was built with CUDA support and the CUDA provider shared library exists\r\n *\r\n * \\param device_id CUDA device id, starts from zero.\r\n */\r\nORT_API_STATUS(OrtSessionOptionsAppendExecutionProvider_CUDA, _In_ OrtSessionOptions* options, int device_id);\r\n\r\n/*\r\n * This is the old way to add the ROCm provider to the session, please use\r\n * SessionOptionsAppendExecutionProvider_ROCM above to access the latest functionality\r\n * This function always exists, but will only succeed if Onnxruntime was built with\r\n * HIP support and the ROCm provider shared library exists\r\n *\r\n * \\param device_id HIP device id, starts from zero.\r\n */\r\nORT_API_STATUS(OrtSessionOptionsAppendExecutionProvider_ROCM, _In_ OrtSessionOptions* options, int device_id);\r\n\r\n/*\r\n * This is the old way to add the MIGraphX provider to the session, please use\r\n * SessionOptionsAppendExecutionProvider_MIGraphX above to access the latest functionality\r\n * This function always exists, but will only succeed if Onnxruntime was built with\r\n * HIP support and the MIGraphX provider shared library exists\r\n *\r\n * \\param device_id HIP device id, starts from zero.\r\n */\r\nORT_API_STATUS(OrtSessionOptionsAppendExecutionProvider_MIGraphX, _In_ OrtSessionOptions* options, int device_id);\r\n\r\n/*\r\n * This is the old way to add the oneDNN provider to the session, please use\r\n * SessionOptionsAppendExecutionProvider_oneDNN above to access the latest functionality\r\n * This function always exists, but will only succeed if Onnxruntime was built with\r\n * oneDNN support and the oneDNN provider shared library exists\r\n *\r\n * \\param use_arena zero: false. non-zero: true.\r\n */\r\nORT_API_STATUS(OrtSessionOptionsAppendExecutionProvider_Dnnl, _In_ OrtSessionOptions* options, int use_arena);\r\n\r\n/*\r\n * This is the old way to add the TensorRT provider to the session, please use SessionOptionsAppendExecutionProvider_TensorRT_V2 above to access the latest functionality\r\n * This function always exists, but will only succeed if Onnxruntime was built with TensorRT support and the TensorRT provider shared library exists\r\n *\r\n * \\param device_id CUDA device id, starts from zero.\r\n */\r\nORT_API_STATUS(OrtSessionOptionsAppendExecutionProvider_Tensorrt, _In_ OrtSessionOptions* options, int device_id);\r\n\r\n#ifdef __cplusplus\r\n}\r\n#endif\r\n/// @}\r\n\r\n#include \"onnxruntime_ep_c_api.h\"\r\n"}
{"path":"\\\\?\\C:\\Users\\Naomi\\Desktop\\ONNX\\onnxruntime\\onnxruntime\\test\\providers\\qnn\\qnn_basic_test.cc","line":null,"char":null,"code":"CLANGFORMAT","severity":"warning","name":"format","description":"See https://clang.llvm.org/docs/ClangFormat.html.\nRun `lintrunner -a` to apply this patch.","original":"// Copyright (c) Microsoft Corporation. All rights reserved.\r\n// Licensed under the MIT License.\r\n\r\n#include <filesystem>\r\n#include <string>\r\n#include <thread>\r\n\r\n#include \"core/graph/constants.h\"\r\n#include \"core/graph/node_attr_utils.h\"\r\n#include \"core/providers/cpu/cpu_provider_factory.h\"  // For OrtSessionOptionsAppendExecutionProvider_CPU\r\n#if BUILD_QNN_EP_STATIC_LIB\r\n#include \"core/providers/qnn/qnn_allocator.h\"  // Used by QnnHTPBackendTests.UseHtpSharedMemoryAllocatorForInputs\r\n#endif\r\n#include \"core/session/inference_session.h\"\r\n#include \"core/session/onnxruntime_cxx_api.h\"\r\n#include \"core/session/onnxruntime_session_options_config_keys.h\"\r\n#include \"core/session/onnxruntime_run_options_config_keys.h\"\r\n\r\n#include \"test/providers/qnn/qnn_test_utils.h\"\r\n#include \"test/util/include/api_asserts.h\"\r\n\r\n#include \"gtest/gtest.h\"\r\n#include \"gmock/gmock.h\"\r\n\r\nusing namespace ONNX_NAMESPACE;\r\nusing namespace onnxruntime::logging;\r\n\r\n#define ORT_MODEL_FOLDER ORT_TSTR(\"testdata/\")\r\n\r\nconstexpr std::string_view kDlcOutputDir(\"dlc_output\");\r\n\r\n// in test_main.cc\r\nextern std::unique_ptr<Ort::Env> ort_env;\r\nextern \"C\" void ortenv_setup();\r\nextern \"C\" void ortenv_teardown();\r\n\r\nnamespace onnxruntime {\r\nnamespace test {\r\n\r\n// test uses ONNX model so can't be run in a minimal build.\r\n// TODO: When we need QNN in a minimal build we should add an ORT format version of the model\r\n#if !defined(ORT_MINIMAL_BUILD)\r\n\r\nstatic bool SessionHasEp(Ort::Session& session, const char* ep_name) {\r\n  // Access the underlying InferenceSession.\r\n  const OrtSession* ort_session = session;\r\n  const InferenceSession* s = reinterpret_cast<const InferenceSession*>(ort_session);\r\n  bool has_ep = false;\r\n\r\n  for (const auto& provider : s->GetRegisteredProviderTypes()) {\r\n    if (provider == ep_name) {\r\n      has_ep = true;\r\n      break;\r\n    }\r\n  }\r\n  return has_ep;\r\n}\r\n\r\n// Tests that the QNN EP is registered when added via the public C++ API.\r\n// Loads a simple ONNX model that adds floats.\r\nTEST_F(QnnHTPBackendTests, TestAddEpUsingPublicApi) {\r\n  onnxruntime::ProviderOptions options;\r\n#if defined(_WIN32)\r\n  options[\"backend_path\"] = \"QnnHtp.dll\";\r\n#else\r\n  options[\"backend_path\"] = \"libQnnHtp.so\";\r\n#endif\r\n\r\n  const ORTCHAR_T* ort_model_path = ORT_MODEL_FOLDER \"constant_floats.onnx\";\r\n\r\n  {\r\n    // Test C++ API to add QNN EP with the short name 'QNN'.\r\n    Ort::SessionOptions so;\r\n\r\n    // Can only enforce that model runs on QNN in linux CI machines\r\n    // because they support the CPU backend and emulate the HTP backend.\r\n    // TODO: Remove #ifdef when Windows Arm64 machines support the CPU backend.\r\n#if defined(__linux__)\r\n    so.AddConfigEntry(kOrtSessionOptionsDisableCPUEPFallback, \"1\");  // Disable fallback to the CPU EP.\r\n#endif\r\n    so.AppendExecutionProvider(\"QNN\", options);\r\n\r\n    Ort::Session session(*ort_env, ort_model_path, so);\r\n    ASSERT_TRUE(SessionHasEp(session, kQnnExecutionProvider))\r\n        << \"QNN EP was not found in registered providers for session \"\r\n        << \"providers for session when added to session with name 'QNN'\";\r\n  }\r\n\r\n  {\r\n    // Test C++ API to add QNN EP with the long canonical name 'QNNExecutionProvider'.\r\n    Ort::SessionOptions so;\r\n\r\n    // TODO: Remove #ifdef when Windows Arm64 machines support the CPU backend.\r\n#if defined(__linux__)\r\n    so.AddConfigEntry(kOrtSessionOptionsDisableCPUEPFallback, \"1\");  // Disable fallback to the CPU EP.\r\n#endif\r\n    so.AppendExecutionProvider(kQnnExecutionProvider, options);\r\n\r\n    Ort::Session session(*ort_env, ort_model_path, so);\r\n    ASSERT_TRUE(SessionHasEp(session, kQnnExecutionProvider))\r\n        << \"QNN EP was not found in registered providers for session \"\r\n        << \"when added to session with name '\" << kQnnExecutionProvider << \"'\";\r\n  }\r\n}\r\n\r\n// Tests the `session.disable_cpu_ep_fallback` configuration option when the backend cannot be loaded.\r\n// When the option is enabled, session creation throws an exception because the backend cannot be found.\r\nTEST(QnnEP, TestDisableCPUFallback_BackendNotFound) {\r\n  {\r\n    Ort::SessionOptions so;\r\n    so.AddConfigEntry(kOrtSessionOptionsDisableCPUEPFallback, \"1\");  // Disable fallback to the CPU EP.\r\n\r\n    onnxruntime::ProviderOptions options;\r\n#if defined(_WIN32)\r\n    options[\"backend_path\"] = \"DoesNotExist.dll\";  // Invalid backend path!\r\n#else\r\n    options[\"backend_path\"] = \"libDoesNotExist.so\";  // Invalid backend path!\r\n#endif\r\n\r\n    so.AppendExecutionProvider(\"QNN\", options);\r\n\r\n    const ORTCHAR_T* ort_model_path = ORT_MODEL_FOLDER \"constant_floats.onnx\";\r\n\r\n    try {\r\n      Ort::Session session(*ort_env, ort_model_path, so);\r\n      FAIL();  // Should not get here!\r\n    } catch (const Ort::Exception& excpt) {\r\n      ASSERT_EQ(excpt.GetOrtErrorCode(), ORT_FAIL);\r\n      ASSERT_THAT(excpt.what(), testing::HasSubstr(\"This session contains graph nodes that are assigned to the default \"\r\n                                                   \"CPU EP, but fallback to CPU EP has been explicitly disabled by \"\r\n                                                   \"the user.\"));\r\n    }\r\n  }\r\n}\r\n\r\n// Tests the `session.disable_cpu_ep_fallback` configuration option when the entire model cannot be assigned to QNN EP.\r\n// When the option is enabled, Session creation should throw an exception.\r\nTEST(QnnEP, TestDisableCPUFallback_ModelNotFullySupported) {\r\n  {\r\n    Ort::SessionOptions so;\r\n    so.AddConfigEntry(kOrtSessionOptionsDisableCPUEPFallback, \"1\");  // Disable fallback to the CPU EP.\r\n\r\n    onnxruntime::ProviderOptions options;\r\n#if defined(_WIN32)\r\n    options[\"backend_path\"] = \"QnnCpu.dll\";\r\n#else\r\n    options[\"backend_path\"] = \"libQnnCpu.so\";\r\n#endif\r\n    options[\"offload_graph_io_quantization\"] = \"0\";\r\n\r\n    so.AppendExecutionProvider(\"QNN\", options);\r\n\r\n    // QNN EP doesn't support MatMulInteger.\r\n    const ORTCHAR_T* ort_model_path = ORT_MODEL_FOLDER \"qnn_ep_partial_support.onnx\";\r\n\r\n    try {\r\n      Ort::Session session(*ort_env, ort_model_path, so);\r\n      FAIL();  // Should not get here!\r\n    } catch (const Ort::Exception& excpt) {\r\n      ASSERT_EQ(excpt.GetOrtErrorCode(), ORT_FAIL);\r\n      ASSERT_THAT(excpt.what(), testing::HasSubstr(\"This session contains graph nodes that are assigned to the default \"\r\n                                                   \"CPU EP, but fallback to CPU EP has been explicitly disabled by \"\r\n                                                   \"the user.\"));\r\n    }\r\n  }\r\n}\r\n\r\n// The model is supported on QNN CPU backend, but CPU fallback is disabled\r\n// QNN EP report error for this scenario also\r\nTEST(QnnEP, TestDisableCPUFallback_TryingToRunOnQnnCPU) {\r\n  SessionOptions so;\r\n  // Disable fallback to the CPU EP.\r\n  ASSERT_STATUS_OK(so.config_options.AddConfigEntry(kOrtSessionOptionsDisableCPUEPFallback, \"1\"));\r\n\r\n  onnxruntime::ProviderOptions options;\r\n#if defined(_WIN32)\r\n  options[\"backend_path\"] = \"QnnCpu.dll\";\r\n#else\r\n  options[\"backend_path\"] = \"libQnnCpu.so\";\r\n#endif\r\n  options[\"offload_graph_io_quantization\"] = \"0\";\r\n\r\n  auto input_defs = {TestInputDef<float>({1, 2, 2, 2}, false, -10.0f, 10.0f),\r\n                     TestInputDef<float>({1, 2, 2, 2}, false, -10.0f, 10.0f)};\r\n  auto model_func = BuildOpTestCase<float>(\"Add\", input_defs, {}, {}, kOnnxDomain);\r\n\r\n  const std::unordered_map<std::string, int> domain_to_version = {{\"\", 13}, {kMSDomain, 1}};\r\n\r\n  auto& logging_manager = DefaultLoggingManager();\r\n  // logging_manager.SetDefaultLoggerSeverity(log_severity);\r\n\r\n  onnxruntime::Model model(\"QNN_EP_TestModel\", false, ModelMetaData(), PathString(),\r\n                           IOnnxRuntimeOpSchemaRegistryList(), domain_to_version, {},\r\n                           logging_manager.DefaultLogger());\r\n  Graph& graph = model.MainGraph();\r\n  ModelTestBuilder helper(graph);\r\n  model_func(helper);\r\n  helper.SetGraphOutputs();\r\n  ASSERT_STATUS_OK(model.MainGraph().Resolve());\r\n\r\n  // Serialize the model to a string.\r\n  std::string model_data;\r\n  model.ToProto().SerializeToString(&model_data);\r\n\r\n  InferenceSession session_object{so, GetEnvironment()};\r\n  auto qnn_ep = QnnExecutionProviderWithOptions(options, &so);\r\n  EXPECT_TRUE(session_object.RegisterExecutionProvider(std::move(qnn_ep)).IsOK());\r\n\r\n  ASSERT_STATUS_OK(session_object.Load(model_data.data(), static_cast<int>(model_data.size())));\r\n  auto status = session_object.Initialize();\r\n  ASSERT_EQ(status.Code(), ORT_FAIL);\r\n  ASSERT_THAT(status.ErrorMessage().c_str(), testing::HasSubstr(\"This session contains graph nodes that are assigned to the default \"\r\n                                                                \"CPU EP, but fallback to CPU EP has been explicitly disabled by \"\r\n                                                                \"the user.\"));\r\n}\r\n\r\n// Tests invalid use of the `session.disable_cpu_ep_fallback` configuration option.\r\n// It is invalid to set the option and explicitly add the CPU EP to the session.\r\nTEST(QnnEP, TestDisableCPUFallback_ConflictingConfig) {\r\n  {\r\n    Ort::SessionOptions so;\r\n    so.AddConfigEntry(kOrtSessionOptionsDisableCPUEPFallback, \"1\");  // Disable fallback to the CPU EP.\r\n\r\n    onnxruntime::ProviderOptions options;\r\n#if defined(_WIN32)\r\n    options[\"backend_path\"] = \"QnnCpu.dll\";\r\n#else\r\n    options[\"backend_path\"] = \"libQnnCpu.so\";\r\n#endif\r\n    options[\"offload_graph_io_quantization\"] = \"0\";\r\n\r\n    so.AppendExecutionProvider(\"QNN\", options);\r\n\r\n    // Invalid! Adds CPU EP to session, but also disables CPU fallback.\r\n    so.AppendExecutionProvider_CPU(1);\r\n\r\n    const ORTCHAR_T* ort_model_path = ORT_MODEL_FOLDER \"constant_floats.onnx\";\r\n\r\n    try {\r\n      Ort::Session session(*ort_env, ort_model_path, so);\r\n      FAIL();  // Should not get here!\r\n    } catch (const Ort::Exception& excpt) {\r\n      ASSERT_EQ(excpt.GetOrtErrorCode(), ORT_INVALID_ARGUMENT);\r\n      ASSERT_THAT(excpt.what(), testing::HasSubstr(\"Conflicting session configuration: explicitly added the CPU EP to the \"\r\n                                                   \"session, but also disabled fallback to the CPU EP via session \"\r\n                                                   \"configuration options.\"));\r\n    }\r\n  }\r\n}\r\n\r\nTEST(QnnEP, TestInvalidSpecificationOfBothBackendTypeAndBackendPath) {\r\n  onnxruntime::ProviderOptions provider_options{};\r\n  provider_options[\"backend_type\"] = \"cpu\";\r\n#if defined(_WIN32)\r\n  provider_options[\"backend_path\"] = \"QnnCpu.dll\";\r\n#else\r\n  provider_options[\"backend_path\"] = \"libQnnCpu.so\";\r\n#endif\r\n\r\n  Ort::SessionOptions so{};\r\n  so.AppendExecutionProvider(\"QNN\", provider_options);\r\n\r\n  const ORTCHAR_T* ort_model_path = ORT_MODEL_FOLDER \"constant_floats.onnx\";\r\n\r\n  try {\r\n    Ort::Session session(*ort_env, ort_model_path, so);\r\n    FAIL();\r\n  } catch (const Ort::Exception& e) {\r\n    ASSERT_EQ(e.GetOrtErrorCode(), ORT_FAIL);\r\n    ASSERT_THAT(e.what(), testing::HasSubstr(\"Only one of 'backend_type' and 'backend_path' should be set.\"));\r\n  }\r\n}\r\n\r\n// Conv node `Conv` is not supported: GetFileLength for conv_qdq_external_ini.bin failed:open file conv_qdq_external_ini.bin fail,\r\n// errcode = 2 - The system cannot find the file specified.\r\nTEST_F(QnnHTPBackendTests, TestConvWithExternalData) {\r\n  Ort::SessionOptions so;\r\n  onnxruntime::ProviderOptions options;\r\n#if defined(_WIN32)\r\n  options[\"backend_path\"] = \"QnnHtp.dll\";\r\n#else\r\n  options[\"backend_path\"] = \"libQnnHtp.so\";\r\n#endif\r\n  options[\"offload_graph_io_quantization\"] = \"0\";\r\n\r\n  so.AppendExecutionProvider(\"QNN\", options);\r\n\r\n  so.AppendExecutionProvider_CPU(1);\r\n\r\n  const ORTCHAR_T* ort_model_path = ORT_MODEL_FOLDER \"conv_qdq_external_ini.onnx\";\r\n\r\n  Ort::Session session(*ort_env, ort_model_path, so);\r\n}\r\n\r\n#if defined(__aarch64__) || defined(_M_ARM64) || defined(__linux__)\r\nTEST_F(QnnHTPBackendTests, RunConvInt4Model) {\r\n  Ort::SessionOptions so;\r\n\r\n  so.AddConfigEntry(kOrtSessionOptionsDisableCPUEPFallback, \"1\");  // Disable fallback to the CPU EP.\r\n  so.SetGraphOptimizationLevel(ORT_ENABLE_ALL);\r\n  onnxruntime::ProviderOptions options;\r\n\r\n#if defined(_WIN32)\r\n  options[\"backend_path\"] = \"QnnHtp.dll\";\r\n#else\r\n  options[\"backend_path\"] = \"libQnnHtp.so\";\r\n#endif\r\n\r\n  so.AppendExecutionProvider(\"QNN\", options);\r\n\r\n  const ORTCHAR_T* ort_model_path = ORT_MODEL_FOLDER \"conv.int4_weights.qdq.onnx\";\r\n  Ort::Session session(*ort_env, ort_model_path, so);\r\n\r\n  TensorShape input_shape = {1, 3, 8, 8};\r\n  std::vector<float> input0_data(input_shape.Size(), 0.2f);\r\n\r\n  auto memory_info = Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeCPU);\r\n  std::vector<Ort::Value> ort_inputs;\r\n  std::vector<const char*> ort_input_names;\r\n\r\n  // Add input0\r\n  ort_inputs.emplace_back(Ort::Value::CreateTensor<float>(\r\n      memory_info, input0_data.data(), input0_data.size(), &input_shape[0], input_shape.NumDimensions()));\r\n  ort_input_names.push_back(\"input_0\");\r\n\r\n  // Run session and get outputs\r\n  std::array<const char*, 1> output_names{\"output_0\"};\r\n  std::vector<Ort::Value> ort_outputs = session.Run(Ort::RunOptions{nullptr}, ort_input_names.data(), ort_inputs.data(),\r\n                                                    ort_inputs.size(), output_names.data(), output_names.size());\r\n\r\n  // Check output shape.\r\n  Ort::Value& ort_output = ort_outputs[0];\r\n  auto typeshape = ort_output.GetTensorTypeAndShapeInfo();\r\n  std::vector<int64_t> output_shape = typeshape.GetShape();\r\n\r\n  EXPECT_THAT(output_shape, ::testing::ElementsAre(1, 5, 6, 6));\r\n}\r\n#endif  // #if defined(__aarch64__) || defined(_M_ARM64) || defined(__linux__)\r\n\r\nenum class TestBackend {\r\n  Cpu,\r\n  Htp,\r\n  Saver,\r\n  Ir,\r\n};\r\n\r\nstatic std::string ToBackendLibName(TestBackend backend) {\r\n  switch (backend) {\r\n    case TestBackend::Cpu:\r\n      return \"Cpu\";\r\n    case TestBackend::Htp:\r\n      return \"Htp\";\r\n    case TestBackend::Saver:\r\n      return \"Saver\";\r\n    case TestBackend::Ir:\r\n      return \"Ir\";\r\n    default:\r\n      assert(false && \"Invalid TestBackend value.\");\r\n      return \"\";\r\n  }\r\n}\r\n\r\nstatic void AddSerializerConfigs(TestBackend serializer_backend, onnxruntime::ProviderOptions& options) {\r\n  std::string serializer_lib = ToBackendLibName(serializer_backend);\r\n  std::string serializer_path_key;\r\n\r\n  switch (serializer_backend) {\r\n    case TestBackend::Ir:\r\n      serializer_path_key = \"qnn_ir_backend_path\";\r\n      options[\"dump_qnn_ir_dlc\"] = \"1\";\r\n      options[\"dump_qnn_ir_dlc_dir\"] = kDlcOutputDir;\r\n      break;\r\n    case TestBackend::Saver:\r\n      serializer_path_key = \"qnn_saver_path\";\r\n      break;\r\n    default:\r\n      assert(false && \"Invalid serializer backend.\");\r\n      return;\r\n  }\r\n\r\n#if defined(_WIN32)\r\n  options[serializer_path_key] = \"Qnn\" + serializer_lib + \".dll\";\r\n#else\r\n  options[serializer_path_key] = \"libQnn\" + serializer_lib + \".so\";\r\n#endif\r\n}\r\n\r\nstatic Ort::Session InitNHWCResizeModel(const ORTCHAR_T* ort_model_path, TestBackend backend,\r\n                                        std::optional<TestBackend> serializer_backend = std::nullopt,\r\n                                        std::string htp_graph_finalization_opt_mode = \"\",\r\n                                        std::string qnn_context_priority = \"\",\r\n                                        std::string soc_model = \"\",\r\n                                        std::string htp_arch = \"\",\r\n                                        std::string device_id = \"\") {\r\n  Ort::SessionOptions so;\r\n\r\n  // Ensure all type/shape inference warnings result in errors!\r\n  so.AddConfigEntry(kOrtSessionOptionsConfigStrictShapeTypeInference, \"1\");\r\n  so.SetGraphOptimizationLevel(ORT_ENABLE_ALL);\r\n\r\n  onnxruntime::ProviderOptions options;\r\n  options[\"offload_graph_io_quantization\"] = \"0\";\r\n\r\n  std::string backend_lib = ToBackendLibName(backend);\r\n\r\n#if defined(_WIN32)\r\n  options[\"backend_path\"] = \"Qnn\" + backend_lib + \".dll\";\r\n#else\r\n  options[\"backend_path\"] = \"libQnn\" + backend_lib + \".so\";\r\n#endif\r\n\r\n  if (serializer_backend) {\r\n    AddSerializerConfigs(*serializer_backend, options);\r\n  }\r\n\r\n  if (!htp_graph_finalization_opt_mode.empty()) {\r\n    options[\"htp_graph_finalization_optimization_mode\"] = std::move(htp_graph_finalization_opt_mode);\r\n  }\r\n\r\n  if (!qnn_context_priority.empty()) {\r\n    options[\"qnn_context_priority\"] = std::move(qnn_context_priority);\r\n  }\r\n\r\n  if (!soc_model.empty()) {\r\n    options[\"soc_model\"] = std::move(soc_model);\r\n  }\r\n\r\n  if (!htp_arch.empty()) {\r\n    options[\"htp_arch\"] = std::move(htp_arch);\r\n  }\r\n\r\n  if (!device_id.empty()) {\r\n    options[\"device_id\"] = std::move(device_id);\r\n  }\r\n\r\n  so.AppendExecutionProvider(\"QNN\", options);\r\n\r\n  Ort::Session session(*ort_env, ort_model_path, so);\r\n\r\n  return session;\r\n}\r\n\r\n// Helper function that runs an ONNX model with a NHWC Resize operator to test that\r\n// type/shape inference succeeds during layout transformation.\r\n// Refer to onnxruntime/core/graph/contrib_ops/nhwc_inference_context.h.\r\n//\r\n// The models passed to this function are subgraphs extracted from a larger model that exhibited\r\n// shape inferencing issues on QNN. Thus, the models are expected to have a specific input/output\r\n// types and shapes.\r\nstatic void RunNHWCResizeModel(const ORTCHAR_T* ort_model_path, TestBackend backend,\r\n                               std::optional<TestBackend> serializer_backend = std::nullopt,\r\n                               std::string htp_graph_finalization_opt_mode = \"\",\r\n                               std::string qnn_context_priority = \"\",\r\n                               std::string soc_model = \"\",\r\n                               std::string htp_arch = \"\",\r\n                               std::string device_id = \"\") {\r\n  Ort::Session session = InitNHWCResizeModel(ort_model_path, backend, serializer_backend, htp_graph_finalization_opt_mode, qnn_context_priority, soc_model, htp_arch, device_id);\r\n\r\n  // Input can be all zeros since we're testing for correct shape inference.\r\n  std::array<float, 1 * 3 * 4 * 5> input0_data = {};\r\n  std::array<float, 1 * 3 * 4 * 5> input1_data = {};\r\n  std::array<float, 1 * 3 * 4 * 5> input2_data = {};\r\n\r\n  auto memory_info = Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeCPU);\r\n  std::vector<Ort::Value> ort_inputs;\r\n  std::vector<const char*> ort_input_names;\r\n\r\n  // Add input0\r\n  std::array<int64_t, 4> inputs_shape{1, 3, 4, 5};\r\n  ort_inputs.emplace_back(Ort::Value::CreateTensor<float>(\r\n      memory_info, input0_data.data(), input0_data.size(), inputs_shape.data(), inputs_shape.size()));\r\n  ort_input_names.push_back(\"input0\");\r\n\r\n  // Add input1\r\n  ort_inputs.emplace_back(Ort::Value::CreateTensor<float>(\r\n      memory_info, input1_data.data(), input1_data.size(), inputs_shape.data(), inputs_shape.size()));\r\n  ort_input_names.push_back(\"input1\");\r\n\r\n  // Add input2\r\n  ort_inputs.emplace_back(Ort::Value::CreateTensor<float>(\r\n      memory_info, input2_data.data(), input2_data.size(), inputs_shape.data(), inputs_shape.size()));\r\n  ort_input_names.push_back(\"input2\");\r\n\r\n  // Run session and get outputs\r\n  std::array<const char*, 2> output_names{\"output0\", \"output1\"};\r\n  std::vector<Ort::Value> ort_outputs = session.Run(Ort::RunOptions{nullptr}, ort_input_names.data(), ort_inputs.data(),\r\n                                                    ort_inputs.size(), output_names.data(), output_names.size());\r\n\r\n  // Check output shape.\r\n  Ort::Value& ort_output = ort_outputs[1];\r\n  auto typeshape = ort_output.GetTensorTypeAndShapeInfo();\r\n  std::vector<int64_t> output_shape = typeshape.GetShape();\r\n\r\n  EXPECT_THAT(output_shape, ::testing::ElementsAre(1, 6, 7, 10));\r\n}\r\n\r\n// Test shape inference of NHWC Resize operator (opset 11) that uses\r\n// the scales input. Use the QNN CPU backend.\r\nTEST_F(QnnCPUBackendTests, TestNHWCResizeShapeInference_scales_opset11) {\r\n  RunNHWCResizeModel(ORT_MODEL_FOLDER \"nhwc_resize_scales_opset11.onnx\", TestBackend::Cpu);\r\n}\r\n\r\n// Test shape inference of NHWC Resize operator (opset 18) that uses\r\n// the scales input. Use the QNN CPU backend.\r\nTEST_F(QnnCPUBackendTests, TestNHWCResizeShapeInference_scales_opset18) {\r\n  RunNHWCResizeModel(ORT_MODEL_FOLDER \"nhwc_resize_scales_opset18.onnx\", TestBackend::Cpu);\r\n}\r\n\r\n// Test shape inference of NHWC Resize operator (opset 11) that uses\r\n// the sizes input. Use the QNN CPU backend.\r\nTEST_F(QnnCPUBackendTests, TestNHWCResizeShapeInference_sizes_opset11) {\r\n  RunNHWCResizeModel(ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset11.onnx\", TestBackend::Cpu);\r\n}\r\n\r\n// Test shape inference of NHWC Resize operator (opset 18) that uses\r\n// the sizes input. Use the QNN CPU backend.\r\nTEST_F(QnnCPUBackendTests, TestNHWCResizeShapeInference_sizes_opset18) {\r\n  RunNHWCResizeModel(ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.onnx\", TestBackend::Cpu);\r\n}\r\n\r\n// Test that QNN Saver generates the expected files for a model meant to run on the QNN CPU backend.\r\nTEST_F(QnnCPUBackendTests, QnnSaver_OutputFiles) {\r\n  const std::filesystem::path qnn_saver_output_dir = \"saver_output\";\r\n\r\n  // Remove pre-existing QNN Saver output files. Note that fs::remove_all() can handle non-existing paths.\r\n  std::filesystem::remove_all(qnn_saver_output_dir);\r\n  ASSERT_FALSE(std::filesystem::exists(qnn_saver_output_dir));\r\n\r\n  RunNHWCResizeModel(ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.onnx\",\r\n                     TestBackend::Cpu,     // backend\r\n                     TestBackend::Saver);  // serializer_backend\r\n\r\n  // Check that QNN Saver output files exist.\r\n  EXPECT_TRUE(std::filesystem::exists(qnn_saver_output_dir / \"saver_output.c\"));\r\n  EXPECT_TRUE(std::filesystem::exists(qnn_saver_output_dir / \"params.bin\"));\r\n}\r\n\r\nstruct ModelAndBuilder {\r\n  ModelAndBuilder(Graph& graph) : builder(graph) {}\r\n  std::string model_data;\r\n  ModelTestBuilder builder;\r\n};\r\n\r\n// Creates a model in memory. Input feeds and output names can be accessed from result.builder.\r\nstatic void CreateModelInMemory(std::unique_ptr<ModelAndBuilder>& result,\r\n                                const GetTestModelFn& model_build_fn,\r\n                                const std::string& model_name,\r\n                                int opset_version = 18) {\r\n  const std::unordered_map<std::string, int> domain_to_version = {{\"\", opset_version}, {kMSDomain, 1}};\r\n  auto& logging_manager = DefaultLoggingManager();\r\n\r\n  // Create float model and serialize it to a string.\r\n  onnxruntime::Model model(model_name, false, ModelMetaData(), PathString(),\r\n                           IOnnxRuntimeOpSchemaRegistryList(), domain_to_version, {},\r\n                           logging_manager.DefaultLogger());\r\n  result = std::make_unique<ModelAndBuilder>(model.MainGraph());\r\n  model_build_fn(result->builder);\r\n  result->builder.SetGraphOutputs();\r\n  ASSERT_STATUS_OK(model.MainGraph().Resolve());\r\n  model.ToProto().SerializeToString(&result->model_data);\r\n}\r\n\r\n// Runs a session and verifies the outputs. Can be run by individual threads.\r\nstatic void RunSessionAndVerify(InferenceSession& session, const RunOptions& run_options, const NameMLValMap& feeds,\r\n                                const std::vector<std::string>& output_names,\r\n                                const std::vector<std::vector<int64_t>>& output_shapes,\r\n                                const std::vector<std::vector<float>>& expected_values,\r\n                                int loop_count = 10) {\r\n  // Let it run for a while\r\n  for (int it = 0; it < loop_count; ++it) {\r\n    std::vector<OrtValue> fetches;\r\n    auto status = session.Run(run_options, feeds, output_names, &fetches);\r\n    ASSERT_TRUE(status.IsOK());\r\n\r\n    for (size_t i = 0; i < fetches.size(); i++) {\r\n      auto& tensor = fetches[i].Get<Tensor>();\r\n      TensorShape expected_shape(output_shapes[i]);\r\n      ASSERT_EQ(expected_shape, tensor.Shape());\r\n\r\n      gsl::span<const float> actual = tensor.DataAsSpan<float>();\r\n      gsl::span<const float> expected(expected_values[i].data(), expected_values[i].size());\r\n      ASSERT_EQ(expected, actual);\r\n    }\r\n  }\r\n}\r\n\r\n// Returns a function that builds a float32 model that adds 3 tensors.\r\nstatic GetTestModelFn F32BuildAdd3Tensors(const TestInputDef<float>& input0_def,\r\n                                          const TestInputDef<float>& input1_def,\r\n                                          const TestInputDef<float>& input2_def) {\r\n  return [input0_def, input1_def, input2_def](ModelTestBuilder& builder) {\r\n    NodeArg* input0 = MakeTestInput<float>(builder, input0_def);\r\n    NodeArg* input1 = MakeTestInput<float>(builder, input1_def);\r\n    NodeArg* input2 = MakeTestInput<float>(builder, input1_def);\r\n\r\n    auto* add0_out = builder.MakeIntermediate();\r\n    builder.AddNode(\"Add\", {input0, input1}, {add0_out});\r\n\r\n    auto* output = builder.MakeOutput();\r\n    builder.AddNode(\"Add\", {add0_out, input2}, {output});\r\n  };\r\n}\r\n\r\n// Tests running a single session in multiple threads on the CPU backend.\r\nTEST_F(QnnCPUBackendTests, MultithreadSessionRun) {\r\n  std::unique_ptr<ModelAndBuilder> model;\r\n  std::vector<float> input_data = {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f};\r\n  std::vector<int64_t> shape = {1, 3, 2};\r\n  std::vector<std::vector<int64_t>> output_shapes = {shape};\r\n  std::vector<std::vector<float>> output_values = {{3.0f, 6.0f, 9.0f, 12.0f, 15.0f, 18.0f}};\r\n\r\n  CreateModelInMemory(model,\r\n                      F32BuildAdd3Tensors(TestInputDef<float>(shape, false, input_data),\r\n                                          TestInputDef<float>(shape, false, input_data),\r\n                                          TestInputDef<float>(shape, false, input_data)),\r\n                      \"add3.f32\");\r\n\r\n  SessionOptions session_opts;\r\n  session_opts.session_logid = \"logger0\";\r\n\r\n  RunOptions run_opts;\r\n  run_opts.run_tag = session_opts.session_logid;\r\n\r\n  InferenceSession session_obj{session_opts, GetEnvironment()};\r\n  onnxruntime::ProviderOptions options;\r\n\r\n#if defined(_WIN32)\r\n  options[\"backend_path\"] = \"QnnCpu.dll\";\r\n#else\r\n  options[\"backend_path\"] = \"libQnnCpu.so\";\r\n#endif\r\n\r\n  auto qnn_ep = QnnExecutionProviderWithOptions(options, &session_opts);\r\n  EXPECT_TRUE(session_obj.RegisterExecutionProvider(std::move(qnn_ep)).IsOK());\r\n\r\n  auto status = session_obj.Load(model->model_data.data(), static_cast<int>(model->model_data.size()));\r\n  ASSERT_TRUE(status.IsOK());\r\n  status = session_obj.Initialize();\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  std::vector<std::thread> threads;\r\n  constexpr int num_threads = 5;\r\n  constexpr int loop_count = 10;\r\n  for (int i = 0; i < num_threads; i++) {\r\n    threads.push_back(std::thread(RunSessionAndVerify, std::ref(session_obj), run_opts,\r\n                                  model->builder.feeds_, model->builder.output_names_,\r\n                                  output_shapes, output_values, loop_count));\r\n  }\r\n\r\n  for (auto& th : threads) {\r\n    th.join();\r\n  }\r\n}\r\n\r\n#if defined(__aarch64__) || defined(_M_ARM64) || defined(__linux__)\r\n\r\n// Returns a function that builds a QDQ model that adds 3 tensors. Forces all scales and zero-points to be (1.0f, 0),\r\n// so it is only accurate when using non-fractional positive inputs.\r\ntemplate <typename QuantType>\r\nstatic GetTestModelFn QDQBuildAdd3Tensors(const TestInputDef<float>& input0_def,\r\n                                          const TestInputDef<float>& input1_def,\r\n                                          const TestInputDef<float>& input2_def) {\r\n  return [input0_def, input1_def, input2_def](ModelTestBuilder& builder) {\r\n    NodeArg* input0 = MakeTestInput<float>(builder, input0_def);\r\n    NodeArg* input0_after_qdq = AddQDQNodePair<QuantType>(builder, input0, 1.0f, 0);\r\n    NodeArg* input1 = MakeTestInput<float>(builder, input1_def);\r\n    NodeArg* input1_after_qdq = AddQDQNodePair<QuantType>(builder, input1, 1.0f, 0);\r\n    NodeArg* input2 = MakeTestInput<float>(builder, input1_def);\r\n    NodeArg* input2_after_qdq = AddQDQNodePair<QuantType>(builder, input2, 1.0f, 0);\r\n\r\n    auto* add0_out = builder.MakeIntermediate();\r\n    builder.AddNode(\"Add\", {input0_after_qdq, input1_after_qdq}, {add0_out});\r\n\r\n    auto* add0_out_dq = AddQDQNodePair<QuantType>(builder, add0_out, 1.0f, 0);\r\n\r\n    auto* add1_out = builder.MakeIntermediate();\r\n    builder.AddNode(\"Add\", {add0_out_dq, input2_after_qdq}, {add1_out});\r\n\r\n    // op_output -> Q -> DQ -> output\r\n    AddQDQNodePairWithOutputAsGraphOutput<QuantType>(builder, add1_out, 1.0f, 0);\r\n  };\r\n}\r\n\r\n// Tests running a single session in multiple threads on the HTP backend.\r\nTEST_F(QnnHTPBackendTests, MultithreadSessionRun) {\r\n  std::unique_ptr<ModelAndBuilder> model;\r\n  std::vector<float> input_data = {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f};\r\n  std::vector<int64_t> shape = {1, 3, 2};\r\n  std::vector<std::vector<int64_t>> output_shapes = {shape};\r\n  std::vector<std::vector<float>> output_values = {{3.0f, 6.0f, 9.0f, 12.0f, 15.0f, 18.0f}};\r\n\r\n  CreateModelInMemory(model,\r\n                      QDQBuildAdd3Tensors<uint8_t>(TestInputDef<float>(shape, false, input_data),\r\n                                                   TestInputDef<float>(shape, false, input_data),\r\n                                                   TestInputDef<float>(shape, false, input_data)),\r\n                      \"add3.qdq\");\r\n\r\n  SessionOptions session_opts;\r\n  session_opts.session_logid = \"logger0\";\r\n\r\n  RunOptions run_opts;\r\n  run_opts.run_tag = session_opts.session_logid;\r\n\r\n  InferenceSession session_obj{session_opts, GetEnvironment()};\r\n  onnxruntime::ProviderOptions options;\r\n\r\n#if defined(_WIN32)\r\n  options[\"backend_path\"] = \"QnnHtp.dll\";\r\n#else\r\n  options[\"backend_path\"] = \"libQnnHtp.so\";\r\n#endif\r\n  options[\"offload_graph_io_quantization\"] = \"0\";\r\n\r\n  auto qnn_ep = QnnExecutionProviderWithOptions(options, &session_opts);\r\n  EXPECT_TRUE(session_obj.RegisterExecutionProvider(std::move(qnn_ep)).IsOK());\r\n\r\n  auto status = session_obj.Load(model->model_data.data(), static_cast<int>(model->model_data.size()));\r\n  ASSERT_TRUE(status.IsOK());\r\n  status = session_obj.Initialize();\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  std::vector<std::thread> threads;\r\n  constexpr int num_threads = 5;\r\n  constexpr int loop_count = 10;\r\n\r\n  for (int i = 0; i < num_threads; i++) {\r\n    threads.push_back(std::thread(RunSessionAndVerify, std::ref(session_obj), run_opts,\r\n                                  model->builder.feeds_, model->builder.output_names_,\r\n                                  output_shapes, output_values, loop_count));\r\n  }\r\n\r\n  for (auto& th : threads) {\r\n    th.join();\r\n  }\r\n}\r\n\r\n// Tests running a single session in multiple threads on the HTP backend with run option to set power config\r\nTEST_F(QnnHTPBackendTests, MultithreadHtpPowerCfgSessionRunOption) {\r\n  std::unique_ptr<ModelAndBuilder> model;\r\n  std::vector<float> input_data = {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f};\r\n  std::vector<int64_t> shape = {1, 3, 2};\r\n  std::vector<std::vector<int64_t>> output_shapes = {shape};\r\n  std::vector<std::vector<float>> output_values = {{3.0f, 6.0f, 9.0f, 12.0f, 15.0f, 18.0f}};\r\n\r\n  CreateModelInMemory(model,\r\n                      QDQBuildAdd3Tensors<uint8_t>(TestInputDef<float>(shape, false, input_data),\r\n                                                   TestInputDef<float>(shape, false, input_data),\r\n                                                   TestInputDef<float>(shape, false, input_data)),\r\n                      \"add3.qdq\");\r\n\r\n  SessionOptions session_opts;\r\n  session_opts.session_logid = \"logger0\";\r\n\r\n  InferenceSession session_obj{session_opts, GetEnvironment()};\r\n  onnxruntime::ProviderOptions options;\r\n\r\n#if defined(_WIN32)\r\n  options[\"backend_path\"] = \"QnnHtp.dll\";\r\n#else\r\n  options[\"backend_path\"] = \"libQnnHtp.so\";\r\n#endif\r\n  options[\"offload_graph_io_quantization\"] = \"0\";\r\n\r\n  auto qnn_ep = QnnExecutionProviderWithOptions(options, &session_opts);\r\n  EXPECT_TRUE(session_obj.RegisterExecutionProvider(std::move(qnn_ep)).IsOK());\r\n\r\n  auto status = session_obj.Load(model->model_data.data(), static_cast<int>(model->model_data.size()));\r\n  ASSERT_TRUE(status.IsOK());\r\n  status = session_obj.Initialize();\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  std::vector<std::thread> threads;\r\n  constexpr int num_threads = 5;\r\n  constexpr int loop_count = 10;\r\n\r\n  std::vector<std::string> perf_modes{\r\n      \"burst\", \"balanced\", \"default\", \"high_performance\", \"high_power_saver\",\r\n      \"low_balanced\", \"extreme_power_saver\", \"low_power_saver\", \"power_saver\"};\r\n\r\n  size_t post_i = perf_modes.size() - 1;\r\n  ASSERT_TRUE(post_i > num_threads);\r\n  for (int i = 0; i < num_threads; ++i, --post_i) {\r\n    RunOptions run_opts;\r\n    run_opts.run_tag = session_opts.session_logid;\r\n    auto rt = run_opts.config_options.AddConfigEntry(kOrtRunOptionsConfigQnnPerfMode, perf_modes[i].c_str());\r\n    ASSERT_TRUE(rt.IsOK());\r\n    rt = run_opts.config_options.AddConfigEntry(kOrtRunOptionsConfigQnnPerfModePostRun, perf_modes[post_i].c_str());\r\n    ASSERT_TRUE(rt.IsOK());\r\n\r\n    threads.push_back(std::thread(RunSessionAndVerify, std::ref(session_obj), run_opts,\r\n                                  model->builder.feeds_, model->builder.output_names_,\r\n                                  output_shapes, output_values, loop_count));\r\n  }\r\n\r\n  for (auto& th : threads) {\r\n    th.join();\r\n  }\r\n}\r\n\r\n// Tests running a single session in multiple threads on the HTP backend with EP option to set default power config\r\nTEST_F(QnnHTPBackendTests, MultithreadDefaultHtpPowerCfgFromEpOption) {\r\n  std::unique_ptr<ModelAndBuilder> model;\r\n  std::vector<float> input_data = {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f};\r\n  std::vector<int64_t> shape = {1, 3, 2};\r\n  std::vector<std::vector<int64_t>> output_shapes = {shape};\r\n  std::vector<std::vector<float>> output_values = {{3.0f, 6.0f, 9.0f, 12.0f, 15.0f, 18.0f}};\r\n\r\n  CreateModelInMemory(model,\r\n                      QDQBuildAdd3Tensors<uint8_t>(TestInputDef<float>(shape, false, input_data),\r\n                                                   TestInputDef<float>(shape, false, input_data),\r\n                                                   TestInputDef<float>(shape, false, input_data)),\r\n                      \"add3.qdq\");\r\n\r\n  SessionOptions session_opts;\r\n  session_opts.session_logid = \"logger0\";\r\n\r\n  RunOptions run_opts;\r\n  run_opts.run_tag = session_opts.session_logid;\r\n\r\n  InferenceSession session_obj{session_opts, GetEnvironment()};\r\n  onnxruntime::ProviderOptions options;\r\n\r\n#if defined(_WIN32)\r\n  options[\"backend_path\"] = \"QnnHtp.dll\";\r\n#else\r\n  options[\"backend_path\"] = \"libQnnHtp.so\";\r\n#endif\r\n  options[\"offload_graph_io_quantization\"] = \"0\";\r\n  options[\"htp_performance_mode\"] = \"burst\";\r\n\r\n  auto qnn_ep = QnnExecutionProviderWithOptions(options, &session_opts);\r\n  EXPECT_TRUE(session_obj.RegisterExecutionProvider(std::move(qnn_ep)).IsOK());\r\n\r\n  auto status = session_obj.Load(model->model_data.data(), static_cast<int>(model->model_data.size()));\r\n  ASSERT_TRUE(status.IsOK());\r\n  status = session_obj.Initialize();\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  std::vector<std::thread> threads;\r\n  constexpr int num_threads = 5;\r\n  constexpr int loop_count = 10;\r\n\r\n  for (int i = 0; i < num_threads; i++) {\r\n    threads.push_back(std::thread(RunSessionAndVerify, std::ref(session_obj), run_opts,\r\n                                  model->builder.feeds_, model->builder.output_names_,\r\n                                  output_shapes, output_values, loop_count));\r\n  }\r\n\r\n  for (auto& th : threads) {\r\n    th.join();\r\n  }\r\n}\r\n\r\n// Tests running a single session in multiple threads on the HTP backend with\r\n// EP option to set default power config + run option to set power config for each run\r\nTEST_F(QnnHTPBackendTests, MultithreadHtpPowerCfgDefaultAndRunOption) {\r\n  std::unique_ptr<ModelAndBuilder> model;\r\n  std::vector<float> input_data = {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f};\r\n  std::vector<int64_t> shape = {1, 3, 2};\r\n  std::vector<std::vector<int64_t>> output_shapes = {shape};\r\n  std::vector<std::vector<float>> output_values = {{3.0f, 6.0f, 9.0f, 12.0f, 15.0f, 18.0f}};\r\n\r\n  CreateModelInMemory(model,\r\n                      QDQBuildAdd3Tensors<uint8_t>(TestInputDef<float>(shape, false, input_data),\r\n                                                   TestInputDef<float>(shape, false, input_data),\r\n                                                   TestInputDef<float>(shape, false, input_data)),\r\n                      \"add3.qdq\");\r\n\r\n  SessionOptions session_opts;\r\n  session_opts.session_logid = \"logger0\";\r\n\r\n  InferenceSession session_obj{session_opts, GetEnvironment()};\r\n  onnxruntime::ProviderOptions options;\r\n\r\n#if defined(_WIN32)\r\n  options[\"backend_path\"] = \"QnnHtp.dll\";\r\n#else\r\n  options[\"backend_path\"] = \"libQnnHtp.so\";\r\n#endif\r\n  options[\"offload_graph_io_quantization\"] = \"0\";\r\n  options[\"htp_performance_mode\"] = \"burst\";\r\n\r\n  auto qnn_ep = QnnExecutionProviderWithOptions(options, &session_opts);\r\n  EXPECT_TRUE(session_obj.RegisterExecutionProvider(std::move(qnn_ep)).IsOK());\r\n\r\n  auto status = session_obj.Load(model->model_data.data(), static_cast<int>(model->model_data.size()));\r\n  ASSERT_TRUE(status.IsOK());\r\n  status = session_obj.Initialize();\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  std::vector<std::thread> threads;\r\n  constexpr int num_threads = 5;\r\n  constexpr int loop_count = 10;\r\n\r\n  std::vector<std::string> perf_modes{\r\n      \"burst\", \"balanced\", \"default\", \"high_performance\", \"high_power_saver\",\r\n      \"low_balanced\", \"extreme_power_saver\", \"low_power_saver\", \"power_saver\"};\r\n\r\n  size_t post_i = perf_modes.size() - 1;\r\n  ASSERT_TRUE(post_i > num_threads);\r\n  for (int i = 0; i < num_threads; ++i, --post_i) {\r\n    RunOptions run_opts;\r\n    run_opts.run_tag = session_opts.session_logid;\r\n    auto rt = run_opts.config_options.AddConfigEntry(kOrtRunOptionsConfigQnnPerfMode, perf_modes[i].c_str());\r\n    ASSERT_TRUE(rt.IsOK());\r\n    rt = run_opts.config_options.AddConfigEntry(kOrtRunOptionsConfigQnnPerfModePostRun, perf_modes[post_i].c_str());\r\n    ASSERT_TRUE(rt.IsOK());\r\n\r\n    threads.push_back(std::thread(RunSessionAndVerify, std::ref(session_obj), run_opts,\r\n                                  model->builder.feeds_, model->builder.output_names_,\r\n                                  output_shapes, output_values, loop_count));\r\n  }\r\n\r\n  for (auto& th : threads) {\r\n    th.join();\r\n  }\r\n}\r\n\r\n// Test shape inference of QDQ NHWC Resize operator (opset 18) that uses\r\n// the sizes input. Use the QNN HTP backend.\r\n// Maps to QNN's ResizeBilinear operator.\r\nTEST_F(QnnHTPBackendTests, TestNHWCResizeShapeInference_qdq_sizes_opset18) {\r\n  RunNHWCResizeModel(ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.quant.onnx\", TestBackend::Htp);\r\n}\r\n\r\n// Test that QNN Ir generates the expected file for a model meant to run on the QNN HTP backend.\r\n\r\nTEST_F(QnnHTPBackendTests, QnnIr_OutputFiles) {\r\n  const auto& logger = DefaultLoggingManager().DefaultLogger();\r\n  if (IsIRBackendSupported() == BackendSupport::UNSUPPORTED) {\r\n    LOGS(logger, WARNING) << \"QNN IR backend is not available! Skipping test.\";\r\n    GTEST_SKIP();\r\n  } else if (IsIRBackendSupported() == BackendSupport::SUPPORT_ERROR) {\r\n    LOGS(logger, ERROR) << \"Failed to check if QNN IR backend is available.\";\r\n    FAIL();\r\n  }\r\n\r\n  const std::filesystem::path qnn_dlc_dir = kDlcOutputDir;\r\n\r\n  // Remove pre-existing QNN Ir output files. Note that fs::remove_all() can handle non-existing paths.\r\n  std::filesystem::remove_all(qnn_dlc_dir);\r\n  ASSERT_FALSE(std::filesystem::exists(qnn_dlc_dir));\r\n\r\n  InitNHWCResizeModel(ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.onnx\",\r\n                      TestBackend::Htp,  // backend\r\n                      TestBackend::Ir);  // serializer backend\r\n\r\n  // File names are taken from graph node names. Just make sure that we got one .dlc\r\n  // in the expected directory.\r\n  ASSERT_TRUE(std::filesystem::exists(qnn_dlc_dir));\r\n\r\n  int file_count = 0;\r\n  for (const auto& entry : std::filesystem::directory_iterator(qnn_dlc_dir)) {\r\n    EXPECT_TRUE(entry.is_regular_file());\r\n    EXPECT_EQ(entry.path().extension(), \".dlc\");\r\n    ++file_count;\r\n  }\r\n  EXPECT_EQ(file_count, 1);\r\n}\r\n\r\n// Test that QNN Saver generates the expected files for a model meant to run on the QNN HTP backend.\r\nTEST_F(QnnHTPBackendTests, QnnSaver_OutputFiles) {\r\n  const std::filesystem::path qnn_saver_output_dir = \"saver_output\";\r\n\r\n  // Remove pre-existing QNN Saver output files. Note that fs::remove_all() can handle non-existing paths.\r\n  std::filesystem::remove_all(qnn_saver_output_dir);\r\n  ASSERT_FALSE(std::filesystem::exists(qnn_saver_output_dir));\r\n\r\n  RunNHWCResizeModel(ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.onnx\",\r\n                     TestBackend::Htp,     // backend\r\n                     TestBackend::Saver);  // serializer_backend\r\n\r\n  // Check that QNN Saver output files exist.\r\n  EXPECT_TRUE(std::filesystem::exists(qnn_saver_output_dir / \"saver_output.c\"));\r\n  EXPECT_TRUE(std::filesystem::exists(qnn_saver_output_dir / \"params.bin\"));\r\n}\r\n\r\n// Test that models run with various HTP graph finalization optimization modes.\r\nTEST_F(QnnHTPBackendTests, HTPGraphFinalizationOptimizationModes) {\r\n  constexpr std::array<const char*, 5> graph_opt_modes = {\"\",    // No explicit mode specified\r\n                                                          \"0\",   // Explicit default mode\r\n                                                          \"1\",   // Mode 1\r\n                                                          \"2\",   // Mode 2\r\n                                                          \"3\"};  // Mode 3\r\n  for (auto mode : graph_opt_modes) {\r\n    RunNHWCResizeModel(ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.quant.onnx\",\r\n                       TestBackend::Htp,  // backend\r\n                       std::nullopt,      // serializer_backend\r\n                       mode);             // htp_graph_finalization_opt_mode\r\n  }\r\n}\r\n\r\n// Test that models run with various SoC model values\r\nTEST_F(QnnHTPBackendTests, HTPSocModels) {\r\n  constexpr std::array<const char*, 3> soc_models = {\"\",   // No explicit SoC model specified\r\n                                                     \"0\",  // \"Unknown\"\r\n#if defined(_M_ARM64)\r\n                                                     \"37\"};  // SC8280X\r\n#elif defined(__linux__)\r\n                                                     \"30\"};  // SM8350\r\n#else\r\n                                                     \"\"};\r\n#endif\r\n\r\n  for (auto soc_model : soc_models) {\r\n    RunNHWCResizeModel(ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.quant.onnx\",\r\n                       TestBackend::Htp,  // backend\r\n                       std::nullopt,      // serializer_backend\r\n                       \"\",                // htp_graph_finalization_opt_mode\r\n                       \"\",                // qnn_context_priority\r\n                       soc_model);\r\n  }\r\n}\r\n\r\n// Test that models run with various HTP architecture values (and set device_id)\r\nTEST_F(QnnHTPBackendTests, HTPArchValues) {\r\n  constexpr std::array<const char*, 3> htp_archs = {\"\",     // No explicit arch specified\r\n                                                    \"0\",    // \"None\"\r\n                                                    \"68\"};  // v68\r\n  for (auto htp_arch : htp_archs) {\r\n    RunNHWCResizeModel(ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.quant.onnx\",\r\n                       TestBackend::Htp,  // backend\r\n                       std::nullopt,      // enable_qnn_saver\r\n                       \"\",                // htp_graph_finalization_opt_mode\r\n                       \"\",                // qnn_context_priority\r\n                       \"\",                // soc_model\r\n                       htp_arch,          // htp_arch\r\n                       \"0\");              // device_id\r\n  }\r\n}\r\n\r\n// Test that models run with high QNN context priority.\r\nTEST_F(QnnHTPBackendTests, QnnContextPriorityHigh) {\r\n  RunNHWCResizeModel(ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.quant.onnx\",\r\n                     TestBackend::Htp,  // use_htp\r\n                     std::nullopt,      // enable_qnn_saver\r\n                     \"\",                // htp_graph_finalization_opt_mode\r\n                     \"high\");           // qnn_context_priority\r\n}\r\n\r\n// Create a model with Cast + Add (quantized)\r\n// cast_input -> Cast -> Q -> DQ ----\r\n//                                   |\r\n//             input2 -> Q -> DQ -> Add -> Q -> DQ -> output\r\ntemplate <typename InputType, typename QuantType>\r\nstatic GetTestQDQModelFn<QuantType> BuildCastAddQDQTestCase() {\r\n  return [](ModelTestBuilder& builder, std::vector<QuantParams<QuantType>>& output_qparams) {\r\n    // Creat Cast node int32 -> float32\r\n    NodeArg* cast_input = MakeTestInput(builder, TestInputDef<InputType>({2, 3}, false, {0, 1, 0, 1, 0, 1}));\r\n\r\n    auto* cast_output = builder.MakeIntermediate();\r\n    Node& cast_node = builder.AddNode(\"Cast\", {cast_input}, {cast_output});\r\n    cast_node.AddAttribute(\"to\", static_cast<int64_t>(ONNX_NAMESPACE::TensorProto_DataType::TensorProto_DataType_FLOAT));\r\n\r\n    // Create Add node\r\n    std::vector<float> data = {0.0f, 0.0f, 1.0f, 0.0f, 1.0f, 0.0f};\r\n    gsl::span<float> data_range = gsl::make_span(data);\r\n    QuantParams<QuantType> q_parameter = GetDataQuantParams<QuantType>(data_range);\r\n    auto* add_input1_qdq = AddQDQNodePair<QuantType>(builder, cast_output, q_parameter.scale, q_parameter.zero_point);\r\n\r\n    NodeArg* add_input2 = MakeTestInput(builder, TestInputDef<float>({2, 3}, false, data));\r\n    auto* add_input2_qdq = AddQDQNodePair<QuantType>(builder, add_input2, q_parameter.scale, q_parameter.zero_point);\r\n\r\n    auto* add_output = builder.MakeIntermediate();\r\n\r\n    builder.AddNode(\"Add\", {add_input1_qdq, add_input2_qdq}, {add_output});\r\n\r\n    // add_output -> Q -> DQ -> output\r\n    AddQDQNodePairWithOutputAsGraphOutput<QuantType>(builder, add_output, output_qparams[0].scale, output_qparams[0].zero_point);\r\n  };\r\n}\r\n\r\ntemplate <typename InputType>\r\nstatic GetTestModelFn BuildCastAddTestCase() {\r\n  return [](ModelTestBuilder& builder) {\r\n    // Creat Cast node int32 -> float32\r\n    NodeArg* cast_input = MakeTestInput(builder, TestInputDef<InputType>({2, 3}, false, {0, 1, 0, 1, 0, 1}));\r\n\r\n    auto* cast_output = builder.MakeIntermediate();\r\n    Node& cast_node = builder.AddNode(\"Cast\", {cast_input}, {cast_output});\r\n    cast_node.AddAttribute(\"to\", static_cast<int64_t>(ONNX_NAMESPACE::TensorProto_DataType::TensorProto_DataType_FLOAT));\r\n\r\n    // Create Add node\r\n    NodeArg* add_input2 = MakeTestInput(builder, TestInputDef<float>({2, 3}, false, {0.0f, 0.0f, 1.0f, 0.0f, 1.0f, 0.0f}));\r\n    auto* add_output = builder.MakeOutput();\r\n\r\n    builder.AddNode(\"Add\", {cast_output, add_input2}, {add_output});\r\n  };\r\n}\r\n\r\nvoid VerifyFileExistsAndIsNonEmpty(const std::string& filepath) {\r\n  std::ifstream csv_file(filepath, std::ifstream::binary);\r\n  ASSERT_TRUE(csv_file.good());\r\n\r\n  csv_file.seekg(0, csv_file.end);\r\n  size_t buffer_size = static_cast<size_t>(csv_file.tellg());\r\n  EXPECT_NE(0, buffer_size);\r\n}\r\n\r\nTEST_F(QnnHTPBackendTests, ProfilingTest) {\r\n  onnxruntime::ProviderOptions provider_options;\r\n\r\n  provider_options[\"backend_type\"] = \"htp\";\r\n  provider_options[\"offload_graph_io_quantization\"] = \"0\";\r\n  provider_options[\"enable_htp_fp16_precision\"] = \"1\";\r\n  provider_options[\"profiling_level\"] = \"detailed\";\r\n  provider_options[\"profiling_file_path\"] = \"detailed_profile.csv\";\r\n\r\n  auto input_defs = {TestInputDef<float>({1, 2, 2, 2}, false, -10.0f, 10.0f),\r\n                     TestInputDef<float>({1, 2, 2, 2}, false, -10.0f, 10.0f)};\r\n  RunQnnModelTest(BuildOpTestCase<float>(\"Add\", input_defs, {}, {}, kOnnxDomain),\r\n                  provider_options,\r\n                  13,\r\n                  ExpectedEPNodeAssignment::All,\r\n                  0.008f);\r\n\r\n  VerifyFileExistsAndIsNonEmpty(provider_options[\"profiling_file_path\"]);\r\n  std::remove(provider_options[\"profiling_file_path\"].c_str());\r\n\r\n#if QNN_API_VERSION_MAJOR > 2 || \\\r\n    (QNN_API_VERSION_MAJOR == 2 && (QNN_API_VERSION_MINOR >= 29))\r\n  VerifyFileExistsAndIsNonEmpty(\"detailed_profile_qnn.log\");\r\n  std::remove(\"detailed_profile_qnn.log\");\r\n#endif\r\n}\r\n\r\nTEST_F(QnnHTPBackendTests, OptraceTest) {\r\n  onnxruntime::ProviderOptions provider_options;\r\n\r\n  provider_options[\"backend_type\"] = \"htp\";\r\n  provider_options[\"offload_graph_io_quantization\"] = \"0\";\r\n  provider_options[\"enable_htp_fp16_precision\"] = \"1\";\r\n  provider_options[\"profiling_level\"] = \"optrace\";\r\n  provider_options[\"profiling_file_path\"] = \"optrace_profile.csv\";\r\n\r\n  auto input_defs = {TestInputDef<float>({1, 2, 2, 2}, false, -10.0f, 10.0f),\r\n                     TestInputDef<float>({1, 2, 2, 2}, false, -10.0f, 10.0f)};\r\n  RunQnnModelTest(BuildOpTestCase<float>(\"Add\", input_defs, {}, {}, kOnnxDomain),\r\n                  provider_options,\r\n                  13,\r\n                  ExpectedEPNodeAssignment::All,\r\n                  0.008f);\r\n\r\n  VerifyFileExistsAndIsNonEmpty(provider_options[\"profiling_file_path\"]);\r\n  std::remove(provider_options[\"profiling_file_path\"].c_str());\r\n\r\n#if QNN_API_VERSION_MAJOR > 2 || \\\r\n    (QNN_API_VERSION_MAJOR == 2 && (QNN_API_VERSION_MINOR >= 29))\r\n  VerifyFileExistsAndIsNonEmpty(\"optrace_profile_qnn.log\");\r\n  std::remove(\"optrace_profile_qnn.log\");\r\n#endif\r\n}\r\n\r\nTEST_F(QnnHTPBackendTests, CastAddQDQU8) {\r\n  ProviderOptions provider_options;\r\n  provider_options[\"backend_type\"] = \"htp\";\r\n  provider_options[\"offload_graph_io_quantization\"] = \"0\";\r\n\r\n  TestQDQModelAccuracy<uint8_t>(BuildCastAddTestCase<uint8_t>(),\r\n                                BuildCastAddQDQTestCase<uint8_t, uint8_t>(),\r\n                                provider_options,\r\n                                21,\r\n                                ExpectedEPNodeAssignment::All);\r\n}\r\n\r\nTEST_F(QnnHTPBackendTests, CastAddQDQU16) {\r\n  ProviderOptions provider_options;\r\n  provider_options[\"backend_type\"] = \"htp\";\r\n  provider_options[\"offload_graph_io_quantization\"] = \"0\";\r\n\r\n  TestQDQModelAccuracy<uint16_t>(BuildCastAddTestCase<uint8_t>(),\r\n                                 BuildCastAddQDQTestCase<uint8_t, uint16_t>(),\r\n                                 provider_options,\r\n                                 21,\r\n                                 ExpectedEPNodeAssignment::All);\r\n}\r\n\r\nTEST_F(QnnHTPBackendTests, CastAddQDQS8) {\r\n  ProviderOptions provider_options;\r\n  provider_options[\"backend_type\"] = \"htp\";\r\n  provider_options[\"offload_graph_io_quantization\"] = \"0\";\r\n\r\n  TestQDQModelAccuracy<int8_t>(BuildCastAddTestCase<uint8_t>(),\r\n                               BuildCastAddQDQTestCase<uint8_t, int8_t>(),\r\n                               provider_options,\r\n                               21,\r\n                               ExpectedEPNodeAssignment::All);\r\n}\r\n\r\nTEST_F(QnnHTPBackendTests, CastAddQDQS16) {\r\n  ProviderOptions provider_options;\r\n  provider_options[\"backend_type\"] = \"htp\";\r\n  provider_options[\"offload_graph_io_quantization\"] = \"0\";\r\n\r\n  TestQDQModelAccuracy<int16_t>(BuildCastAddTestCase<uint8_t>(),\r\n                                BuildCastAddQDQTestCase<uint8_t, int16_t>(),\r\n                                provider_options,\r\n                                21,\r\n                                // QNN has not yet supported S16 Quantize/Dequantize\r\n                                ExpectedEPNodeAssignment::Some);\r\n}\r\n\r\n// Test float32 model with FP16 precision\r\nTEST_F(QnnHTPBackendTests, Float32ModelWithFP16PrecisionTest) {\r\n  ProviderOptions provider_options;\r\n#if defined(_WIN32)\r\n  provider_options[\"backend_path\"] = \"QnnHtp.dll\";\r\n#else\r\n  provider_options[\"backend_path\"] = \"libQnnHtp.so\";\r\n#endif\r\n  provider_options[\"enable_htp_fp16_precision\"] = \"1\";\r\n\r\n  auto input_defs = {TestInputDef<float>({1, 2, 2, 2}, false, -10.0f, 10.0f),\r\n                     TestInputDef<float>({1, 2, 2, 2}, false, -10.0f, 10.0f)};\r\n  RunQnnModelTest(BuildOpTestCase<float>(\"Add\", input_defs, {}, {}, kOnnxDomain),\r\n                  provider_options,\r\n                  13,\r\n                  ExpectedEPNodeAssignment::All,\r\n                  0.008f);\r\n}\r\n\r\n// Test that QNN EP only handles nodes with static shapes and rejects nodes with dynamic shape I/O.\r\nTEST_F(QnnHTPBackendTests, EPRejectsDynamicShapesF32) {\r\n  // Local function that builds a model in which the last two nodes use dynamic shapes.\r\n  auto model_build_fn = [](ModelTestBuilder& builder) {\r\n    NodeArg* input1 = builder.MakeInput<float>(std::vector<int64_t>{1, 2, 8, 8},\r\n                                               GetFloatDataInRange(0.0f, 1.0f, 128));\r\n    NodeArg* input2 = builder.MakeInput<int64_t>(std::vector<int64_t>{3}, std::vector<int64_t>{1, 2, 49});\r\n\r\n    // Add a Conv with known shapes. QNN EP should support it.\r\n    NodeArg* weight = builder.MakeInitializer<float>(std::vector<int64_t>{2, 2, 2, 2},\r\n                                                     GetFloatDataInRange(-0.3f, 0.3f, 16));\r\n    NodeArg* bias = builder.MakeInitializer<float>(std::vector<int64_t>{2}, {0.0f, 1.0f});\r\n\r\n    auto* conv_output = builder.MakeIntermediate();\r\n    builder.AddNode(\"Conv\", {input1, weight, bias}, {conv_output});\r\n\r\n    // Add a Reshape to a dynamic shape. QNN EP should reject this node.\r\n    auto* reshape_output = builder.MakeIntermediate();\r\n    builder.AddNode(\"Reshape\", {conv_output, input2}, {reshape_output});\r\n\r\n    // Add a Softmax. QNN EP should reject this node because its input has a dynamic shape.\r\n    NodeArg* output = builder.MakeOutput();\r\n    builder.AddNode(\"Softmax\", {reshape_output}, {output});\r\n  };\r\n\r\n  // Local function that checks that the nodes with dynamic shape I/O were assigned to CPU EP.\r\n  std::function<void(const Graph&)> ep_graph_checker = [](const Graph& graph) {\r\n    for (const Node& node : graph.Nodes()) {\r\n      const std::string& ep_name = node.GetExecutionProviderType();\r\n      const std::string& op_type = node.OpType();\r\n      if (op_type == \"Reshape\" || op_type == \"Softmax\") {\r\n        EXPECT_EQ(ep_name, kCpuExecutionProvider);\r\n      } else {\r\n        EXPECT_EQ(ep_name, kQnnExecutionProvider);\r\n      }\r\n    }\r\n  };\r\n\r\n  ProviderOptions provider_options;\r\n#if defined(_WIN32)\r\n  provider_options[\"backend_path\"] = \"QnnHtp.dll\";\r\n#else\r\n  provider_options[\"backend_path\"] = \"libQnnHtp.so\";\r\n#endif\r\n  provider_options[\"offload_graph_io_quantization\"] = \"0\";\r\n  provider_options[\"enable_htp_fp16_precision\"] = \"1\";  // QNN EP will use fp16 precision.\r\n                                                        // CPU EP will use fp32, so we can relax accuracy requirements.\r\n\r\n  RunQnnModelTest(model_build_fn,\r\n                  provider_options,\r\n                  /*opset*/ 19,\r\n                  ExpectedEPNodeAssignment::Some,\r\n                  /*abs_err*/ 1e-4f,\r\n                  logging::Severity::kERROR,\r\n                  /*verify_output*/ true,\r\n                  &ep_graph_checker);\r\n}\r\n\r\nTEST_F(QnnHTPBackendTests, DumpJsonQNNGraph) {\r\n  const ORTCHAR_T* ort_model_path = ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.quant.onnx\";\r\n  Ort::SessionOptions so;\r\n  onnxruntime::ProviderOptions options;\r\n#if defined(_WIN32)\r\n  options[\"backend_path\"] = \"QnnHtp.dll\";\r\n#else\r\n  options[\"backend_path\"] = \"libQnnHtp.so\";\r\n#endif\r\n  options[\"offload_graph_io_quantization\"] = \"0\";\r\n\r\n  const std::filesystem::path dump_dir = \"test_qnn_graphs_\";\r\n  options[\"json_qnn_graph_dir\"] = dump_dir.string();\r\n  options[\"dump_json_qnn_graph\"] = \"1\";\r\n\r\n  // Remove pre-existing json files. Note that fs::remove_all() can handle non-existing paths.\r\n  std::filesystem::remove_all(dump_dir);\r\n  ASSERT_TRUE(std::filesystem::create_directory(dump_dir));\r\n\r\n  so.AppendExecutionProvider(\"QNN\", options);\r\n  Ort::Session session(*ort_env, ort_model_path, so);\r\n\r\n  // Check that QNN JSON file(s) exist.\r\n  bool has_a_json_file = false;\r\n  for (auto const& dir_entry : std::filesystem::directory_iterator{dump_dir}) {\r\n    EXPECT_TRUE(dir_entry.is_regular_file());\r\n    EXPECT_EQ(dir_entry.path().extension().string(), \".json\");\r\n    has_a_json_file = true;\r\n  }\r\n  EXPECT_TRUE(has_a_json_file);\r\n\r\n  // Cleaup generated files.\r\n  // Comment the following line to inspect generated JSON files.\r\n  std::filesystem::remove_all(dump_dir);\r\n}\r\n\r\n// Test option for offloading quantization of graph inputs and dequantization of graph outputs to the CPU EP.\r\nTEST_F(QnnHTPBackendTests, EPOffloadsGraphIOQuantDequant) {\r\n  // Returns a function that checks that the Q/DQ ops at the graph IO boundary are offloaded to CPU\r\n  // if the corresponding provider option is enabled.\r\n  auto graph_checker_builder = [](bool offload_graph_io_quantization) -> std::function<void(const Graph&)> {\r\n    return [offload_graph_io_quantization](const Graph& graph) {\r\n      size_t num_q = 0;\r\n      size_t num_dq = 0;\r\n      size_t num_qnn_fused_node = 0;\r\n\r\n      for (const Node& node : graph.Nodes()) {\r\n        const std::string& ep_name = node.GetExecutionProviderType();\r\n        const std::string& op_type = node.OpType();\r\n\r\n        if (offload_graph_io_quantization && op_type == \"QuantizeLinear\") {\r\n          const bool consumes_graph_input = graph.IsInputsIncludingInitializers(node.InputDefs()[0]);\r\n          EXPECT_EQ(ep_name, kCpuExecutionProvider);\r\n          EXPECT_TRUE(consumes_graph_input);\r\n          num_q += 1;\r\n        } else if (offload_graph_io_quantization && op_type == \"DequantizeLinear\") {\r\n          const bool produces_graph_output = graph.IsOutput(node.OutputDefs()[0]);\r\n          EXPECT_EQ(ep_name, kCpuExecutionProvider);\r\n          EXPECT_TRUE(produces_graph_output);\r\n          num_dq += 1;\r\n        } else {\r\n          EXPECT_EQ(ep_name, kQnnExecutionProvider);\r\n          num_qnn_fused_node += 1;\r\n        }\r\n      }\r\n\r\n      EXPECT_EQ(num_q, static_cast<size_t>(offload_graph_io_quantization));\r\n      EXPECT_EQ(num_dq, static_cast<size_t>(offload_graph_io_quantization));\r\n      EXPECT_EQ(num_qnn_fused_node, 1);\r\n    };\r\n  };\r\n\r\n  ProviderOptions provider_options;\r\n#if defined(_WIN32)\r\n  provider_options[\"backend_path\"] = \"QnnHtp.dll\";\r\n#else\r\n  provider_options[\"backend_path\"] = \"libQnnHtp.so\";\r\n#endif\r\n  const std::vector<std::string> op_types = {\r\n      \"Sigmoid\",\r\n      \"Transpose\",\r\n      \"Softmax\",\r\n      \"Sqrt\",\r\n      \"Elu\",\r\n  };\r\n\r\n  // Test various QDQ ops with offloading of I/O quantization enabled and disabled.\r\n  for (auto op_type : op_types) {\r\n    for (int offload_io_quant = 0; offload_io_quant <= 1; offload_io_quant++) {\r\n      provider_options[\"offload_graph_io_quantization\"] = offload_io_quant ? \"1\" : \"0\";\r\n      auto graph_checker = graph_checker_builder(offload_io_quant);\r\n      auto expected_ep_assignment = offload_io_quant ? ExpectedEPNodeAssignment::Some : ExpectedEPNodeAssignment::All;\r\n\r\n      float min_val = (op_type == \"Sqrt\") ? 0.0f : -10.0f;\r\n      TestInputDef<float> input_def({1, 2, 2, 2}, false, GetFloatDataInRange(min_val, 10.0f, 8));\r\n      auto f32_model_build_fn = BuildOpTestCase<float>(op_type, {input_def}, {}, {});\r\n      auto qdq_model_build_fn = BuildQDQOpTestCase<uint8_t>(op_type, {input_def}, {}, {});\r\n      TestQDQModelAccuracy<uint8_t>(f32_model_build_fn,\r\n                                    qdq_model_build_fn,\r\n                                    provider_options,\r\n                                    /*opset*/ 21,\r\n                                    expected_ep_assignment,\r\n                                    /*abs_err*/ QDQTolerance(),\r\n                                    logging::Severity::kERROR,\r\n                                    /*qnn_ctx_model_path*/ \"\",\r\n                                    /*session_option_pairs*/ {},\r\n                                    &graph_checker);\r\n    }\r\n  }\r\n}\r\n\r\n// Only compile this test when QNN EP is built as a static library. When QNN EP is a shared library,\r\n// we cannot include internal QNN EP headers that use the provider-bridge API.\r\n#if BUILD_QNN_EP_STATIC_LIB\r\nTEST_F(QnnHTPBackendTests, UseHtpSharedMemoryAllocatorForInputs) {\r\n  ProviderOptions provider_options;\r\n#if defined(_WIN32)\r\n  provider_options[\"backend_path\"] = \"QnnHtp.dll\";\r\n#else\r\n  provider_options[\"backend_path\"] = \"libQnnHtp.so\";\r\n#endif\r\n  provider_options[\"offload_graph_io_quantization\"] = \"0\";\r\n  provider_options[\"enable_htp_shared_memory_allocator\"] = \"1\";\r\n\r\n  std::unique_ptr<IExecutionProvider> qnn_ep;\r\n  try {\r\n    qnn_ep = QnnExecutionProviderWithOptions(provider_options);\r\n  } catch (const OnnxRuntimeException& e) {\r\n    // handle exception that indicates that the libcdsprpc.so / dll can't be loaded\r\n    constexpr const char* expected_error_message = \"Failed to initialize RPCMEM dynamic library handle\";\r\n    ASSERT_THAT(e.what(), testing::HasSubstr(expected_error_message));\r\n    GTEST_SKIP() << \"HTP shared memory allocator is unavailable.\";\r\n  }\r\n\r\n  AllocatorPtr htp_shared_memory_allocator{};\r\n  {\r\n    auto allocators = qnn_ep->CreatePreferredAllocators();\r\n    ASSERT_FALSE(allocators.empty());\r\n    auto& allocator = allocators[0];\r\n    ASSERT_EQ(allocator->Info(), qnn::HtpSharedMemoryAllocator::AssociatedMemoryInfo());\r\n    htp_shared_memory_allocator = std::move(allocator);\r\n  }\r\n\r\n  auto input_defs = {TestInputDef<float>({1, 2, 2, 2}, false, -10.0f, 10.0f),\r\n                     TestInputDef<float>({1, 2, 2, 2}, false, -10.0f, 10.0f)};\r\n  RunQnnModelTest(BuildOpTestCase<float>(\"Add\", input_defs, {}, {}, kOnnxDomain, htp_shared_memory_allocator),\r\n                  provider_options,\r\n                  13,\r\n                  ExpectedEPNodeAssignment::All,\r\n                  0.008f);\r\n}\r\n#endif  // BUILD_QNN_EP_STATIC_LIB\r\n\r\n#if !BUILD_QNN_EP_STATIC_LIB\r\n// Tests that loading and unloading of an EP library in the same process does not cause a segfault.\r\nTEST_F(QnnHTPBackendTests, LoadingAndUnloadingOfQnnLibrary_FixSegFault) {\r\n  const ORTCHAR_T* ort_model_path = ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.quant.onnx\";\r\n\r\n  onnxruntime::ProviderOptions options;\r\n  options[\"backend_type\"] = \"htp\";\r\n  options[\"offload_graph_io_quantization\"] = \"0\";\r\n\r\n  // This first session will load the QNN EP library for the first time.\r\n  {\r\n    Ort::SessionOptions so;\r\n    so.AppendExecutionProvider(\"QNN\", options);\r\n\r\n    EXPECT_NO_THROW(Ort::Session session(*ort_env, ort_model_path, so));\r\n  }\r\n\r\n  {\r\n    ortenv_teardown();  // Destroy Env to force unloading of EP libraries.\r\n    ortenv_setup();\r\n\r\n    // This next session will reload the QNN EP library.\r\n    // Should not get a segfault.\r\n    Ort::SessionOptions so;\r\n    so.AppendExecutionProvider(\"QNN\", options);\r\n\r\n    EXPECT_NO_THROW(Ort::Session session(*ort_env, ort_model_path, so));\r\n  }\r\n}\r\n#endif  // !BUILD_QNN_EP_STATIC_LIB\r\n\r\n#if defined(WIN32) && !BUILD_QNN_EP_STATIC_LIB\r\n// Tests autoEP feature to automatically select an EP that supports the NPU.\r\n// Currently only works on Windows.\r\nTEST_F(QnnHTPBackendTests, AutoEp_PreferNpu) {\r\n  ASSERT_ORTSTATUS_OK(Ort::GetApi().RegisterExecutionProviderLibrary(*ort_env, kQnnExecutionProvider,\r\n                                                                     ORT_TSTR(\"onnxruntime_providers_qnn.dll\")));\r\n\r\n  Ort::SessionOptions so;\r\n  so.SetEpSelectionPolicy(OrtExecutionProviderDevicePolicy_PREFER_NPU);\r\n\r\n  const ORTCHAR_T* ort_model_path = ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.quant.onnx\";\r\n  Ort::Session session(*ort_env, ort_model_path, so);\r\n  EXPECT_TRUE(SessionHasEp(session, kQnnExecutionProvider));\r\n\r\n  ASSERT_ORTSTATUS_OK(Ort::GetApi().UnregisterExecutionProviderLibrary(*ort_env, kQnnExecutionProvider));\r\n}\r\n\r\nTEST_F(QnnGPUBackendTests, AutoEp_PreferGpu) {\r\n  ASSERT_ORTSTATUS_OK(Ort::GetApi().RegisterExecutionProviderLibrary(*ort_env, kQnnExecutionProvider,\r\n                                                                     ORT_TSTR(\"onnxruntime_providers_qnn.dll\")));\r\n\r\n  Ort::SessionOptions so;\r\n  so.SetEpSelectionPolicy(OrtExecutionProviderDevicePolicy_PREFER_GPU);\r\n\r\n  const ORTCHAR_T* ort_model_path = ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.onnx\";\r\n  Ort::Session session(*ort_env, ort_model_path, so);\r\n  EXPECT_TRUE(SessionHasEp(session, kQnnExecutionProvider));\r\n\r\n  ASSERT_ORTSTATUS_OK(Ort::GetApi().UnregisterExecutionProviderLibrary(*ort_env, kQnnExecutionProvider));\r\n}\r\n#endif  // defined(WIN32) && !BUILD_QNN_EP_STATIC_LIB\r\n\r\n// Test whether QNN EP can handle the case where the number of graph inputs and\r\n// the number of tensor wrappers do not match.\r\n// Take Resize op as an example.\r\n// - Qnn only cares about the 1st input, so the rest of the inputs are not converted\r\n//   to tensor wrappers.\r\n// - However, these remaining inputs still appear in the graph inputs,\r\n//   resulting in a discrepancy in the input quantities.\r\nTEST_F(QnnHTPBackendTests, TestMismatchedGraphInputAndTensorWrapperCount) {\r\n  onnxruntime::ProviderOptions provider_options;\r\n  provider_options[\"backend_type\"] = \"htp\";\r\n\r\n  auto input_defs = {TestInputDef<float>({1, 3, 10, 10}, false, -10.0f, 10.0f),\r\n                     TestInputDef<float>({0}, false, {}),\r\n                     TestInputDef<float>({4}, true, {1.0f, 1.0f, 2.0f, 2.0f})};\r\n  auto attrs = {utils::MakeAttribute(\"mode\", \"nearest\"),\r\n                utils::MakeAttribute(\"coordinate_transformation_mode\", \"asymmetric\"),\r\n                utils::MakeAttribute(\"nearest_mode\", \"floor\")};\r\n  RunQnnModelTest(BuildOpTestCase<float>(\"Resize\",\r\n                                         input_defs,\r\n                                         {},\r\n                                         attrs,\r\n                                         kOnnxDomain),\r\n                  provider_options,\r\n                  11,\r\n                  ExpectedEPNodeAssignment::All,\r\n                  0.008f);\r\n}\r\n\r\n#endif  // defined(__aarch64__) || defined(_M_ARM64) || defined(__linux__)\r\n\r\n// Test that QNN Ir generates the expected files for a model meant to run on any QNN backend.\r\nTEST_F(QnnIRBackendTests, QnnIr_OutputFiles) {\r\n  const std::filesystem::path qnn_dlc_dir = kDlcOutputDir;\r\n\r\n  // Remove pre-existing QNN Ir output files. Note that fs::remove_all() can handle non-existing paths.\r\n  std::filesystem::remove_all(qnn_dlc_dir);\r\n  ASSERT_FALSE(std::filesystem::exists(qnn_dlc_dir));\r\n\r\n  InitNHWCResizeModel(ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.onnx\",\r\n                      TestBackend::Ir,   // backend\r\n                      TestBackend::Ir);  // serializer backend\r\n\r\n  // File names are taken from graph node names. Just make sure that we got one .dlc\r\n  // in the expected directory.\r\n  ASSERT_TRUE(std::filesystem::exists(qnn_dlc_dir));\r\n\r\n  int file_count = 0;\r\n  for (const auto& entry : std::filesystem::directory_iterator(qnn_dlc_dir)) {\r\n    EXPECT_TRUE(entry.is_regular_file());\r\n    EXPECT_EQ(entry.path().extension(), \".dlc\");\r\n    ++file_count;\r\n  }\r\n  EXPECT_EQ(file_count, 1);\r\n}\r\n\r\n// Test that QNN Saver generates the expected files for a model meant to run on any QNN backend.\r\nTEST(QnnSaverBackendTests, QnnSaver_OutputFiles) {\r\n  const std::filesystem::path qnn_saver_output_dir = \"saver_output\";\r\n\r\n  // Remove pre-existing QNN Saver output files. Note that fs::remove_all() can handle non-existing paths.\r\n  std::filesystem::remove_all(qnn_saver_output_dir);\r\n  ASSERT_FALSE(std::filesystem::exists(qnn_saver_output_dir));\r\n\r\n  InitNHWCResizeModel(ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.onnx\",\r\n                      TestBackend::Saver,   // backend\r\n                      TestBackend::Saver);  // serializer_backend\r\n\r\n  // Check that QNN Saver output files exist.\r\n  EXPECT_TRUE(std::filesystem::exists(qnn_saver_output_dir / \"saver_output.c\"));\r\n  EXPECT_TRUE(std::filesystem::exists(qnn_saver_output_dir / \"params.bin\"));\r\n}\r\n\r\n#endif  // !defined(ORT_MINIMAL_BUILD)\r\n\r\n}  // namespace test\r\n}  // namespace onnxruntime\r\n","replacement":"// Copyright (c) Microsoft Corporation. All rights reserved.\r\n// Licensed under the MIT License.\r\n\r\n#include <filesystem>\r\n#include <string>\r\n#include <thread>\r\n\r\n#include \"core/graph/constants.h\"\r\n#include \"core/graph/node_attr_utils.h\"\r\n#include \"core/providers/cpu/cpu_provider_factory.h\"  // For OrtSessionOptionsAppendExecutionProvider_CPU\r\n#if BUILD_QNN_EP_STATIC_LIB\r\n#include \"core/providers/qnn/qnn_allocator.h\"  // Used by QnnHTPBackendTests.UseHtpSharedMemoryAllocatorForInputs\r\n#endif\r\n#include \"core/session/inference_session.h\"\r\n#include \"core/session/onnxruntime_cxx_api.h\"\r\n#include \"core/session/onnxruntime_session_options_config_keys.h\"\r\n#include \"core/session/onnxruntime_run_options_config_keys.h\"\r\n\r\n#include \"test/providers/qnn/qnn_test_utils.h\"\r\n#include \"test/util/include/api_asserts.h\"\r\n\r\n#include \"gtest/gtest.h\"\r\n#include \"gmock/gmock.h\"\r\n\r\nusing namespace ONNX_NAMESPACE;\r\nusing namespace onnxruntime::logging;\r\n\r\n#define ORT_MODEL_FOLDER ORT_TSTR(\"testdata/\")\r\n\r\nconstexpr std::string_view kDlcOutputDir(\"dlc_output\");\r\n\r\n// in test_main.cc\r\nextern std::unique_ptr<Ort::Env> ort_env;\r\nextern \"C\" void ortenv_setup();\r\nextern \"C\" void ortenv_teardown();\r\n\r\nnamespace onnxruntime {\r\nnamespace test {\r\n\r\n// test uses ONNX model so can't be run in a minimal build.\r\n// TODO: When we need QNN in a minimal build we should add an ORT format version of the model\r\n#if !defined(ORT_MINIMAL_BUILD)\r\n\r\nstatic bool SessionHasEp(Ort::Session& session, const char* ep_name) {\r\n  // Access the underlying InferenceSession.\r\n  const OrtSession* ort_session = session;\r\n  const InferenceSession* s = reinterpret_cast<const InferenceSession*>(ort_session);\r\n  bool has_ep = false;\r\n\r\n  for (const auto& provider : s->GetRegisteredProviderTypes()) {\r\n    if (provider == ep_name) {\r\n      has_ep = true;\r\n      break;\r\n    }\r\n  }\r\n  return has_ep;\r\n}\r\n\r\n// Tests that the QNN EP is registered when added via the public C++ API.\r\n// Loads a simple ONNX model that adds floats.\r\nTEST_F(QnnHTPBackendTests, TestAddEpUsingPublicApi) {\r\n  onnxruntime::ProviderOptions options;\r\n#if defined(_WIN32)\r\n  options[\"backend_path\"] = \"QnnHtp.dll\";\r\n#else\r\n  options[\"backend_path\"] = \"libQnnHtp.so\";\r\n#endif\r\n\r\n  const ORTCHAR_T* ort_model_path = ORT_MODEL_FOLDER \"constant_floats.onnx\";\r\n\r\n  {\r\n    // Test C++ API to add QNN EP with the short name 'QNN'.\r\n    Ort::SessionOptions so;\r\n\r\n    // Can only enforce that model runs on QNN in linux CI machines\r\n    // because they support the CPU backend and emulate the HTP backend.\r\n    // TODO: Remove #ifdef when Windows Arm64 machines support the CPU backend.\r\n#if defined(__linux__)\r\n    so.AddConfigEntry(kOrtSessionOptionsDisableCPUEPFallback, \"1\");  // Disable fallback to the CPU EP.\r\n#endif\r\n    so.AppendExecutionProvider(\"QNN\", options);\r\n\r\n    Ort::Session session(*ort_env, ort_model_path, so);\r\n    ASSERT_TRUE(SessionHasEp(session, kQnnExecutionProvider))\r\n        << \"QNN EP was not found in registered providers for session \"\r\n        << \"providers for session when added to session with name 'QNN'\";\r\n  }\r\n\r\n  {\r\n    // Test C++ API to add QNN EP with the long canonical name 'QNNExecutionProvider'.\r\n    Ort::SessionOptions so;\r\n\r\n    // TODO: Remove #ifdef when Windows Arm64 machines support the CPU backend.\r\n#if defined(__linux__)\r\n    so.AddConfigEntry(kOrtSessionOptionsDisableCPUEPFallback, \"1\");  // Disable fallback to the CPU EP.\r\n#endif\r\n    so.AppendExecutionProvider(kQnnExecutionProvider, options);\r\n\r\n    Ort::Session session(*ort_env, ort_model_path, so);\r\n    ASSERT_TRUE(SessionHasEp(session, kQnnExecutionProvider))\r\n        << \"QNN EP was not found in registered providers for session \"\r\n        << \"when added to session with name '\" << kQnnExecutionProvider << \"'\";\r\n  }\r\n}\r\n\r\n// Tests the `session.disable_cpu_ep_fallback` configuration option when the backend cannot be loaded.\r\n// When the option is enabled, session creation throws an exception because the backend cannot be found.\r\nTEST(QnnEP, TestDisableCPUFallback_BackendNotFound) {\r\n  {\r\n    Ort::SessionOptions so;\r\n    so.AddConfigEntry(kOrtSessionOptionsDisableCPUEPFallback, \"1\");  // Disable fallback to the CPU EP.\r\n\r\n    onnxruntime::ProviderOptions options;\r\n#if defined(_WIN32)\r\n    options[\"backend_path\"] = \"DoesNotExist.dll\";  // Invalid backend path!\r\n#else\r\n    options[\"backend_path\"] = \"libDoesNotExist.so\";  // Invalid backend path!\r\n#endif\r\n\r\n    so.AppendExecutionProvider(\"QNN\", options);\r\n\r\n    const ORTCHAR_T* ort_model_path = ORT_MODEL_FOLDER \"constant_floats.onnx\";\r\n\r\n    try {\r\n      Ort::Session session(*ort_env, ort_model_path, so);\r\n      FAIL();  // Should not get here!\r\n    } catch (const Ort::Exception& excpt) {\r\n      ASSERT_EQ(excpt.GetOrtErrorCode(), ORT_FAIL);\r\n      ASSERT_THAT(excpt.what(), testing::HasSubstr(\"This session contains graph nodes that are assigned to the default \"\r\n                                                   \"CPU EP, but fallback to CPU EP has been explicitly disabled by \"\r\n                                                   \"the user.\"));\r\n    }\r\n  }\r\n}\r\n\r\n// Tests the `session.disable_cpu_ep_fallback` configuration option when the entire model cannot be assigned to QNN EP.\r\n// When the option is enabled, Session creation should throw an exception.\r\nTEST(QnnEP, TestDisableCPUFallback_ModelNotFullySupported) {\r\n  {\r\n    Ort::SessionOptions so;\r\n    so.AddConfigEntry(kOrtSessionOptionsDisableCPUEPFallback, \"1\");  // Disable fallback to the CPU EP.\r\n\r\n    onnxruntime::ProviderOptions options;\r\n#if defined(_WIN32)\r\n    options[\"backend_path\"] = \"QnnCpu.dll\";\r\n#else\r\n    options[\"backend_path\"] = \"libQnnCpu.so\";\r\n#endif\r\n    options[\"offload_graph_io_quantization\"] = \"0\";\r\n\r\n    so.AppendExecutionProvider(\"QNN\", options);\r\n\r\n    // QNN EP doesn't support MatMulInteger.\r\n    const ORTCHAR_T* ort_model_path = ORT_MODEL_FOLDER \"qnn_ep_partial_support.onnx\";\r\n\r\n    try {\r\n      Ort::Session session(*ort_env, ort_model_path, so);\r\n      FAIL();  // Should not get here!\r\n    } catch (const Ort::Exception& excpt) {\r\n      ASSERT_EQ(excpt.GetOrtErrorCode(), ORT_FAIL);\r\n      ASSERT_THAT(excpt.what(), testing::HasSubstr(\"This session contains graph nodes that are assigned to the default \"\r\n                                                   \"CPU EP, but fallback to CPU EP has been explicitly disabled by \"\r\n                                                   \"the user.\"));\r\n    }\r\n  }\r\n}\r\n\r\n// The model is supported on QNN CPU backend, but CPU fallback is disabled\r\n// QNN EP report error for this scenario also\r\nTEST(QnnEP, TestDisableCPUFallback_TryingToRunOnQnnCPU) {\r\n  SessionOptions so;\r\n  // Disable fallback to the CPU EP.\r\n  ASSERT_STATUS_OK(so.config_options.AddConfigEntry(kOrtSessionOptionsDisableCPUEPFallback, \"1\"));\r\n\r\n  onnxruntime::ProviderOptions options;\r\n#if defined(_WIN32)\r\n  options[\"backend_path\"] = \"QnnCpu.dll\";\r\n#else\r\n  options[\"backend_path\"] = \"libQnnCpu.so\";\r\n#endif\r\n  options[\"offload_graph_io_quantization\"] = \"0\";\r\n\r\n  auto input_defs = {TestInputDef<float>({1, 2, 2, 2}, false, -10.0f, 10.0f),\r\n                     TestInputDef<float>({1, 2, 2, 2}, false, -10.0f, 10.0f)};\r\n  auto model_func = BuildOpTestCase<float>(\"Add\", input_defs, {}, {}, kOnnxDomain);\r\n\r\n  const std::unordered_map<std::string, int> domain_to_version = {{\"\", 13}, {kMSDomain, 1}};\r\n\r\n  auto& logging_manager = DefaultLoggingManager();\r\n  // logging_manager.SetDefaultLoggerSeverity(log_severity);\r\n\r\n  onnxruntime::Model model(\"QNN_EP_TestModel\", false, ModelMetaData(), PathString(),\r\n                           IOnnxRuntimeOpSchemaRegistryList(), domain_to_version, {},\r\n                           logging_manager.DefaultLogger());\r\n  Graph& graph = model.MainGraph();\r\n  ModelTestBuilder helper(graph);\r\n  model_func(helper);\r\n  helper.SetGraphOutputs();\r\n  ASSERT_STATUS_OK(model.MainGraph().Resolve());\r\n\r\n  // Serialize the model to a string.\r\n  std::string model_data;\r\n  model.ToProto().SerializeToString(&model_data);\r\n\r\n  InferenceSession session_object{so, GetEnvironment()};\r\n  auto qnn_ep = QnnExecutionProviderWithOptions(options, &so);\r\n  EXPECT_TRUE(session_object.RegisterExecutionProvider(std::move(qnn_ep)).IsOK());\r\n\r\n  ASSERT_STATUS_OK(session_object.Load(model_data.data(), static_cast<int>(model_data.size())));\r\n  auto status = session_object.Initialize();\r\n  ASSERT_EQ(status.Code(), ORT_FAIL);\r\n  ASSERT_THAT(status.ErrorMessage().c_str(), testing::HasSubstr(\"This session contains graph nodes that are assigned to the default \"\r\n                                                                \"CPU EP, but fallback to CPU EP has been explicitly disabled by \"\r\n                                                                \"the user.\"));\r\n}\r\n\r\n// Tests invalid use of the `session.disable_cpu_ep_fallback` configuration option.\r\n// It is invalid to set the option and explicitly add the CPU EP to the session.\r\nTEST(QnnEP, TestDisableCPUFallback_ConflictingConfig) {\r\n  {\r\n    Ort::SessionOptions so;\r\n    so.AddConfigEntry(kOrtSessionOptionsDisableCPUEPFallback, \"1\");  // Disable fallback to the CPU EP.\r\n\r\n    onnxruntime::ProviderOptions options;\r\n#if defined(_WIN32)\r\n    options[\"backend_path\"] = \"QnnCpu.dll\";\r\n#else\r\n    options[\"backend_path\"] = \"libQnnCpu.so\";\r\n#endif\r\n    options[\"offload_graph_io_quantization\"] = \"0\";\r\n\r\n    so.AppendExecutionProvider(\"QNN\", options);\r\n\r\n    // Invalid! Adds CPU EP to session, but also disables CPU fallback.\r\n    so.AppendExecutionProvider_CPU(1);\r\n\r\n    const ORTCHAR_T* ort_model_path = ORT_MODEL_FOLDER \"constant_floats.onnx\";\r\n\r\n    try {\r\n      Ort::Session session(*ort_env, ort_model_path, so);\r\n      FAIL();  // Should not get here!\r\n    } catch (const Ort::Exception& excpt) {\r\n      ASSERT_EQ(excpt.GetOrtErrorCode(), ORT_INVALID_ARGUMENT);\r\n      ASSERT_THAT(excpt.what(), testing::HasSubstr(\"Conflicting session configuration: explicitly added the CPU EP to the \"\r\n                                                   \"session, but also disabled fallback to the CPU EP via session \"\r\n                                                   \"configuration options.\"));\r\n    }\r\n  }\r\n}\r\n\r\nTEST(QnnEP, TestInvalidSpecificationOfBothBackendTypeAndBackendPath) {\r\n  onnxruntime::ProviderOptions provider_options{};\r\n  provider_options[\"backend_type\"] = \"cpu\";\r\n#if defined(_WIN32)\r\n  provider_options[\"backend_path\"] = \"QnnCpu.dll\";\r\n#else\r\n  provider_options[\"backend_path\"] = \"libQnnCpu.so\";\r\n#endif\r\n\r\n  Ort::SessionOptions so{};\r\n  so.AppendExecutionProvider(\"QNN\", provider_options);\r\n\r\n  const ORTCHAR_T* ort_model_path = ORT_MODEL_FOLDER \"constant_floats.onnx\";\r\n\r\n  try {\r\n    Ort::Session session(*ort_env, ort_model_path, so);\r\n    FAIL();\r\n  } catch (const Ort::Exception& e) {\r\n    ASSERT_EQ(e.GetOrtErrorCode(), ORT_FAIL);\r\n    ASSERT_THAT(e.what(), testing::HasSubstr(\"Only one of 'backend_type' and 'backend_path' should be set.\"));\r\n  }\r\n}\r\n\r\n// Conv node `Conv` is not supported: GetFileLength for conv_qdq_external_ini.bin failed:open file conv_qdq_external_ini.bin fail,\r\n// errcode = 2 - The system cannot find the file specified.\r\nTEST_F(QnnHTPBackendTests, TestConvWithExternalData) {\r\n  Ort::SessionOptions so;\r\n  onnxruntime::ProviderOptions options;\r\n#if defined(_WIN32)\r\n  options[\"backend_path\"] = \"QnnHtp.dll\";\r\n#else\r\n  options[\"backend_path\"] = \"libQnnHtp.so\";\r\n#endif\r\n  options[\"offload_graph_io_quantization\"] = \"0\";\r\n\r\n  so.AppendExecutionProvider(\"QNN\", options);\r\n\r\n  so.AppendExecutionProvider_CPU(1);\r\n\r\n  const ORTCHAR_T* ort_model_path = ORT_MODEL_FOLDER \"conv_qdq_external_ini.onnx\";\r\n\r\n  Ort::Session session(*ort_env, ort_model_path, so);\r\n}\r\n\r\n#if defined(__aarch64__) || defined(_M_ARM64) || defined(__linux__)\r\nTEST_F(QnnHTPBackendTests, RunConvInt4Model) {\r\n  Ort::SessionOptions so;\r\n\r\n  so.AddConfigEntry(kOrtSessionOptionsDisableCPUEPFallback, \"1\");  // Disable fallback to the CPU EP.\r\n  so.SetGraphOptimizationLevel(ORT_ENABLE_ALL);\r\n  onnxruntime::ProviderOptions options;\r\n\r\n#if defined(_WIN32)\r\n  options[\"backend_path\"] = \"QnnHtp.dll\";\r\n#else\r\n  options[\"backend_path\"] = \"libQnnHtp.so\";\r\n#endif\r\n\r\n  so.AppendExecutionProvider(\"QNN\", options);\r\n\r\n  const ORTCHAR_T* ort_model_path = ORT_MODEL_FOLDER \"conv.int4_weights.qdq.onnx\";\r\n  Ort::Session session(*ort_env, ort_model_path, so);\r\n\r\n  TensorShape input_shape = {1, 3, 8, 8};\r\n  std::vector<float> input0_data(input_shape.Size(), 0.2f);\r\n\r\n  auto memory_info = Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeCPU);\r\n  std::vector<Ort::Value> ort_inputs;\r\n  std::vector<const char*> ort_input_names;\r\n\r\n  // Add input0\r\n  ort_inputs.emplace_back(Ort::Value::CreateTensor<float>(\r\n      memory_info, input0_data.data(), input0_data.size(), &input_shape[0], input_shape.NumDimensions()));\r\n  ort_input_names.push_back(\"input_0\");\r\n\r\n  // Run session and get outputs\r\n  std::array<const char*, 1> output_names{\"output_0\"};\r\n  std::vector<Ort::Value> ort_outputs = session.Run(Ort::RunOptions{nullptr}, ort_input_names.data(), ort_inputs.data(),\r\n                                                    ort_inputs.size(), output_names.data(), output_names.size());\r\n\r\n  // Check output shape.\r\n  Ort::Value& ort_output = ort_outputs[0];\r\n  auto typeshape = ort_output.GetTensorTypeAndShapeInfo();\r\n  std::vector<int64_t> output_shape = typeshape.GetShape();\r\n\r\n  EXPECT_THAT(output_shape, ::testing::ElementsAre(1, 5, 6, 6));\r\n}\r\n#endif  // #if defined(__aarch64__) || defined(_M_ARM64) || defined(__linux__)\r\n\r\nenum class TestBackend {\r\n  Cpu,\r\n  Htp,\r\n  Saver,\r\n  Ir,\r\n};\r\n\r\nstatic std::string ToBackendLibName(TestBackend backend) {\r\n  switch (backend) {\r\n    case TestBackend::Cpu:\r\n      return \"Cpu\";\r\n    case TestBackend::Htp:\r\n      return \"Htp\";\r\n    case TestBackend::Saver:\r\n      return \"Saver\";\r\n    case TestBackend::Ir:\r\n      return \"Ir\";\r\n    default:\r\n      assert(false && \"Invalid TestBackend value.\");\r\n      return \"\";\r\n  }\r\n}\r\n\r\nstatic void AddSerializerConfigs(TestBackend serializer_backend, onnxruntime::ProviderOptions& options) {\r\n  std::string serializer_lib = ToBackendLibName(serializer_backend);\r\n  std::string serializer_path_key;\r\n\r\n  switch (serializer_backend) {\r\n    case TestBackend::Ir:\r\n      serializer_path_key = \"qnn_ir_backend_path\";\r\n      options[\"dump_qnn_ir_dlc\"] = \"1\";\r\n      options[\"dump_qnn_ir_dlc_dir\"] = kDlcOutputDir;\r\n      break;\r\n    case TestBackend::Saver:\r\n      serializer_path_key = \"qnn_saver_path\";\r\n      break;\r\n    default:\r\n      assert(false && \"Invalid serializer backend.\");\r\n      return;\r\n  }\r\n\r\n#if defined(_WIN32)\r\n  options[serializer_path_key] = \"Qnn\" + serializer_lib + \".dll\";\r\n#else\r\n  options[serializer_path_key] = \"libQnn\" + serializer_lib + \".so\";\r\n#endif\r\n}\r\n\r\nstatic Ort::Session InitNHWCResizeModel(const ORTCHAR_T* ort_model_path, TestBackend backend,\r\n                                        std::optional<TestBackend> serializer_backend = std::nullopt,\r\n                                        std::string htp_graph_finalization_opt_mode = \"\",\r\n                                        std::string qnn_context_priority = \"\",\r\n                                        std::string soc_model = \"\",\r\n                                        std::string htp_arch = \"\",\r\n                                        std::string device_id = \"\") {\r\n  Ort::SessionOptions so;\r\n\r\n  // Ensure all type/shape inference warnings result in errors!\r\n  so.AddConfigEntry(kOrtSessionOptionsConfigStrictShapeTypeInference, \"1\");\r\n  so.SetGraphOptimizationLevel(ORT_ENABLE_ALL);\r\n\r\n  onnxruntime::ProviderOptions options;\r\n  options[\"offload_graph_io_quantization\"] = \"0\";\r\n\r\n  std::string backend_lib = ToBackendLibName(backend);\r\n\r\n#if defined(_WIN32)\r\n  options[\"backend_path\"] = \"Qnn\" + backend_lib + \".dll\";\r\n#else\r\n  options[\"backend_path\"] = \"libQnn\" + backend_lib + \".so\";\r\n#endif\r\n\r\n  if (serializer_backend) {\r\n    AddSerializerConfigs(*serializer_backend, options);\r\n  }\r\n\r\n  if (!htp_graph_finalization_opt_mode.empty()) {\r\n    options[\"htp_graph_finalization_optimization_mode\"] = std::move(htp_graph_finalization_opt_mode);\r\n  }\r\n\r\n  if (!qnn_context_priority.empty()) {\r\n    options[\"qnn_context_priority\"] = std::move(qnn_context_priority);\r\n  }\r\n\r\n  if (!soc_model.empty()) {\r\n    options[\"soc_model\"] = std::move(soc_model);\r\n  }\r\n\r\n  if (!htp_arch.empty()) {\r\n    options[\"htp_arch\"] = std::move(htp_arch);\r\n  }\r\n\r\n  if (!device_id.empty()) {\r\n    options[\"device_id\"] = std::move(device_id);\r\n  }\r\n\r\n  so.AppendExecutionProvider(\"QNN\", options);\r\n\r\n  Ort::Session session(*ort_env, ort_model_path, so);\r\n\r\n  return session;\r\n}\r\n\r\n// Helper function that runs an ONNX model with a NHWC Resize operator to test that\r\n// type/shape inference succeeds during layout transformation.\r\n// Refer to onnxruntime/core/graph/contrib_ops/nhwc_inference_context.h.\r\n//\r\n// The models passed to this function are subgraphs extracted from a larger model that exhibited\r\n// shape inferencing issues on QNN. Thus, the models are expected to have a specific input/output\r\n// types and shapes.\r\nstatic void RunNHWCResizeModel(const ORTCHAR_T* ort_model_path, TestBackend backend,\r\n                               std::optional<TestBackend> serializer_backend = std::nullopt,\r\n                               std::string htp_graph_finalization_opt_mode = \"\",\r\n                               std::string qnn_context_priority = \"\",\r\n                               std::string soc_model = \"\",\r\n                               std::string htp_arch = \"\",\r\n                               std::string device_id = \"\") {\r\n  Ort::Session session = InitNHWCResizeModel(ort_model_path, backend, serializer_backend, htp_graph_finalization_opt_mode, qnn_context_priority, soc_model, htp_arch, device_id);\r\n\r\n  // Input can be all zeros since we're testing for correct shape inference.\r\n  std::array<float, 1 * 3 * 4 * 5> input0_data = {};\r\n  std::array<float, 1 * 3 * 4 * 5> input1_data = {};\r\n  std::array<float, 1 * 3 * 4 * 5> input2_data = {};\r\n\r\n  auto memory_info = Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeCPU);\r\n  std::vector<Ort::Value> ort_inputs;\r\n  std::vector<const char*> ort_input_names;\r\n\r\n  // Add input0\r\n  std::array<int64_t, 4> inputs_shape{1, 3, 4, 5};\r\n  ort_inputs.emplace_back(Ort::Value::CreateTensor<float>(\r\n      memory_info, input0_data.data(), input0_data.size(), inputs_shape.data(), inputs_shape.size()));\r\n  ort_input_names.push_back(\"input0\");\r\n\r\n  // Add input1\r\n  ort_inputs.emplace_back(Ort::Value::CreateTensor<float>(\r\n      memory_info, input1_data.data(), input1_data.size(), inputs_shape.data(), inputs_shape.size()));\r\n  ort_input_names.push_back(\"input1\");\r\n\r\n  // Add input2\r\n  ort_inputs.emplace_back(Ort::Value::CreateTensor<float>(\r\n      memory_info, input2_data.data(), input2_data.size(), inputs_shape.data(), inputs_shape.size()));\r\n  ort_input_names.push_back(\"input2\");\r\n\r\n  // Run session and get outputs\r\n  std::array<const char*, 2> output_names{\"output0\", \"output1\"};\r\n  std::vector<Ort::Value> ort_outputs = session.Run(Ort::RunOptions{nullptr}, ort_input_names.data(), ort_inputs.data(),\r\n                                                    ort_inputs.size(), output_names.data(), output_names.size());\r\n\r\n  // Check output shape.\r\n  Ort::Value& ort_output = ort_outputs[1];\r\n  auto typeshape = ort_output.GetTensorTypeAndShapeInfo();\r\n  std::vector<int64_t> output_shape = typeshape.GetShape();\r\n\r\n  EXPECT_THAT(output_shape, ::testing::ElementsAre(1, 6, 7, 10));\r\n}\r\n\r\n// Test shape inference of NHWC Resize operator (opset 11) that uses\r\n// the scales input. Use the QNN CPU backend.\r\nTEST_F(QnnCPUBackendTests, TestNHWCResizeShapeInference_scales_opset11) {\r\n  RunNHWCResizeModel(ORT_MODEL_FOLDER \"nhwc_resize_scales_opset11.onnx\", TestBackend::Cpu);\r\n}\r\n\r\n// Test shape inference of NHWC Resize operator (opset 18) that uses\r\n// the scales input. Use the QNN CPU backend.\r\nTEST_F(QnnCPUBackendTests, TestNHWCResizeShapeInference_scales_opset18) {\r\n  RunNHWCResizeModel(ORT_MODEL_FOLDER \"nhwc_resize_scales_opset18.onnx\", TestBackend::Cpu);\r\n}\r\n\r\n// Test shape inference of NHWC Resize operator (opset 11) that uses\r\n// the sizes input. Use the QNN CPU backend.\r\nTEST_F(QnnCPUBackendTests, TestNHWCResizeShapeInference_sizes_opset11) {\r\n  RunNHWCResizeModel(ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset11.onnx\", TestBackend::Cpu);\r\n}\r\n\r\n// Test shape inference of NHWC Resize operator (opset 18) that uses\r\n// the sizes input. Use the QNN CPU backend.\r\nTEST_F(QnnCPUBackendTests, TestNHWCResizeShapeInference_sizes_opset18) {\r\n  RunNHWCResizeModel(ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.onnx\", TestBackend::Cpu);\r\n}\r\n\r\n// Test that QNN Saver generates the expected files for a model meant to run on the QNN CPU backend.\r\nTEST_F(QnnCPUBackendTests, QnnSaver_OutputFiles) {\r\n  const std::filesystem::path qnn_saver_output_dir = \"saver_output\";\r\n\r\n  // Remove pre-existing QNN Saver output files. Note that fs::remove_all() can handle non-existing paths.\r\n  std::filesystem::remove_all(qnn_saver_output_dir);\r\n  ASSERT_FALSE(std::filesystem::exists(qnn_saver_output_dir));\r\n\r\n  RunNHWCResizeModel(ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.onnx\",\r\n                     TestBackend::Cpu,     // backend\r\n                     TestBackend::Saver);  // serializer_backend\r\n\r\n  // Check that QNN Saver output files exist.\r\n  EXPECT_TRUE(std::filesystem::exists(qnn_saver_output_dir / \"saver_output.c\"));\r\n  EXPECT_TRUE(std::filesystem::exists(qnn_saver_output_dir / \"params.bin\"));\r\n}\r\n\r\nstruct ModelAndBuilder {\r\n  ModelAndBuilder(Graph& graph) : builder(graph) {}\r\n  std::string model_data;\r\n  ModelTestBuilder builder;\r\n};\r\n\r\n// Creates a model in memory. Input feeds and output names can be accessed from result.builder.\r\nstatic void CreateModelInMemory(std::unique_ptr<ModelAndBuilder>& result,\r\n                                const GetTestModelFn& model_build_fn,\r\n                                const std::string& model_name,\r\n                                int opset_version = 18) {\r\n  const std::unordered_map<std::string, int> domain_to_version = {{\"\", opset_version}, {kMSDomain, 1}};\r\n  auto& logging_manager = DefaultLoggingManager();\r\n\r\n  // Create float model and serialize it to a string.\r\n  onnxruntime::Model model(model_name, false, ModelMetaData(), PathString(),\r\n                           IOnnxRuntimeOpSchemaRegistryList(), domain_to_version, {},\r\n                           logging_manager.DefaultLogger());\r\n  result = std::make_unique<ModelAndBuilder>(model.MainGraph());\r\n  model_build_fn(result->builder);\r\n  result->builder.SetGraphOutputs();\r\n  ASSERT_STATUS_OK(model.MainGraph().Resolve());\r\n  model.ToProto().SerializeToString(&result->model_data);\r\n}\r\n\r\n// Runs a session and verifies the outputs. Can be run by individual threads.\r\nstatic void RunSessionAndVerify(InferenceSession& session, const RunOptions& run_options, const NameMLValMap& feeds,\r\n                                const std::vector<std::string>& output_names,\r\n                                const std::vector<std::vector<int64_t>>& output_shapes,\r\n                                const std::vector<std::vector<float>>& expected_values,\r\n                                int loop_count = 10) {\r\n  // Let it run for a while\r\n  for (int it = 0; it < loop_count; ++it) {\r\n    std::vector<OrtValue> fetches;\r\n    auto status = session.Run(run_options, feeds, output_names, &fetches);\r\n    ASSERT_TRUE(status.IsOK());\r\n\r\n    for (size_t i = 0; i < fetches.size(); i++) {\r\n      auto& tensor = fetches[i].Get<Tensor>();\r\n      TensorShape expected_shape(output_shapes[i]);\r\n      ASSERT_EQ(expected_shape, tensor.Shape());\r\n\r\n      gsl::span<const float> actual = tensor.DataAsSpan<float>();\r\n      gsl::span<const float> expected(expected_values[i].data(), expected_values[i].size());\r\n      ASSERT_EQ(expected, actual);\r\n    }\r\n  }\r\n}\r\n\r\n// Returns a function that builds a float32 model that adds 3 tensors.\r\nstatic GetTestModelFn F32BuildAdd3Tensors(const TestInputDef<float>& input0_def,\r\n                                          const TestInputDef<float>& input1_def,\r\n                                          const TestInputDef<float>& input2_def) {\r\n  return [input0_def, input1_def, input2_def](ModelTestBuilder& builder) {\r\n    NodeArg* input0 = MakeTestInput<float>(builder, input0_def);\r\n    NodeArg* input1 = MakeTestInput<float>(builder, input1_def);\r\n    NodeArg* input2 = MakeTestInput<float>(builder, input1_def);\r\n\r\n    auto* add0_out = builder.MakeIntermediate();\r\n    builder.AddNode(\"Add\", {input0, input1}, {add0_out});\r\n\r\n    auto* output = builder.MakeOutput();\r\n    builder.AddNode(\"Add\", {add0_out, input2}, {output});\r\n  };\r\n}\r\n\r\n// Tests running a single session in multiple threads on the CPU backend.\r\nTEST_F(QnnCPUBackendTests, MultithreadSessionRun) {\r\n  std::unique_ptr<ModelAndBuilder> model;\r\n  std::vector<float> input_data = {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f};\r\n  std::vector<int64_t> shape = {1, 3, 2};\r\n  std::vector<std::vector<int64_t>> output_shapes = {shape};\r\n  std::vector<std::vector<float>> output_values = {{3.0f, 6.0f, 9.0f, 12.0f, 15.0f, 18.0f}};\r\n\r\n  CreateModelInMemory(model,\r\n                      F32BuildAdd3Tensors(TestInputDef<float>(shape, false, input_data),\r\n                                          TestInputDef<float>(shape, false, input_data),\r\n                                          TestInputDef<float>(shape, false, input_data)),\r\n                      \"add3.f32\");\r\n\r\n  SessionOptions session_opts;\r\n  session_opts.session_logid = \"logger0\";\r\n\r\n  RunOptions run_opts;\r\n  run_opts.run_tag = session_opts.session_logid;\r\n\r\n  InferenceSession session_obj{session_opts, GetEnvironment()};\r\n  onnxruntime::ProviderOptions options;\r\n\r\n#if defined(_WIN32)\r\n  options[\"backend_path\"] = \"QnnCpu.dll\";\r\n#else\r\n  options[\"backend_path\"] = \"libQnnCpu.so\";\r\n#endif\r\n\r\n  auto qnn_ep = QnnExecutionProviderWithOptions(options, &session_opts);\r\n  EXPECT_TRUE(session_obj.RegisterExecutionProvider(std::move(qnn_ep)).IsOK());\r\n\r\n  auto status = session_obj.Load(model->model_data.data(), static_cast<int>(model->model_data.size()));\r\n  ASSERT_TRUE(status.IsOK());\r\n  status = session_obj.Initialize();\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  std::vector<std::thread> threads;\r\n  constexpr int num_threads = 5;\r\n  constexpr int loop_count = 10;\r\n  for (int i = 0; i < num_threads; i++) {\r\n    threads.push_back(std::thread(RunSessionAndVerify, std::ref(session_obj), run_opts,\r\n                                  model->builder.feeds_, model->builder.output_names_,\r\n                                  output_shapes, output_values, loop_count));\r\n  }\r\n\r\n  for (auto& th : threads) {\r\n    th.join();\r\n  }\r\n}\r\n\r\n#if defined(__aarch64__) || defined(_M_ARM64) || defined(__linux__)\r\n\r\n// Returns a function that builds a QDQ model that adds 3 tensors. Forces all scales and zero-points to be (1.0f, 0),\r\n// so it is only accurate when using non-fractional positive inputs.\r\ntemplate <typename QuantType>\r\nstatic GetTestModelFn QDQBuildAdd3Tensors(const TestInputDef<float>& input0_def,\r\n                                          const TestInputDef<float>& input1_def,\r\n                                          const TestInputDef<float>& input2_def) {\r\n  return [input0_def, input1_def, input2_def](ModelTestBuilder& builder) {\r\n    NodeArg* input0 = MakeTestInput<float>(builder, input0_def);\r\n    NodeArg* input0_after_qdq = AddQDQNodePair<QuantType>(builder, input0, 1.0f, 0);\r\n    NodeArg* input1 = MakeTestInput<float>(builder, input1_def);\r\n    NodeArg* input1_after_qdq = AddQDQNodePair<QuantType>(builder, input1, 1.0f, 0);\r\n    NodeArg* input2 = MakeTestInput<float>(builder, input1_def);\r\n    NodeArg* input2_after_qdq = AddQDQNodePair<QuantType>(builder, input2, 1.0f, 0);\r\n\r\n    auto* add0_out = builder.MakeIntermediate();\r\n    builder.AddNode(\"Add\", {input0_after_qdq, input1_after_qdq}, {add0_out});\r\n\r\n    auto* add0_out_dq = AddQDQNodePair<QuantType>(builder, add0_out, 1.0f, 0);\r\n\r\n    auto* add1_out = builder.MakeIntermediate();\r\n    builder.AddNode(\"Add\", {add0_out_dq, input2_after_qdq}, {add1_out});\r\n\r\n    // op_output -> Q -> DQ -> output\r\n    AddQDQNodePairWithOutputAsGraphOutput<QuantType>(builder, add1_out, 1.0f, 0);\r\n  };\r\n}\r\n\r\n// Tests running a single session in multiple threads on the HTP backend.\r\nTEST_F(QnnHTPBackendTests, MultithreadSessionRun) {\r\n  std::unique_ptr<ModelAndBuilder> model;\r\n  std::vector<float> input_data = {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f};\r\n  std::vector<int64_t> shape = {1, 3, 2};\r\n  std::vector<std::vector<int64_t>> output_shapes = {shape};\r\n  std::vector<std::vector<float>> output_values = {{3.0f, 6.0f, 9.0f, 12.0f, 15.0f, 18.0f}};\r\n\r\n  CreateModelInMemory(model,\r\n                      QDQBuildAdd3Tensors<uint8_t>(TestInputDef<float>(shape, false, input_data),\r\n                                                   TestInputDef<float>(shape, false, input_data),\r\n                                                   TestInputDef<float>(shape, false, input_data)),\r\n                      \"add3.qdq\");\r\n\r\n  SessionOptions session_opts;\r\n  session_opts.session_logid = \"logger0\";\r\n\r\n  RunOptions run_opts;\r\n  run_opts.run_tag = session_opts.session_logid;\r\n\r\n  InferenceSession session_obj{session_opts, GetEnvironment()};\r\n  onnxruntime::ProviderOptions options;\r\n\r\n#if defined(_WIN32)\r\n  options[\"backend_path\"] = \"QnnHtp.dll\";\r\n#else\r\n  options[\"backend_path\"] = \"libQnnHtp.so\";\r\n#endif\r\n  options[\"offload_graph_io_quantization\"] = \"0\";\r\n\r\n  auto qnn_ep = QnnExecutionProviderWithOptions(options, &session_opts);\r\n  EXPECT_TRUE(session_obj.RegisterExecutionProvider(std::move(qnn_ep)).IsOK());\r\n\r\n  auto status = session_obj.Load(model->model_data.data(), static_cast<int>(model->model_data.size()));\r\n  ASSERT_TRUE(status.IsOK());\r\n  status = session_obj.Initialize();\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  std::vector<std::thread> threads;\r\n  constexpr int num_threads = 5;\r\n  constexpr int loop_count = 10;\r\n\r\n  for (int i = 0; i < num_threads; i++) {\r\n    threads.push_back(std::thread(RunSessionAndVerify, std::ref(session_obj), run_opts,\r\n                                  model->builder.feeds_, model->builder.output_names_,\r\n                                  output_shapes, output_values, loop_count));\r\n  }\r\n\r\n  for (auto& th : threads) {\r\n    th.join();\r\n  }\r\n}\r\n\r\n// Tests running a single session in multiple threads on the HTP backend with run option to set power config\r\nTEST_F(QnnHTPBackendTests, MultithreadHtpPowerCfgSessionRunOption) {\r\n  std::unique_ptr<ModelAndBuilder> model;\r\n  std::vector<float> input_data = {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f};\r\n  std::vector<int64_t> shape = {1, 3, 2};\r\n  std::vector<std::vector<int64_t>> output_shapes = {shape};\r\n  std::vector<std::vector<float>> output_values = {{3.0f, 6.0f, 9.0f, 12.0f, 15.0f, 18.0f}};\r\n\r\n  CreateModelInMemory(model,\r\n                      QDQBuildAdd3Tensors<uint8_t>(TestInputDef<float>(shape, false, input_data),\r\n                                                   TestInputDef<float>(shape, false, input_data),\r\n                                                   TestInputDef<float>(shape, false, input_data)),\r\n                      \"add3.qdq\");\r\n\r\n  SessionOptions session_opts;\r\n  session_opts.session_logid = \"logger0\";\r\n\r\n  InferenceSession session_obj{session_opts, GetEnvironment()};\r\n  onnxruntime::ProviderOptions options;\r\n\r\n#if defined(_WIN32)\r\n  options[\"backend_path\"] = \"QnnHtp.dll\";\r\n#else\r\n  options[\"backend_path\"] = \"libQnnHtp.so\";\r\n#endif\r\n  options[\"offload_graph_io_quantization\"] = \"0\";\r\n\r\n  auto qnn_ep = QnnExecutionProviderWithOptions(options, &session_opts);\r\n  EXPECT_TRUE(session_obj.RegisterExecutionProvider(std::move(qnn_ep)).IsOK());\r\n\r\n  auto status = session_obj.Load(model->model_data.data(), static_cast<int>(model->model_data.size()));\r\n  ASSERT_TRUE(status.IsOK());\r\n  status = session_obj.Initialize();\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  std::vector<std::thread> threads;\r\n  constexpr int num_threads = 5;\r\n  constexpr int loop_count = 10;\r\n\r\n  std::vector<std::string> perf_modes{\r\n      \"burst\", \"balanced\", \"default\", \"high_performance\", \"high_power_saver\",\r\n      \"low_balanced\", \"extreme_power_saver\", \"low_power_saver\", \"power_saver\"};\r\n\r\n  size_t post_i = perf_modes.size() - 1;\r\n  ASSERT_TRUE(post_i > num_threads);\r\n  for (int i = 0; i < num_threads; ++i, --post_i) {\r\n    RunOptions run_opts;\r\n    run_opts.run_tag = session_opts.session_logid;\r\n    auto rt = run_opts.config_options.AddConfigEntry(kOrtRunOptionsConfigQnnPerfMode, perf_modes[i].c_str());\r\n    ASSERT_TRUE(rt.IsOK());\r\n    rt = run_opts.config_options.AddConfigEntry(kOrtRunOptionsConfigQnnPerfModePostRun, perf_modes[post_i].c_str());\r\n    ASSERT_TRUE(rt.IsOK());\r\n\r\n    threads.push_back(std::thread(RunSessionAndVerify, std::ref(session_obj), run_opts,\r\n                                  model->builder.feeds_, model->builder.output_names_,\r\n                                  output_shapes, output_values, loop_count));\r\n  }\r\n\r\n  for (auto& th : threads) {\r\n    th.join();\r\n  }\r\n}\r\n\r\n// Tests running a single session in multiple threads on the HTP backend with EP option to set default power config\r\nTEST_F(QnnHTPBackendTests, MultithreadDefaultHtpPowerCfgFromEpOption) {\r\n  std::unique_ptr<ModelAndBuilder> model;\r\n  std::vector<float> input_data = {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f};\r\n  std::vector<int64_t> shape = {1, 3, 2};\r\n  std::vector<std::vector<int64_t>> output_shapes = {shape};\r\n  std::vector<std::vector<float>> output_values = {{3.0f, 6.0f, 9.0f, 12.0f, 15.0f, 18.0f}};\r\n\r\n  CreateModelInMemory(model,\r\n                      QDQBuildAdd3Tensors<uint8_t>(TestInputDef<float>(shape, false, input_data),\r\n                                                   TestInputDef<float>(shape, false, input_data),\r\n                                                   TestInputDef<float>(shape, false, input_data)),\r\n                      \"add3.qdq\");\r\n\r\n  SessionOptions session_opts;\r\n  session_opts.session_logid = \"logger0\";\r\n\r\n  RunOptions run_opts;\r\n  run_opts.run_tag = session_opts.session_logid;\r\n\r\n  InferenceSession session_obj{session_opts, GetEnvironment()};\r\n  onnxruntime::ProviderOptions options;\r\n\r\n#if defined(_WIN32)\r\n  options[\"backend_path\"] = \"QnnHtp.dll\";\r\n#else\r\n  options[\"backend_path\"] = \"libQnnHtp.so\";\r\n#endif\r\n  options[\"offload_graph_io_quantization\"] = \"0\";\r\n  options[\"htp_performance_mode\"] = \"burst\";\r\n\r\n  auto qnn_ep = QnnExecutionProviderWithOptions(options, &session_opts);\r\n  EXPECT_TRUE(session_obj.RegisterExecutionProvider(std::move(qnn_ep)).IsOK());\r\n\r\n  auto status = session_obj.Load(model->model_data.data(), static_cast<int>(model->model_data.size()));\r\n  ASSERT_TRUE(status.IsOK());\r\n  status = session_obj.Initialize();\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  std::vector<std::thread> threads;\r\n  constexpr int num_threads = 5;\r\n  constexpr int loop_count = 10;\r\n\r\n  for (int i = 0; i < num_threads; i++) {\r\n    threads.push_back(std::thread(RunSessionAndVerify, std::ref(session_obj), run_opts,\r\n                                  model->builder.feeds_, model->builder.output_names_,\r\n                                  output_shapes, output_values, loop_count));\r\n  }\r\n\r\n  for (auto& th : threads) {\r\n    th.join();\r\n  }\r\n}\r\n\r\n// Tests running a single session in multiple threads on the HTP backend with\r\n// EP option to set default power config + run option to set power config for each run\r\nTEST_F(QnnHTPBackendTests, MultithreadHtpPowerCfgDefaultAndRunOption) {\r\n  std::unique_ptr<ModelAndBuilder> model;\r\n  std::vector<float> input_data = {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f};\r\n  std::vector<int64_t> shape = {1, 3, 2};\r\n  std::vector<std::vector<int64_t>> output_shapes = {shape};\r\n  std::vector<std::vector<float>> output_values = {{3.0f, 6.0f, 9.0f, 12.0f, 15.0f, 18.0f}};\r\n\r\n  CreateModelInMemory(model,\r\n                      QDQBuildAdd3Tensors<uint8_t>(TestInputDef<float>(shape, false, input_data),\r\n                                                   TestInputDef<float>(shape, false, input_data),\r\n                                                   TestInputDef<float>(shape, false, input_data)),\r\n                      \"add3.qdq\");\r\n\r\n  SessionOptions session_opts;\r\n  session_opts.session_logid = \"logger0\";\r\n\r\n  InferenceSession session_obj{session_opts, GetEnvironment()};\r\n  onnxruntime::ProviderOptions options;\r\n\r\n#if defined(_WIN32)\r\n  options[\"backend_path\"] = \"QnnHtp.dll\";\r\n#else\r\n  options[\"backend_path\"] = \"libQnnHtp.so\";\r\n#endif\r\n  options[\"offload_graph_io_quantization\"] = \"0\";\r\n  options[\"htp_performance_mode\"] = \"burst\";\r\n\r\n  auto qnn_ep = QnnExecutionProviderWithOptions(options, &session_opts);\r\n  EXPECT_TRUE(session_obj.RegisterExecutionProvider(std::move(qnn_ep)).IsOK());\r\n\r\n  auto status = session_obj.Load(model->model_data.data(), static_cast<int>(model->model_data.size()));\r\n  ASSERT_TRUE(status.IsOK());\r\n  status = session_obj.Initialize();\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  std::vector<std::thread> threads;\r\n  constexpr int num_threads = 5;\r\n  constexpr int loop_count = 10;\r\n\r\n  std::vector<std::string> perf_modes{\r\n      \"burst\", \"balanced\", \"default\", \"high_performance\", \"high_power_saver\",\r\n      \"low_balanced\", \"extreme_power_saver\", \"low_power_saver\", \"power_saver\"};\r\n\r\n  size_t post_i = perf_modes.size() - 1;\r\n  ASSERT_TRUE(post_i > num_threads);\r\n  for (int i = 0; i < num_threads; ++i, --post_i) {\r\n    RunOptions run_opts;\r\n    run_opts.run_tag = session_opts.session_logid;\r\n    auto rt = run_opts.config_options.AddConfigEntry(kOrtRunOptionsConfigQnnPerfMode, perf_modes[i].c_str());\r\n    ASSERT_TRUE(rt.IsOK());\r\n    rt = run_opts.config_options.AddConfigEntry(kOrtRunOptionsConfigQnnPerfModePostRun, perf_modes[post_i].c_str());\r\n    ASSERT_TRUE(rt.IsOK());\r\n\r\n    threads.push_back(std::thread(RunSessionAndVerify, std::ref(session_obj), run_opts,\r\n                                  model->builder.feeds_, model->builder.output_names_,\r\n                                  output_shapes, output_values, loop_count));\r\n  }\r\n\r\n  for (auto& th : threads) {\r\n    th.join();\r\n  }\r\n}\r\n\r\n// Test shape inference of QDQ NHWC Resize operator (opset 18) that uses\r\n// the sizes input. Use the QNN HTP backend.\r\n// Maps to QNN's ResizeBilinear operator.\r\nTEST_F(QnnHTPBackendTests, TestNHWCResizeShapeInference_qdq_sizes_opset18) {\r\n  RunNHWCResizeModel(ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.quant.onnx\", TestBackend::Htp);\r\n}\r\n\r\n// Test that QNN Ir generates the expected file for a model meant to run on the QNN HTP backend.\r\n\r\nTEST_F(QnnHTPBackendTests, QnnIr_OutputFiles) {\r\n  const auto& logger = DefaultLoggingManager().DefaultLogger();\r\n  if (IsIRBackendSupported() == BackendSupport::UNSUPPORTED) {\r\n    LOGS(logger, WARNING) << \"QNN IR backend is not available! Skipping test.\";\r\n    GTEST_SKIP();\r\n  } else if (IsIRBackendSupported() == BackendSupport::SUPPORT_ERROR) {\r\n    LOGS(logger, ERROR) << \"Failed to check if QNN IR backend is available.\";\r\n    FAIL();\r\n  }\r\n\r\n  const std::filesystem::path qnn_dlc_dir = kDlcOutputDir;\r\n\r\n  // Remove pre-existing QNN Ir output files. Note that fs::remove_all() can handle non-existing paths.\r\n  std::filesystem::remove_all(qnn_dlc_dir);\r\n  ASSERT_FALSE(std::filesystem::exists(qnn_dlc_dir));\r\n\r\n  InitNHWCResizeModel(ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.onnx\",\r\n                      TestBackend::Htp,  // backend\r\n                      TestBackend::Ir);  // serializer backend\r\n\r\n  // File names are taken from graph node names. Just make sure that we got one .dlc\r\n  // in the expected directory.\r\n  ASSERT_TRUE(std::filesystem::exists(qnn_dlc_dir));\r\n\r\n  int file_count = 0;\r\n  for (const auto& entry : std::filesystem::directory_iterator(qnn_dlc_dir)) {\r\n    EXPECT_TRUE(entry.is_regular_file());\r\n    EXPECT_EQ(entry.path().extension(), \".dlc\");\r\n    ++file_count;\r\n  }\r\n  EXPECT_EQ(file_count, 1);\r\n}\r\n\r\n// Test that QNN Saver generates the expected files for a model meant to run on the QNN HTP backend.\r\nTEST_F(QnnHTPBackendTests, QnnSaver_OutputFiles) {\r\n  const std::filesystem::path qnn_saver_output_dir = \"saver_output\";\r\n\r\n  // Remove pre-existing QNN Saver output files. Note that fs::remove_all() can handle non-existing paths.\r\n  std::filesystem::remove_all(qnn_saver_output_dir);\r\n  ASSERT_FALSE(std::filesystem::exists(qnn_saver_output_dir));\r\n\r\n  RunNHWCResizeModel(ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.onnx\",\r\n                     TestBackend::Htp,     // backend\r\n                     TestBackend::Saver);  // serializer_backend\r\n\r\n  // Check that QNN Saver output files exist.\r\n  EXPECT_TRUE(std::filesystem::exists(qnn_saver_output_dir / \"saver_output.c\"));\r\n  EXPECT_TRUE(std::filesystem::exists(qnn_saver_output_dir / \"params.bin\"));\r\n}\r\n\r\n// Test that models run with various HTP graph finalization optimization modes.\r\nTEST_F(QnnHTPBackendTests, HTPGraphFinalizationOptimizationModes) {\r\n  constexpr std::array<const char*, 5> graph_opt_modes = {\"\",    // No explicit mode specified\r\n                                                          \"0\",   // Explicit default mode\r\n                                                          \"1\",   // Mode 1\r\n                                                          \"2\",   // Mode 2\r\n                                                          \"3\"};  // Mode 3\r\n  for (auto mode : graph_opt_modes) {\r\n    RunNHWCResizeModel(ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.quant.onnx\",\r\n                       TestBackend::Htp,  // backend\r\n                       std::nullopt,      // serializer_backend\r\n                       mode);             // htp_graph_finalization_opt_mode\r\n  }\r\n}\r\n\r\n// Test that models run with various SoC model values\r\nTEST_F(QnnHTPBackendTests, HTPSocModels) {\r\n  constexpr std::array<const char*, 3> soc_models = { \"\",   // No explicit SoC model specified\r\n                                                      \"0\",  // \"Unknown\"\r\n#if defined(_M_ARM64)\r\n                                                      \"37\" };  // SC8280X\r\n#elif defined(__linux__)\r\n                                                      \"30\" };  // SM8350\r\n#else\r\n                                                      \"\" };\r\n#endif\r\n\r\n  for (auto soc_model : soc_models) {\r\n    RunNHWCResizeModel(ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.quant.onnx\",\r\n                       TestBackend::Htp,  // backend\r\n                       std::nullopt,      // serializer_backend\r\n                       \"\",                // htp_graph_finalization_opt_mode\r\n                       \"\",                // qnn_context_priority\r\n                       soc_model);\r\n  }\r\n}\r\n\r\n// Test that models run with various HTP architecture values (and set device_id)\r\nTEST_F(QnnHTPBackendTests, HTPArchValues) {\r\n  constexpr std::array<const char*, 3> htp_archs = {\"\",     // No explicit arch specified\r\n                                                    \"0\",    // \"None\"\r\n                                                    \"68\"};  // v68\r\n  for (auto htp_arch : htp_archs) {\r\n    RunNHWCResizeModel(ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.quant.onnx\",\r\n                       TestBackend::Htp,  // backend\r\n                       std::nullopt,      // enable_qnn_saver\r\n                       \"\",                // htp_graph_finalization_opt_mode\r\n                       \"\",                // qnn_context_priority\r\n                       \"\",                // soc_model\r\n                       htp_arch,          // htp_arch\r\n                       \"0\");              // device_id\r\n  }\r\n}\r\n\r\n// Test that models run with high QNN context priority.\r\nTEST_F(QnnHTPBackendTests, QnnContextPriorityHigh) {\r\n  RunNHWCResizeModel(ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.quant.onnx\",\r\n                     TestBackend::Htp,  // use_htp\r\n                     std::nullopt,      // enable_qnn_saver\r\n                     \"\",                // htp_graph_finalization_opt_mode\r\n                     \"high\");           // qnn_context_priority\r\n}\r\n\r\n// Create a model with Cast + Add (quantized)\r\n// cast_input -> Cast -> Q -> DQ ----\r\n//                                   |\r\n//             input2 -> Q -> DQ -> Add -> Q -> DQ -> output\r\ntemplate <typename InputType, typename QuantType>\r\nstatic GetTestQDQModelFn<QuantType> BuildCastAddQDQTestCase() {\r\n  return [](ModelTestBuilder& builder, std::vector<QuantParams<QuantType>>& output_qparams) {\r\n    // Creat Cast node int32 -> float32\r\n    NodeArg* cast_input = MakeTestInput(builder, TestInputDef<InputType>({2, 3}, false, {0, 1, 0, 1, 0, 1}));\r\n\r\n    auto* cast_output = builder.MakeIntermediate();\r\n    Node& cast_node = builder.AddNode(\"Cast\", {cast_input}, {cast_output});\r\n    cast_node.AddAttribute(\"to\", static_cast<int64_t>(ONNX_NAMESPACE::TensorProto_DataType::TensorProto_DataType_FLOAT));\r\n\r\n    // Create Add node\r\n    std::vector<float> data = {0.0f, 0.0f, 1.0f, 0.0f, 1.0f, 0.0f};\r\n    gsl::span<float> data_range = gsl::make_span(data);\r\n    QuantParams<QuantType> q_parameter = GetDataQuantParams<QuantType>(data_range);\r\n    auto* add_input1_qdq = AddQDQNodePair<QuantType>(builder, cast_output, q_parameter.scale, q_parameter.zero_point);\r\n\r\n    NodeArg* add_input2 = MakeTestInput(builder, TestInputDef<float>({2, 3}, false, data));\r\n    auto* add_input2_qdq = AddQDQNodePair<QuantType>(builder, add_input2, q_parameter.scale, q_parameter.zero_point);\r\n\r\n    auto* add_output = builder.MakeIntermediate();\r\n\r\n    builder.AddNode(\"Add\", {add_input1_qdq, add_input2_qdq}, {add_output});\r\n\r\n    // add_output -> Q -> DQ -> output\r\n    AddQDQNodePairWithOutputAsGraphOutput<QuantType>(builder, add_output, output_qparams[0].scale, output_qparams[0].zero_point);\r\n  };\r\n}\r\n\r\ntemplate <typename InputType>\r\nstatic GetTestModelFn BuildCastAddTestCase() {\r\n  return [](ModelTestBuilder& builder) {\r\n    // Creat Cast node int32 -> float32\r\n    NodeArg* cast_input = MakeTestInput(builder, TestInputDef<InputType>({2, 3}, false, {0, 1, 0, 1, 0, 1}));\r\n\r\n    auto* cast_output = builder.MakeIntermediate();\r\n    Node& cast_node = builder.AddNode(\"Cast\", {cast_input}, {cast_output});\r\n    cast_node.AddAttribute(\"to\", static_cast<int64_t>(ONNX_NAMESPACE::TensorProto_DataType::TensorProto_DataType_FLOAT));\r\n\r\n    // Create Add node\r\n    NodeArg* add_input2 = MakeTestInput(builder, TestInputDef<float>({2, 3}, false, {0.0f, 0.0f, 1.0f, 0.0f, 1.0f, 0.0f}));\r\n    auto* add_output = builder.MakeOutput();\r\n\r\n    builder.AddNode(\"Add\", {cast_output, add_input2}, {add_output});\r\n  };\r\n}\r\n\r\nvoid VerifyFileExistsAndIsNonEmpty(const std::string& filepath) {\r\n  std::ifstream csv_file(filepath, std::ifstream::binary);\r\n  ASSERT_TRUE(csv_file.good());\r\n\r\n  csv_file.seekg(0, csv_file.end);\r\n  size_t buffer_size = static_cast<size_t>(csv_file.tellg());\r\n  EXPECT_NE(0, buffer_size);\r\n}\r\n\r\nTEST_F(QnnHTPBackendTests, ProfilingTest) {\r\n  onnxruntime::ProviderOptions provider_options;\r\n\r\n  provider_options[\"backend_type\"] = \"htp\";\r\n  provider_options[\"offload_graph_io_quantization\"] = \"0\";\r\n  provider_options[\"enable_htp_fp16_precision\"] = \"1\";\r\n  provider_options[\"profiling_level\"] = \"detailed\";\r\n  provider_options[\"profiling_file_path\"] = \"detailed_profile.csv\";\r\n\r\n  auto input_defs = {TestInputDef<float>({1, 2, 2, 2}, false, -10.0f, 10.0f),\r\n                     TestInputDef<float>({1, 2, 2, 2}, false, -10.0f, 10.0f)};\r\n  RunQnnModelTest(BuildOpTestCase<float>(\"Add\", input_defs, {}, {}, kOnnxDomain),\r\n                  provider_options,\r\n                  13,\r\n                  ExpectedEPNodeAssignment::All,\r\n                  0.008f);\r\n\r\n  VerifyFileExistsAndIsNonEmpty(provider_options[\"profiling_file_path\"]);\r\n  std::remove(provider_options[\"profiling_file_path\"].c_str());\r\n\r\n#if QNN_API_VERSION_MAJOR > 2 || \\\r\n    (QNN_API_VERSION_MAJOR == 2 && (QNN_API_VERSION_MINOR >= 29))\r\n  VerifyFileExistsAndIsNonEmpty(\"detailed_profile_qnn.log\");\r\n  std::remove(\"detailed_profile_qnn.log\");\r\n#endif\r\n}\r\n\r\nTEST_F(QnnHTPBackendTests, OptraceTest) {\r\n  onnxruntime::ProviderOptions provider_options;\r\n\r\n  provider_options[\"backend_type\"] = \"htp\";\r\n  provider_options[\"offload_graph_io_quantization\"] = \"0\";\r\n  provider_options[\"enable_htp_fp16_precision\"] = \"1\";\r\n  provider_options[\"profiling_level\"] = \"optrace\";\r\n  provider_options[\"profiling_file_path\"] = \"optrace_profile.csv\";\r\n\r\n  auto input_defs = {TestInputDef<float>({1, 2, 2, 2}, false, -10.0f, 10.0f),\r\n                     TestInputDef<float>({1, 2, 2, 2}, false, -10.0f, 10.0f)};\r\n  RunQnnModelTest(BuildOpTestCase<float>(\"Add\", input_defs, {}, {}, kOnnxDomain),\r\n                  provider_options,\r\n                  13,\r\n                  ExpectedEPNodeAssignment::All,\r\n                  0.008f);\r\n\r\n  VerifyFileExistsAndIsNonEmpty(provider_options[\"profiling_file_path\"]);\r\n  std::remove(provider_options[\"profiling_file_path\"].c_str());\r\n\r\n#if QNN_API_VERSION_MAJOR > 2 || \\\r\n    (QNN_API_VERSION_MAJOR == 2 && (QNN_API_VERSION_MINOR >= 29))\r\n  VerifyFileExistsAndIsNonEmpty(\"optrace_profile_qnn.log\");\r\n  std::remove(\"optrace_profile_qnn.log\");\r\n#endif\r\n}\r\n\r\nTEST_F(QnnHTPBackendTests, CastAddQDQU8) {\r\n  ProviderOptions provider_options;\r\n  provider_options[\"backend_type\"] = \"htp\";\r\n  provider_options[\"offload_graph_io_quantization\"] = \"0\";\r\n\r\n  TestQDQModelAccuracy<uint8_t>(BuildCastAddTestCase<uint8_t>(),\r\n                                BuildCastAddQDQTestCase<uint8_t, uint8_t>(),\r\n                                provider_options,\r\n                                21,\r\n                                ExpectedEPNodeAssignment::All);\r\n}\r\n\r\nTEST_F(QnnHTPBackendTests, CastAddQDQU16) {\r\n  ProviderOptions provider_options;\r\n  provider_options[\"backend_type\"] = \"htp\";\r\n  provider_options[\"offload_graph_io_quantization\"] = \"0\";\r\n\r\n  TestQDQModelAccuracy<uint16_t>(BuildCastAddTestCase<uint8_t>(),\r\n                                 BuildCastAddQDQTestCase<uint8_t, uint16_t>(),\r\n                                 provider_options,\r\n                                 21,\r\n                                 ExpectedEPNodeAssignment::All);\r\n}\r\n\r\nTEST_F(QnnHTPBackendTests, CastAddQDQS8) {\r\n  ProviderOptions provider_options;\r\n  provider_options[\"backend_type\"] = \"htp\";\r\n  provider_options[\"offload_graph_io_quantization\"] = \"0\";\r\n\r\n  TestQDQModelAccuracy<int8_t>(BuildCastAddTestCase<uint8_t>(),\r\n                               BuildCastAddQDQTestCase<uint8_t, int8_t>(),\r\n                               provider_options,\r\n                               21,\r\n                               ExpectedEPNodeAssignment::All);\r\n}\r\n\r\nTEST_F(QnnHTPBackendTests, CastAddQDQS16) {\r\n  ProviderOptions provider_options;\r\n  provider_options[\"backend_type\"] = \"htp\";\r\n  provider_options[\"offload_graph_io_quantization\"] = \"0\";\r\n\r\n  TestQDQModelAccuracy<int16_t>(BuildCastAddTestCase<uint8_t>(),\r\n                                BuildCastAddQDQTestCase<uint8_t, int16_t>(),\r\n                                provider_options,\r\n                                21,\r\n                                // QNN has not yet supported S16 Quantize/Dequantize\r\n                                ExpectedEPNodeAssignment::Some);\r\n}\r\n\r\n// Test float32 model with FP16 precision\r\nTEST_F(QnnHTPBackendTests, Float32ModelWithFP16PrecisionTest) {\r\n  ProviderOptions provider_options;\r\n#if defined(_WIN32)\r\n  provider_options[\"backend_path\"] = \"QnnHtp.dll\";\r\n#else\r\n  provider_options[\"backend_path\"] = \"libQnnHtp.so\";\r\n#endif\r\n  provider_options[\"enable_htp_fp16_precision\"] = \"1\";\r\n\r\n  auto input_defs = {TestInputDef<float>({1, 2, 2, 2}, false, -10.0f, 10.0f),\r\n                     TestInputDef<float>({1, 2, 2, 2}, false, -10.0f, 10.0f)};\r\n  RunQnnModelTest(BuildOpTestCase<float>(\"Add\", input_defs, {}, {}, kOnnxDomain),\r\n                  provider_options,\r\n                  13,\r\n                  ExpectedEPNodeAssignment::All,\r\n                  0.008f);\r\n}\r\n\r\n// Test that QNN EP only handles nodes with static shapes and rejects nodes with dynamic shape I/O.\r\nTEST_F(QnnHTPBackendTests, EPRejectsDynamicShapesF32) {\r\n  // Local function that builds a model in which the last two nodes use dynamic shapes.\r\n  auto model_build_fn = [](ModelTestBuilder& builder) {\r\n    NodeArg* input1 = builder.MakeInput<float>(std::vector<int64_t>{1, 2, 8, 8},\r\n                                               GetFloatDataInRange(0.0f, 1.0f, 128));\r\n    NodeArg* input2 = builder.MakeInput<int64_t>(std::vector<int64_t>{3}, std::vector<int64_t>{1, 2, 49});\r\n\r\n    // Add a Conv with known shapes. QNN EP should support it.\r\n    NodeArg* weight = builder.MakeInitializer<float>(std::vector<int64_t>{2, 2, 2, 2},\r\n                                                     GetFloatDataInRange(-0.3f, 0.3f, 16));\r\n    NodeArg* bias = builder.MakeInitializer<float>(std::vector<int64_t>{2}, {0.0f, 1.0f});\r\n\r\n    auto* conv_output = builder.MakeIntermediate();\r\n    builder.AddNode(\"Conv\", {input1, weight, bias}, {conv_output});\r\n\r\n    // Add a Reshape to a dynamic shape. QNN EP should reject this node.\r\n    auto* reshape_output = builder.MakeIntermediate();\r\n    builder.AddNode(\"Reshape\", {conv_output, input2}, {reshape_output});\r\n\r\n    // Add a Softmax. QNN EP should reject this node because its input has a dynamic shape.\r\n    NodeArg* output = builder.MakeOutput();\r\n    builder.AddNode(\"Softmax\", {reshape_output}, {output});\r\n  };\r\n\r\n  // Local function that checks that the nodes with dynamic shape I/O were assigned to CPU EP.\r\n  std::function<void(const Graph&)> ep_graph_checker = [](const Graph& graph) {\r\n    for (const Node& node : graph.Nodes()) {\r\n      const std::string& ep_name = node.GetExecutionProviderType();\r\n      const std::string& op_type = node.OpType();\r\n      if (op_type == \"Reshape\" || op_type == \"Softmax\") {\r\n        EXPECT_EQ(ep_name, kCpuExecutionProvider);\r\n      } else {\r\n        EXPECT_EQ(ep_name, kQnnExecutionProvider);\r\n      }\r\n    }\r\n  };\r\n\r\n  ProviderOptions provider_options;\r\n#if defined(_WIN32)\r\n  provider_options[\"backend_path\"] = \"QnnHtp.dll\";\r\n#else\r\n  provider_options[\"backend_path\"] = \"libQnnHtp.so\";\r\n#endif\r\n  provider_options[\"offload_graph_io_quantization\"] = \"0\";\r\n  provider_options[\"enable_htp_fp16_precision\"] = \"1\";  // QNN EP will use fp16 precision.\r\n                                                        // CPU EP will use fp32, so we can relax accuracy requirements.\r\n\r\n  RunQnnModelTest(model_build_fn,\r\n                  provider_options,\r\n                  /*opset*/ 19,\r\n                  ExpectedEPNodeAssignment::Some,\r\n                  /*abs_err*/ 1e-4f,\r\n                  logging::Severity::kERROR,\r\n                  /*verify_output*/ true,\r\n                  &ep_graph_checker);\r\n}\r\n\r\nTEST_F(QnnHTPBackendTests, DumpJsonQNNGraph) {\r\n  const ORTCHAR_T* ort_model_path = ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.quant.onnx\";\r\n  Ort::SessionOptions so;\r\n  onnxruntime::ProviderOptions options;\r\n#if defined(_WIN32)\r\n  options[\"backend_path\"] = \"QnnHtp.dll\";\r\n#else\r\n  options[\"backend_path\"] = \"libQnnHtp.so\";\r\n#endif\r\n  options[\"offload_graph_io_quantization\"] = \"0\";\r\n\r\n  const std::filesystem::path dump_dir = \"test_qnn_graphs_\";\r\n  options[\"json_qnn_graph_dir\"] = dump_dir.string();\r\n  options[\"dump_json_qnn_graph\"] = \"1\";\r\n\r\n  // Remove pre-existing json files. Note that fs::remove_all() can handle non-existing paths.\r\n  std::filesystem::remove_all(dump_dir);\r\n  ASSERT_TRUE(std::filesystem::create_directory(dump_dir));\r\n\r\n  so.AppendExecutionProvider(\"QNN\", options);\r\n  Ort::Session session(*ort_env, ort_model_path, so);\r\n\r\n  // Check that QNN JSON file(s) exist.\r\n  bool has_a_json_file = false;\r\n  for (auto const& dir_entry : std::filesystem::directory_iterator{dump_dir}) {\r\n    EXPECT_TRUE(dir_entry.is_regular_file());\r\n    EXPECT_EQ(dir_entry.path().extension().string(), \".json\");\r\n    has_a_json_file = true;\r\n  }\r\n  EXPECT_TRUE(has_a_json_file);\r\n\r\n  // Cleaup generated files.\r\n  // Comment the following line to inspect generated JSON files.\r\n  std::filesystem::remove_all(dump_dir);\r\n}\r\n\r\n// Test option for offloading quantization of graph inputs and dequantization of graph outputs to the CPU EP.\r\nTEST_F(QnnHTPBackendTests, EPOffloadsGraphIOQuantDequant) {\r\n  // Returns a function that checks that the Q/DQ ops at the graph IO boundary are offloaded to CPU\r\n  // if the corresponding provider option is enabled.\r\n  auto graph_checker_builder = [](bool offload_graph_io_quantization) -> std::function<void(const Graph&)> {\r\n    return [offload_graph_io_quantization](const Graph& graph) {\r\n      size_t num_q = 0;\r\n      size_t num_dq = 0;\r\n      size_t num_qnn_fused_node = 0;\r\n\r\n      for (const Node& node : graph.Nodes()) {\r\n        const std::string& ep_name = node.GetExecutionProviderType();\r\n        const std::string& op_type = node.OpType();\r\n\r\n        if (offload_graph_io_quantization && op_type == \"QuantizeLinear\") {\r\n          const bool consumes_graph_input = graph.IsInputsIncludingInitializers(node.InputDefs()[0]);\r\n          EXPECT_EQ(ep_name, kCpuExecutionProvider);\r\n          EXPECT_TRUE(consumes_graph_input);\r\n          num_q += 1;\r\n        } else if (offload_graph_io_quantization && op_type == \"DequantizeLinear\") {\r\n          const bool produces_graph_output = graph.IsOutput(node.OutputDefs()[0]);\r\n          EXPECT_EQ(ep_name, kCpuExecutionProvider);\r\n          EXPECT_TRUE(produces_graph_output);\r\n          num_dq += 1;\r\n        } else {\r\n          EXPECT_EQ(ep_name, kQnnExecutionProvider);\r\n          num_qnn_fused_node += 1;\r\n        }\r\n      }\r\n\r\n      EXPECT_EQ(num_q, static_cast<size_t>(offload_graph_io_quantization));\r\n      EXPECT_EQ(num_dq, static_cast<size_t>(offload_graph_io_quantization));\r\n      EXPECT_EQ(num_qnn_fused_node, 1);\r\n    };\r\n  };\r\n\r\n  ProviderOptions provider_options;\r\n#if defined(_WIN32)\r\n  provider_options[\"backend_path\"] = \"QnnHtp.dll\";\r\n#else\r\n  provider_options[\"backend_path\"] = \"libQnnHtp.so\";\r\n#endif\r\n  const std::vector<std::string> op_types = {\r\n      \"Sigmoid\",\r\n      \"Transpose\",\r\n      \"Softmax\",\r\n      \"Sqrt\",\r\n      \"Elu\",\r\n  };\r\n\r\n  // Test various QDQ ops with offloading of I/O quantization enabled and disabled.\r\n  for (auto op_type : op_types) {\r\n    for (int offload_io_quant = 0; offload_io_quant <= 1; offload_io_quant++) {\r\n      provider_options[\"offload_graph_io_quantization\"] = offload_io_quant ? \"1\" : \"0\";\r\n      auto graph_checker = graph_checker_builder(offload_io_quant);\r\n      auto expected_ep_assignment = offload_io_quant ? ExpectedEPNodeAssignment::Some : ExpectedEPNodeAssignment::All;\r\n\r\n      float min_val = (op_type == \"Sqrt\") ? 0.0f : -10.0f;\r\n      TestInputDef<float> input_def({1, 2, 2, 2}, false, GetFloatDataInRange(min_val, 10.0f, 8));\r\n      auto f32_model_build_fn = BuildOpTestCase<float>(op_type, {input_def}, {}, {});\r\n      auto qdq_model_build_fn = BuildQDQOpTestCase<uint8_t>(op_type, {input_def}, {}, {});\r\n      TestQDQModelAccuracy<uint8_t>(f32_model_build_fn,\r\n                                    qdq_model_build_fn,\r\n                                    provider_options,\r\n                                    /*opset*/ 21,\r\n                                    expected_ep_assignment,\r\n                                    /*abs_err*/ QDQTolerance(),\r\n                                    logging::Severity::kERROR,\r\n                                    /*qnn_ctx_model_path*/ \"\",\r\n                                    /*session_option_pairs*/ {},\r\n                                    &graph_checker);\r\n    }\r\n  }\r\n}\r\n\r\n// Only compile this test when QNN EP is built as a static library. When QNN EP is a shared library,\r\n// we cannot include internal QNN EP headers that use the provider-bridge API.\r\n#if BUILD_QNN_EP_STATIC_LIB\r\nTEST_F(QnnHTPBackendTests, UseHtpSharedMemoryAllocatorForInputs) {\r\n  ProviderOptions provider_options;\r\n#if defined(_WIN32)\r\n  provider_options[\"backend_path\"] = \"QnnHtp.dll\";\r\n#else\r\n  provider_options[\"backend_path\"] = \"libQnnHtp.so\";\r\n#endif\r\n  provider_options[\"offload_graph_io_quantization\"] = \"0\";\r\n  provider_options[\"enable_htp_shared_memory_allocator\"] = \"1\";\r\n\r\n  std::unique_ptr<IExecutionProvider> qnn_ep;\r\n  try {\r\n    qnn_ep = QnnExecutionProviderWithOptions(provider_options);\r\n  } catch (const OnnxRuntimeException& e) {\r\n    // handle exception that indicates that the libcdsprpc.so / dll can't be loaded\r\n    constexpr const char* expected_error_message = \"Failed to initialize RPCMEM dynamic library handle\";\r\n    ASSERT_THAT(e.what(), testing::HasSubstr(expected_error_message));\r\n    GTEST_SKIP() << \"HTP shared memory allocator is unavailable.\";\r\n  }\r\n\r\n  AllocatorPtr htp_shared_memory_allocator{};\r\n  {\r\n    auto allocators = qnn_ep->CreatePreferredAllocators();\r\n    ASSERT_FALSE(allocators.empty());\r\n    auto& allocator = allocators[0];\r\n    ASSERT_EQ(allocator->Info(), qnn::HtpSharedMemoryAllocator::AssociatedMemoryInfo());\r\n    htp_shared_memory_allocator = std::move(allocator);\r\n  }\r\n\r\n  auto input_defs = {TestInputDef<float>({1, 2, 2, 2}, false, -10.0f, 10.0f),\r\n                     TestInputDef<float>({1, 2, 2, 2}, false, -10.0f, 10.0f)};\r\n  RunQnnModelTest(BuildOpTestCase<float>(\"Add\", input_defs, {}, {}, kOnnxDomain, htp_shared_memory_allocator),\r\n                  provider_options,\r\n                  13,\r\n                  ExpectedEPNodeAssignment::All,\r\n                  0.008f);\r\n}\r\n#endif  // BUILD_QNN_EP_STATIC_LIB\r\n\r\n#if !BUILD_QNN_EP_STATIC_LIB\r\n// Tests that loading and unloading of an EP library in the same process does not cause a segfault.\r\nTEST_F(QnnHTPBackendTests, LoadingAndUnloadingOfQnnLibrary_FixSegFault) {\r\n  const ORTCHAR_T* ort_model_path = ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.quant.onnx\";\r\n\r\n  onnxruntime::ProviderOptions options;\r\n  options[\"backend_type\"] = \"htp\";\r\n  options[\"offload_graph_io_quantization\"] = \"0\";\r\n\r\n  // This first session will load the QNN EP library for the first time.\r\n  {\r\n    Ort::SessionOptions so;\r\n    so.AppendExecutionProvider(\"QNN\", options);\r\n\r\n    EXPECT_NO_THROW(Ort::Session session(*ort_env, ort_model_path, so));\r\n  }\r\n\r\n  {\r\n    ortenv_teardown();  // Destroy Env to force unloading of EP libraries.\r\n    ortenv_setup();\r\n\r\n    // This next session will reload the QNN EP library.\r\n    // Should not get a segfault.\r\n    Ort::SessionOptions so;\r\n    so.AppendExecutionProvider(\"QNN\", options);\r\n\r\n    EXPECT_NO_THROW(Ort::Session session(*ort_env, ort_model_path, so));\r\n  }\r\n}\r\n#endif  // !BUILD_QNN_EP_STATIC_LIB\r\n\r\n#if defined(WIN32) && !BUILD_QNN_EP_STATIC_LIB\r\n// Tests autoEP feature to automatically select an EP that supports the NPU.\r\n// Currently only works on Windows.\r\nTEST_F(QnnHTPBackendTests, AutoEp_PreferNpu) {\r\n  ASSERT_ORTSTATUS_OK(Ort::GetApi().RegisterExecutionProviderLibrary(*ort_env, kQnnExecutionProvider,\r\n                                                                     ORT_TSTR(\"onnxruntime_providers_qnn.dll\")));\r\n\r\n  Ort::SessionOptions so;\r\n  so.SetEpSelectionPolicy(OrtExecutionProviderDevicePolicy_PREFER_NPU);\r\n\r\n  const ORTCHAR_T* ort_model_path = ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.quant.onnx\";\r\n  Ort::Session session(*ort_env, ort_model_path, so);\r\n  EXPECT_TRUE(SessionHasEp(session, kQnnExecutionProvider));\r\n\r\n  ASSERT_ORTSTATUS_OK(Ort::GetApi().UnregisterExecutionProviderLibrary(*ort_env, kQnnExecutionProvider));\r\n}\r\n\r\nTEST_F(QnnGPUBackendTests, AutoEp_PreferGpu) {\r\n  ASSERT_ORTSTATUS_OK(Ort::GetApi().RegisterExecutionProviderLibrary(*ort_env, kQnnExecutionProvider,\r\n                                                                     ORT_TSTR(\"onnxruntime_providers_qnn.dll\")));\r\n\r\n  Ort::SessionOptions so;\r\n  so.SetEpSelectionPolicy(OrtExecutionProviderDevicePolicy_PREFER_GPU);\r\n\r\n  const ORTCHAR_T* ort_model_path = ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.onnx\";\r\n  Ort::Session session(*ort_env, ort_model_path, so);\r\n  EXPECT_TRUE(SessionHasEp(session, kQnnExecutionProvider));\r\n\r\n  ASSERT_ORTSTATUS_OK(Ort::GetApi().UnregisterExecutionProviderLibrary(*ort_env, kQnnExecutionProvider));\r\n}\r\n#endif  // defined(WIN32) && !BUILD_QNN_EP_STATIC_LIB\r\n\r\n// Test whether QNN EP can handle the case where the number of graph inputs and\r\n// the number of tensor wrappers do not match.\r\n// Take Resize op as an example.\r\n// - Qnn only cares about the 1st input, so the rest of the inputs are not converted\r\n//   to tensor wrappers.\r\n// - However, these remaining inputs still appear in the graph inputs,\r\n//   resulting in a discrepancy in the input quantities.\r\nTEST_F(QnnHTPBackendTests, TestMismatchedGraphInputAndTensorWrapperCount) {\r\n  onnxruntime::ProviderOptions provider_options;\r\n  provider_options[\"backend_type\"] = \"htp\";\r\n\r\n  auto input_defs = {TestInputDef<float>({1, 3, 10, 10}, false, -10.0f, 10.0f),\r\n                     TestInputDef<float>({0}, false, {}),\r\n                     TestInputDef<float>({4}, true, {1.0f, 1.0f, 2.0f, 2.0f})};\r\n  auto attrs = {utils::MakeAttribute(\"mode\", \"nearest\"),\r\n                utils::MakeAttribute(\"coordinate_transformation_mode\", \"asymmetric\"),\r\n                utils::MakeAttribute(\"nearest_mode\", \"floor\")};\r\n  RunQnnModelTest(BuildOpTestCase<float>(\"Resize\",\r\n                                         input_defs,\r\n                                         {},\r\n                                         attrs,\r\n                                         kOnnxDomain),\r\n                  provider_options,\r\n                  11,\r\n                  ExpectedEPNodeAssignment::All,\r\n                  0.008f);\r\n}\r\n\r\n#endif  // defined(__aarch64__) || defined(_M_ARM64) || defined(__linux__)\r\n\r\n// Test that QNN Ir generates the expected files for a model meant to run on any QNN backend.\r\nTEST_F(QnnIRBackendTests, QnnIr_OutputFiles) {\r\n  const std::filesystem::path qnn_dlc_dir = kDlcOutputDir;\r\n\r\n  // Remove pre-existing QNN Ir output files. Note that fs::remove_all() can handle non-existing paths.\r\n  std::filesystem::remove_all(qnn_dlc_dir);\r\n  ASSERT_FALSE(std::filesystem::exists(qnn_dlc_dir));\r\n\r\n  InitNHWCResizeModel(ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.onnx\",\r\n                      TestBackend::Ir,   // backend\r\n                      TestBackend::Ir);  // serializer backend\r\n\r\n  // File names are taken from graph node names. Just make sure that we got one .dlc\r\n  // in the expected directory.\r\n  ASSERT_TRUE(std::filesystem::exists(qnn_dlc_dir));\r\n\r\n  int file_count = 0;\r\n  for (const auto& entry : std::filesystem::directory_iterator(qnn_dlc_dir)) {\r\n    EXPECT_TRUE(entry.is_regular_file());\r\n    EXPECT_EQ(entry.path().extension(), \".dlc\");\r\n    ++file_count;\r\n  }\r\n  EXPECT_EQ(file_count, 1);\r\n}\r\n\r\n// Test that QNN Saver generates the expected files for a model meant to run on any QNN backend.\r\nTEST(QnnSaverBackendTests, QnnSaver_OutputFiles) {\r\n  const std::filesystem::path qnn_saver_output_dir = \"saver_output\";\r\n\r\n  // Remove pre-existing QNN Saver output files. Note that fs::remove_all() can handle non-existing paths.\r\n  std::filesystem::remove_all(qnn_saver_output_dir);\r\n  ASSERT_FALSE(std::filesystem::exists(qnn_saver_output_dir));\r\n\r\n  InitNHWCResizeModel(ORT_MODEL_FOLDER \"nhwc_resize_sizes_opset18.onnx\",\r\n                      TestBackend::Saver,   // backend\r\n                      TestBackend::Saver);  // serializer_backend\r\n\r\n  // Check that QNN Saver output files exist.\r\n  EXPECT_TRUE(std::filesystem::exists(qnn_saver_output_dir / \"saver_output.c\"));\r\n  EXPECT_TRUE(std::filesystem::exists(qnn_saver_output_dir / \"params.bin\"));\r\n}\r\n\r\n#endif  // !defined(ORT_MINIMAL_BUILD)\r\n\r\n}  // namespace test\r\n}  // namespace onnxruntime\r\n"}
{"path":"\\\\?\\C:\\Users\\Naomi\\Desktop\\ONNX\\onnxruntime\\onnxruntime\\test\\python\\quantization\\test_op_matmul_2bits.py","line":8,"char":1,"code":"RUFF","severity":"warning","name":"I001","description":"Import block is un-sorted or un-formatted.\nSee https://docs.astral.sh/ruff/rules/unsorted-imports"}
{"path":"\\\\?\\C:\\Users\\Naomi\\Desktop\\ONNX\\onnxruntime\\onnxruntime\\test\\python\\quantization\\test_op_matmul_2bits.py","line":null,"char":null,"code":"RUFF","severity":"warning","name":"format","description":"Run `lintrunner -a` to apply this patch.","original":"#!/usr/bin/env python\r\n# -------------------------------------------------------------------------\r\n# Copyright (c) Microsoft Corporation. All rights reserved.\r\n# Licensed under the MIT License. See License.txt in the project root for\r\n# license information.\r\n# --------------------------------------------------------------------------\r\n\r\nimport tempfile\r\nimport unittest\r\nfrom pathlib import Path\r\n\r\nimport numpy as np\r\nimport onnx\r\nfrom onnx import TensorProto, helper\r\nfrom op_test_utils import TestDataFeeds, check_model_correctness, check_op_type_count, check_qtype_by_node_type\r\n\r\nfrom onnxruntime.quantization import quant_utils\r\n\r\n\r\nclass TestOpMatMul2Bits(unittest.TestCase):\r\n    @classmethod\r\n    def setUpClass(cls):\r\n        cls._tmp_model_dir = tempfile.TemporaryDirectory(prefix=\"test_matmul2bits.\")\r\n\r\n    @classmethod\r\n    def tearDownClass(cls):\r\n        cls._tmp_model_dir.cleanup()\r\n\r\n    def fill_int2_data(self, shape: int | tuple[int, ...], symmetric: bool) -> np.ndarray:\r\n        line = np.zeros(shape)\r\n        line = line.reshape(-1)\r\n\r\n        if symmetric:\r\n            # For 2-bit symmetric: values in range [-2, 1] (excluding 0)\r\n            v = -2.0\r\n            for i in range(line.shape[0]):\r\n                if v == 0:  # Skip 0 for symmetric quantization\r\n                    v += 1\r\n                line[i] = v\r\n                v += 1\r\n                if v >= 2:\r\n                    v = -2\r\n        else:\r\n            # For 2-bit unsigned: values in range [0, 3]\r\n            v = 0.0\r\n            for i in range(line.shape[0]):\r\n                line[i] = v\r\n                v += 1\r\n                if v >= 4:\r\n                    v = 0\r\n\r\n        return line.reshape(shape)\r\n\r\n    def input_feeds(\r\n        self,\r\n        n: int,\r\n        name2shape: dict[str, int | tuple[int, ...]],\r\n        low: int = -1,\r\n        high: int = 2,\r\n        dtype: type = np.float32,\r\n    ) -> TestDataFeeds:\r\n        input_data_list = []\r\n        for _i in range(n):\r\n            inputs = {}\r\n            for name, shape in name2shape.items():\r\n                inputs.update({name: np.random.randint(low, high, shape).astype(dtype)})\r\n            input_data_list.extend([inputs])\r\n        dr = TestDataFeeds(input_data_list)\r\n        return dr\r\n\r\n    def construct_model_matmul(self, output_model_path: str, symmetric: bool, k: int = 52, n: int = 288) -> None:\r\n        #      (input)\r\n        #         |\r\n        #       MatMul\r\n        #         |\r\n        #      (output)\r\n        input_name = \"input\"\r\n        output_name = \"output\"\r\n        initializers = []\r\n\r\n        def make_matmul(\r\n            input_name, weight_shape: int | tuple[int, ...], weight_name: str, output_name: str, node_name: str\r\n        ):\r\n            weight_data = self.fill_int2_data(weight_shape, symmetric).astype(np.float32)\r\n            initializers.append(onnx.numpy_helper.from_array(weight_data, name=weight_name))\r\n            return onnx.helper.make_node(\r\n                \"MatMul\",\r\n                [input_name, weight_name],\r\n                [output_name],\r\n                node_name,\r\n            )\r\n\r\n        in_features = k\r\n        out_features = n\r\n        # make MatMul node\r\n        matmul_node = make_matmul(\r\n            input_name,\r\n            [in_features, out_features],\r\n            \"linear1.weight\",\r\n            output_name,\r\n            \"MatMul_0\",\r\n        )\r\n\r\n        # make graph\r\n        input_tensor = helper.make_tensor_value_info(input_name, TensorProto.FLOAT, [-1, in_features])\r\n        output_tensor = helper.make_tensor_value_info(output_name, TensorProto.FLOAT, [-1, out_features])\r\n        graph_name = \"matmul_2bits_test\"\r\n        graph = helper.make_graph(\r\n            [matmul_node],\r\n            graph_name,\r\n            [input_tensor],\r\n            [output_tensor],\r\n            initializer=initializers,\r\n        )\r\n        # blocked quantization requires DQ op set >= 21\r\n        model = helper.make_model(graph, opset_imports=[helper.make_opsetid(\"\", 21)])\r\n        model.ir_version = 10  # use stable onnx ir version\r\n\r\n        onnx.save(model, output_model_path)\r\n\r\n    def quant_test(\r\n        self,\r\n        model_fp32_path: str,\r\n        data_reader: TestDataFeeds,\r\n        block_size: int,\r\n        is_symmetric: bool,\r\n        quant_format: quant_utils.QuantFormat = quant_utils.QuantFormat.QOperator,\r\n        op_types_to_quantize: tuple[str, ...] = (\"MatMul\",),\r\n        quant_axes: tuple[tuple[str, int], ...] = ((\"MatMul\", 0), (\"Gather\", 1)),\r\n        rtol: float = 0.01,\r\n        atol: float = 0.05,\r\n        suffix: str = \"\",\r\n    ):\r\n        use_qdq = quant_format == quant_utils.QuantFormat.QDQ\r\n        name_prefix = \"QDQ\" if use_qdq else \"QOperator\"\r\n        model_int2_path = str(\r\n            Path(self._tmp_model_dir.name)\r\n            .joinpath(f\"{name_prefix}_bs{block_size}_{is_symmetric}{suffix}.onnx\")\r\n            .absolute()\r\n        )\r\n\r\n        # Quantize fp32 model to int2 model\r\n        from onnxruntime.quantization import matmul_nbits_quantizer  # noqa: PLC0415\r\n\r\n        model = quant_utils.load_model_with_shape_infer(Path(model_fp32_path))\r\n\r\n        quant_config = matmul_nbits_quantizer.DefaultWeightOnlyQuantConfig(\r\n            block_size=block_size,\r\n            is_symmetric=is_symmetric,\r\n            quant_format=quant_format,\r\n            op_types_to_quantize=op_types_to_quantize,\r\n            quant_axes=quant_axes,\r\n            bits=2,\r\n        )\r\n\r\n        quant = matmul_nbits_quantizer.MatMulNBitsQuantizer(model, bits=2, algo_config=quant_config)\r\n        quant.process()\r\n        quant.model.save_model_to_file(model_int2_path, False)\r\n\r\n        if \"Gather\" in op_types_to_quantize:\r\n            quant_nodes = {\"GatherBlockQuantized\": 1}\r\n        else:\r\n            quant_nodes = {\"DequantizeLinear\": 1, \"MatMul\": 1} if use_qdq else {\"MatMulNBits\": 1}\r\n        check_op_type_count(self, model_int2_path, **quant_nodes)\r\n\r\n        if use_qdq:\r\n            # Note: For 2-bit, we might need to use INT8/UINT8 as the actual storage type\r\n            # since INT2/UINT2 might not be directly supported\r\n            dq_qtype = onnx.TensorProto.INT8 if is_symmetric else onnx.TensorProto.UINT8\r\n            dqnode_io_qtypes = (\r\n                {\r\n                    \"DequantizeLinear\": [\r\n                        [\"i\", 0, dq_qtype],\r\n                    ]\r\n                }\r\n                if is_symmetric\r\n                else {\r\n                    \"DequantizeLinear\": [\r\n                        [\"i\", 0, dq_qtype],\r\n                        [\"i\", 2, dq_qtype],\r\n                    ]\r\n                }\r\n            )\r\n            check_qtype_by_node_type(self, model_int2_path, dqnode_io_qtypes)\r\n            for op in quant.model.opset_import():\r\n                if op.domain in [None, \"\", \"ai.onnx\"] and op.version < 21:\r\n                    self.fail(f\"In QDQ format {op.domain} opset should be >= 21\")\r\n\r\n        data_reader.rewind()\r\n\r\n        try:\r\n            check_model_correctness(\r\n                self,\r\n                model_fp32_path,\r\n                model_int2_path,\r\n                data_reader.get_next(),\r\n                rtol,\r\n                atol,\r\n            )\r\n        except Exception as exception:\r\n            if \"2b quantization not yet supported on this hardware platform!\" in exception.args[0]:\r\n                # Currently we don't have int2 quantization support on all platforms, has to tolerate this exception\r\n                pass\r\n            else:\r\n                raise exception\r\n\r\n    def test_quantize_matmul_int2_symmetric(self):\r\n        np.random.seed(13)\r\n\r\n        model_fp32_path = str(Path(self._tmp_model_dir.name).joinpath(\"matmul_fp32_symmetric.onnx\").absolute())\r\n        self.construct_model_matmul(model_fp32_path, symmetric=True)\r\n        data_reader = self.input_feeds(1, {\"input\": (100, 52)})\r\n        self.quant_test(model_fp32_path, data_reader, 32, True, rtol=0.02, atol=0.1)\r\n\r\n    def test_quantize_matmul_int2_offsets(self):\r\n        model_fp32_path = str(Path(self._tmp_model_dir.name).joinpath(\"matmul_fp32_offset.onnx\").absolute())\r\n        self.construct_model_matmul(model_fp32_path, symmetric=False)\r\n        data_reader = self.input_feeds(1, {\"input\": (100, 52)})\r\n        self.quant_test(model_fp32_path, data_reader, 32, False, rtol=0.02, atol=0.1)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    unittest.main()\r\n","replacement":"#!/usr/bin/env python\r\n# -------------------------------------------------------------------------\r\n# Copyright (c) Microsoft Corporation. All rights reserved.\r\n# Licensed under the MIT License. See License.txt in the project root for\r\n# license information.\r\n# --------------------------------------------------------------------------\r\n\r\nimport tempfile\r\nimport unittest\r\nfrom pathlib import Path\r\n\r\nimport numpy as np\r\nimport onnx\r\nfrom onnx import TensorProto, helper\r\nfrom onnxruntime.quantization import quant_utils\r\nfrom op_test_utils import TestDataFeeds, check_model_correctness, check_op_type_count, check_qtype_by_node_type\r\n\r\n\r\nclass TestOpMatMul2Bits(unittest.TestCase):\r\n    @classmethod\r\n    def setUpClass(cls):\r\n        cls._tmp_model_dir = tempfile.TemporaryDirectory(prefix=\"test_matmul2bits.\")\r\n\r\n    @classmethod\r\n    def tearDownClass(cls):\r\n        cls._tmp_model_dir.cleanup()\r\n\r\n    def fill_int2_data(self, shape: int | tuple[int, ...], symmetric: bool) -> np.ndarray:\r\n        line = np.zeros(shape)\r\n        line = line.reshape(-1)\r\n\r\n        if symmetric:\r\n            # For 2-bit symmetric: values in range [-2, 1] (excluding 0)\r\n            v = -2.0\r\n            for i in range(line.shape[0]):\r\n                if v == 0:  # Skip 0 for symmetric quantization\r\n                    v += 1\r\n                line[i] = v\r\n                v += 1\r\n                if v >= 2:\r\n                    v = -2\r\n        else:\r\n            # For 2-bit unsigned: values in range [0, 3]\r\n            v = 0.0\r\n            for i in range(line.shape[0]):\r\n                line[i] = v\r\n                v += 1\r\n                if v >= 4:\r\n                    v = 0\r\n\r\n        return line.reshape(shape)\r\n\r\n    def input_feeds(\r\n        self,\r\n        n: int,\r\n        name2shape: dict[str, int | tuple[int, ...]],\r\n        low: int = -1,\r\n        high: int = 2,\r\n        dtype: type = np.float32,\r\n    ) -> TestDataFeeds:\r\n        input_data_list = []\r\n        for _i in range(n):\r\n            inputs = {}\r\n            for name, shape in name2shape.items():\r\n                inputs.update({name: np.random.randint(low, high, shape).astype(dtype)})\r\n            input_data_list.extend([inputs])\r\n        dr = TestDataFeeds(input_data_list)\r\n        return dr\r\n\r\n    def construct_model_matmul(self, output_model_path: str, symmetric: bool, k: int = 52, n: int = 288) -> None:\r\n        #      (input)\r\n        #         |\r\n        #       MatMul\r\n        #         |\r\n        #      (output)\r\n        input_name = \"input\"\r\n        output_name = \"output\"\r\n        initializers = []\r\n\r\n        def make_matmul(\r\n            input_name, weight_shape: int | tuple[int, ...], weight_name: str, output_name: str, node_name: str\r\n        ):\r\n            weight_data = self.fill_int2_data(weight_shape, symmetric).astype(np.float32)\r\n            initializers.append(onnx.numpy_helper.from_array(weight_data, name=weight_name))\r\n            return onnx.helper.make_node(\r\n                \"MatMul\",\r\n                [input_name, weight_name],\r\n                [output_name],\r\n                node_name,\r\n            )\r\n\r\n        in_features = k\r\n        out_features = n\r\n        # make MatMul node\r\n        matmul_node = make_matmul(\r\n            input_name,\r\n            [in_features, out_features],\r\n            \"linear1.weight\",\r\n            output_name,\r\n            \"MatMul_0\",\r\n        )\r\n\r\n        # make graph\r\n        input_tensor = helper.make_tensor_value_info(input_name, TensorProto.FLOAT, [-1, in_features])\r\n        output_tensor = helper.make_tensor_value_info(output_name, TensorProto.FLOAT, [-1, out_features])\r\n        graph_name = \"matmul_2bits_test\"\r\n        graph = helper.make_graph(\r\n            [matmul_node],\r\n            graph_name,\r\n            [input_tensor],\r\n            [output_tensor],\r\n            initializer=initializers,\r\n        )\r\n        # blocked quantization requires DQ op set >= 21\r\n        model = helper.make_model(graph, opset_imports=[helper.make_opsetid(\"\", 21)])\r\n        model.ir_version = 10  # use stable onnx ir version\r\n\r\n        onnx.save(model, output_model_path)\r\n\r\n    def quant_test(\r\n        self,\r\n        model_fp32_path: str,\r\n        data_reader: TestDataFeeds,\r\n        block_size: int,\r\n        is_symmetric: bool,\r\n        quant_format: quant_utils.QuantFormat = quant_utils.QuantFormat.QOperator,\r\n        op_types_to_quantize: tuple[str, ...] = (\"MatMul\",),\r\n        quant_axes: tuple[tuple[str, int], ...] = ((\"MatMul\", 0), (\"Gather\", 1)),\r\n        rtol: float = 0.01,\r\n        atol: float = 0.05,\r\n        suffix: str = \"\",\r\n    ):\r\n        use_qdq = quant_format == quant_utils.QuantFormat.QDQ\r\n        name_prefix = \"QDQ\" if use_qdq else \"QOperator\"\r\n        model_int2_path = str(\r\n            Path(self._tmp_model_dir.name)\r\n            .joinpath(f\"{name_prefix}_bs{block_size}_{is_symmetric}{suffix}.onnx\")\r\n            .absolute()\r\n        )\r\n\r\n        # Quantize fp32 model to int2 model\r\n        from onnxruntime.quantization import matmul_nbits_quantizer  # noqa: PLC0415\r\n\r\n        model = quant_utils.load_model_with_shape_infer(Path(model_fp32_path))\r\n\r\n        quant_config = matmul_nbits_quantizer.DefaultWeightOnlyQuantConfig(\r\n            block_size=block_size,\r\n            is_symmetric=is_symmetric,\r\n            quant_format=quant_format,\r\n            op_types_to_quantize=op_types_to_quantize,\r\n            quant_axes=quant_axes,\r\n            bits=2,\r\n        )\r\n\r\n        quant = matmul_nbits_quantizer.MatMulNBitsQuantizer(model, bits=2, algo_config=quant_config)\r\n        quant.process()\r\n        quant.model.save_model_to_file(model_int2_path, False)\r\n\r\n        if \"Gather\" in op_types_to_quantize:\r\n            quant_nodes = {\"GatherBlockQuantized\": 1}\r\n        else:\r\n            quant_nodes = {\"DequantizeLinear\": 1, \"MatMul\": 1} if use_qdq else {\"MatMulNBits\": 1}\r\n        check_op_type_count(self, model_int2_path, **quant_nodes)\r\n\r\n        if use_qdq:\r\n            # Note: For 2-bit, we might need to use INT8/UINT8 as the actual storage type\r\n            # since INT2/UINT2 might not be directly supported\r\n            dq_qtype = onnx.TensorProto.INT8 if is_symmetric else onnx.TensorProto.UINT8\r\n            dqnode_io_qtypes = (\r\n                {\r\n                    \"DequantizeLinear\": [\r\n                        [\"i\", 0, dq_qtype],\r\n                    ]\r\n                }\r\n                if is_symmetric\r\n                else {\r\n                    \"DequantizeLinear\": [\r\n                        [\"i\", 0, dq_qtype],\r\n                        [\"i\", 2, dq_qtype],\r\n                    ]\r\n                }\r\n            )\r\n            check_qtype_by_node_type(self, model_int2_path, dqnode_io_qtypes)\r\n            for op in quant.model.opset_import():\r\n                if op.domain in [None, \"\", \"ai.onnx\"] and op.version < 21:\r\n                    self.fail(f\"In QDQ format {op.domain} opset should be >= 21\")\r\n\r\n        data_reader.rewind()\r\n\r\n        try:\r\n            check_model_correctness(\r\n                self,\r\n                model_fp32_path,\r\n                model_int2_path,\r\n                data_reader.get_next(),\r\n                rtol,\r\n                atol,\r\n            )\r\n        except Exception as exception:\r\n            if \"2b quantization not yet supported on this hardware platform!\" in exception.args[0]:\r\n                # Currently we don't have int2 quantization support on all platforms, has to tolerate this exception\r\n                pass\r\n            else:\r\n                raise exception\r\n\r\n    def test_quantize_matmul_int2_symmetric(self):\r\n        np.random.seed(13)\r\n\r\n        model_fp32_path = str(Path(self._tmp_model_dir.name).joinpath(\"matmul_fp32_symmetric.onnx\").absolute())\r\n        self.construct_model_matmul(model_fp32_path, symmetric=True)\r\n        data_reader = self.input_feeds(1, {\"input\": (100, 52)})\r\n        self.quant_test(model_fp32_path, data_reader, 32, True, rtol=0.02, atol=0.1)\r\n\r\n    def test_quantize_matmul_int2_offsets(self):\r\n        model_fp32_path = str(Path(self._tmp_model_dir.name).joinpath(\"matmul_fp32_offset.onnx\").absolute())\r\n        self.construct_model_matmul(model_fp32_path, symmetric=False)\r\n        data_reader = self.input_feeds(1, {\"input\": (100, 52)})\r\n        self.quant_test(model_fp32_path, data_reader, 32, False, rtol=0.02, atol=0.1)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    unittest.main()\r\n"}
{"path":"\\\\?\\C:\\Users\\Naomi\\Desktop\\ONNX\\onnxruntime\\onnxruntime\\core\\providers\\webgpu\\nn\\conv2d_mm.cc","line":null,"char":null,"code":"CLANGFORMAT","severity":"warning","name":"format","description":"See https://clang.llvm.org/docs/ClangFormat.html.\nRun `lintrunner -a` to apply this patch.","original":"// Copyright (c) Microsoft Corporation. All rights reserved.\r\n// Licensed under the MIT License.\r\n#include <string>\r\n#include <vector>\r\n#include <iterator>\r\n#include <algorithm>\r\n#include \"core/providers/webgpu/nn/conv2d_mm.h\"\r\n#include \"core/providers/webgpu/shader_helper.h\"\r\n#include \"core/providers/webgpu/webgpu_supported_types.h\"\r\n#include \"core/providers/webgpu/nn/activation_util.h\"\r\n#include \"core/providers/webgpu/math/matmul_packed.h\"\r\n#include \"core/providers/webgpu/nn/conv_utils.h\"\r\n#include \"core/providers/webgpu/nn/fuse_utils.h\"\r\n#include \"core/providers/webgpu/webgpu_utils.h\"\r\n#include \"core/providers/webgpu/math/gemm_utils.h\"\r\n\r\nnamespace onnxruntime {\r\nnamespace webgpu {\r\nstd::string Conv2dMMProgram::Conv2dCommonSnippet(const ShaderVariableHelper& x, const ShaderVariableHelper& w, const Activation& activation, std::string data_type, uint32_t inner_element_size_x, uint32_t inner_element_size_w, uint32_t inner_element_size) const {\r\n  auto get_x_snippet = [&](int32_t inner_element_size) -> std::string {\r\n    switch (inner_element_size) {\r\n      case 1:\r\n        return \"resData = \" + x.GetByOffset(\"xIndex\") + \";\";\r\n      case 3:\r\n        return \"resData = vec3<x_element_t>(\" + x.GetByOffset(\"xIndex\") + \", \" + x.GetByOffset(\"xIndex + 1\") + \", \" + x.GetByOffset(\"xIndex + 2\") + \");\";\r\n      case 4:\r\n        return \"resData = \" + x.GetByOffset(\"xIndex\") + \";\\n \";\r\n      default:\r\n        ORT_THROW(\"inner_element_size\", inner_element_size, \" is not supported.\");\r\n    }\r\n  };\r\n  auto get_w_snippet = [&](int32_t inner_element_size) -> std::string {\r\n    switch (inner_element_size) {\r\n      case 1:\r\n        return \"return \" + w.GetByOffset(\"row * i32(uniforms.w_shape[3]) + colIn\") + \";\\n\";\r\n      case 4:\r\n        return \"return \" + w.GetByOffset(\"row * i32(uniforms.w_shape[3])  + colIn\") + \";\\n\";\r\n      default:\r\n        ORT_THROW(\"inner_element_size \", inner_element_size, \" is not supported.\");\r\n    }\r\n  };\r\n  const std::string coord_a_snippet = is_channels_last_ ? \"let coord = vec4<i32>(batch, xRow, xCol, xCh / \" + std::to_string(inner_element_size_x == 3 ? 4 : inner_element_size_x) + \");\" : \"let coord = vec4<i32>(batch, xCh, xRow, xCol);\";\r\n  const std::string coord_res_snippet = is_channels_last_ ? \"let coords = vec4<i32>(batch, row / outWidth, row % outWidth, col / \" + std::to_string(inner_element_size) + \");\" : \"let coords = vec4<i32>(batch, row, col / outWidth, col % outWidth);\";\r\n\r\n  const std::string xHeight = is_channels_last_ ? \"i32(uniforms.x_shape[1])\" : \"i32(uniforms.x_shape[2])\";\r\n  const std::string xWidth = is_channels_last_ ? \"i32(uniforms.x_shape[2])\" : \"i32(uniforms.x_shape[3])\";\r\n  const std::string row = is_channels_last_ ? \"row\" : \"col\";\r\n  const std::string col = is_channels_last_ ? \"col\" : \"row\";\r\n  std::stringstream read_x_snippet;\r\n  read_x_snippet\r\n      << \"let inChannels = i32(uniforms.w_shape[2]);\\n\"\r\n      << \"let outWidth = \" << (is_channels_last_ ? \"i32(uniforms.result_shape[2])\" : \"i32(uniforms.result_shape[3])\") << \";\\n\"\r\n      << \"let outRow = \" << row << \" / outWidth;\\n \"\r\n      << \"let outCol = \" << row << \" % outWidth;\\n\"\r\n      << \"let WRow = \" << col << \" / (i32(uniforms.w_shape[1]) * inChannels);\\n\"\r\n      << \"let WCol = \" << col << \" / inChannels % i32(uniforms.w_shape[1]);\\n\"\r\n      << \"let xRow = outRow * i32(uniforms.strides[0]) + i32(uniforms.dilations[0]) * WRow - i32(uniforms.pads[0]);\\n\"\r\n      << \"let xCol = outCol * i32(uniforms.strides[1]) + i32(uniforms.dilations[1]) * WCol - i32(uniforms.pads[1]);\\n\"\r\n      << \"let xCh = \" << col << \" % inChannels;\\n\"\r\n      << \"var resData = \" << TypeSnippet(inner_element_size_x, data_type) << \"(0.0);\\n \"\r\n      << \"// The bounds checking is always needed since we use it to pad zero for\\n\"\r\n      << \"// the \\\" same \\\" padding type.\\n\"\r\n      << \"if (xRow >= 0 && xRow < \" << xHeight << \" && xCol >= 0 && xCol < \" << xWidth << \") {\\n\"\r\n      << \"  \" << coord_a_snippet << \"\\n\"\r\n      << \"  let xIndex = getIndexFromCoords4D(coord, vec4<i32>(uniforms.x_shape));\\n\"\r\n      << \"  \" << get_x_snippet(inner_element_size_x)\r\n      << \"}\\n\"\r\n      << \"return resData;\";\r\n  std::stringstream sample_x;\r\n  if (is_channels_last_) {\r\n    if (fit_a_outer_ && fit_inner_) {\r\n      sample_x << \"let col = colIn * \" << inner_element_size_x << \";\\n\"\r\n               << read_x_snippet.str();\r\n    } else {\r\n      sample_x << \"let col = colIn * \" << inner_element_size_x << \";\\n\"\r\n               << \"if(row < i32(uniforms.dim_a_outer) && col < i32(uniforms.dim_inner)) {\\n\"\r\n               << \"  \" << read_x_snippet.str() << \"\\n\"\r\n               << \"}\\n\"\r\n               << \"return \" << TypeSnippet(inner_element_size_x, data_type) << \"(0.0);\\n\";\r\n    }\r\n  } else {\r\n    if (fit_inner_ && fit_b_outer_) {\r\n      sample_x << \"let col = colIn * \" << inner_element_size_x << \";\\n\"\r\n               << read_x_snippet.str();\r\n    } else {\r\n      sample_x << \"let col = colIn * \" << inner_element_size_x << \";\\n\"\r\n               << \"if (row < i32(uniforms.dim_inner) && col < i32(uniforms.dim_b_outer)) {\\n\"\r\n               << \"  \" << read_x_snippet.str() << \"\\n\"\r\n               << \"}\\n\"\r\n               << \"return \" << TypeSnippet(inner_element_size_x, data_type) << \"(0.0);\\n\";\r\n    }\r\n  }\r\n  std::stringstream sample_w;\r\n  if (is_channels_last_) {\r\n    if (fit_inner_ && fit_b_outer_) {\r\n      sample_w << get_w_snippet(inner_element_size_w);\r\n    } else {\r\n      sample_w << \"let col = colIn * \" << inner_element_size_w << \";\\n\"\r\n               << \"if(row < i32(uniforms.dim_inner) && col < i32(uniforms.dim_b_outer)) {\\n\"\r\n               << \"  \" << get_w_snippet(inner_element_size_w) << \"\\n\"\r\n               << \"}\\n\"\r\n               << \"return \" << TypeSnippet(inner_element_size_w, data_type) << \"(0.0);\\n\";\r\n    }\r\n  } else {\r\n    sample_w << \"let col = colIn * \" << inner_element_size_w << \";\\n\"\r\n             << \"if (row < i32(uniforms.dim_inner) && col < i32(uniforms.dim_a_outer)) {\\n\"\r\n             << \"  \" << get_w_snippet(inner_element_size_w) << \"\\n\"\r\n             << \"}\\n\"\r\n             << \"return \" << TypeSnippet(inner_element_size_w, data_type) << \"(0.0);\\n\";\r\n  }\r\n  const std::string res_type = TypeSnippet(inner_element_size, data_type);\r\n  const std::string a_type = is_channels_last_ ? TypeSnippet(inner_element_size_x, data_type) : TypeSnippet(inner_element_size_w, data_type);\r\n  const std::string b_type = is_channels_last_ ? TypeSnippet(inner_element_size_w, data_type) : TypeSnippet(inner_element_size_x, data_type);\r\n  const std::string apply_activation = GetActivationSnippet(activation, res_type, data_type);\r\n  std::stringstream user_code;\r\n  user_code << \"fn mm_readA(batch : i32, row : i32, colIn : i32) -> \" << a_type << \" {\\n\"\r\n            << (is_channels_last_ ? sample_x.str() : sample_w.str())\r\n            << \"}\\n\"\r\n            << \"\\n\"\r\n            << \"fn mm_readB(batch : i32, row : i32, colIn : i32) -> \" << b_type << \" {\\n\"\r\n            << (is_channels_last_ ? sample_w.str() : sample_x.str())\r\n            << \"}\\n\"\r\n            << \"\\n\"\r\n            << \"fn mm_write(batch : i32, row : i32, colIn : i32, valueIn : \" << res_type << \") {\\n\"\r\n            << \"  let col = colIn * \" << inner_element_size << \";\\n\"\r\n            << \"  if(row < i32(uniforms.dim_a_outer) && col < i32(uniforms.dim_b_outer)) {\\n\"\r\n            << \"    var value = valueIn;\\n\"\r\n            << \"    let outWidth = \" << (is_channels_last_ ? \" i32(uniforms.result_shape[2]) \" : \" i32(uniforms.result_shape[3]) \") << \";\\n\"\r\n            << \"    \" << coord_res_snippet << \"\\n\"\r\n            << \"    \" << BiasSnippet(has_bias_) << \"\\n\"\r\n            << \"    \" << apply_activation << \"\\n\"\r\n            << \"    setOutputAtCoords(coords[0], coords[1], coords[2], coords[3], value);\\n\"\r\n            << \"  }\\n\"\r\n            << \"}\\n\";\r\n  return user_code.str();\r\n}\r\n\r\nStatus Conv2dMMProgram::GenerateShaderCode(ShaderHelper& shader) const {\r\n  std::stringstream declaration_functions;\r\n  declaration_functions << \"fn setOutputAtIndex(flatIndex : i32, value : \" << (is_vec4_ ? \"vec4<x_element_t>\" : \"x_element_t\") << \") {\\n\"\r\n                        << \"  result[flatIndex] = \" << (is_vec4_ ? \"vec4<x_element_t>\" : \"x_element_t\") << \"(value);\\n\"\r\n                        << \"}\\n\"\r\n                        << \"fn setOutputAtCoords(d0 : i32, d1 : i32, d2 : i32, d3 : i32, value : \" << (is_vec4_ ? \"vec4<x_element_t>\" : \"x_element_t\") << \"){\\n\"\r\n                        << \"  let flatIndex = getOutputIndexFromCoords(vec4<i32>(d0, d1, d2, d3));\\n\"\r\n                        << \"  setOutputAtIndex(flatIndex, value);\\n\"\r\n                        << \"}\\n\";\r\n  const auto& x = shader.AddInput(\"x\", ShaderUsage::UseUniform | ShaderUsage::UseShapeAndStride | ShaderUsage::UseIndicesTypeAlias | ShaderUsage::UseValueTypeAlias | ShaderUsage::UseElementTypeAlias);\r\n  const auto& w = shader.AddInput(\"w\", ShaderUsage::UseUniform | ShaderUsage::UseShapeAndStride | ShaderUsage::UseIndicesTypeAlias | ShaderUsage::UseValueTypeAlias);\r\n  std::vector<const ShaderVariableHelper*> inputs = {&x, &w};\r\n  ORT_IGNORE_RETURN_VALUE(shader.AddOutput(\"result\", ShaderUsage::UseUniform | ShaderUsage::UseShapeAndStride | ShaderUsage::UseIndicesTypeAlias));\r\n  if (has_bias_) {\r\n    const auto& bias = shader.AddInput(\"bias\", ShaderUsage::UseUniform | ShaderUsage::UseIndicesTypeAlias | ShaderUsage::UseValueTypeAlias | ShaderUsage::UseElementTypeAlias);\r\n    inputs.push_back(&bias);\r\n    declaration_functions << \"fn getBiasByOutputCoords(coords : vec4<i32>) -> bias_value_t {\" << \"\\n\"\r\n                          << \"  return bias[\" << (is_channels_last_ ? \"coords.w\" : \"coords.y\") << \"];\\n\"\r\n                          << \"}\";\r\n  }\r\n  shader.AdditionalImplementation()\r\n      << UtilFunctions(\"uniforms.result_stride\")\r\n      << declaration_functions.str()\r\n      << Conv2dCommonSnippet(x, w, activation_, \"x_element_t\", element_size_[0], element_size_[1], element_size_[2]);\r\n  std::string data_type = \"x_element_t\";\r\n\r\n  return is_vec4_ ? MakeMatMulPackedVec4Source(shader, elements_per_thread_, WorkgroupSizeX(), WorkgroupSizeY(), data_type, /* batch_dims = */ nullptr, /* transpose_a = */ !is_channels_last_, /* transpose_b = */ false, 1.0f, true, 4, tile_inner_)\r\n                  : MakeMatMulPackedSource(shader, elements_per_thread_, WorkgroupSizeX(), WorkgroupSizeY(), data_type, /* batch_dims = */ nullptr, /*transpose_a = */ !is_channels_last_, /* transpose_b = */ false, 1.0f, true, tile_inner_, /* split_t = */ false, 0);\r\n}\r\n\r\nConv2dMMProgram CreateConv2dMMProgram(const Activation& activation, const std::vector<const Tensor*>& inputs, const std::vector<uint32_t>& pads, const std::vector<uint32_t>& strides, const std::vector<uint32_t>& dilations, Tensor* output, uint32_t dim_a_outer, uint32_t dim_b_outer, uint32_t dim_inner, bool is_channels_last, const std::vector<TensorShape>& input_output_shapes) {\r\n  const auto* input = inputs[0];\r\n  const auto* weight = inputs[1];\r\n  bool has_bias = inputs.size() > 2;\r\n  const auto* bias = has_bias ? inputs[2] : nullptr;\r\n  const auto& input_shape = input_output_shapes[0];\r\n  auto in_channels = is_channels_last ? input_shape[3] : input_shape[1];\r\n  const auto& output_shape = has_bias ? input_output_shapes[3] : input_output_shapes[2];\r\n  auto batch_size = output_shape[0];\r\n  const auto output_width = is_channels_last ? output_shape[2] : output_shape[3];\r\n  const auto output_height = is_channels_last ? output_shape[1] : output_shape[2];\r\n  const auto output_channels = is_channels_last ? output_shape[3] : output_shape[1];\r\n  // TODO: enable vec4 for NCHW\r\n  const bool is_vec4 = is_channels_last && (in_channels % 4 == 0 || in_channels % 3 == 0) && output_channels % 4 == 0;\r\n\r\n  // TODO: fine tune size\r\n  const auto dispatch_x = is_channels_last ? output_channels : output_width * output_height;\r\n  const auto dispatch_y = is_channels_last ? output_width * output_height : output_channels;\r\n  std::vector<uint32_t> workgroup_size = {8, 8, 1};\r\n  InlinedVector<int64_t> elements_per_thread = {4, static_cast<int64_t>(dim_a_outer <= 8 ? 1 : 4), 1};\r\n  auto integer_ceil = [](int64_t a, int64_t b) -> int64_t { return (a + b - 1) / b; };\r\n\r\n  const std::vector<uint32_t> dispatch = {\r\n      static_cast<uint32_t>(integer_ceil(integer_ceil(dispatch_x, workgroup_size[0]), elements_per_thread[0])),\r\n      static_cast<uint32_t>(integer_ceil(integer_ceil(dispatch_y, workgroup_size[1]), elements_per_thread[1])),\r\n      static_cast<uint32_t>(integer_ceil(integer_ceil(batch_size, workgroup_size[2]), elements_per_thread[2])),\r\n  };\r\n\r\n  uint32_t inner_element_size = is_vec4 ? (is_channels_last && in_channels % 4 != 0 ? 3 : 4) : 1;\r\n  auto tile_a_outer = static_cast<uint32_t>(workgroup_size[1] * elements_per_thread[1]);\r\n  auto tile_b_outer = static_cast<uint32_t>(workgroup_size[0] * elements_per_thread[0]);\r\n  auto tile_inner = std::max(workgroup_size[0] * inner_element_size, workgroup_size[1]);\r\n  bool fit_a_outer = dim_a_outer % tile_a_outer == 0;\r\n  bool fit_b_outer = dim_b_outer % tile_b_outer == 0;\r\n  bool fit_inner = dim_inner % tile_inner == 0;\r\n  std::vector<uint32_t> element_size = {is_vec4 ? inner_element_size : 1, static_cast<uint32_t>(is_vec4 ? 4 : 1), static_cast<uint32_t>(is_vec4 ? 4 : 1)};\r\n  const auto components = is_vec4 ? 4 : 1;\r\n  const auto input_components = static_cast<int>(inner_element_size == 3 ? 1 : inner_element_size);\r\n  Conv2dMMProgram program(activation, tile_inner, fit_a_outer, fit_b_outer, fit_inner, is_channels_last, is_vec4, has_bias, std::move(element_size), std::move(elements_per_thread));\r\n  TensorShape reduced_input_shape = ReduceShapeByComponents(input_output_shapes[0], input_components);\r\n  TensorShape reduced_weight_shape = ReduceShapeByComponents(input_output_shapes[1], components);\r\n  TensorShape reduced_output_shape = ReduceShapeByComponents(input_output_shapes[has_bias ? 3 : 2], components);\r\n  program.AddInputs({{input, ProgramTensorMetadataDependency::TypeAndRank, reduced_input_shape, input_components}, {weight, ProgramTensorMetadataDependency::TypeAndRank, reduced_weight_shape, components}});\r\n  if (has_bias) {\r\n    TensorShape reduced_bias_shape = ReduceShapeByComponents(input_output_shapes[2], components);\r\n    program.AddInput({bias, ProgramTensorMetadataDependency::TypeAndRank, reduced_bias_shape, components});\r\n  }\r\n  const auto stringify = [](const std::vector<uint32_t>& vec) -> std::string {\r\n    std::ostringstream oss;\r\n    std::transform(vec.begin(), vec.end(), std::ostream_iterator<std::string>(oss, \",\"), [](uint32_t i) { return std::to_string(i); });\r\n    return oss.str();\r\n  };\r\n  program.CacheHint(activation.ToString(), is_channels_last, stringify({inner_element_size, static_cast<uint32_t>(is_vec4 ? 1 : 0), fit_a_outer, fit_b_outer, fit_inner, tile_a_outer, tile_a_outer, tile_inner, static_cast<uint32_t>(components)}))\r\n      .AddOutput({output, ProgramTensorMetadataDependency::TypeAndRank, reduced_output_shape, components})\r\n      .SetDispatchGroupSize(dispatch[0], dispatch[1], dispatch[2])\r\n      .SetWorkgroupSize(workgroup_size[0], workgroup_size[1], workgroup_size[2])\r\n      .AddUniformVariables({{static_cast<uint32_t>(dim_a_outer)},\r\n                            {static_cast<uint32_t>(dim_b_outer)},\r\n                            {static_cast<uint32_t>(dim_inner)},\r\n                            {pads},\r\n                            {strides},\r\n                            {dilations}});\r\n\r\n  return program;\r\n}\r\n\r\n}  // namespace webgpu\r\n}  // namespace onnxruntime\r\n","replacement":"// Copyright (c) Microsoft Corporation. All rights reserved.\r\n// Licensed under the MIT License.\r\n#include <string>\r\n#include <vector>\r\n#include <iterator>\r\n#include <algorithm>\r\n#include \"core/providers/webgpu/nn/conv2d_mm.h\"\r\n#include \"core/providers/webgpu/shader_helper.h\"\r\n#include \"core/providers/webgpu/webgpu_supported_types.h\"\r\n#include \"core/providers/webgpu/nn/activation_util.h\"\r\n#include \"core/providers/webgpu/math/matmul_packed.h\"\r\n#include \"core/providers/webgpu/nn/conv_utils.h\"\r\n#include \"core/providers/webgpu/nn/fuse_utils.h\"\r\n#include \"core/providers/webgpu/webgpu_utils.h\"\r\n#include \"core/providers/webgpu/math/gemm_utils.h\"\r\n\r\nnamespace onnxruntime {\r\nnamespace webgpu {\r\nstd::string Conv2dMMProgram::Conv2dCommonSnippet(const ShaderVariableHelper& x, const ShaderVariableHelper& w, const Activation& activation, std::string data_type, uint32_t inner_element_size_x, uint32_t inner_element_size_w, uint32_t inner_element_size) const {\r\n  auto get_x_snippet = [&](int32_t inner_element_size) -> std::string {\r\n    switch (inner_element_size) {\r\n      case 1:\r\n        return \"resData = \" + x.GetByOffset(\"xIndex\") + \";\";\r\n      case 3:\r\n        return \"resData = vec3<x_element_t>(\" + x.GetByOffset(\"xIndex\") + \", \" + x.GetByOffset(\"xIndex + 1\") + \", \" + x.GetByOffset(\"xIndex + 2\") + \");\";\r\n      case 4:\r\n        return \"resData = \" + x.GetByOffset(\"xIndex\") + \";\\n \";\r\n      default:\r\n        ORT_THROW(\"inner_element_size\", inner_element_size, \" is not supported.\");\r\n    }\r\n  };\r\n  auto get_w_snippet = [&](int32_t inner_element_size) -> std::string {\r\n    switch (inner_element_size) {\r\n      case 1:\r\n        return \"return \" + w.GetByOffset(\"row * i32(uniforms.w_shape[3]) + colIn\") + \";\\n\";\r\n      case 4:\r\n        return \"return \" + w.GetByOffset(\"row * i32(uniforms.w_shape[3])  + colIn\") + \";\\n\";\r\n      default:\r\n        ORT_THROW(\"inner_element_size \", inner_element_size, \" is not supported.\");\r\n    }\r\n  };\r\n  const std::string coord_a_snippet = is_channels_last_ ? \"let coord = vec4<i32>(batch, xRow, xCol, xCh / \" + std::to_string(inner_element_size_x == 3 ? 4 : inner_element_size_x) + \");\" : \"let coord = vec4<i32>(batch, xCh, xRow, xCol);\";\r\n  const std::string coord_res_snippet = is_channels_last_ ? \"let coords = vec4<i32>(batch, row / outWidth, row % outWidth, col / \" + std::to_string(inner_element_size) + \");\" : \"let coords = vec4<i32>(batch, row, col / outWidth, col % outWidth);\";\r\n\r\n  const std::string xHeight = is_channels_last_ ? \"i32(uniforms.x_shape[1])\" : \"i32(uniforms.x_shape[2])\";\r\n  const std::string xWidth = is_channels_last_ ? \"i32(uniforms.x_shape[2])\" : \"i32(uniforms.x_shape[3])\";\r\n  const std::string row = is_channels_last_ ? \"row\" : \"col\";\r\n  const std::string col = is_channels_last_ ? \"col\" : \"row\";\r\n  std::stringstream read_x_snippet;\r\n  read_x_snippet\r\n      << \"let inChannels = i32(uniforms.w_shape[2]);\\n\"\r\n      << \"let outWidth = \" << (is_channels_last_ ? \"i32(uniforms.result_shape[2])\" : \"i32(uniforms.result_shape[3])\") << \";\\n\"\r\n      << \"let outRow = \" << row << \" / outWidth;\\n \"\r\n      << \"let outCol = \" << row << \" % outWidth;\\n\"\r\n      << \"let WRow = \" << col << \" / (i32(uniforms.w_shape[1]) * inChannels);\\n\"\r\n      << \"let WCol = \" << col << \" / inChannels % i32(uniforms.w_shape[1]);\\n\"\r\n      << \"let xRow = outRow * i32(uniforms.strides[0]) + i32(uniforms.dilations[0]) * WRow - i32(uniforms.pads[0]);\\n\"\r\n      << \"let xCol = outCol * i32(uniforms.strides[1]) + i32(uniforms.dilations[1]) * WCol - i32(uniforms.pads[1]);\\n\"\r\n      << \"let xCh = \" << col << \" % inChannels;\\n\"\r\n      << \"var resData = \" << TypeSnippet(inner_element_size_x, data_type) << \"(0.0);\\n \"\r\n      << \"// The bounds checking is always needed since we use it to pad zero for\\n\"\r\n      << \"// the \\\" same \\\" padding type.\\n\"\r\n      << \"if (xRow >= 0 && xRow < \" << xHeight << \" && xCol >= 0 && xCol < \" << xWidth << \") {\\n\"\r\n      << \"  \" << coord_a_snippet << \"\\n\"\r\n      << \"  let xIndex = getIndexFromCoords4D(coord, vec4<i32>(uniforms.x_shape));\\n\"\r\n      << \"  \" << get_x_snippet(inner_element_size_x)\r\n      << \"}\\n\"\r\n      << \"return resData;\";\r\n  std::stringstream sample_x;\r\n  if (is_channels_last_) {\r\n    if (fit_a_outer_ && fit_inner_) {\r\n      sample_x << \"let col = colIn * \" << inner_element_size_x << \";\\n\"\r\n               << read_x_snippet.str();\r\n    } else {\r\n      sample_x << \"let col = colIn * \" << inner_element_size_x << \";\\n\"\r\n               << \"if(row < i32(uniforms.dim_a_outer) && col < i32(uniforms.dim_inner)) {\\n\"\r\n               << \"  \" << read_x_snippet.str() << \"\\n\"\r\n               << \"}\\n\"\r\n               << \"return \" << TypeSnippet(inner_element_size_x, data_type) << \"(0.0);\\n\";\r\n    }\r\n  } else {\r\n    if (fit_inner_ && fit_b_outer_) {\r\n      sample_x << \"let col = colIn * \" << inner_element_size_x << \";\\n\"\r\n               << read_x_snippet.str();\r\n    } else {\r\n      sample_x << \"let col = colIn * \" << inner_element_size_x << \";\\n\"\r\n               << \"if (row < i32(uniforms.dim_inner) && col < i32(uniforms.dim_b_outer)) {\\n\"\r\n               << \"  \" << read_x_snippet.str() << \"\\n\"\r\n               << \"}\\n\"\r\n               << \"return \" << TypeSnippet(inner_element_size_x, data_type) << \"(0.0);\\n\";\r\n    }\r\n  }\r\n  std::stringstream sample_w;\r\n  if (is_channels_last_) {\r\n    if (fit_inner_ && fit_b_outer_) {\r\n      sample_w << get_w_snippet(inner_element_size_w);\r\n    } else {\r\n      sample_w << \"let col = colIn * \" << inner_element_size_w << \";\\n\"\r\n               << \"if(row < i32(uniforms.dim_inner) && col < i32(uniforms.dim_b_outer)) {\\n\"\r\n               << \"  \" << get_w_snippet(inner_element_size_w) << \"\\n\"\r\n               << \"}\\n\"\r\n               << \"return \" << TypeSnippet(inner_element_size_w, data_type) << \"(0.0);\\n\";\r\n    }\r\n  } else {\r\n    sample_w << \"let col = colIn * \" << inner_element_size_w << \";\\n\"\r\n             << \"if (row < i32(uniforms.dim_inner) && col < i32(uniforms.dim_a_outer)) {\\n\"\r\n             << \"  \" << get_w_snippet(inner_element_size_w) << \"\\n\"\r\n             << \"}\\n\"\r\n             << \"return \" << TypeSnippet(inner_element_size_w, data_type) << \"(0.0);\\n\";\r\n  }\r\n  const std::string res_type = TypeSnippet(inner_element_size, data_type);\r\n  const std::string a_type = is_channels_last_ ? TypeSnippet(inner_element_size_x, data_type) : TypeSnippet(inner_element_size_w, data_type);\r\n  const std::string b_type = is_channels_last_ ? TypeSnippet(inner_element_size_w, data_type) : TypeSnippet(inner_element_size_x, data_type);\r\n  const std::string apply_activation = GetActivationSnippet(activation, res_type, data_type);\r\n  std::stringstream user_code;\r\n  user_code << \"fn mm_readA(batch : i32, row : i32, colIn : i32) -> \" << a_type << \" {\\n\"\r\n            << (is_channels_last_ ? sample_x.str() : sample_w.str())\r\n            << \"}\\n\"\r\n            << \"\\n\"\r\n            << \"fn mm_readB(batch : i32, row : i32, colIn : i32) -> \" << b_type << \" {\\n\"\r\n            << (is_channels_last_ ? sample_w.str() : sample_x.str())\r\n            << \"}\\n\"\r\n            << \"\\n\"\r\n            << \"fn mm_write(batch : i32, row : i32, colIn : i32, valueIn : \" << res_type << \") {\\n\"\r\n            << \"  let col = colIn * \" << inner_element_size << \";\\n\"\r\n            << \"  if(row < i32(uniforms.dim_a_outer) && col < i32(uniforms.dim_b_outer)) {\\n\"\r\n            << \"    var value = valueIn;\\n\"\r\n            << \"    let outWidth = \" << (is_channels_last_ ? \" i32(uniforms.result_shape[2]) \" : \" i32(uniforms.result_shape[3]) \") << \";\\n\"\r\n            << \"    \" << coord_res_snippet << \"\\n\"\r\n            << \"    \" << BiasSnippet(has_bias_) << \"\\n\"\r\n            << \"    \" << apply_activation << \"\\n\"\r\n            << \"    setOutputAtCoords(coords[0], coords[1], coords[2], coords[3], value);\\n\"\r\n            << \"  }\\n\"\r\n            << \"}\\n\";\r\n  return user_code.str();\r\n}\r\n\r\nStatus Conv2dMMProgram::GenerateShaderCode(ShaderHelper& shader) const {\r\n  std::stringstream declaration_functions;\r\n  declaration_functions << \"fn setOutputAtIndex(flatIndex : i32, value : \" << (is_vec4_ ? \"vec4<x_element_t>\" : \"x_element_t\") << \") {\\n\"\r\n                        << \"  result[flatIndex] = \" << (is_vec4_ ? \"vec4<x_element_t>\" : \"x_element_t\") << \"(value);\\n\"\r\n                        << \"}\\n\"\r\n                        << \"fn setOutputAtCoords(d0 : i32, d1 : i32, d2 : i32, d3 : i32, value : \" << (is_vec4_ ? \"vec4<x_element_t>\" : \"x_element_t\") << \"){\\n\"\r\n                        << \"  let flatIndex = getOutputIndexFromCoords(vec4<i32>(d0, d1, d2, d3));\\n\"\r\n                        << \"  setOutputAtIndex(flatIndex, value);\\n\"\r\n                        << \"}\\n\";\r\n  const auto& x = shader.AddInput(\"x\", ShaderUsage::UseUniform | ShaderUsage::UseShapeAndStride | ShaderUsage::UseIndicesTypeAlias | ShaderUsage::UseValueTypeAlias | ShaderUsage::UseElementTypeAlias);\r\n  const auto& w = shader.AddInput(\"w\", ShaderUsage::UseUniform | ShaderUsage::UseShapeAndStride | ShaderUsage::UseIndicesTypeAlias | ShaderUsage::UseValueTypeAlias);\r\n  std::vector<const ShaderVariableHelper*> inputs = {&x, &w};\r\n  ORT_IGNORE_RETURN_VALUE(shader.AddOutput(\"result\", ShaderUsage::UseUniform | ShaderUsage::UseShapeAndStride | ShaderUsage::UseIndicesTypeAlias));\r\n  if (has_bias_) {\r\n    const auto& bias = shader.AddInput(\"bias\", ShaderUsage::UseUniform | ShaderUsage::UseIndicesTypeAlias | ShaderUsage::UseValueTypeAlias | ShaderUsage::UseElementTypeAlias);\r\n    inputs.push_back(&bias);\r\n    declaration_functions << \"fn getBiasByOutputCoords(coords : vec4<i32>) -> bias_value_t {\"\r\n                          << \"\\n\"\r\n                          << \"  return bias[\" << (is_channels_last_ ? \"coords.w\" : \"coords.y\") << \"];\\n\"\r\n                          << \"}\";\r\n  }\r\n  shader.AdditionalImplementation()\r\n      << UtilFunctions(\"uniforms.result_stride\")\r\n      << declaration_functions.str()\r\n      << Conv2dCommonSnippet(x, w, activation_, \"x_element_t\", element_size_[0], element_size_[1], element_size_[2]);\r\n  std::string data_type = \"x_element_t\";\r\n\r\n  return is_vec4_ ? MakeMatMulPackedVec4Source(shader, elements_per_thread_, WorkgroupSizeX(), WorkgroupSizeY(), data_type, /* batch_dims = */ nullptr, /* transpose_a = */ !is_channels_last_, /* transpose_b = */ false, 1.0f, true, 4, tile_inner_)\r\n                  : MakeMatMulPackedSource(shader, elements_per_thread_, WorkgroupSizeX(), WorkgroupSizeY(), data_type, /* batch_dims = */ nullptr, /*transpose_a = */ !is_channels_last_, /* transpose_b = */ false, 1.0f, true, tile_inner_, /* split_t = */ false, 0);\r\n}\r\n\r\nConv2dMMProgram CreateConv2dMMProgram(const Activation& activation, const std::vector<const Tensor*>& inputs, const std::vector<uint32_t>& pads, const std::vector<uint32_t>& strides, const std::vector<uint32_t>& dilations, Tensor* output, uint32_t dim_a_outer, uint32_t dim_b_outer, uint32_t dim_inner, bool is_channels_last, const std::vector<TensorShape>& input_output_shapes) {\r\n  const auto* input = inputs[0];\r\n  const auto* weight = inputs[1];\r\n  bool has_bias = inputs.size() > 2;\r\n  const auto* bias = has_bias ? inputs[2] : nullptr;\r\n  const auto& input_shape = input_output_shapes[0];\r\n  auto in_channels = is_channels_last ? input_shape[3] : input_shape[1];\r\n  const auto& output_shape = has_bias ? input_output_shapes[3] : input_output_shapes[2];\r\n  auto batch_size = output_shape[0];\r\n  const auto output_width = is_channels_last ? output_shape[2] : output_shape[3];\r\n  const auto output_height = is_channels_last ? output_shape[1] : output_shape[2];\r\n  const auto output_channels = is_channels_last ? output_shape[3] : output_shape[1];\r\n  // TODO: enable vec4 for NCHW\r\n  const bool is_vec4 = is_channels_last && (in_channels % 4 == 0 || in_channels % 3 == 0) && output_channels % 4 == 0;\r\n\r\n  // TODO: fine tune size\r\n  const auto dispatch_x = is_channels_last ? output_channels : output_width * output_height;\r\n  const auto dispatch_y = is_channels_last ? output_width * output_height : output_channels;\r\n  std::vector<uint32_t> workgroup_size = {8, 8, 1};\r\n  InlinedVector<int64_t> elements_per_thread = {4, static_cast<int64_t>(dim_a_outer <= 8 ? 1 : 4), 1};\r\n  auto integer_ceil = [](int64_t a, int64_t b) -> int64_t { return (a + b - 1) / b; };\r\n\r\n  const std::vector<uint32_t> dispatch = {\r\n      static_cast<uint32_t>(integer_ceil(integer_ceil(dispatch_x, workgroup_size[0]), elements_per_thread[0])),\r\n      static_cast<uint32_t>(integer_ceil(integer_ceil(dispatch_y, workgroup_size[1]), elements_per_thread[1])),\r\n      static_cast<uint32_t>(integer_ceil(integer_ceil(batch_size, workgroup_size[2]), elements_per_thread[2])),\r\n  };\r\n\r\n  uint32_t inner_element_size = is_vec4 ? (is_channels_last && in_channels % 4 != 0 ? 3 : 4) : 1;\r\n  auto tile_a_outer = static_cast<uint32_t>(workgroup_size[1] * elements_per_thread[1]);\r\n  auto tile_b_outer = static_cast<uint32_t>(workgroup_size[0] * elements_per_thread[0]);\r\n  auto tile_inner = std::max(workgroup_size[0] * inner_element_size, workgroup_size[1]);\r\n  bool fit_a_outer = dim_a_outer % tile_a_outer == 0;\r\n  bool fit_b_outer = dim_b_outer % tile_b_outer == 0;\r\n  bool fit_inner = dim_inner % tile_inner == 0;\r\n  std::vector<uint32_t> element_size = {is_vec4 ? inner_element_size : 1, static_cast<uint32_t>(is_vec4 ? 4 : 1), static_cast<uint32_t>(is_vec4 ? 4 : 1)};\r\n  const auto components = is_vec4 ? 4 : 1;\r\n  const auto input_components = static_cast<int>(inner_element_size == 3 ? 1 : inner_element_size);\r\n  Conv2dMMProgram program(activation, tile_inner, fit_a_outer, fit_b_outer, fit_inner, is_channels_last, is_vec4, has_bias, std::move(element_size), std::move(elements_per_thread));\r\n  TensorShape reduced_input_shape = ReduceShapeByComponents(input_output_shapes[0], input_components);\r\n  TensorShape reduced_weight_shape = ReduceShapeByComponents(input_output_shapes[1], components);\r\n  TensorShape reduced_output_shape = ReduceShapeByComponents(input_output_shapes[has_bias ? 3 : 2], components);\r\n  program.AddInputs({{input, ProgramTensorMetadataDependency::TypeAndRank, reduced_input_shape, input_components}, {weight, ProgramTensorMetadataDependency::TypeAndRank, reduced_weight_shape, components}});\r\n  if (has_bias) {\r\n    TensorShape reduced_bias_shape = ReduceShapeByComponents(input_output_shapes[2], components);\r\n    program.AddInput({bias, ProgramTensorMetadataDependency::TypeAndRank, reduced_bias_shape, components});\r\n  }\r\n  const auto stringify = [](const std::vector<uint32_t>& vec) -> std::string {\r\n    std::ostringstream oss;\r\n    std::transform(vec.begin(), vec.end(), std::ostream_iterator<std::string>(oss, \",\"), [](uint32_t i) { return std::to_string(i); });\r\n    return oss.str();\r\n  };\r\n  program.CacheHint(activation.ToString(), is_channels_last, stringify({inner_element_size, static_cast<uint32_t>(is_vec4 ? 1 : 0), fit_a_outer, fit_b_outer, fit_inner, tile_a_outer, tile_a_outer, tile_inner, static_cast<uint32_t>(components)}))\r\n      .AddOutput({output, ProgramTensorMetadataDependency::TypeAndRank, reduced_output_shape, components})\r\n      .SetDispatchGroupSize(dispatch[0], dispatch[1], dispatch[2])\r\n      .SetWorkgroupSize(workgroup_size[0], workgroup_size[1], workgroup_size[2])\r\n      .AddUniformVariables({{static_cast<uint32_t>(dim_a_outer)},\r\n                            {static_cast<uint32_t>(dim_b_outer)},\r\n                            {static_cast<uint32_t>(dim_inner)},\r\n                            {pads},\r\n                            {strides},\r\n                            {dilations}});\r\n\r\n  return program;\r\n}\r\n\r\n}  // namespace webgpu\r\n}  // namespace onnxruntime\r\n"}
{"path":"\\\\?\\C:\\Users\\Naomi\\Desktop\\ONNX\\onnxruntime\\onnxruntime\\test\\framework\\allocation_planner_test.cc","line":null,"char":null,"code":"CLANGFORMAT","severity":"warning","name":"format","description":"See https://clang.llvm.org/docs/ClangFormat.html.\nRun `lintrunner -a` to apply this patch.","original":"// Copyright (c) Microsoft Corporation. All rights reserved.\r\n// Licensed under the MIT License.\r\n\r\n#include <string>\r\n#include <unordered_map>\r\n#include <unordered_set>\r\n#include <sstream>\r\n#include \"gtest/gtest.h\"\r\n\r\n#ifdef ORT_ENABLE_STREAM\r\n#include \"nlohmann/json.hpp\"\r\nusing json = nlohmann::json;\r\n#endif\r\n\r\n#include \"core/framework/session_state.h\"\r\n#include \"core/framework/kernel_registry.h\"\r\n#include \"core/framework/op_kernel.h\"\r\n#include \"test/framework/model_builder_utils.h\"\r\n#include \"core/framework/allocation_planner.h\"\r\n#include \"core/session/inference_session.h\"\r\n#include \"core/graph/model.h\"\r\n#include \"core/graph/graph_utils.h\"\r\n#include \"core/providers/cpu/cpu_execution_provider.h\"\r\n#include \"core/util/thread_utils.h\"\r\n\r\n#include \"test/test_environment.h\"\r\n#include \"test/util/include/asserts.h\"\r\n#include \"test/util/include/default_providers.h\"\r\n#ifdef USE_CUDA\r\n#include \"core/providers/cuda/cuda_execution_provider.h\"\r\n#include \"core/providers/cuda/cuda_provider_factory.h\"\r\n#endif  // USE_CUDA\r\n#include \"core/session/onnxruntime_session_options_config_keys.h\"\r\nusing namespace ONNX_NAMESPACE;\r\n\r\nnamespace onnxruntime {\r\n#ifdef USE_CUDA\r\nProviderInfo_CUDA& GetProviderInfo_CUDA();\r\n#endif\r\nnamespace test {\r\n\r\nnamespace modelbuilder {\r\n\r\nclass NodeCounter {\r\n private:\r\n  static int node_count_;\r\n\r\n public:\r\n  static int Next() { return ++node_count_; }\r\n};\r\n\r\nint NodeCounter::node_count_ = 0;\r\n\r\nstruct UnaryNode {\r\n  std::vector<onnxruntime::NodeArg*> input_args;\r\n  std::vector<onnxruntime::NodeArg*> output_args;\r\n  onnxruntime::Node* p_node;\r\n\r\n  UnaryNode(onnxruntime::Graph& graph, const std::string& op, onnxruntime::NodeArg* p_input_arg,\r\n            onnxruntime::NodeArg* p_output_arg)\r\n      : input_args({p_input_arg}), output_args({p_output_arg}) {\r\n    int num = NodeCounter::Next();\r\n    p_node = &graph.AddNode(\"node\" + std::to_string(num), op, \"test op\", input_args, output_args);\r\n  }\r\n\r\n  UnaryNode(onnxruntime::Graph& graph, onnxruntime::NodeArg* p_input_arg, onnxruntime::NodeArg* p_output_arg)\r\n      : UnaryNode(graph, \"Transpose\", p_input_arg, p_output_arg) {}\r\n\r\n  UnaryNode(onnxruntime::Graph& graph, std::string& node_name, const std::string& op, std::vector<onnxruntime::NodeArg*>& inputs,\r\n            std::vector<onnxruntime::NodeArg*>& outputs) : input_args(inputs), output_args(outputs) {\r\n    p_node = &graph.AddNode(node_name, op, \"test op\", input_args, output_args);\r\n  }\r\n};\r\n\r\nclass DummyOpKernel : public OpKernel {\r\n public:\r\n  DummyOpKernel(const OpKernelInfo& p) : OpKernel(p) {}\r\n  Status Compute(OpKernelContext* context) const {\r\n    ORT_UNUSED_PARAMETER(context);\r\n    return Status::OK();\r\n  }\r\n  Status ComputeAsync(OpKernelContext* context, DoneCallback done) const {\r\n    ORT_UNUSED_PARAMETER(context);\r\n    ORT_UNUSED_PARAMETER(done);\r\n    return Status::OK();\r\n  }\r\n};\r\n\r\n}  // namespace modelbuilder\r\n\r\nusing namespace modelbuilder;\r\n\r\nclass AllocationPlanTestUtility {\r\n public:\r\n  static void CheckAllocationKind(const SequentialExecutionPlan& plan, std::vector<AllocKind>& expected) {\r\n    ASSERT_EQ(plan.allocation_plan.size(), expected.size()) << \"Allocation plan of wrong size\";\r\n    for (size_t i = 0; i < expected.size(); ++i) {\r\n      EXPECT_EQ(plan.allocation_plan[i].alloc_kind, expected[i]) << \"Error in allocation kind at position \" << i;\r\n    }\r\n  }\r\n  // The free list has been re-implmented.\r\n  // remove those checkers first.\r\n  // TODO: add the tests for new release plan.\r\n};\r\n\r\ntypedef std::unordered_map<const onnxruntime::NodeArg*, TensorShapeProto*> ShapeMap;\r\n\r\nclass SequentialPlannerTestContext : public ISequentialPlannerContext {\r\n public:\r\n  SequentialPlannerTestContext(ShapeMap* shape_map) : shape_map_(shape_map) {}\r\n\r\n  TensorShapeProto* GetShape(const onnxruntime::NodeArg& arg) const override {\r\n    auto iter = shape_map_->find(&arg);\r\n    return (shape_map_->end() != iter) ? iter->second : nullptr;\r\n  }\r\n\r\n private:\r\n  ShapeMap* shape_map_;\r\n};\r\n\r\nclass ParallelPlannerTestContext : public SequentialPlannerTestContext {\r\n public:\r\n  ParallelPlannerTestContext(ShapeMap* shape_map) : SequentialPlannerTestContext(shape_map) {\r\n  }\r\n  bool IsParallelExecutionEnabled() const override { return true; }\r\n  ExecutionOrder GetExecutionOrder() const override { return ExecutionOrder::DEFAULT; }\r\n  bool GetEnableMemoryReuse() const override { return false; }\r\n};\r\n\r\nclass PlannerTest : public ::testing::Test {\r\n private:\r\n  void index(const std::string& name, int& out) {\r\n    ASSERT_TRUE(state_->GetOrtValueNameIdxMap().GetIdx(name, out).IsOK());\r\n  }\r\n\r\n  onnxruntime::Model model_;\r\n  onnxruntime::Graph& graph_;\r\n\r\n  // some standard components used to build test-cases:\r\n  Type float_type_;\r\n\r\n  std::unique_ptr<::onnxruntime::KernelDef> std_kernel_;               // a unary kernel with no-aliasing and no-in-place\r\n  std::unique_ptr<::onnxruntime::KernelDef> in_place_kernel_;          // a unary kernel with in-place\r\n  std::unique_ptr<::onnxruntime::KernelDef> external_outputs_kernel_;  // an unary kernel with external outputs\r\n#ifdef ENABLE_STRIDED_TENSORS\r\n  std::unique_ptr<::onnxruntime::KernelDef> may_strided_input_kernel_;   // an uinary kernel with may_strided_input\r\n  std::unique_ptr<::onnxruntime::KernelDef> may_strided_output_kernel_;  // an unary kernel with may_strided_output\r\n#endif\r\n\r\n  std::unordered_map<std::string, onnxruntime::NodeArg*> name_to_arg_;\r\n  std::vector<std::unique_ptr<UnaryNode>> nodes_;\r\n  std::vector<std::unique_ptr<OpKernelInfo>> op_kernel_infos_;\r\n  std::vector<std::pair<onnxruntime::Node*, KernelDef&>> kernel_bindings_;\r\n  ExecutionProviders execution_providers_;\r\n  std::unique_ptr<concurrency::ThreadPool> tp_;\r\n  DataTransferManager dtm_;\r\n  ExternalDataLoaderManager edlm_;\r\n  profiling::Profiler profiler_;\r\n  std::unique_ptr<SessionOptions> sess_options_;\r\n  std::unique_ptr<SessionState> state_;\r\n  ShapeMap shape_map_;\r\n  std::optional<SequentialExecutionPlan> plan_;\r\n\r\n public:\r\n  PlannerTest()\r\n      : model_(\"test\", false, ModelMetaData(), PathString(), IOnnxRuntimeOpSchemaRegistryList(), {{kOnnxDomain, 10}}, {}, DefaultLoggingManager().DefaultLogger()),\r\n        graph_(model_.MainGraph()),\r\n        tp_(concurrency::CreateThreadPool(&onnxruntime::Env::Default(), OrtThreadPoolParams(),\r\n                                          concurrency::ThreadPoolType::INTRA_OP)) {\r\n    std_kernel_ = KernelDefBuilder().SetName(\"Transpose\").Provider(kCpuExecutionProvider).SinceVersion(1, 10).Build();\r\n    in_place_kernel_ =\r\n        KernelDefBuilder().SetName(\"Relu\").Provider(kCpuExecutionProvider).SinceVersion(1, 10).MayInplace(0, 0).Build();\r\n    external_outputs_kernel_ =\r\n        KernelDefBuilder().SetName(\"Tanh\").Provider(kCpuExecutionProvider).SinceVersion(1, 10).ExternalOutputs().Build();\r\n#ifdef ENABLE_STRIDED_TENSORS\r\n    may_strided_input_kernel_ = KernelDefBuilder()\r\n                                    .SetName(\"Abs\")\r\n                                    .Provider(kCpuExecutionProvider)\r\n                                    .SinceVersion(1, 10)\r\n                                    .MayStridedInput(0)\r\n                                    .Build();\r\n    may_strided_output_kernel_ = KernelDefBuilder()\r\n                                     .SetName(\"Neg\")\r\n                                     .Provider(kCpuExecutionProvider)\r\n                                     .SinceVersion(1, 10)\r\n                                     .MayStridedOutput(0, 0)\r\n                                     .Build();\r\n#endif\r\n    CPUExecutionProviderInfo epi;\r\n    auto execution_provider = std::make_unique<CPUExecutionProvider>(epi);\r\n    ORT_THROW_IF_ERROR(execution_providers_.Add(\"CPUExecutionProvider\", std::move(execution_provider)));\r\n    sess_options_ = std::make_unique<SessionOptions>();\r\n    sess_options_->enable_mem_pattern = false;\r\n    sess_options_->use_deterministic_compute = false;\r\n    sess_options_->enable_mem_reuse = true;\r\n    state_.reset(new SessionState(graph_, execution_providers_, tp_.get(), nullptr, dtm_, edlm_,\r\n                                  DefaultLoggingManager().DefaultLogger(), profiler_, *sess_options_));\r\n  }\r\n\r\n  onnxruntime::NodeArg* Arg(const std::string& name) {\r\n    auto iter = name_to_arg_.find(name);\r\n    if (name_to_arg_.end() != iter) return iter->second;\r\n    return (name_to_arg_[name] = &graph_.GetOrCreateNodeArg(name, &float_type_.value));\r\n  }\r\n\r\n  onnxruntime::Node* AddNode(::onnxruntime::KernelDef& kernel_def, std::string& input, std::string& output) {\r\n    auto node = std::make_unique<UnaryNode>(graph_, kernel_def.OpName(), Arg(input), Arg(output));\r\n    auto* p_node = node->p_node;\r\n    p_node->SetExecutionProviderType(kernel_def.Provider());\r\n    nodes_.push_back(std::move(node));\r\n    kernel_bindings_.emplace_back(p_node, kernel_def);\r\n    return p_node;\r\n  }\r\n\r\n  onnxruntime::Node* AddNode(::onnxruntime::KernelDef& kernel_def, std::string& node_name, std::vector<onnxruntime::NodeArg*>& input, std::vector<onnxruntime::NodeArg*>& output) {\r\n    auto node = std::make_unique<UnaryNode>(graph_, node_name, kernel_def.OpName(), input, output);\r\n    auto* p_node = node->p_node;\r\n    p_node->SetExecutionProviderType(kernel_def.Provider());\r\n    nodes_.push_back(std::move(node));\r\n    kernel_bindings_.emplace_back(p_node, kernel_def);\r\n    return p_node;\r\n  }\r\n\r\n  onnxruntime::Node* AddNormalNode(std::string& input, std::string& output) {\r\n    return AddNode(*std_kernel_, input, output);\r\n  }\r\n\r\n  onnxruntime::Node* AddInplaceNode(std::string& input, std::string& output) {\r\n    return AddNode(*in_place_kernel_, input, output);\r\n  }\r\n\r\n  onnxruntime::Node* AddExternalOutputsNode(std::string& input, std::string& output) {\r\n    return AddNode(*external_outputs_kernel_, input, output);\r\n  }\r\n\r\n#ifdef ENABLE_STRIDED_TENSORS\r\n  onnxruntime::Node* AddMayStridedInputNode(std::string& input, std::string& output) {\r\n    return AddNode(*may_strided_input_kernel_, input, output);\r\n  }\r\n\r\n  onnxruntime::Node* AddMayStridedOutputNode(std::string& input, std::string& output) {\r\n    return AddNode(*may_strided_output_kernel_, input, output);\r\n  }\r\n#endif\r\n\r\n  void BindKernel(onnxruntime::Node* p_node, ::onnxruntime::KernelDef& kernel_def, KernelRegistry* reg,\r\n                  std::unordered_map<NodeIndex, gsl::not_null<const KernelCreateInfo*>>& kernel_create_info_map) {\r\n    const auto& logger = DefaultLoggingManager().DefaultLogger();\r\n    const IExecutionProvider* ep = execution_providers_.Get(*p_node);\r\n    ASSERT_NE(ep, nullptr);\r\n    auto info = std::make_unique<OpKernelInfo>(\r\n        *p_node, kernel_def, *ep, state_->GetInitializedTensors(), state_->GetOrtValueNameIdxMap(),\r\n        state_->GetDataTransferMgr(), state_->GetAllocators(), state_->GetSessionOptions().config_options);\r\n\r\n    op_kernel_infos_.push_back(std::move(info));\r\n    const auto kernel_type_str_resolver = OpSchemaKernelTypeStrResolver{};\r\n    if (!KernelRegistry::HasImplementationOf(*reg, *p_node, onnxruntime::kCpuExecutionProvider,\r\n                                             kernel_type_str_resolver, logger)) {\r\n      ASSERT_STATUS_OK(reg->Register(\r\n          KernelCreateInfo(std::make_unique<KernelDef>(kernel_def),\r\n                           [](FuncManager&, const OpKernelInfo& info, std::unique_ptr<OpKernel>& out) -> Status {\r\n                             out = std::make_unique<DummyOpKernel>(info);\r\n                             return Status::OK();\r\n                           })));\r\n    }\r\n\r\n    const KernelCreateInfo* kci;\r\n    ASSERT_STATUS_OK(reg->TryFindKernel(*p_node, \"\", kernel_type_str_resolver, logger, &kci));\r\n    kernel_create_info_map.insert({p_node->Index(), gsl::not_null<const KernelCreateInfo*>(kci)});\r\n  }\r\n\r\n  void SetShape(std::string& name, TensorShapeProto* shape) { shape_map_[Arg(name)] = shape; }\r\n\r\n  void SetShape(std::initializer_list<std::pair<std::string&, TensorShapeProto*>> shapes) {\r\n    for (auto& pair : shapes) {\r\n      SetShape(pair.first, pair.second);\r\n    }\r\n  }\r\n\r\n  void CreatePlan(const std::vector<const NodeArg*>& outer_scope_node_args = {},\r\n                  bool invoke_createPlan_explicityly = true) {\r\n    state_.reset(new SessionState(graph_, execution_providers_, tp_.get(), nullptr, dtm_, edlm_,\r\n                                  DefaultLoggingManager().DefaultLogger(), profiler_, *sess_options_));\r\n    EXPECT_EQ(graph_.Resolve(), Status::OK());\r\n\r\n    std::shared_ptr<KernelRegistry> reg = std::make_shared<KernelRegistry>();\r\n    std::unordered_map<NodeIndex, gsl::not_null<const KernelCreateInfo*>> kernel_create_info_map;\r\n\r\n    for (auto& binding : kernel_bindings_) {\r\n      BindKernel(binding.first, binding.second, reg.get(), kernel_create_info_map);\r\n    }\r\n\r\n    auto cpu_execution_provider = std::make_unique<CPUExecutionProvider>(CPUExecutionProviderInfo());\r\n    KernelRegistryManager kernel_registry_manager;\r\n    kernel_registry_manager.RegisterKernelRegistry(reg);\r\n    auto status = kernel_registry_manager.RegisterKernels(execution_providers_);\r\n    EXPECT_TRUE(status.IsOK()) << status.ErrorMessage();\r\n\r\n    // CreatePlan is called inside FinalizeSessionState and usually the initializers are removed following that.\r\n    // Leave initializers so we can duplicate the call to CreatePlan from here to validate.\r\n    constexpr bool remove_initializers = false;\r\n    status = state_->FinalizeSessionState(ORT_TSTR(\"\"), kernel_registry_manager, {}, remove_initializers);\r\n\r\n    EXPECT_TRUE(status.IsOK()) << status.ErrorMessage();\r\n    SequentialPlannerTestContext test_context(&shape_map_);\r\n    plan_.emplace();\r\n\r\n    class MockStreamHandleRegsitry : public IStreamCommandHandleRegistry {\r\n     public:\r\n      // Wait is a little special as we need to consider the source stream the notification generated, and the stream we are waiting.\r\n      // i.e., for an cuda event what notify the memory copy, it could be wait on a CPU stream, or on another cuda stream.\r\n      virtual WaitNotificationFn GetWaitHandle(const OrtDevice& /*notification_owner_device*/,\r\n                                               const OrtDevice& /*executor_device*/) const override {\r\n        return nullptr;\r\n      }\r\n\r\n      virtual CreateStreamFn GetCreateStreamFn(const OrtDevice::DeviceType /*execution_provider_type*/) const override {\r\n        return nullptr;\r\n      }\r\n\r\n      virtual void RegisterWaitFn(const OrtDevice::DeviceType /*notification_ep_type*/, const OrtDevice::DeviceType /*ep_type*/, WaitNotificationFn /*fn*/) override {}\r\n\r\n      virtual void RegisterCreateStreamFn(const OrtDevice::DeviceType /*ep_type*/, CreateStreamFn /*f*/) override {}\r\n    };\r\n\r\n    if (invoke_createPlan_explicityly) {\r\n      onnxruntime::GraphViewer graph_viewer{graph_};\r\n      status = SequentialPlanner::CreatePlan(\r\n          nullptr,\r\n          graph_viewer,\r\n          outer_scope_node_args,\r\n          execution_providers_,\r\n          kernel_create_info_map,\r\n          {},\r\n          {},\r\n          state_->GetOrtValueNameIdxMap(),\r\n          test_context,\r\n#ifdef ORT_ENABLE_STREAM\r\n          MockStreamHandleRegsitry(),\r\n#endif\r\n          /* {{kCpuExecutionProvider, 1}}, {},*/\r\n          ORT_TSTR(\"\"),\r\n          DefaultLoggingManager().DefaultLogger(),\r\n          plan_);\r\n\r\n      EXPECT_TRUE(status.IsOK()) << status.ErrorMessage();\r\n      // AllocationPlanTestUtility::BasicIntegrityCheck(*plan_, name_to_arg_.size());\r\n    }\r\n  }\r\n\r\n  void CheckAllocKind(const std::string& name, AllocKind kind) {\r\n    int id;\r\n    index(name, id);\r\n    EXPECT_EQ(plan_->allocation_plan[id].alloc_kind, kind) << \"Error in allocation kind for \" << name;\r\n  }\r\n\r\n  void CheckFreed(int step_number, std::initializer_list<std::string> freed_items) {\r\n    // TODO: add the checker for new implementation of release plan\r\n    //// create set and check equality\r\n    std::unordered_set<int> expected;\r\n    for (auto& name : freed_items) {\r\n      int id;\r\n      index(name, id);\r\n      expected.insert(id);\r\n    }\r\n    std::unordered_set<int> plan_result;\r\n    // todo - support multi-stream\r\n    EXPECT_EQ(plan_->execution_plan.size(), 1U);\r\n    int list_size = static_cast<int>(plan_->node_release_list.size());\r\n    EXPECT_GT(list_size, step_number);\r\n    for (auto action_idx : plan_->node_release_list[step_number]) {\r\n      const size_t ortvalue_id = plan_->release_actions[action_idx].value_index;\r\n      plan_result.insert(static_cast<int>(ortvalue_id));\r\n    }\r\n    EXPECT_EQ(plan_result, expected) << \"Freed items incorrect for step \" << step_number;\r\n  }\r\n\r\n protected:\r\n  Graph& GetGraph() { return graph_; }\r\n  const SequentialExecutionPlan& GetPlan() const { return *plan_; }\r\n  const SessionState& GetState() const { return *state_; }\r\n  ExecutionProviders& GetExecutionProviders() { return execution_providers_; }\r\n  void SetNodePartitionConfigFilePath(const char* config_file_path) {\r\n    ORT_THROW_IF_ERROR(sess_options_->config_options.AddConfigEntry(kNodePartitionConfigFile, config_file_path));\r\n  }\r\n  std::unique_ptr<::onnxruntime::KernelDef>& GetStdKernel() { return std_kernel_; }\r\n#ifdef USE_CUDA\r\n  void MemcpyToHostInCuda_TransposeInCudaAndCpu(const char* partitionConfigFile = nullptr) {\r\n    std::unique_ptr<::onnxruntime::KernelDef> cudaKernel = KernelDefBuilder().SetName(\"MemcpyToHost\").Provider(kCudaExecutionProvider).SetDefaultOutputMemoryType(OrtMemTypeCPUOutput).Build();\r\n    std::unique_ptr<::onnxruntime::KernelDef> cudaKernelTrans = KernelDefBuilder().SetName(\"Transpose\").Provider(kCudaExecutionProvider).SinceVersion(1, 10).Build();\r\n    std::string Graph_input(\"Graph_input\"), Arg1(\"Arg1\"), Arg2(\"Arg2\"), Arg3(\"Arg3\"), node1(\"node1\"), node2(\"node2\"), node3(\"node3\");\r\n    std::vector<onnxruntime::NodeArg*> input1{Arg(Graph_input)}, output1{Arg(Arg1)}, output2{Arg(Arg2)}, output3{Arg(Arg3)};\r\n    AddNode(*cudaKernel, node1, input1, output1);\r\n    AddNode(*GetStdKernel(), node2, output1, output2);\r\n    AddNode(*cudaKernelTrans, node3, output1, output3);\r\n\r\n    CUDAExecutionProviderInfo epi;\r\n    onnxruntime::ProviderInfo_CUDA& ep = onnxruntime::GetProviderInfo_CUDA();\r\n    auto epFactory = ep.CreateExecutionProviderFactory(epi);\r\n    std::unique_ptr<IExecutionProvider> execution_provider = epFactory->CreateProvider();\r\n    ORT_THROW_IF_ERROR(GetExecutionProviders().Add(\"CUDAExecutionProvider\", std::move(execution_provider)));\r\n\r\n    if (partitionConfigFile != nullptr) SetNodePartitionConfigFilePath(partitionConfigFile);\r\n    CreatePlan({}, false);\r\n  }\r\n#endif  // USE_CUDA\r\n};\r\n\r\nTEST_F(PlannerTest, ChainTest) {\r\n  // tensor variables:\r\n  std::string W(\"W\"), X(\"X\"), B(\"B\"), Y(\"Y\"), Z(\"Z\");\r\n\r\n  // graph structure:\r\n\r\n  ONNX_NAMESPACE::TensorProto tensor;\r\n  tensor.add_dims(1);\r\n  tensor.add_float_data(1.0f);\r\n  tensor.set_data_type(TensorProto_DataType_FLOAT);\r\n  tensor.set_name(\"W\");\r\n  GetGraph().AddInitializedTensor(tensor);\r\n\r\n  AddNormalNode(W, X);\r\n  AddNormalNode(X, B);\r\n  AddNormalNode(B, Y);\r\n  AddNormalNode(Y, Z);\r\n\r\n  // simulate shape-inference results:\r\n  Shape shape1{50, 100};\r\n  auto shape = &shape1.value;\r\n  SetShape({{X, shape}, {B, shape}, {Y, shape}, {Z, shape}});\r\n\r\n  CreatePlan();\r\n\r\n  // Expected plan:\r\n  //   W: kAllocateStatically; X: kAllocate; B: kAllocate; Y: kReuse (X); post-node3: free(B); Z is returned output\r\n  CheckAllocKind(W, AllocKind::kAllocateStatically);\r\n  CheckAllocKind(X, AllocKind::kAllocate);\r\n  CheckAllocKind(B, AllocKind::kAllocate);\r\n  CheckAllocKind(Y, AllocKind::kReuse);\r\n  CheckAllocKind(Z, AllocKind::kAllocateOutput);\r\n\r\n  CheckFreed(0, {});\r\n  CheckFreed(1, {});\r\n  CheckFreed(2, {B});\r\n  CheckFreed(3, {X});\r\n}\r\n\r\n/* InputOutputTest: Test that:\r\n(a) All inputs are classified as kPreExisting,\r\n(b) All outer scope node args are classified as kPreExisting,\r\n(c) All outputs are classified as kAllocate (in this example),\r\n(d) Neither input nor outputs are freed.\r\n*/\r\nTEST_F(PlannerTest, InputOutputTest) {\r\n  // tensor variables:\r\n  std::string X1(\"X1\"), X2(\"X2\"), Y1(\"Y1\"), Y2(\"Y2\"), Outer1(\"Outer1\"), Y3(\"Y3\");\r\n\r\n  // graph structure:\r\n  AddNormalNode(X1, Y1);\r\n  AddNormalNode(X2, Y2);\r\n\r\n  // add node that consumes an outer scope node arg\r\n  auto outer_node = AddNormalNode(Outer1, Y3);\r\n  const NodeArg* outer_scope_node_arg = outer_node->InputDefs().at(0);\r\n  GetGraph().AddOuterScopeNodeArg(Outer1);\r\n\r\n  // simulate no shape-inference:\r\n\r\n  CreatePlan({outer_scope_node_arg});\r\n\r\n  // X1: kPreExisting, X2: kPreExisting, Outer1: kPreExisting, Y1: kAllocate, Y2: kAllocate, Y3: kAllocate\r\n  CheckAllocKind(X1, AllocKind::kPreExisting);\r\n  CheckAllocKind(X2, AllocKind::kPreExisting);\r\n  CheckAllocKind(Outer1, AllocKind::kPreExisting);\r\n  CheckAllocKind(Y1, AllocKind::kAllocateOutput);\r\n  CheckAllocKind(Y2, AllocKind::kAllocateOutput);\r\n  CheckAllocKind(Y3, AllocKind::kAllocateOutput);\r\n\r\n  // Nothing should be freed (since they are either inputs or outputs)\r\n  CheckFreed(0, {});\r\n  CheckFreed(1, {});\r\n  CheckFreed(2, {});\r\n}\r\n\r\n// InPlaceTest: Check that we reuse when Inplace allows us to.\r\n\r\nTEST_F(PlannerTest, InPlaceTest) {\r\n  // tensor variables:\r\n  std::string X1(\"X1\"), X2(\"X2\"), X3(\"X3\"), X4(\"X4\");\r\n\r\n  // graph structure:\r\n  AddNormalNode(X1, X2);   // no in-place operator; X1: input; X2: temporary\r\n  AddInplaceNode(X2, X3);  // may-in-place operator; X3: temporary\r\n  AddNormalNode(X3, X4);   // no in-place operator; X4: output\r\n\r\n  // simulate shape-inference results:\r\n  Shape shape1{\"M\", \"N\"};\r\n  auto shape = &shape1.value;\r\n  SetShape({{X1, shape}, {X2, shape}, {X3, shape}, {X4, shape}});\r\n\r\n  CreatePlan();\r\n\r\n  // check allocation kind:\r\n  CheckAllocKind(X1, AllocKind::kPreExisting);\r\n  CheckAllocKind(X2, AllocKind::kAllocate);\r\n  CheckAllocKind(X3, AllocKind::kReuse);\r\n  CheckAllocKind(X4, AllocKind::kAllocateOutput);\r\n\r\n  // check each ml-value is freed at appropriate step\r\n  CheckFreed(0, {});\r\n  CheckFreed(1, {});\r\n  CheckFreed(2, {X2});\r\n}\r\n\r\nTEST_F(PlannerTest, ExternalOutputsTest) {\r\n  // tensor variables:\r\n  std::string X1(\"X1\"), X2(\"X2\"), X3(\"X3\"), X4(\"X4\");\r\n\r\n  // graph structure:\r\n  AddExternalOutputsNode(X1, X2);  // external-outputs operator; X1: input; X2: temporary\r\n  AddNormalNode(X2, X3);           // normal operator; X3: temporary\r\n  AddNormalNode(X3, X4);           // normal operator; X4: output\r\n\r\n  // simulate shape-inference results:\r\n  Shape shape1{\"M\", \"N\"};\r\n  auto shape = &shape1.value;\r\n  SetShape({{X1, shape}, {X2, shape}, {X3, shape}, {X4, shape}});\r\n\r\n  CreatePlan();\r\n\r\n  // check allocation kind:\r\n  CheckAllocKind(X1, AllocKind::kPreExisting);\r\n  CheckAllocKind(X2, AllocKind::kAllocatedExternally);\r\n  CheckAllocKind(X3, AllocKind::kAllocate);\r\n  CheckAllocKind(X4, AllocKind::kAllocateOutput);\r\n\r\n  // check each ml-value is freed at appropriate step\r\n  // X2 will not be reused but will be freed (to release the current reference). X3 will be allocated and will be freed.\r\n  CheckFreed(0, {});\r\n  CheckFreed(1, {X2});\r\n  CheckFreed(2, {X3});\r\n}\r\n\r\nTEST_F(PlannerTest, ExternalOutputsNoReuseTest) {\r\n  // tensor variables:\r\n  std::string X1(\"X1\"), X2(\"X2\"), X3(\"X3\"), X4(\"X4\"), X5(\"X5\");\r\n\r\n  // graph structure:\r\n  AddExternalOutputsNode(X1, X2);  // external-outputs operator; X1: input; X2: temporary\r\n  AddInplaceNode(X2, X3);          // may-in-place operator; X3: temporary\r\n  AddNormalNode(X3, X4);           // normal operator; X4: temporary\r\n  AddNormalNode(X4, X5);           // normal operator; X5: output\r\n\r\n  // simulate shape-inference results:\r\n  Shape shape1{\"M\", \"N\"};\r\n  auto shape = &shape1.value;\r\n  SetShape({{X1, shape}, {X2, shape}, {X3, shape}, {X4, shape}, {X5, shape}});\r\n\r\n  CreatePlan();\r\n\r\n  // check allocation kind:\r\n  CheckAllocKind(X1, AllocKind::kPreExisting);\r\n  CheckAllocKind(X2, AllocKind::kAllocatedExternally);\r\n  CheckAllocKind(X3, AllocKind::kAllocate);  // Should not be Reused.\r\n  CheckAllocKind(X4, AllocKind::kAllocate);\r\n  CheckAllocKind(X5, AllocKind::kAllocateOutput);\r\n\r\n  // check each ml-value is freed at appropriate step\r\n  // X2 will not be reused. X3 will be allocated and will be freed.\r\n  CheckFreed(0, {});\r\n  CheckFreed(1, {X2});\r\n  CheckFreed(2, {X3});\r\n  CheckFreed(3, {X4});\r\n}\r\n\r\n#ifdef ENABLE_STRIDED_TENSORS\r\nTEST_F(PlannerTest, MayStridedTest1) {\r\n  // tensor variables:\r\n  std::string X1(\"X1\"), X2(\"X2\"), X3(\"X3\");\r\n\r\n  // graph structure:\r\n  AddNormalNode(X1, X2);\r\n  AddMayStridedOutputNode(X2, X3);  // may_strided_output as graph output.\r\n\r\n  // simulate shape-inference results:\r\n  Shape shape1{\"M\", \"N\"};\r\n  auto shape = &shape1.value;\r\n  SetShape({{X1, shape}, {X2, shape}, {X3, shape}});\r\n\r\n  CreatePlan();\r\n\r\n  // check allocation kind:\r\n  CheckAllocKind(X1, AllocKind::kPreExisting);\r\n  CheckAllocKind(X2, AllocKind::kAllocate);\r\n  CheckAllocKind(X3, AllocKind::kAllocateOutput);\r\n\r\n  // check each ml-value is freed at appropriate step\r\n  // X2 will not be reused because X3 is a graph output. X3 will be allocated and will be freed.\r\n  CheckFreed(0, {});\r\n  CheckFreed(1, {X2});\r\n}\r\n\r\nTEST_F(PlannerTest, MayStridedTest2) {\r\n  // tensor variables:\r\n  std::string X1(\"X1\"), X2(\"X2\"), X3(\"X3\"), X4(\"X4\");\r\n\r\n  // graph structure:\r\n  AddMayStridedOutputNode(X1, X2);  // X2 can reuse X1, and is a strided output.\r\n  AddMayStridedInputNode(X2, X3);   // X3 is a graph output, cannot reuse.\r\n  AddMayStridedInputNode(X2, X4);   // X4 is a graph output, cannot reuse.\r\n\r\n  // simulate shape-inference results:\r\n  Shape shape1{\"M\", \"N\"};\r\n  auto shape = &shape1.value;\r\n  SetShape({{X1, shape}, {X2, shape}, {X3, shape}, {X4, shape}});\r\n\r\n  CreatePlan();\r\n\r\n  // check allocation kind:\r\n  CheckAllocKind(X1, AllocKind::kPreExisting);\r\n  CheckAllocKind(X2, AllocKind::kReuse);\r\n  CheckAllocKind(X3, AllocKind::kAllocateOutput);\r\n  CheckAllocKind(X4, AllocKind::kAllocateOutput);\r\n\r\n  // check each ml-value is freed at appropriate step\r\n  // X2 will not be reused and will not be freed. X3 will be allocated and will be freed.\r\n  CheckFreed(0, {});\r\n  CheckFreed(1, {});\r\n  CheckFreed(2, {});\r\n}\r\n\r\nTEST_F(PlannerTest, MayStridedTest3) {\r\n  // tensor variables:\r\n  std::string X1(\"X1\"), X2(\"X2\"), X3(\"X3\"), X4(\"X4\");\r\n\r\n  // graph structure:\r\n  AddMayStridedOutputNode(X1, X2);  // X2 cannot strided reuse X1 because,\r\n  // one of X2's consumers is a node not supporting strided input. So X2 is a allocate.\r\n  AddMayStridedInputNode(X2, X3);  // X3 is a graph output, cannot reuse.\r\n  AddNormalNode(X2, X4);\r\n\r\n  // simulate shape-inference results:\r\n  Shape shape1{\"M\", \"N\"};\r\n  auto shape = &shape1.value;\r\n  SetShape({{X1, shape}, {X2, shape}, {X3, shape}, {X4, shape}});\r\n\r\n  CreatePlan();\r\n\r\n  // check allocation kind:\r\n  CheckAllocKind(X1, AllocKind::kPreExisting);\r\n  CheckAllocKind(X2, AllocKind::kAllocate);\r\n  CheckAllocKind(X3, AllocKind::kAllocateOutput);\r\n  CheckAllocKind(X4, AllocKind::kAllocateOutput);\r\n\r\n  // Be noted: the last two nodes added can run in two different orders, we need figure out the exact order\r\n  // we planned then we know how to check the free order.\r\n  const GraphViewer& graph_viewer = GetState().GetGraphViewer();\r\n  // Normal node index is 2.\r\n  bool does_normal_node_run_at_last = graph_viewer.GetNodesInTopologicalOrder()[2] == 2;\r\n\r\n  // check each ml-value is freed at appropriate step\r\n\r\n  CheckFreed(0, {});\r\n\r\n  if (does_normal_node_run_at_last) {\r\n    // Normal node has node index to be 2, but it is possible that the normal node is executed after the strided node.\r\n    // Then X2 will released once the normal node is executed.\r\n    CheckFreed(1, {});\r\n    CheckFreed(2, {X2});\r\n  } else {\r\n    // Normal node has node index to be 2, and is executed before the strided node.\r\n    // So X2 will be released after the strided node (node index to be 1) is executed.\r\n    CheckFreed(2, {});\r\n    CheckFreed(1, {X2});\r\n  }\r\n}\r\n#endif\r\n\r\n// InPlaceSizeMismatchTest: Check that Inplace reuse is not allowed when sizes don't match.\r\n// Also tests reuse of disjoint lifetime tensors.\r\nTEST_F(PlannerTest, InPlaceSizeMismatchTest) {\r\n  // tensor variables:\r\n  std::string X1(\"X1\"), X2(\"X2\"), X3(\"X3\"), X4(\"X4\"), X5(\"X5\");\r\n\r\n  // graph structure:\r\n  AddNormalNode(X1, X2);   // no in-place operator; X1: input; X2: temporary\r\n  AddInplaceNode(X2, X3);  // may-in-place operator; X3: temporary\r\n  AddNormalNode(X3, X4);   // no in-place operator; X4: temporary (reuse X2)\r\n  AddInplaceNode(X4, X5);  // may-in-place operator; X5 output\r\n\r\n  // simulate shape-inference results:\r\n  Shape shape1w{\"M\", \"N\"};\r\n  auto shape1 = &shape1w.value;\r\n  Shape shape2w{\"M\", \"K\"};\r\n  auto shape2 = &shape2w.value;\r\n  SetShape({{X1, shape1}, {X2, shape1}, {X3, shape2}, {X4, shape1}, {X5, shape1}});\r\n\r\n  CreatePlan();\r\n\r\n  // check allocation kind:\r\n  CheckAllocKind(X1, AllocKind::kPreExisting);\r\n  CheckAllocKind(X2, AllocKind::kAllocate);\r\n  CheckAllocKind(X3, AllocKind::kAllocate);\r\n  CheckAllocKind(X4, AllocKind::kReuse);\r\n  CheckAllocKind(X5, AllocKind::kAllocateOutput);\r\n\r\n  // check each ml-value is freed at appropriate step\r\n  CheckFreed(0, {});\r\n  CheckFreed(1, {});\r\n  CheckFreed(2, {X3});\r\n  CheckFreed(3, {X2});\r\n}\r\n\r\n// Test operator<< to output details of an allocation & execution plan.\r\nTEST_F(PlannerTest, PlanOutputTest) {\r\n  // tensor variables:\r\n  std::string X1(\"X1\"), X2(\"X2\"), X3(\"X3\"), X4(\"X4\");\r\n\r\n  // graph structure:\r\n  AddNormalNode(X1, X2);   // no in-place operator; X1: input; X2: temporary\r\n  AddInplaceNode(X2, X3);  // may-in-place operator; X3: temporary\r\n  AddNormalNode(X3, X4);   // no in-place operator; X4: output\r\n\r\n  // simulate shape-inference results:\r\n  Shape shape1{\"M\", \"N\"};\r\n  auto shape = &shape1.value;\r\n  SetShape({{X1, shape}, {X2, shape}, {X3, shape}, {X4, shape}});\r\n\r\n  CreatePlan();\r\n\r\n  ORT_TRY {\r\n    std::ostringstream output;\r\n    output << std::make_pair(&GetPlan(), &GetState());\r\n    auto output_size = output.str().size();\r\n    // Currently, we don't check details of the output, as it may change over time.\r\n    EXPECT_GT(output_size, 0u);\r\n  }\r\n  ORT_CATCH(const std::exception& ex) {\r\n    ORT_HANDLE_EXCEPTION([&ex]() {\r\n      EXPECT_TRUE(false) << \"Exception in producing output: \" << ex.what();\r\n    });\r\n  }\r\n}\r\n\r\n#ifdef USE_CUDA\r\nTEST_F(PlannerTest, LocationPlanningForPassThroughExplicitAndImplicitSubgraphInputs) {\r\n  // Types\r\n  TypeProto float_tensor;\r\n  float_tensor.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  float_tensor.mutable_tensor_type()->mutable_shape()->add_dim()->set_dim_param(\"dim_param\");\r\n\r\n  TypeProto int64_scalar;\r\n  int64_scalar.mutable_tensor_type()->set_elem_type(TensorProto_DataType_INT64);\r\n  int64_scalar.mutable_tensor_type()->mutable_shape()->add_dim()->set_dim_value(1);\r\n\r\n  TypeProto bool_scalar;\r\n  bool_scalar.mutable_tensor_type()->set_elem_type(TensorProto_DataType_BOOL);\r\n  bool_scalar.mutable_tensor_type()->mutable_shape()->add_dim()->set_dim_value(1);\r\n\r\n  // The model has a main graph and 2 levels of nested subgraphs\r\n  // Main graph: 2 Abs nodes + one Loop node\r\n  // First level (Loop) subgraph: Identity (condition pass-through) + If node\r\n  // Second level subgraph(s): Then and Else branches: Both have an Add node\r\n  // The Add node adds 2 values:\r\n  // One value from the main graph (\"abs_data_0_out\") that is \"implicitly\"\r\n  // consumed by the Loop node and \"passed through\" to the If subgraphs.\r\n  // Another value from the main graph (\"abs_data_1_out\") that is \"explicitly\"\r\n  // consumed by the Loop node as a loop carried dependency and its name in\r\n  // the scope of the Loop node is \"loop_state_var\".\r\n\r\n  // In the Loop subgraph, there are no explicit consumers of \"abs_data_0_out\"\r\n  // and \"loop_state_var\", there is only one implicit consumer - \"If\".\r\n  // We want to ensure that since there are no explicit consumers, the planned locations\r\n  // for these values in this subgraph are the same locations as their corresponding\r\n  // values in the outer scope, thus deferring any copies (if required) till the actual\r\n  // subgraph(s) they are explicitly consumed in.\r\n  auto create_model = [&float_tensor, &int64_scalar, &bool_scalar]() -> Model {\r\n    auto create_if_subgraph = [&float_tensor](bool is_then) -> GraphProto {\r\n      Model model(\"if_branch_subgraph\", true, DefaultLoggingManager().DefaultLogger());\r\n      auto& graph = model.MainGraph();\r\n\r\n      auto& outer_scope_0 = graph.GetOrCreateNodeArg(\"loop_state_var\", &float_tensor);\r\n      graph.AddOuterScopeNodeArg(\"loop_state_var\");\r\n\r\n      auto& outer_scope_1 = graph.GetOrCreateNodeArg(\"abs_data_0_out\", &float_tensor);\r\n      graph.AddOuterScopeNodeArg(\"abs_data_0_out\");\r\n\r\n      auto& if_out = graph.GetOrCreateNodeArg(is_then ? \"if_then_out\" : \"if_else_out\", &float_tensor);\r\n      graph.AddNode(\"if_out\", \"Add\", \"add\", {&outer_scope_0, &outer_scope_1}, {&if_out});\r\n\r\n      auto status = graph.Resolve();\r\n      EXPECT_EQ(status, Status::OK());\r\n\r\n      return graph.ToGraphProto();\r\n    };\r\n\r\n    auto create_loop_subgraph = [&create_if_subgraph, &float_tensor, &int64_scalar, &bool_scalar]() -> GraphProto {\r\n      Model model(\"loop_subgraph\", true, DefaultLoggingManager().DefaultLogger());\r\n      auto& graph = model.MainGraph();\r\n\r\n      std::vector<NodeArg*> inputs;\r\n      std::vector<NodeArg*> outputs;\r\n\r\n      /*  Inputs: iter_num, cond_in, loop carried state variables.\r\n         iter_num_in    cond_in     [loop_state_var]\r\n           (unused)        |               |\r\n                       [Identity]         [If]\r\n                           |               |\r\n                        cond_out     loop_state_var_out\r\n    */\r\n\r\n      // graph inputs\r\n      auto& iter_num_in = graph.GetOrCreateNodeArg(\"iter_num_in\", &int64_scalar);\r\n      auto& cond_in = graph.GetOrCreateNodeArg(\"cond_in\", &bool_scalar);\r\n      auto& loop_state_var = graph.GetOrCreateNodeArg(\"loop_state_var\", &float_tensor);\r\n\r\n      // graph outputs\r\n      auto& cond_out = graph.GetOrCreateNodeArg(\"cond_out\", &bool_scalar);\r\n      auto& loop_state_var_out = graph.GetOrCreateNodeArg(\"loop_state_var_out\", &float_tensor);\r\n\r\n      // outer scope args\r\n      ORT_IGNORE_RETURN_VALUE(graph.GetOrCreateNodeArg(\"abs_data_0_out\", &float_tensor));\r\n      graph.AddOuterScopeNodeArg(\"abs_data_0_out\");\r\n\r\n      // cond_in -> cond_out\r\n      {\r\n        inputs = {&cond_in};\r\n        outputs = {&cond_out};\r\n\r\n        graph.AddNode(\"cond_in_identity\", \"Identity\", \"Forward cond_in to cond_out\", inputs, outputs);\r\n      }\r\n\r\n      // loop_state_var -> If(cond_in) -> loop_state_var_out\r\n      {\r\n        inputs = {&cond_in};\r\n        outputs = {&loop_state_var_out};\r\n\r\n        auto& node = graph.AddNode(\"loop_var_out\", \"If\", \"If with loop_state_var as implicit_input\", inputs, outputs);\r\n        node.AddAttribute(\"then_branch\", create_if_subgraph(true));\r\n        node.AddAttribute(\"else_branch\", create_if_subgraph(false));\r\n      }\r\n\r\n      graph.SetInputs({&iter_num_in, &cond_in, &loop_state_var});\r\n      graph.SetOutputs({&cond_out, &loop_state_var_out});\r\n\r\n      auto status = graph.Resolve();\r\n      EXPECT_EQ(status, Status::OK());\r\n\r\n      return graph.ToGraphProto();\r\n    };\r\n\r\n    onnxruntime::Model model(\"main_graph\", false, ModelMetaData(),\r\n                             PathString(), IOnnxRuntimeOpSchemaRegistryList(),\r\n                             {{kOnnxDomain, 12}}, {}, DefaultLoggingManager().DefaultLogger());\r\n    auto& main_graph = model.MainGraph();\r\n\r\n    // Abs-0\r\n    auto& abs_data_0_in = main_graph.GetOrCreateNodeArg(\"abs_data_0_in\", &float_tensor);\r\n    auto& abs_data_0_out = main_graph.GetOrCreateNodeArg(\"abs_data_0_out\", &float_tensor);\r\n    std::vector<onnxruntime::NodeArg*> abs_0_inputs = {&abs_data_0_in};\r\n    std::vector<onnxruntime::NodeArg*> abs_0_outputs = {&abs_data_0_out};\r\n    main_graph.AddNode(\"abs_0\", \"Abs\", \"node abs\", abs_0_inputs, abs_0_outputs);\r\n\r\n    // Abs-1\r\n    auto& abs_data_1_in = main_graph.GetOrCreateNodeArg(\"abs_data_1_in\", &float_tensor);\r\n    auto& abs_data_1_out = main_graph.GetOrCreateNodeArg(\"abs_data_1_out\", &float_tensor);\r\n    const std::array<onnxruntime::NodeArg*, 1> abs_1_inputs = {&abs_data_1_in};\r\n    const std::array<onnxruntime::NodeArg*, 1> abs_1_outputs = {&abs_data_1_out};\r\n    main_graph.AddNode(\"abs_1\", \"Abs\", \"node abs\", abs_1_inputs, abs_1_outputs);\r\n\r\n    // Loop\r\n    auto& iter_num_in = main_graph.GetOrCreateNodeArg(\"iter_num_in\", &int64_scalar);\r\n    auto& cond_in = main_graph.GetOrCreateNodeArg(\"cond_in\", &bool_scalar);\r\n    auto& loop_state_out_var = main_graph.GetOrCreateNodeArg(\"loop_state_out_var\", &float_tensor);\r\n\r\n    auto& loop_node = main_graph.AddNode(\"loop\", \"Loop\", \"Loop node\",\r\n                                         {&iter_num_in, &cond_in, &abs_data_1_out},\r\n                                         {&loop_state_out_var});\r\n    loop_node.AddAttribute(\"body\", create_loop_subgraph());\r\n\r\n    main_graph.SetInputs({&abs_data_0_in, &abs_data_1_in, &iter_num_in, &cond_in});\r\n    main_graph.SetOutputs({&loop_state_out_var});\r\n\r\n    auto status = main_graph.Resolve();\r\n    EXPECT_EQ(status, Status::OK());\r\n\r\n    return model;\r\n  };\r\n\r\n  // Create and load session\r\n  SessionOptions so;\r\n  InferenceSession sess{so, GetEnvironment()};\r\n\r\n  auto status = sess.RegisterExecutionProvider(DefaultCudaExecutionProvider());\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  std::string s1;\r\n  const bool rc = create_model().ToProto().SerializeToString(&s1);\r\n  EXPECT_EQ(rc, true);\r\n  std::stringstream sstr(s1);\r\n\r\n  status = sess.Load(sstr);\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  status = sess.Initialize();\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  // Check planned locations of values in the main graph that are implicit subgraph inputs\r\n  // and explicit subgraph inputs to the Loop node\r\n\r\n  // Main graph (L0 graph)\r\n  const auto& main_graph_session_state = sess.GetSessionState();\r\n\r\n  {\r\n    const auto& main_graph_ort_value_index_map = main_graph_session_state.GetOrtValueNameIdxMap();\r\n    const auto* main_graph_plan = main_graph_session_state.GetExecutionPlan();\r\n\r\n    OrtValueIndex abs_data_0_out_index;\r\n    ASSERT_STATUS_OK(main_graph_ort_value_index_map.GetIdx(\"abs_data_0_out\", abs_data_0_out_index));\r\n\r\n    OrtValueIndex abs_data_1_out_index;\r\n    ASSERT_STATUS_OK(main_graph_ort_value_index_map.GetIdx(\"abs_data_1_out\", abs_data_1_out_index));\r\n\r\n    EXPECT_EQ(main_graph_plan->allocation_plan[abs_data_0_out_index].location.Type(), OrtDevice::GPU);\r\n    EXPECT_EQ(main_graph_plan->allocation_plan[abs_data_1_out_index].location.Type(), OrtDevice::GPU);\r\n  }\r\n\r\n  // First subgraph (Loop) (L1 graph)\r\n  // There are 3 nodes in the main level- Only one of them has a subgraph (Loop).\r\n  // Find that.\r\n  const SessionState* find_first_subgraph_session_state = nullptr;\r\n  for (size_t i = 0; i < 3; ++i) {\r\n    find_first_subgraph_session_state = main_graph_session_state.GetSubgraphSessionState(i, \"body\");\r\n    if (find_first_subgraph_session_state) {\r\n      break;\r\n    }\r\n  }\r\n\r\n  const auto& first_subgraph_session_state = *find_first_subgraph_session_state;\r\n\r\n  {\r\n    const auto& first_subgraph_ort_value_index_map = first_subgraph_session_state.GetOrtValueNameIdxMap();\r\n    const auto* first_subgraph_plan = first_subgraph_session_state.GetExecutionPlan();\r\n\r\n    OrtValueIndex abs_data_0_out_index;\r\n    ASSERT_STATUS_OK(first_subgraph_ort_value_index_map.GetIdx(\"abs_data_0_out\", abs_data_0_out_index));\r\n\r\n    // \"abs_data_1_out\" is \"loop_state_var\" in this scope as it was consumed as an explicit subgraph input\r\n    // to Loop's body subgraph\r\n    OrtValueIndex abs_data_1_out_index;\r\n    ASSERT_STATUS_OK(first_subgraph_ort_value_index_map.GetIdx(\"loop_state_var\", abs_data_1_out_index));\r\n\r\n    // There are no explicit consumers of \"abs_data_0_out\" and \"loop_state_var (abs_data_1_out)\" in this scope.\r\n    // There is only one implicit consumer \"If\". Hence, check that we are preserving the locations of these values\r\n    // from the outer scope, thus deferring any copies till the actual nested subgraph these values are used in.\r\n    EXPECT_EQ(first_subgraph_plan->allocation_plan[abs_data_0_out_index].location.Type(), OrtDevice::GPU);\r\n    EXPECT_EQ(first_subgraph_plan->allocation_plan[abs_data_1_out_index].location.Type(), OrtDevice::GPU);\r\n  }\r\n}\r\n\r\nTEST_F(PlannerTest, LocationPlanningForInitializersOnlyUsedInANestedSubgraph) {\r\n  // This a simple model that has one outer scope initializer and an `If` node\r\n  // and that initializer is ONLY used in nested subgraphs (both the `If` subgraphs).\r\n  // We want to test that the location planned for this initializer accounts for\r\n  // its usage in the nested subgraphs and statically determines the right location\r\n  // for it (without defaulting to CPU).\r\n\r\n  // Types\r\n  TypeProto float_tensor;\r\n  float_tensor.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  float_tensor.mutable_tensor_type()->mutable_shape()->add_dim()->set_dim_param(\"dim_param\");\r\n\r\n  TypeProto bool_scalar;\r\n  bool_scalar.mutable_tensor_type()->set_elem_type(TensorProto_DataType_BOOL);\r\n  bool_scalar.mutable_tensor_type()->mutable_shape()->add_dim()->set_dim_value(1);\r\n\r\n  auto create_model = [&float_tensor, &bool_scalar]() -> Model {\r\n    auto create_if_subgraph = [&float_tensor](bool is_then) -> GraphProto {\r\n      Model model(\"if_branch_subgraph\", true, DefaultLoggingManager().DefaultLogger());\r\n      auto& graph = model.MainGraph();\r\n\r\n      auto& outer_scope_0 = graph.GetOrCreateNodeArg(\"abs_data_out\", &float_tensor);\r\n      graph.AddOuterScopeNodeArg(\"abs_data_out\");\r\n\r\n      auto& outer_scope_1 = graph.GetOrCreateNodeArg(\"init_data\", &float_tensor);\r\n      graph.AddOuterScopeNodeArg(\"init_data\");\r\n\r\n      auto& if_out = graph.GetOrCreateNodeArg(is_then ? \"if_then_out\" : \"if_else_out\", &float_tensor);\r\n      graph.AddNode(\"if_out\", \"Add\", \"add\", {&outer_scope_0, &outer_scope_1}, {&if_out});\r\n\r\n      auto status = graph.Resolve();\r\n      EXPECT_EQ(status, Status::OK());\r\n\r\n      return graph.ToGraphProto();\r\n    };\r\n\r\n    onnxruntime::Model model(\"main_graph\", false, ModelMetaData(),\r\n                             PathString(), IOnnxRuntimeOpSchemaRegistryList(),\r\n                             {{kOnnxDomain, 12}}, {}, DefaultLoggingManager().DefaultLogger());\r\n    auto& main_graph = model.MainGraph();\r\n\r\n    // Abs-0\r\n    auto& abs_data_in = main_graph.GetOrCreateNodeArg(\"abs_data_in\", &float_tensor);\r\n    auto& abs_data_out = main_graph.GetOrCreateNodeArg(\"abs_data_out\", &float_tensor);\r\n    main_graph.AddNode(\"abs_0\", \"Abs\", \"node abs\", {&abs_data_in}, {&abs_data_out});\r\n\r\n    // If\r\n    auto& if_in = main_graph.GetOrCreateNodeArg(\"if_in\", &bool_scalar);\r\n    auto& if_out = main_graph.GetOrCreateNodeArg(\"if_out\", &float_tensor);\r\n    auto& node = main_graph.AddNode(\"if_out\", \"If\", \"If\", {&if_in}, {&if_out});\r\n    node.AddAttribute(\"then_branch\", create_if_subgraph(true));\r\n    node.AddAttribute(\"else_branch\", create_if_subgraph(false));\r\n\r\n    // Add initializer to the graph\r\n    ONNX_NAMESPACE::TensorProto tensor;\r\n    tensor.add_dims(1);\r\n    tensor.add_float_data(1.0f);\r\n    tensor.set_data_type(TensorProto_DataType_FLOAT);\r\n    tensor.set_name(\"init_data\");\r\n    graph_utils::AddInitializerWithOrtValue(main_graph, tensor);\r\n\r\n    // Main graph's inputs/outputs\r\n    main_graph.SetInputs({&abs_data_in, &if_in});\r\n    main_graph.SetOutputs({&if_out});\r\n\r\n    auto status = main_graph.Resolve();\r\n    EXPECT_EQ(status, Status::OK());\r\n\r\n    return model;\r\n  };\r\n\r\n  // Create and load session\r\n  SessionOptions so;\r\n  InferenceSession sess{so, GetEnvironment()};\r\n\r\n  auto status = sess.RegisterExecutionProvider(DefaultCudaExecutionProvider());\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  std::string s1;\r\n  const bool rc = create_model().ToProto().SerializeToString(&s1);\r\n  EXPECT_EQ(rc, true);\r\n  std::stringstream sstr(s1);\r\n\r\n  status = sess.Load(sstr);\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  status = sess.Initialize();\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  // Check planned locations for the initializer\r\n  const auto& main_graph_session_state = sess.GetSessionState();\r\n  const auto& main_graph_ort_value_index_map = main_graph_session_state.GetOrtValueNameIdxMap();\r\n  const auto* main_graph_plan = main_graph_session_state.GetExecutionPlan();\r\n\r\n  OrtValueIndex init_data_index;\r\n  ASSERT_STATUS_OK(main_graph_ort_value_index_map.GetIdx(\"init_data\", init_data_index));\r\n\r\n  EXPECT_EQ(main_graph_plan->allocation_plan[init_data_index].location.Type(), OrtDevice::GPU);\r\n}\r\n\r\nTEST_F(PlannerTest, LocationPlanningForInitializersUsedOnDifferentDevicesInMainGraphAndSubgraph) {\r\n  // This a simple model that has one outer scope initializer, an `If` node followed\r\n  // by a `TopK` node. The initializer is used in both nested subgraphs(`Add` consumes it\r\n  // and requires it on GPU) and main graph(the second input of `TopK` is required on CPU).\r\n  // The right location for the initializer should be CPU as no Memcpy will be inserted\r\n  // for a node in main graph that requires the input(initializer) on CPU if that initializer\r\n  // is placed on GPU by allocation planner.\r\n  TypeProto int_tensor;\r\n  int_tensor.mutable_tensor_type()->set_elem_type(TensorProto_DataType_INT64);\r\n  int_tensor.mutable_tensor_type()->mutable_shape()->add_dim()->set_dim_param(\"dim_param\");\r\n\r\n  TypeProto bool_scalar;\r\n  bool_scalar.mutable_tensor_type()->set_elem_type(TensorProto_DataType_BOOL);\r\n  bool_scalar.mutable_tensor_type()->mutable_shape()->add_dim()->set_dim_value(1);\r\n\r\n  auto create_model = [&int_tensor, &bool_scalar]() -> Model {\r\n    auto create_if_subgraph = [&int_tensor](bool is_then) -> GraphProto {\r\n      Model model(\"if_branch_subgraph\", true, DefaultLoggingManager().DefaultLogger());\r\n      auto& graph = model.MainGraph();\r\n\r\n      auto& outer_scope_0 = graph.GetOrCreateNodeArg(\"abs_data_out\", &int_tensor);\r\n      graph.AddOuterScopeNodeArg(\"abs_data_out\");\r\n\r\n      auto& outer_scope_1 = graph.GetOrCreateNodeArg(\"init_data\", &int_tensor);\r\n      graph.AddOuterScopeNodeArg(\"init_data\");\r\n\r\n      auto& if_out = graph.GetOrCreateNodeArg(is_then ? \"if_then_out\" : \"if_else_out\", &int_tensor);\r\n      graph.AddNode(\"if_out\", \"Add\", \"add\", {&outer_scope_0, &outer_scope_1}, {&if_out});\r\n\r\n      auto status = graph.Resolve();\r\n      EXPECT_EQ(status, Status::OK());\r\n\r\n      return graph.ToGraphProto();\r\n    };\r\n\r\n    onnxruntime::Model model(\"main_graph\", false, ModelMetaData(),\r\n                             PathString(), IOnnxRuntimeOpSchemaRegistryList(),\r\n                             {{kOnnxDomain, 12}}, {}, DefaultLoggingManager().DefaultLogger());\r\n    auto& main_graph = model.MainGraph();\r\n\r\n    // Abs-0\r\n    auto& abs_data_in = main_graph.GetOrCreateNodeArg(\"abs_data_in\", &int_tensor);\r\n    auto& abs_data_out = main_graph.GetOrCreateNodeArg(\"abs_data_out\", &int_tensor);\r\n    main_graph.AddNode(\"abs_0\", \"Abs\", \"node abs\", {&abs_data_in}, {&abs_data_out});\r\n\r\n    // If\r\n    auto& if_in = main_graph.GetOrCreateNodeArg(\"if_in\", &bool_scalar);\r\n    auto& if_out = main_graph.GetOrCreateNodeArg(\"if_out\", &int_tensor);\r\n    auto& node = main_graph.AddNode(\"if_out\", \"If\", \"If\", {&if_in}, {&if_out});\r\n    node.AddAttribute(\"then_branch\", create_if_subgraph(true));\r\n    node.AddAttribute(\"else_branch\", create_if_subgraph(false));\r\n\r\n    // TopK\r\n    auto& topk_data_in_0 = main_graph.GetOrCreateNodeArg(\"if_out\", &int_tensor);\r\n    auto& topk_data_in_1 = main_graph.GetOrCreateNodeArg(\"init_data\", &int_tensor);\r\n    auto& topk_data_out_0 = main_graph.GetOrCreateNodeArg(\"topk_data_out_0\", &int_tensor);\r\n    auto& topk_data_out_1 = main_graph.GetOrCreateNodeArg(\"topk_data_out_1\", &int_tensor);\r\n    main_graph.AddNode(\"topk_0\", \"TopK\", \"node topk\", {&topk_data_in_0, &topk_data_in_1},\r\n                       {&topk_data_out_0, &topk_data_out_1});\r\n\r\n    // Add initializer to the graph\r\n    ONNX_NAMESPACE::TensorProto tensor;\r\n    tensor.add_dims(1);\r\n    tensor.add_int64_data(1);\r\n    tensor.set_data_type(TensorProto_DataType_INT64);\r\n    tensor.set_name(\"init_data\");\r\n    graph_utils::AddInitializerWithOrtValue(main_graph, tensor);\r\n\r\n    // Main graph's inputs/outputs\r\n    main_graph.SetInputs({&abs_data_in, &if_in});\r\n    main_graph.SetOutputs({&topk_data_out_0, &topk_data_out_1});\r\n\r\n    auto status = main_graph.Resolve();\r\n    EXPECT_EQ(status, Status::OK());\r\n\r\n    return model;\r\n  };\r\n\r\n  // Create and load session\r\n  SessionOptions so;\r\n  InferenceSession sess{so, GetEnvironment()};\r\n\r\n  auto status = sess.RegisterExecutionProvider(DefaultCudaExecutionProvider());\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  std::string s1;\r\n  const bool rc = create_model().ToProto().SerializeToString(&s1);\r\n  EXPECT_EQ(rc, true);\r\n  std::stringstream sstr(s1);\r\n\r\n  status = sess.Load(sstr);\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  status = sess.Initialize();\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  // Check planned locations for the initializer\r\n  const auto& main_graph_session_state = sess.GetSessionState();\r\n  const auto& main_graph_ort_value_index_map = main_graph_session_state.GetOrtValueNameIdxMap();\r\n  const auto* main_graph_plan = main_graph_session_state.GetExecutionPlan();\r\n\r\n  OrtValueIndex init_data_index;\r\n  ASSERT_STATUS_OK(main_graph_ort_value_index_map.GetIdx(\"init_data\", init_data_index));\r\n\r\n  EXPECT_EQ(main_graph_plan->allocation_plan[init_data_index].location.Type(), OrtDevice::CPU);\r\n\r\n  // TODO: test para exe plan on subgraph supported\r\n  // const auto* para_graph_plan = const_cast<SessionState&>(main_graph_session_state).GetParallelExecutionPlan();\r\n  // EXPECT_EQ(para_graph_plan->allocation_plan[init_data_index].location.device.Type(), OrtDevice::GPU);\r\n}\r\n\r\nTEST_F(PlannerTest, LocationPlanningForImplicitInputsWithoutExplicitConsumersInMainGraph) {\r\n  // This a simple model that has two inputs and an `If` node.\r\n  // The first input is the condition for the `If` node and the second input\r\n  // is an input consumed implicitly by the `If` node to be used in its subgraphs.\r\n  // Note that there are no other explicit consumers of this input in the main graph.\r\n\r\n  // We want to test that the location planned for this implicit input is the default device\r\n  // of the EP that the `If` node is partitioned to (which will be CUDA)\r\n  // and that it doesn't default to CPU.\r\n\r\n  // Types\r\n  TypeProto float_tensor;\r\n  float_tensor.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  float_tensor.mutable_tensor_type()->mutable_shape()->add_dim()->set_dim_param(\"dim_param\");\r\n\r\n  TypeProto bool_scalar;\r\n  bool_scalar.mutable_tensor_type()->set_elem_type(TensorProto_DataType_BOOL);\r\n  bool_scalar.mutable_tensor_type()->mutable_shape()->add_dim()->set_dim_value(1);\r\n\r\n  auto create_model = [&float_tensor, &bool_scalar]() -> Model {\r\n    auto create_if_subgraph = [&float_tensor](bool is_then) -> GraphProto {\r\n      Model model(\"if_branch_subgraph\", true, DefaultLoggingManager().DefaultLogger());\r\n      auto& graph = model.MainGraph();\r\n\r\n      auto& outer_scope_0 = graph.GetOrCreateNodeArg(\"image_data_in\", &float_tensor);\r\n      graph.AddOuterScopeNodeArg(\"image_data_in\");\r\n\r\n      auto& if_out = graph.GetOrCreateNodeArg(is_then ? \"if_then_out\" : \"if_else_out\", &float_tensor);\r\n      graph.AddNode(\"if_out\", \"Relu\", \"relu\", {&outer_scope_0}, {&if_out});\r\n\r\n      auto status = graph.Resolve();\r\n      EXPECT_EQ(status, Status::OK());\r\n\r\n      return graph.ToGraphProto();\r\n    };\r\n\r\n    onnxruntime::Model model(\"main_graph\", false, ModelMetaData(),\r\n                             PathString(), IOnnxRuntimeOpSchemaRegistryList(),\r\n                             {{kOnnxDomain, 12}}, {}, DefaultLoggingManager().DefaultLogger());\r\n    auto& main_graph = model.MainGraph();\r\n    auto& image_data_in = main_graph.GetOrCreateNodeArg(\"image_data_in\", &float_tensor);\r\n\r\n    // If\r\n    auto& if_in = main_graph.GetOrCreateNodeArg(\"if_in\", &bool_scalar);\r\n    auto& if_out = main_graph.GetOrCreateNodeArg(\"if_out\", &float_tensor);\r\n    auto& node = main_graph.AddNode(\"if_out\", \"If\", \"If\", {&if_in}, {&if_out});\r\n    node.AddAttribute(\"then_branch\", create_if_subgraph(true));\r\n    node.AddAttribute(\"else_branch\", create_if_subgraph(false));\r\n\r\n    // Main graph's inputs/outputs\r\n    main_graph.SetInputs({&image_data_in, &if_in});\r\n    main_graph.SetOutputs({&if_out});\r\n\r\n    auto status = main_graph.Resolve();\r\n    EXPECT_EQ(status, Status::OK());\r\n\r\n    return model;\r\n  };\r\n\r\n  // Create and load session\r\n  SessionOptions so;\r\n  InferenceSession sess{so, GetEnvironment()};\r\n\r\n  auto status = sess.RegisterExecutionProvider(DefaultCudaExecutionProvider());\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  std::string s1;\r\n  const bool rc = create_model().ToProto().SerializeToString(&s1);\r\n  EXPECT_EQ(rc, true);\r\n  std::stringstream sstr(s1);\r\n\r\n  status = sess.Load(sstr);\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  status = sess.Initialize();\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  // Check planned locations for the implicit input\r\n  const auto& main_graph_session_state = sess.GetSessionState();\r\n  const auto& main_graph_ort_value_index_map = main_graph_session_state.GetOrtValueNameIdxMap();\r\n  const auto* main_graph_plan = main_graph_session_state.GetExecutionPlan();\r\n\r\n  OrtValueIndex input_data_index;\r\n  ASSERT_STATUS_OK(main_graph_ort_value_index_map.GetIdx(\"image_data_in\", input_data_index));\r\n\r\n  EXPECT_EQ(main_graph_plan->allocation_plan[input_data_index].location.Type(), OrtDevice::GPU);\r\n\r\n  // TODO: test para exe plan on subgraph supported\r\n  // const auto* para_graph_plan = const_cast<SessionState&>(main_graph_session_state).GetParallelExecutionPlan();\r\n  // EXPECT_EQ(para_graph_plan->allocation_plan[input_data_index].location.device.Type(), OrtDevice::GPU);\r\n}\r\n\r\n// Test MultiStream scenario for the graph:\r\n// node1(CPU ep)->node2(CPU ep)->node3(CUDA ep)->node4(CPU ep)\r\nTEST_F(PlannerTest, MultiStream) {\r\n  ONNX_NAMESPACE::TensorProto tensor;\r\n  tensor.add_dims(1);\r\n  tensor.add_float_data(1.0f);\r\n  tensor.set_data_type(TensorProto_DataType_FLOAT);\r\n  tensor.set_name(\"Graph_input\");\r\n  GetGraph().AddInitializedTensor(tensor);\r\n\r\n  std::string Graph_input(\"Graph_input\"), Arg1(\"Arg1\"), Arg2(\"Arg2\"), Arg3(\"Arg3\"), Arg4(\"Arg4\");\r\n  AddNormalNode(Graph_input, Arg1);\r\n  AddNormalNode(Arg1, Arg2);\r\n  std::unique_ptr<::onnxruntime::KernelDef> cudaKernel = KernelDefBuilder().SetName(\"Transpose\").Provider(kCudaExecutionProvider).SinceVersion(1, 10).Build();\r\n  AddNode(*cudaKernel, Arg2, Arg3);\r\n  AddNormalNode(Arg3, Arg4);\r\n\r\n  CUDAExecutionProviderInfo epi;\r\n  onnxruntime::ProviderInfo_CUDA& ep = onnxruntime::GetProviderInfo_CUDA();\r\n  auto epFactory = ep.CreateExecutionProviderFactory(epi);\r\n  std::unique_ptr<IExecutionProvider> execution_provider = epFactory->CreateProvider();\r\n  ORT_THROW_IF_ERROR(GetExecutionProviders().Add(\"CUDAExecutionProvider\", std::move(execution_provider)));\r\n\r\n  CreatePlan({}, false);\r\n\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan.size(), 2) << \"2 logic streams for CPU and CUDA separately\";\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan[0]->steps_.size(), 6) << \"CPU stream has 6 steps\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[0]).name(), \"LaunchKernelStep\"), nullptr) << \"0th step: LaunchKernelStep for node 1\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[1]).name(), \"LaunchKernelStep\"), nullptr) << \"1st step: LaunchKernelStep for node 2\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[2]).name(), \"TriggerDownstreamStep\"), nullptr) << \"2nd step: TriggerDownstreamStep for node 3, no Activate/Wait step between node 2 and node 3\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[3]).name(), \"BarrierStep\"), nullptr) << \"3rd step: BarrierStep for node 4\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[4]).name(), \"WaitOnEPStep\"), nullptr) << \"4th step: WaitOnEPStep for node 4\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[5]).name(), \"LaunchKernelStep\"), nullptr) << \"5th step: LaunchKernelStep for node 4\";\r\n\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan[1]->steps_.size(), 4) << \"CUDA stream has 4 steps\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[0]).name(), \"BarrierStep\"), nullptr) << \"0th step: BarrierStep for node 3\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[1]).name(), \"LaunchKernelStep\"), nullptr) << \"1st step: LaunchKernelStep for node 3\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[2]).name(), \"ActivateNotificationStep\"), nullptr) << \"2nd step: ActivateNofiticationStep by node 3\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[3]).name(), \"TriggerDownstreamStep\"), nullptr) << \"3rd step: TriggerDownstreamStep for node 4\";\r\n}\r\n\r\n// Test execution plan for the graph:\r\n// node1   node2\r\n//   \\       /\r\n//    \\     /\r\n//      node3\r\n// All 3 nodes are CUDA EP, node1 is in stream0, node2 is in stream1, node3 is in stream2\r\nTEST_F(PlannerTest, MultiStream1StreamWaitFor2Streams) {\r\n  std::unique_ptr<::onnxruntime::KernelDef> cudaKernel = KernelDefBuilder().SetName(\"Transpose\").Provider(kCudaExecutionProvider).SinceVersion(1, 10).Build();\r\n  std::unique_ptr<::onnxruntime::KernelDef> cudaKernelAdd = KernelDefBuilder().SetName(\"Add\").Provider(kCudaExecutionProvider).SinceVersion(1, 10).Build();\r\n  std::string Graph_input(\"Graph_input\"), Arg1(\"Arg1\"), Arg2(\"Arg2\"), Arg3(\"Arg3\"), node1(\"node1\"), node2(\"node2\"), node3(\"node3\");\r\n  std::vector<onnxruntime::NodeArg*> input1{Arg(Graph_input)}, output1{Arg(Arg1)}, output2{Arg(Arg2)}, input3{Arg(Arg1), Arg(Arg2)}, output3{Arg(Arg3)};\r\n  AddNode(*cudaKernel, node1, input1, output1);\r\n  AddNode(*cudaKernel, node2, input1, output2);\r\n  AddNode(*cudaKernelAdd, node3, input3, output3);\r\n\r\n  CUDAExecutionProviderInfo epi;\r\n  onnxruntime::ProviderInfo_CUDA& ep = onnxruntime::GetProviderInfo_CUDA();\r\n  auto epFactory = ep.CreateExecutionProviderFactory(epi);\r\n  std::unique_ptr<IExecutionProvider> execution_provider = epFactory->CreateProvider();\r\n  ORT_THROW_IF_ERROR(GetExecutionProviders().Add(\"CUDAExecutionProvider\", std::move(execution_provider)));\r\n\r\n  SetNodePartitionConfigFilePath(\"./testdata/multi_stream_models/3_gpu_streams.json\");\r\n  CreatePlan({}, false);\r\n\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan.size(), 3) << \"3 logic streams\";\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan[0]->steps_.size(), 3) << \"stream 0 has 3 steps\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[0]).name(), \"LaunchKernelStep\"), nullptr) << \"0th step: LaunchKernelStep for node 1\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[1]).name(), \"ActivateNotificationStep\"), nullptr) << \"1st step: ActivateNofiticationStep by node 1\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[2]).name(), \"TriggerDownstreamStep\"), nullptr) << \"2nd step: TriggerDownstreamStep for node 3\";\r\n\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan[1]->steps_.size(), 3) << \"stream 1 has 3 steps\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[0]).name(), \"LaunchKernelStep\"), nullptr) << \"0th step: LaunchKernelStep for node 2\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[1]).name(), \"ActivateNotificationStep\"), nullptr) << \"1st step: ActivateNofiticationStep by node 2\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[2]).name(), \"TriggerDownstreamStep\"), nullptr) << \"2nd step: TriggerDownstreamStep for node 3\";\r\n\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan[2]->steps_.size(), 5) << \"stream 2 has 5 steps\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[2]->steps_[0]).name(), \"BarrierStep\"), nullptr) << \"0th step: BarrierStep for node 3, for TriggerDownstreamStep in stream 1\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[2]->steps_[1]).name(), \"BarrierStep\"), nullptr) << \"1st step: BarrierStep for node 3, for TriggerDownstreamStep in stream 2\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[2]->steps_[2]).name(), \"WaitOnEPStep\"), nullptr) << \"2nd step: WaitOnEPStep for node 3, for ActivateNotificationStep in stream 1\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[2]->steps_[3]).name(), \"WaitOnEPStep\"), nullptr) << \"3rd step: WaitOnEPStep for node 3, for ActivateNotificationStep in stream 2\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[2]->steps_[4]).name(), \"LaunchKernelStep\"), nullptr) << \"4th step: LaunchKernelStep for node 3\";\r\n}\r\n\r\n// Test execution plan for the graph:\r\n// stream 0: node1 (MemcpyToHost, CUDA EP) -> node3 (Transpose, CUDA EP)\r\n// stream 1: node2 (CPU EP)\r\n// node1's output, which is consumed by both node2 and node3, is in CPU.\r\nTEST_F(PlannerTest, MultiStreamCudaEPNodeCPUOutput) {\r\n  MemcpyToHostInCuda_TransposeInCudaAndCpu(\"./testdata/multi_stream_models/memcpyToHost_same_stream_with_transpose.json\");\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan.size(), 2) << \"2 logic streams\";\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan[0]->steps_.size(), 5) << \"stream 0 has 5 steps\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[0]).name(), \"LaunchKernelStep\"), nullptr) << \"0th step: LaunchKernelStep for node 1\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[1]).name(), \"ActivateNotificationStep\"), nullptr) << \"1st step: ActivateNofiticationStep by node 1\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[2]).name(), \"TriggerDownstreamStep\"), nullptr) << \"2nd step: TriggerDownstreamStep for node 3\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[3]).name(), \"WaitOnEPStep\"), nullptr) << \"3rd step: WaitOnEPStep for node 3 in the same stream, as node 1's output is to CPU\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[4]).name(), \"LaunchKernelStep\"), nullptr) << \"4th step: LaunchKernelStep for node 3\";\r\n\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan[1]->steps_.size(), 3) << \"stream 1 has 3 steps\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[0]).name(), \"BarrierStep\"), nullptr) << \"0th step: BarrierStep for node 2, for TriggerDownstreamStep in stream 0\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[1]).name(), \"WaitOnEPStep\"), nullptr) << \"1st step: WaitOnEPStep for node 2, for ActivateNotificationStep in stream 0\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[2]).name(), \"LaunchKernelStep\"), nullptr) << \"2nd step: LaunchKernelStep for node 2\";\r\n}\r\n\r\n// Test execution plan for the graph:\r\n// node1 has 2 outputs which are both consumed by node2, node1 and node2 are in different streams\r\n// Only 1 WaitOnEPStep is expected before launching node2\r\n// TODO(leca): there is a bug in the corresponding graph that node2 will be visited twice when traversing node1's output nodes\r\n// (see: for (auto it = node->OutputNodesBegin(); it != node->OutputNodesEnd(); ++it) in BuildExecutionPlan()). We can just break the loop and don't need the extra variables once it is fixed\r\nTEST_F(PlannerTest, MultiStreamMultiOutput) {\r\n  std::unique_ptr<::onnxruntime::KernelDef> cudaKernel = KernelDefBuilder().SetName(\"RNN\").Provider(kCudaExecutionProvider).SinceVersion(7).Build();\r\n  std::string Graph_input1(\"Graph_input1\"), Graph_input2(\"Graph_input2\"), Graph_input3(\"Graph_input3\"), Arg1(\"Arg1\"), Arg2(\"Arg2\"), Arg3(\"Arg3\"), node1(\"node1\"), node2(\"node2\");\r\n  std::vector<onnxruntime::NodeArg*> input1{Arg(Graph_input1), Arg(Graph_input2), Arg(Graph_input3)}, output1{Arg(Arg1), Arg(Arg2)}, input2{Arg(Arg1), Arg(Arg2)}, output2{Arg(Arg3)};\r\n  AddNode(*cudaKernel, node1, input1, output1);\r\n\r\n  std::unique_ptr<::onnxruntime::KernelDef> cpuKernel = KernelDefBuilder().SetName(\"Add\").Provider(kCpuExecutionProvider).SinceVersion(7, 12).Build();\r\n  AddNode(*cpuKernel, node2, input2, output2);\r\n\r\n  CUDAExecutionProviderInfo epi;\r\n  onnxruntime::ProviderInfo_CUDA& ep = onnxruntime::GetProviderInfo_CUDA();\r\n  auto epFactory = ep.CreateExecutionProviderFactory(epi);\r\n  std::unique_ptr<IExecutionProvider> execution_provider = epFactory->CreateProvider();\r\n  ORT_THROW_IF_ERROR(GetExecutionProviders().Add(\"CUDAExecutionProvider\", std::move(execution_provider)));\r\n\r\n  CreatePlan({}, false);\r\n\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan.size(), 2) << \"2 logic streams\";\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan[0]->steps_.size(), 3) << \"stream 0 has 3 steps\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[0]).name(), \"LaunchKernelStep\"), nullptr) << \"0th step: LaunchKernelStep for node 1\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[1]).name(), \"ActivateNotificationStep\"), nullptr) << \"1st step: ActivateNofiticationStep by node 1\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[2]).name(), \"TriggerDownstreamStep\"), nullptr) << \"2nd step: TriggerDownstreamStep for node 2\";\r\n\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan[1]->steps_.size(), 3) << \"stream 1 has 3 steps\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[0]).name(), \"BarrierStep\"), nullptr) << \"0th step: BarrierStep for node 2, for TriggerDownstreamStep in stream 0\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[1]).name(), \"WaitOnEPStep\"), nullptr) << \"1st step: WaitOnEPStep for node 2, for ActivateNotificationStep in stream 0\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[2]).name(), \"LaunchKernelStep\"), nullptr) << \"2nd step: LaunchKernelStep for node 2\";\r\n}\r\n\r\n// Test execution plan for the graph:\r\n// node1   node2\r\n//   \\       /\r\n//    \\     /\r\n//      node3\r\n// node1 and node2 are in the same stream, both has an output which will be consumed by node3 in a different stream\r\n// TODO(leca): the ideal case is there is only 1 wait step before launching node3,\r\n// as there is a specific order between node1 and node2 if they are in the same stream, thus node3 will only need to wait the latter one\r\nTEST_F(PlannerTest, MultiStream2NodesSameStreamConsumedBy1NodeInDifferentStream) {\r\n  std::unique_ptr<::onnxruntime::KernelDef> cudaKernel = KernelDefBuilder().SetName(\"Transpose\").Provider(kCudaExecutionProvider).SinceVersion(1, 10).Build();\r\n  std::string Graph_input1(\"Graph_input1\"), Graph_input2(\"Graph_input2\"), Graph_input3(\"Graph_input3\"), Arg1(\"Arg1\"), Arg2(\"Arg2\"), Arg3(\"Arg3\"), node1(\"node1\"), node2(\"node2\"), node3(\"node3\");\r\n  std::vector<onnxruntime::NodeArg*> input1{Arg(Graph_input1)}, input2{Arg(Graph_input2)}, output1{Arg(Arg1)}, output2{Arg(Arg2)}, input3{Arg(Arg1), Arg(Arg2)}, output3{Arg(Arg3)};\r\n  AddNode(*cudaKernel, node1, input1, output1);\r\n  AddNode(*cudaKernel, node2, input2, output2);\r\n\r\n  std::unique_ptr<::onnxruntime::KernelDef> cpuKernel = KernelDefBuilder().SetName(\"Add\").Provider(kCpuExecutionProvider).SinceVersion(7, 12).Build();\r\n  AddNode(*cpuKernel, node3, input3, output3);\r\n\r\n  CUDAExecutionProviderInfo epi;\r\n  onnxruntime::ProviderInfo_CUDA& ep = onnxruntime::GetProviderInfo_CUDA();\r\n  auto epFactory = ep.CreateExecutionProviderFactory(epi);\r\n  std::unique_ptr<IExecutionProvider> execution_provider = epFactory->CreateProvider();\r\n  ORT_THROW_IF_ERROR(GetExecutionProviders().Add(\"CUDAExecutionProvider\", std::move(execution_provider)));\r\n\r\n  CreatePlan({}, false);\r\n\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan.size(), 2) << \"2 logic streams\";\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan[0]->steps_.size(), 6) << \"stream 0 has 6 steps\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[0]).name(), \"LaunchKernelStep\"), nullptr) << \"0th step: LaunchKernelStep for node 1\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[1]).name(), \"ActivateNotificationStep\"), nullptr) << \"1st step: ActivateNofiticationStep by node 1\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[2]).name(), \"TriggerDownstreamStep\"), nullptr) << \"2nd step: TriggerDownstreamStep for node 3\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[3]).name(), \"LaunchKernelStep\"), nullptr) << \"3rd step: LaunchKernelStep for node 2\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[4]).name(), \"ActivateNotificationStep\"), nullptr) << \"4th step: ActivateNofiticationStep by node 2\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[5]).name(), \"TriggerDownstreamStep\"), nullptr) << \"5th step: TriggerDownstreamStep for node 3\";\r\n\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan[1]->steps_.size(), 5) << \"stream 1 has 5 steps\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[0]).name(), \"BarrierStep\"), nullptr) << \"0th step: BarrierStep for node 1, for TriggerDownstreamStep in stream 0\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[1]).name(), \"BarrierStep\"), nullptr) << \"1st step: BarrierStep for node 2, for TriggerDownstreamStep in stream 0\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[2]).name(), \"WaitOnEPStep\"), nullptr) << \"2nd step: WaitOnEPStep for node 1, for ActivateNotificationStep in stream 0\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[3]).name(), \"WaitOnEPStep\"), nullptr) << \"3rd step: WaitOnEPStep for node 2, for ActivateNotificationStep in stream 0\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[4]).name(), \"LaunchKernelStep\"), nullptr) << \"4th step: LaunchKernelStep for node 3\";\r\n}\r\n#endif\r\n\r\n#if !defined(__wasm__) && defined(ORT_ENABLE_STREAM)\r\nTEST_F(PlannerTest, ParaPlanCreation) {\r\n  TypeProto graph_in_type;\r\n  graph_in_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  auto* graph_in_shape = graph_in_type.mutable_tensor_type()->mutable_shape();\r\n  graph_in_shape->add_dim()->set_dim_value(3L);\r\n  graph_in_shape->add_dim()->set_dim_value(3L);\r\n  graph_in_shape->add_dim()->set_dim_value(300L);\r\n  graph_in_shape->add_dim()->set_dim_value(300L);\r\n\r\n  TypeProto relu_0_out_type, relu_1_out_type, relu_2_out_type;\r\n  relu_0_out_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  relu_1_out_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  relu_2_out_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n\r\n  TypeProto maxpool_0_out_type;\r\n  maxpool_0_out_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n\r\n  TypeProto conv_0_weight_type, conv_1_weight_type, conv_2_weight_type, conv_3_weight_type, conv_4_weight_type;\r\n\r\n  conv_0_weight_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  auto* conv_0_weight_shape = conv_0_weight_type.mutable_tensor_type()->mutable_shape();\r\n  conv_0_weight_shape->add_dim()->set_dim_value(64L);\r\n  conv_0_weight_shape->add_dim()->set_dim_value(3L);\r\n  conv_0_weight_shape->add_dim()->set_dim_value(7L);\r\n  conv_0_weight_shape->add_dim()->set_dim_value(7L);\r\n\r\n  conv_1_weight_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  auto* conv_1_weight_shape = conv_1_weight_type.mutable_tensor_type()->mutable_shape();\r\n  conv_1_weight_shape->add_dim()->set_dim_value(64L);\r\n  conv_1_weight_shape->add_dim()->set_dim_value(64L);\r\n  conv_1_weight_shape->add_dim()->set_dim_value(1L);\r\n  conv_1_weight_shape->add_dim()->set_dim_value(1L);\r\n\r\n  conv_2_weight_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  auto* conv_2_weight_shape = conv_2_weight_type.mutable_tensor_type()->mutable_shape();\r\n  conv_2_weight_shape->add_dim()->set_dim_value(64L);\r\n  conv_2_weight_shape->add_dim()->set_dim_value(64L);\r\n  conv_2_weight_shape->add_dim()->set_dim_value(3L);\r\n  conv_2_weight_shape->add_dim()->set_dim_value(3L);\r\n\r\n  conv_3_weight_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  auto* conv_3_weight_shape = conv_3_weight_type.mutable_tensor_type()->mutable_shape();\r\n  conv_3_weight_shape->add_dim()->set_dim_value(256L);\r\n  conv_3_weight_shape->add_dim()->set_dim_value(64L);\r\n  conv_3_weight_shape->add_dim()->set_dim_value(1L);\r\n  conv_3_weight_shape->add_dim()->set_dim_value(1L);\r\n\r\n  conv_4_weight_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  auto* conv_4_weight_shape = conv_4_weight_type.mutable_tensor_type()->mutable_shape();\r\n  conv_4_weight_shape->add_dim()->set_dim_value(256L);\r\n  conv_4_weight_shape->add_dim()->set_dim_value(64L);\r\n  conv_4_weight_shape->add_dim()->set_dim_value(1L);\r\n  conv_4_weight_shape->add_dim()->set_dim_value(1L);\r\n\r\n  TypeProto conv_0_bias_type, conv_1_bias_type, conv_2_bias_type, conv_3_bias_type, conv_4_bias_type;\r\n  conv_0_bias_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  auto* conv_0_bias_shape = conv_0_bias_type.mutable_tensor_type()->mutable_shape();\r\n  conv_0_bias_shape->add_dim()->set_dim_value(64L);\r\n\r\n  conv_1_bias_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  auto* conv_1_bias_shape = conv_1_bias_type.mutable_tensor_type()->mutable_shape();\r\n  conv_1_bias_shape->add_dim()->set_dim_value(64L);\r\n\r\n  conv_2_bias_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  auto* conv_2_bias_shape = conv_2_bias_type.mutable_tensor_type()->mutable_shape();\r\n  conv_2_bias_shape->add_dim()->set_dim_value(64L);\r\n\r\n  conv_3_bias_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  auto* conv_3_bias_shape = conv_3_bias_type.mutable_tensor_type()->mutable_shape();\r\n  conv_3_bias_shape->add_dim()->set_dim_value(256L);\r\n\r\n  conv_4_bias_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  auto* conv_4_bias_shape = conv_4_bias_type.mutable_tensor_type()->mutable_shape();\r\n  conv_4_bias_shape->add_dim()->set_dim_value(256L);\r\n\r\n  TypeProto conv_0_out_type, conv_1_out_type, conv_2_out_type, conv_3_out_type, conv_4_out_type;\r\n  conv_0_out_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  conv_1_out_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  conv_2_out_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  conv_3_out_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  conv_4_out_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n\r\n  TypeProto graph_out_type;\r\n  graph_out_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n\r\n  onnxruntime::Model model(\"main_graph\", false, ModelMetaData(),\r\n                           PathString(), IOnnxRuntimeOpSchemaRegistryList(),\r\n                           {{kOnnxDomain, 14}}, {}, DefaultLoggingManager().DefaultLogger());\r\n  auto& main_graph = model.MainGraph();\r\n\r\n  auto& graph_in = main_graph.GetOrCreateNodeArg(\"graph_in\", &graph_in_type);\r\n\r\n  auto& maxpool_0_out = main_graph.GetOrCreateNodeArg(\"maxpool_out\", &maxpool_0_out_type);\r\n  auto& relu_0_out = main_graph.GetOrCreateNodeArg(\"relu_0_out\", &relu_0_out_type);\r\n  auto& relu_1_out = main_graph.GetOrCreateNodeArg(\"relu_1_out\", &relu_1_out_type);\r\n  auto& relu_2_out = main_graph.GetOrCreateNodeArg(\"relu_2_out\", &relu_2_out_type);\r\n\r\n  ONNX_NAMESPACE::TensorProto conv_0_weight_tensor;\r\n  conv_0_weight_tensor.add_dims(64L);\r\n  conv_0_weight_tensor.add_dims(3L);\r\n  conv_0_weight_tensor.add_dims(7L);\r\n  conv_0_weight_tensor.add_dims(7L);\r\n  for (int i = 0; i < 64 * 3 * 7 * 7; ++i) conv_0_weight_tensor.add_float_data(0.234f);\r\n  conv_0_weight_tensor.set_data_type(TensorProto_DataType_FLOAT);\r\n  conv_0_weight_tensor.set_name(\"conv_0_weight\");\r\n  graph_utils::AddInitializerWithOrtValue(main_graph, conv_0_weight_tensor);\r\n\r\n  ONNX_NAMESPACE::TensorProto conv_1_weight_tensor;\r\n  conv_1_weight_tensor.add_dims(64L);\r\n  conv_1_weight_tensor.add_dims(64L);\r\n  conv_1_weight_tensor.add_dims(1L);\r\n  conv_1_weight_tensor.add_dims(1L);\r\n  conv_1_weight_tensor.set_data_type(TensorProto_DataType_FLOAT);\r\n  for (int i = 0; i < 64 * 64; ++i) conv_1_weight_tensor.add_float_data(1.017f);\r\n  conv_1_weight_tensor.set_name(\"conv_1_weight\");\r\n  graph_utils::AddInitializerWithOrtValue(main_graph, conv_1_weight_tensor);\r\n\r\n  ONNX_NAMESPACE::TensorProto conv_2_weight_tensor;\r\n  conv_2_weight_tensor.add_dims(64L);\r\n  conv_2_weight_tensor.add_dims(64L);\r\n  conv_2_weight_tensor.add_dims(3L);\r\n  conv_2_weight_tensor.add_dims(3L);\r\n  for (int i = 0; i < 64 * 64 * 3 * 3; ++i) conv_2_weight_tensor.add_float_data(2.317f);\r\n  conv_2_weight_tensor.set_data_type(TensorProto_DataType_FLOAT);\r\n  conv_2_weight_tensor.set_name(\"conv_2_weight\");\r\n  graph_utils::AddInitializerWithOrtValue(main_graph, conv_2_weight_tensor);\r\n\r\n  ONNX_NAMESPACE::TensorProto conv_3_weight_tensor;\r\n  conv_3_weight_tensor.add_dims(256L);\r\n  conv_3_weight_tensor.add_dims(64L);\r\n  conv_3_weight_tensor.add_dims(1L);\r\n  conv_3_weight_tensor.add_dims(1L);\r\n  for (int i = 0; i < 256 * 64; ++i) conv_3_weight_tensor.add_float_data(1.256f);\r\n  conv_3_weight_tensor.set_data_type(TensorProto_DataType_FLOAT);\r\n  conv_3_weight_tensor.set_name(\"conv_3_weight\");\r\n  graph_utils::AddInitializerWithOrtValue(main_graph, conv_3_weight_tensor);\r\n\r\n  ONNX_NAMESPACE::TensorProto conv_4_weight_tensor;\r\n  conv_4_weight_tensor.add_dims(256L);\r\n  conv_4_weight_tensor.add_dims(64L);\r\n  conv_4_weight_tensor.add_dims(1L);\r\n  conv_4_weight_tensor.add_dims(1L);\r\n  for (int i = 0; i < 256 * 64; ++i) conv_4_weight_tensor.add_float_data(1.913f);\r\n  conv_4_weight_tensor.set_data_type(TensorProto_DataType_FLOAT);\r\n  conv_4_weight_tensor.set_name(\"conv_4_weight\");\r\n  graph_utils::AddInitializerWithOrtValue(main_graph, conv_4_weight_tensor);\r\n\r\n  auto& conv_0_weight = main_graph.GetOrCreateNodeArg(\"conv_0_weight\", &conv_0_weight_type);\r\n  auto& conv_1_weight = main_graph.GetOrCreateNodeArg(\"conv_1_weight\", &conv_1_weight_type);\r\n  auto& conv_2_weight = main_graph.GetOrCreateNodeArg(\"conv_2_weight\", &conv_2_weight_type);\r\n  auto& conv_3_weight = main_graph.GetOrCreateNodeArg(\"conv_3_weight\", &conv_3_weight_type);\r\n  auto& conv_4_weight = main_graph.GetOrCreateNodeArg(\"conv_4_weight\", &conv_4_weight_type);\r\n\r\n  ONNX_NAMESPACE::TensorProto conv_0_bias_tensor;\r\n  conv_0_bias_tensor.add_dims(64L);\r\n  conv_0_bias_tensor.set_data_type(TensorProto_DataType_FLOAT);\r\n  conv_0_bias_tensor.set_name(\"conv_0_bias\");\r\n  for (int i = 0; i < 64; ++i) conv_0_bias_tensor.add_float_data(1.123f);\r\n  graph_utils::AddInitializerWithOrtValue(main_graph, conv_0_bias_tensor);\r\n\r\n  ONNX_NAMESPACE::TensorProto conv_1_bias_tensor;\r\n  conv_1_bias_tensor.add_dims(64L);\r\n  for (int i = 0; i < 64; ++i) conv_1_bias_tensor.add_float_data(2.234f);\r\n  conv_1_bias_tensor.set_data_type(TensorProto_DataType_FLOAT);\r\n  conv_1_bias_tensor.set_name(\"conv_1_bias\");\r\n  graph_utils::AddInitializerWithOrtValue(main_graph, conv_1_bias_tensor);\r\n\r\n  ONNX_NAMESPACE::TensorProto conv_2_bias_tensor;\r\n  conv_2_bias_tensor.add_dims(64L);\r\n  for (int i = 0; i < 64; ++i) conv_2_bias_tensor.add_float_data(0.121f);\r\n  conv_2_bias_tensor.set_data_type(TensorProto_DataType_FLOAT);\r\n  conv_2_bias_tensor.set_name(\"conv_2_bias\");\r\n  graph_utils::AddInitializerWithOrtValue(main_graph, conv_2_bias_tensor);\r\n\r\n  ONNX_NAMESPACE::TensorProto conv_3_bias_tensor;\r\n  conv_3_bias_tensor.add_dims(256L);\r\n  for (int i = 0; i < 256; ++i) conv_3_bias_tensor.add_float_data(1.201f);\r\n  conv_3_bias_tensor.set_data_type(TensorProto_DataType_FLOAT);\r\n  conv_3_bias_tensor.set_name(\"conv_3_bias\");\r\n  graph_utils::AddInitializerWithOrtValue(main_graph, conv_3_bias_tensor);\r\n\r\n  ONNX_NAMESPACE::TensorProto conv_4_bias_tensor;\r\n  conv_4_bias_tensor.add_dims(256L);\r\n  for (int i = 0; i < 256; ++i) conv_4_bias_tensor.add_float_data(0.897f);\r\n  conv_4_bias_tensor.set_data_type(TensorProto_DataType_FLOAT);\r\n  conv_4_bias_tensor.set_name(\"conv_4_bias\");\r\n  graph_utils::AddInitializerWithOrtValue(main_graph, conv_4_bias_tensor);\r\n\r\n  auto& conv_0_bias = main_graph.GetOrCreateNodeArg(\"conv_0_bias\", &conv_0_bias_type);\r\n  auto& conv_1_bias = main_graph.GetOrCreateNodeArg(\"conv_1_bias\", &conv_1_bias_type);\r\n  auto& conv_2_bias = main_graph.GetOrCreateNodeArg(\"conv_2_bias\", &conv_2_bias_type);\r\n  auto& conv_3_bias = main_graph.GetOrCreateNodeArg(\"conv_3_bias\", &conv_3_bias_type);\r\n  auto& conv_4_bias = main_graph.GetOrCreateNodeArg(\"conv_4_bias\", &conv_4_bias_type);\r\n\r\n  auto& conv_0_out = main_graph.GetOrCreateNodeArg(\"conv_0_out\", &conv_0_out_type);\r\n  auto& conv_1_out = main_graph.GetOrCreateNodeArg(\"conv_1_out\", &conv_1_out_type);\r\n  auto& conv_2_out = main_graph.GetOrCreateNodeArg(\"conv_2_out\", &conv_2_out_type);\r\n  auto& conv_3_out = main_graph.GetOrCreateNodeArg(\"conv_3_out\", &conv_3_out_type);\r\n  auto& conv_4_out = main_graph.GetOrCreateNodeArg(\"conv_4_out\", &conv_4_out_type);\r\n\r\n  auto& graph_out = main_graph.GetOrCreateNodeArg(\"graph_out\", &graph_out_type);\r\n\r\n  NodeAttributes conv_0_attributes;\r\n\r\n  ONNX_NAMESPACE::AttributeProto dilation;\r\n  dilation.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  dilation.add_ints(1);\r\n  dilation.add_ints(1);\r\n  dilation.set_name(\"dilations\");\r\n  conv_0_attributes[\"dilations\"] = dilation;\r\n\r\n  ONNX_NAMESPACE::AttributeProto group;\r\n  group.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INT);\r\n  group.set_i(1);\r\n  group.set_name(\"group\");\r\n  conv_0_attributes[\"group\"] = group;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_0_kernel_shape;\r\n  conv_0_kernel_shape.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_0_kernel_shape.add_ints(7);\r\n  conv_0_kernel_shape.add_ints(7);\r\n  conv_0_kernel_shape.set_name(\"kernel_shape\");\r\n  conv_0_attributes[\"kernel_shape\"] = conv_0_kernel_shape;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_0_pads;\r\n  conv_0_pads.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_0_pads.add_ints(3);\r\n  conv_0_pads.add_ints(3);\r\n  conv_0_pads.add_ints(3);\r\n  conv_0_pads.add_ints(3);\r\n  conv_0_pads.set_name(\"pads\");\r\n  conv_0_attributes[\"pads\"] = conv_0_pads;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_0_strides;\r\n  conv_0_strides.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_0_strides.add_ints(2);\r\n  conv_0_strides.add_ints(2);\r\n  conv_0_strides.set_name(\"strides\");\r\n  conv_0_attributes[\"strides\"] = conv_0_strides;\r\n\r\n  main_graph.AddNode(\"conv_0\", \"Conv\", \"\", {&graph_in, &conv_0_weight, &conv_0_bias}, {&conv_0_out}, &conv_0_attributes);\r\n  main_graph.AddNode(\"relu_0\", \"Relu\", \"\", {&conv_0_out}, {&relu_0_out});\r\n\r\n  NodeAttributes maxpool_0_attributes;\r\n  ONNX_NAMESPACE::AttributeProto ceil_mode;\r\n  ceil_mode.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INT);\r\n  ceil_mode.set_i(0);\r\n  ceil_mode.set_name(\"ceil_mode\");\r\n  maxpool_0_attributes[\"ceil_mode\"] = ceil_mode;\r\n\r\n  ONNX_NAMESPACE::AttributeProto maxpool_0_kernel_shape;\r\n  maxpool_0_kernel_shape.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  maxpool_0_kernel_shape.add_ints(3);\r\n  maxpool_0_kernel_shape.add_ints(3);\r\n  maxpool_0_kernel_shape.set_name(\"kernel_shape\");\r\n  maxpool_0_attributes[\"kernel_shape\"] = maxpool_0_kernel_shape;\r\n\r\n  ONNX_NAMESPACE::AttributeProto maxpool_0_pads;\r\n  maxpool_0_pads.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  maxpool_0_pads.add_ints(1);\r\n  maxpool_0_pads.add_ints(1);\r\n  maxpool_0_pads.add_ints(1);\r\n  maxpool_0_pads.add_ints(1);\r\n  maxpool_0_pads.set_name(\"pads\");\r\n  maxpool_0_attributes[\"pads\"] = maxpool_0_pads;\r\n\r\n  ONNX_NAMESPACE::AttributeProto maxpool_0_strides;\r\n  maxpool_0_strides.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  maxpool_0_strides.add_ints(1);\r\n  maxpool_0_strides.add_ints(1);\r\n  maxpool_0_strides.set_name(\"strides\");\r\n  maxpool_0_attributes[\"strides\"] = maxpool_0_strides;\r\n\r\n  main_graph.AddNode(\"maxpool_0\", \"MaxPool\", \"\", {&relu_0_out}, {&maxpool_0_out}, &maxpool_0_attributes);\r\n\r\n  NodeAttributes conv_1_attributes;\r\n  conv_1_attributes[\"dilations\"] = dilation;\r\n  conv_1_attributes[\"group\"] = group;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_1_kernel_shape;\r\n  conv_1_kernel_shape.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_1_kernel_shape.add_ints(1);\r\n  conv_1_kernel_shape.add_ints(1);\r\n  conv_1_kernel_shape.set_name(\"kernel_shape\");\r\n  conv_1_attributes[\"kernel_shape\"] = conv_1_kernel_shape;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_1_pads;\r\n  conv_1_pads.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_1_pads.add_ints(0);\r\n  conv_1_pads.add_ints(0);\r\n  conv_1_pads.add_ints(0);\r\n  conv_1_pads.add_ints(0);\r\n  conv_1_pads.set_name(\"pads\");\r\n  conv_1_attributes[\"pads\"] = conv_1_pads;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_1_strides;\r\n  conv_1_strides.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_1_strides.add_ints(1);\r\n  conv_1_strides.add_ints(1);\r\n  conv_1_strides.set_name(\"strides\");\r\n  conv_1_attributes[\"strides\"] = conv_1_strides;\r\n\r\n  main_graph.AddNode(\"conv_1\", \"Conv\", \"\", {&maxpool_0_out, &conv_1_weight, &conv_1_bias}, {&conv_1_out}, &conv_1_attributes);\r\n  main_graph.AddNode(\"relu_1\", \"Relu\", \"\", {&conv_1_out}, {&relu_1_out});\r\n\r\n  NodeAttributes conv_2_attributes;\r\n  conv_2_attributes[\"dilations\"] = dilation;\r\n  conv_2_attributes[\"group\"] = group;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_2_kernel_shape;\r\n  conv_2_kernel_shape.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_2_kernel_shape.add_ints(3);\r\n  conv_2_kernel_shape.add_ints(3);\r\n  conv_2_kernel_shape.set_name(\"kernel_shape\");\r\n  conv_2_attributes[\"kernel_shape\"] = conv_2_kernel_shape;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_2_pads;\r\n  conv_2_pads.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_2_pads.add_ints(1);\r\n  conv_2_pads.add_ints(1);\r\n  conv_2_pads.add_ints(1);\r\n  conv_2_pads.add_ints(1);\r\n  conv_2_pads.set_name(\"pads\");\r\n  conv_2_attributes[\"pads\"] = conv_2_pads;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_2_strides;\r\n  conv_2_strides.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_2_strides.add_ints(1);\r\n  conv_2_strides.add_ints(1);\r\n  conv_2_strides.set_name(\"strides\");\r\n  conv_2_attributes[\"strides\"] = conv_2_strides;\r\n\r\n  main_graph.AddNode(\"conv_2\", \"Conv\", \"\", {&relu_1_out, &conv_2_weight, &conv_2_bias}, {&conv_2_out}, &conv_2_attributes);\r\n  main_graph.AddNode(\"relu_2\", \"Relu\", \"\", {&conv_2_out}, {&relu_2_out});\r\n\r\n  NodeAttributes conv_3_attributes;\r\n  conv_3_attributes[\"dilations\"] = dilation;\r\n  conv_3_attributes[\"group\"] = group;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_3_kernel_shape;\r\n  conv_3_kernel_shape.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_3_kernel_shape.add_ints(1);\r\n  conv_3_kernel_shape.add_ints(1);\r\n  conv_3_kernel_shape.set_name(\"kernel_shape\");\r\n  conv_3_attributes[\"kernel_shape\"] = conv_3_kernel_shape;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_3_pads;\r\n  conv_3_pads.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_3_pads.add_ints(0);\r\n  conv_3_pads.add_ints(0);\r\n  conv_3_pads.add_ints(0);\r\n  conv_3_pads.add_ints(0);\r\n  conv_3_pads.set_name(\"pads\");\r\n  conv_3_attributes[\"pads\"] = conv_3_pads;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_3_strides;\r\n  conv_3_strides.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_3_strides.add_ints(1);\r\n  conv_3_strides.add_ints(1);\r\n  conv_3_strides.set_name(\"strides\");\r\n  conv_3_attributes[\"strides\"] = conv_3_strides;\r\n\r\n  main_graph.AddNode(\"conv_3\", \"Conv\", \"\", {&relu_2_out, &conv_3_weight, &conv_3_bias}, {&conv_3_out}, &conv_3_attributes);\r\n\r\n  NodeAttributes conv_4_attributes;\r\n  conv_4_attributes[\"dilations\"] = dilation;\r\n  conv_4_attributes[\"group\"] = group;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_4_kernel_shape;\r\n  conv_4_kernel_shape.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_4_kernel_shape.add_ints(1);\r\n  conv_4_kernel_shape.add_ints(1);\r\n  conv_4_kernel_shape.set_name(\"kernel_shape\");\r\n  conv_4_attributes[\"kernel_shape\"] = conv_4_kernel_shape;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_4_pads;\r\n  conv_4_pads.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_4_pads.add_ints(0);\r\n  conv_4_pads.add_ints(0);\r\n  conv_4_pads.add_ints(0);\r\n  conv_4_pads.add_ints(0);\r\n  conv_4_pads.set_name(\"pads\");\r\n  conv_4_attributes[\"pads\"] = conv_4_pads;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_4_strides;\r\n  conv_4_strides.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_4_strides.add_ints(1);\r\n  conv_4_strides.add_ints(1);\r\n  conv_4_strides.set_name(\"strides\");\r\n  conv_4_attributes[\"strides\"] = conv_4_strides;\r\n\r\n  main_graph.AddNode(\"conv_4\", \"Conv\", \"\", {&maxpool_0_out, &conv_4_weight, &conv_4_bias}, {&conv_4_out}, &conv_4_attributes);\r\n  main_graph.AddNode(\"add_0\", \"Add\", \"\", {&conv_3_out, &conv_4_out}, {&graph_out});\r\n\r\n  main_graph.SetInputs({&graph_in});\r\n  main_graph.SetOutputs({&graph_out});\r\n\r\n  auto status = main_graph.Resolve();\r\n  EXPECT_EQ(status, Status::OK());\r\n\r\n  SessionOptions so;\r\n  so.graph_optimization_level = TransformerLevel::Default;\r\n  ASSERT_TRUE(so.config_options.AddConfigEntry(kNodePartitionConfigFile,\r\n                                               \"./testdata/multi_stream_models/simplified_ssd_cpu.json\")\r\n                  .IsOK());\r\n  InferenceSession sess{so, GetEnvironment()};\r\n\r\n  status = sess.RegisterExecutionProvider(DefaultCpuExecutionProvider());\r\n  ASSERT_TRUE(status.IsOK());\r\n  ASSERT_TRUE(model.Save(model, ORT_TSTR(\"./simplified_ssd.onnx\")).IsOK());\r\n\r\n  std::string s1;\r\n  const bool rc = model.ToProto().SerializeToString(&s1);\r\n  EXPECT_EQ(rc, true);\r\n  std::stringstream sstr(s1);\r\n\r\n  status = sess.Load(sstr);\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  status = sess.Initialize();\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  const auto& main_graph_session_state = sess.GetSessionState();\r\n  const auto& main_graph_ort_value_index_map = main_graph_session_state.GetOrtValueNameIdxMap();\r\n  auto* exe_plan = const_cast<onnxruntime::SessionState&>(main_graph_session_state).GetExecutionPlan();\r\n  auto& per_value_plans = exe_plan->GetAllocationPlan();\r\n  InlinedHashMap<std::string, std::string> reuse_pairs;\r\n  reuse_pairs.emplace(\"conv_0_out\", \"relu_0_out\");  // conv_0_out is reused by relu_0_out\r\n  reuse_pairs.emplace(\"conv_1_out\", \"relu_1_out\");  // conv_1_out is reused by relu_1_out\r\n  reuse_pairs.emplace(\"conv_2_out\", \"relu_2_out\");  // conv_2_out is reused by relu_2_out\r\n  for (size_t i = 0; i < per_value_plans.size(); ++i) {\r\n    auto& per_value_plan = per_value_plans[i];\r\n    if (per_value_plan.alloc_kind == AllocKind::kReuse) {\r\n      std::string reused;\r\n      ORT_ENFORCE(main_graph_ort_value_index_map.GetName(per_value_plan.reused_buffer, reused).IsOK());\r\n      reuse_pairs.erase(reused);\r\n    }  // if\r\n  }  // for\r\n  ASSERT_TRUE(reuse_pairs.empty());\r\n}\r\n\r\nTEST_F(PlannerTest, TestMultiStreamConfig) {\r\n  const char* type = \"DeviceBasedPartitioner\";\r\n  constexpr size_t type_len = 22;\r\n\r\n  auto graph_partitioner_cpu = IGraphPartitioner::CreateGraphPartitioner(\r\n      DefaultLoggingManager().DefaultLogger(),\r\n      ORT_TSTR(\"./testdata/multi_stream_models/multi_stream_single_stream.json\"));\r\n\r\n  ASSERT_TRUE(graph_partitioner_cpu &&\r\n              strncmp(graph_partitioner_cpu->Type(), type, type_len) == 0 &&\r\n              graph_partitioner_cpu->Streams() == 1);\r\n\r\n  auto graph_partitioner_cpu_gpu = IGraphPartitioner::CreateGraphPartitioner(\r\n      DefaultLoggingManager().DefaultLogger(),\r\n      ORT_TSTR(\"./testdata/multi_stream_models/multi_stream_double_stream.json\"));\r\n\r\n  ASSERT_TRUE(graph_partitioner_cpu_gpu &&\r\n              strncmp(graph_partitioner_cpu_gpu->Type(), type, type_len) == 0 &&\r\n              graph_partitioner_cpu_gpu->Streams() == 2);\r\n}\r\n\r\n// Save partition config to a file and check its completeness\r\nTEST_F(PlannerTest, TestMultiStreamSaveConfig) {\r\n  const char* config_file_path = \"./testdata/multi_stream_models/conv_add_relu_single_stream.json\";\r\n  {\r\n    SessionOptions sess_opt;\r\n    sess_opt.graph_optimization_level = TransformerLevel::Default;\r\n    ASSERT_TRUE(sess_opt.config_options.AddConfigEntry(kNodePartitionConfigFile,\r\n                                                       config_file_path)\r\n                    .IsOK());\r\n\r\n    InferenceSession sess(sess_opt, GetEnvironment(), ORT_TSTR(\"./testdata/multi_stream_models/conv_add_relu.onnx\"));\r\n    auto status = sess.RegisterExecutionProvider(DefaultCpuExecutionProvider());\r\n    ASSERT_TRUE(status.IsOK());\r\n\r\n    status = sess.Load();\r\n    ASSERT_TRUE(status.IsOK());\r\n\r\n    status = sess.Initialize();\r\n    ASSERT_TRUE(status.IsOK());\r\n  }\r\n\r\n  std::ifstream if_stream(config_file_path);\r\n  ASSERT_TRUE(if_stream.is_open());\r\n  std::set<std::string> node_set{\"model_41/conv2d_34/Conv2D__2321\",\r\n                                 \"model_41/conv2d_34/Conv2D\",\r\n                                 \"model_41/lambda_9/add\",\r\n                                 \"model_41/activation_27/Relu\",\r\n                                 \"Transpose__2331\"};\r\n\r\n  try {\r\n    json json_config = json::parse(if_stream);\r\n    ASSERT_TRUE(json_config[\"type\"] == \"DeviceBasedPartitioner\");\r\n\r\n    for (const auto& node_stream : json_config[\"streams\"]) {\r\n      ASSERT_TRUE(node_stream.is_array());\r\n\r\n      for (const auto& node_name : node_stream) {\r\n        ASSERT_TRUE(node_name.is_string());\r\n        auto iter = node_set.find(node_name);\r\n\r\n        ASSERT_TRUE(iter != node_set.end());\r\n        node_set.erase(iter);\r\n      }\r\n    }\r\n  } catch (...) {\r\n    ASSERT_TRUE(false);\r\n  }\r\n  if_stream.close();\r\n  ASSERT_TRUE(node_set.empty());\r\n}\r\n\r\n// Load with partition config where a node is missing, session load expected to fail.\r\nTEST_F(PlannerTest, TestMultiStreamMissingNodeConfig) {\r\n  const char* config_file_path = \"./testdata/multi_stream_models/conv_add_relu_single_stream_missing_node.json\";\r\n  SessionOptions sess_opt;\r\n  sess_opt.graph_optimization_level = TransformerLevel::Default;\r\n  ASSERT_TRUE(sess_opt.config_options.AddConfigEntry(kNodePartitionConfigFile,\r\n                                                     config_file_path)\r\n                  .IsOK());\r\n\r\n  InferenceSession sess(sess_opt, GetEnvironment(), ORT_TSTR(\"./testdata/multi_stream_models/conv_add_relu.onnx\"));\r\n  auto status = sess.RegisterExecutionProvider(DefaultCpuExecutionProvider());\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  status = sess.Load();\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  status = sess.Initialize();\r\n  ASSERT_TRUE(!status.IsOK());\r\n}\r\n\r\n// Load with partition config where streams and devices has mismatch\r\nTEST_F(PlannerTest, TestMultiStreamMismatchDevice) {\r\n  const char* config_file_path = \"./testdata/multi_stream_models/conv_add_relu_single_stream_mismatch_device.json\";\r\n  SessionOptions sess_opt;\r\n  sess_opt.graph_optimization_level = TransformerLevel::Default;\r\n  ASSERT_TRUE(sess_opt.config_options.AddConfigEntry(kNodePartitionConfigFile,\r\n                                                     config_file_path)\r\n                  .IsOK());\r\n\r\n  InferenceSession sess(sess_opt, GetEnvironment(), ORT_TSTR(\"./testdata/multi_stream_models/conv_add_relu.onnx\"));\r\n  auto status = sess.RegisterExecutionProvider(DefaultCpuExecutionProvider());\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  status = sess.Load();\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  status = sess.Initialize();\r\n  ASSERT_TRUE(!status.IsOK());\r\n}\r\n#endif\r\n\r\n#if defined(USE_CUDA) && defined(ORT_ENABLE_STREAM)\r\nTEST_F(PlannerTest, TestCpuIf) {\r\n  SessionOptions sess_opt;\r\n  sess_opt.graph_optimization_level = TransformerLevel::Default;\r\n\r\n  InferenceSession sess(sess_opt, GetEnvironment(), ORT_TSTR(\"./testdata/multi_stream_models/cpu_if.onnx\"));\r\n  ASSERT_STATUS_OK(sess.RegisterExecutionProvider(DefaultCudaExecutionProvider()));\r\n  ASSERT_STATUS_OK(sess.Load());\r\n  ASSERT_STATUS_OK(sess.Initialize());\r\n\r\n  auto& sess_state = const_cast<onnxruntime::SessionState&>(sess.GetSessionState());\r\n  const auto& exe_plan = sess_state.GetExecutionPlan()->execution_plan;\r\n  if (exe_plan.size() == 2 &&\r\n      exe_plan[1]->device_.Type() == OrtDevice::CPU &&\r\n      exe_plan[1]->steps_.size() == 9 &&\r\n      exe_plan[1]->steps_[7]->GetNodeIndex() == 7) {\r\n    // there must be a wait before cpu If node\r\n    static const std::string WaitOnEPStep = \"WaitOnEPStep\";\r\n    ASSERT_EQ(exe_plan[1]->steps_[6]->ToString().substr(0, WaitOnEPStep.size()), WaitOnEPStep);\r\n  }\r\n}\r\n\r\n// model looks like:\r\n//                                                 |-----------> Gather\r\n//                                                 |-----------> Gather\r\n//                                                 |-----------> Gather\r\n//                                                 |-----------> Gather\r\n// Shape ----------------> Reshape --> Shape ------------------> Reshape\r\n//                           ^                                     ^\r\n// InstanceNormalization ----|         InstanceNormalization ------|\r\n//\r\n// Python script to create this model:\r\n// def CreateModelFor19480():\r\n//    #shape->reshape->shape->reshape, 4 gather\r\n//    graphNodes = []\r\n//    graphNodes.append(h.make_node('Shape', inputs=['shape_input'], outputs=['9']))\r\n//    graphNodes.append(h.make_node('InstanceNormalization', inputs=['in0_input', 'scale0', 'B0'], outputs=['8']))\r\n//    graphNodes.append(h.make_node('Reshape', inputs=['8', '9'], outputs=['Reshape15_output']))\r\n//    graphNodes.append(h.make_node('Shape', inputs=['Reshape15_output'], outputs=['281']))\r\n//    graphNodes.append(h.make_node('InstanceNormalization', inputs=['in1_input', 'scale1', 'B1'], outputs=['293']))\r\n//    graphNodes.append(h.make_node('Reshape', inputs=['293', '281'], outputs=['output0']))\r\n//    graphNodes.append(h.make_node('Gather', inputs=['281', 'indices1'], outputs=['output1']))\r\n//    graphNodes.append(h.make_node('Gather', inputs=['281', 'indices2'], outputs=['output2']))\r\n//    graphNodes.append(h.make_node('Gather', inputs=['281', 'indices3'], outputs=['output3']))\r\n//    graphNodes.append(h.make_node('Gather', inputs=['281', 'indices4'], outputs=['output4']))\r\n//    g = h.make_graph(graphNodes, 'issue_19480',\r\n//                     [h.make_tensor_value_info('shape_input', tp.FLOAT, ['batch', 128, None, None]),\r\n//                      h.make_tensor_value_info('in0_input', tp.FLOAT, ['batch', 32, None]),\r\n//                      h.make_tensor_value_info('scale0', tp.FLOAT, [32]),\r\n//                      h.make_tensor_value_info('B0', tp.FLOAT, [32]),\r\n//                      h.make_tensor_value_info('in1_input', tp.FLOAT, ['batch', 32, None]),\r\n//                      h.make_tensor_value_info('scale1', tp.FLOAT, [32]),\r\n//                      h.make_tensor_value_info('B1', tp.FLOAT, [32]),\r\n//                      h.make_tensor_value_info('indices1', tp.INT32, []),\r\n//                      h.make_tensor_value_info('indices2', tp.INT32, []),\r\n//                      h.make_tensor_value_info('indices3', tp.INT32, []),\r\n//                      h.make_tensor_value_info('indices4', tp.INT32, [])],\r\n//                     [h.make_tensor_value_info('output0', tp.FLOAT, None),\r\n//                      h.make_tensor_value_info('output1', tp.INT64, None),\r\n//                      h.make_tensor_value_info('output2', tp.INT64, None),\r\n//                      h.make_tensor_value_info('output3', tp.INT64, None),\r\n//                      h.make_tensor_value_info('output4', tp.INT64, None)])\r\n//    model = h.make_model(g, opset_imports=[h.make_operatorsetid(\"\", 17)], producer_name='producer_name')\r\n//    onnx.save(model, 'issue_19480.onnx')\r\n//\r\nTEST(AllocationPlannerTest, ReusedInputCrossDifferentStreams) {\r\n  SessionOptions sess_opt;\r\n  sess_opt.graph_optimization_level = TransformerLevel::Default;\r\n\r\n  InferenceSession sess(sess_opt, GetEnvironment(), ORT_TSTR(\"./testdata/multi_stream_models/issue_19480.onnx\"));\r\n  auto status = sess.RegisterExecutionProvider(DefaultCudaExecutionProvider());\r\n  status = sess.Load();\r\n  status = sess.Initialize();\r\n  ASSERT_TRUE(status.IsOK()) << \"No crash\";\r\n  const SequentialExecutionPlan* plan = sess.GetSessionState().GetExecutionPlan();\r\n  ASSERT_EQ(plan->allocation_plan[14].alloc_kind, AllocKind::kReuse) << \"The input of reshape and gather will reuse the output of shape\";\r\n\r\n  int gather_count = 0;\r\n  ASSERT_GT(plan->execution_plan.size(), 1) << \"Number of execution plans should be greater than 1\";\r\n  for (size_t i = 0; i < plan->execution_plan[1]->steps_.size(); i++) {\r\n    if (strstr(typeid(*(plan->execution_plan[1]->steps_[i])).name(), \"LaunchKernelStep\")) {\r\n      const Node* node = sess.GetSessionState().GetGraphViewer().GetNode(plan->execution_plan[1]->steps_[i]->GetNodeIndex());\r\n      if (node->OpType() == \"Gather\")\r\n        gather_count++;\r\n      else\r\n        FAIL() << \"CPU stream should contain only gather ops\";\r\n    }\r\n  }\r\n  ASSERT_EQ(gather_count, 4) << \"4 gather ops are all placed in CPU stream\";\r\n}\r\n#endif\r\n\r\n#ifdef ENABLE_TRAINING_OPS\r\n// use a carefully constructed model to re-produce a customer reported issue where a model produced invalid output.\r\n// this issue required:\r\n// - buffer A that is re-used later in the model\r\n//   - output of the first Shape node\r\n//   - first usage completes after the following Cast node\r\n// - buffer B which has the same size requirement and is used after the first usage of A is complete\r\n//   - buffer B is used for the output from `squeeze2` and a number of other nodes in that part of the model.\r\n// - re-use of buffer A for an output of a node that has no consumers whilst buffer B is still in use\r\n//   - this is the `per_input_length` output of the ConcatTraining node\r\n//\r\n// Because the logic to determine when a buffer can be freed is based on consumers, buffer A gets freed after the\r\n// Cast node. It is then re-used as buffer B because the memory pattern planner believes that block to be available.\r\n// When we re-use buffer A for the ConcatTraining output we are using the same address for two different node output\r\n// buffers, leading to corruption of the output.\r\n// This tests that the change in allocation planner to not re-use a buffer for outputs with no consumers prevents this.\r\nTEST(AllocationPlannerTest, AvoidReuseOfBufferForNodeOutputWithNoConsumers) {\r\n  SessionOptions sess_opt;\r\n  sess_opt.graph_optimization_level = TransformerLevel::Default;\r\n\r\n  InferenceSession sess(sess_opt, GetEnvironment(), ORT_TSTR(\"./testdata/avoid_reuse_of_buffer_for_node_output_with_no_consumers.onnx\"));\r\n  auto status = sess.Load();\r\n  status = sess.Initialize();\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  const auto& session_state = sess.GetSessionState();\r\n  const auto& ort_value_index_map = session_state.GetOrtValueNameIdxMap();\r\n  const SequentialExecutionPlan* plan = session_state.GetExecutionPlan();\r\n\r\n  OrtValueIndex concat_training_unused_out_index;\r\n  // Here per_input_length output of the ConcatTraining node has no consumers, so it should not reuse the buffer.\r\n  ASSERT_STATUS_OK(ort_value_index_map.GetIdx(\"per_input_length\", concat_training_unused_out_index));\r\n  EXPECT_EQ(plan->allocation_plan[concat_training_unused_out_index].alloc_kind, AllocKind::kAllocate);\r\n}\r\n#endif\r\n\r\n}  // namespace test\r\n}  // namespace onnxruntime\r\n","replacement":"// Copyright (c) Microsoft Corporation. All rights reserved.\r\n// Licensed under the MIT License.\r\n\r\n#include <string>\r\n#include <unordered_map>\r\n#include <unordered_set>\r\n#include <sstream>\r\n#include \"gtest/gtest.h\"\r\n\r\n#ifdef ORT_ENABLE_STREAM\r\n#include \"nlohmann/json.hpp\"\r\nusing json = nlohmann::json;\r\n#endif\r\n\r\n#include \"core/framework/session_state.h\"\r\n#include \"core/framework/kernel_registry.h\"\r\n#include \"core/framework/op_kernel.h\"\r\n#include \"test/framework/model_builder_utils.h\"\r\n#include \"core/framework/allocation_planner.h\"\r\n#include \"core/session/inference_session.h\"\r\n#include \"core/graph/model.h\"\r\n#include \"core/graph/graph_utils.h\"\r\n#include \"core/providers/cpu/cpu_execution_provider.h\"\r\n#include \"core/util/thread_utils.h\"\r\n\r\n#include \"test/test_environment.h\"\r\n#include \"test/util/include/asserts.h\"\r\n#include \"test/util/include/default_providers.h\"\r\n#ifdef USE_CUDA\r\n#include \"core/providers/cuda/cuda_execution_provider.h\"\r\n#include \"core/providers/cuda/cuda_provider_factory.h\"\r\n#endif  // USE_CUDA\r\n#include \"core/session/onnxruntime_session_options_config_keys.h\"\r\nusing namespace ONNX_NAMESPACE;\r\n\r\nnamespace onnxruntime {\r\n#ifdef USE_CUDA\r\nProviderInfo_CUDA& GetProviderInfo_CUDA();\r\n#endif\r\nnamespace test {\r\n\r\nnamespace modelbuilder {\r\n\r\nclass NodeCounter {\r\n private:\r\n  static int node_count_;\r\n\r\n public:\r\n  static int Next() { return ++node_count_; }\r\n};\r\n\r\nint NodeCounter::node_count_ = 0;\r\n\r\nstruct UnaryNode {\r\n  std::vector<onnxruntime::NodeArg*> input_args;\r\n  std::vector<onnxruntime::NodeArg*> output_args;\r\n  onnxruntime::Node* p_node;\r\n\r\n  UnaryNode(onnxruntime::Graph& graph, const std::string& op, onnxruntime::NodeArg* p_input_arg,\r\n            onnxruntime::NodeArg* p_output_arg)\r\n      : input_args({p_input_arg}), output_args({p_output_arg}) {\r\n    int num = NodeCounter::Next();\r\n    p_node = &graph.AddNode(\"node\" + std::to_string(num), op, \"test op\", input_args, output_args);\r\n  }\r\n\r\n  UnaryNode(onnxruntime::Graph& graph, onnxruntime::NodeArg* p_input_arg, onnxruntime::NodeArg* p_output_arg)\r\n      : UnaryNode(graph, \"Transpose\", p_input_arg, p_output_arg) {}\r\n\r\n  UnaryNode(onnxruntime::Graph& graph, std::string& node_name, const std::string& op, std::vector<onnxruntime::NodeArg*>& inputs,\r\n            std::vector<onnxruntime::NodeArg*>& outputs) : input_args(inputs), output_args(outputs) {\r\n    p_node = &graph.AddNode(node_name, op, \"test op\", input_args, output_args);\r\n  }\r\n};\r\n\r\nclass DummyOpKernel : public OpKernel {\r\n public:\r\n  DummyOpKernel(const OpKernelInfo& p) : OpKernel(p) {}\r\n  Status Compute(OpKernelContext* context) const {\r\n    ORT_UNUSED_PARAMETER(context);\r\n    return Status::OK();\r\n  }\r\n  Status ComputeAsync(OpKernelContext* context, DoneCallback done) const {\r\n    ORT_UNUSED_PARAMETER(context);\r\n    ORT_UNUSED_PARAMETER(done);\r\n    return Status::OK();\r\n  }\r\n};\r\n\r\n}  // namespace modelbuilder\r\n\r\nusing namespace modelbuilder;\r\n\r\nclass AllocationPlanTestUtility {\r\n public:\r\n  static void CheckAllocationKind(const SequentialExecutionPlan& plan, std::vector<AllocKind>& expected) {\r\n    ASSERT_EQ(plan.allocation_plan.size(), expected.size()) << \"Allocation plan of wrong size\";\r\n    for (size_t i = 0; i < expected.size(); ++i) {\r\n      EXPECT_EQ(plan.allocation_plan[i].alloc_kind, expected[i]) << \"Error in allocation kind at position \" << i;\r\n    }\r\n  }\r\n  // The free list has been re-implmented.\r\n  // remove those checkers first.\r\n  // TODO: add the tests for new release plan.\r\n};\r\n\r\ntypedef std::unordered_map<const onnxruntime::NodeArg*, TensorShapeProto*> ShapeMap;\r\n\r\nclass SequentialPlannerTestContext : public ISequentialPlannerContext {\r\n public:\r\n  SequentialPlannerTestContext(ShapeMap* shape_map) : shape_map_(shape_map) {}\r\n\r\n  TensorShapeProto* GetShape(const onnxruntime::NodeArg& arg) const override {\r\n    auto iter = shape_map_->find(&arg);\r\n    return (shape_map_->end() != iter) ? iter->second : nullptr;\r\n  }\r\n\r\n private:\r\n  ShapeMap* shape_map_;\r\n};\r\n\r\nclass ParallelPlannerTestContext : public SequentialPlannerTestContext {\r\n public:\r\n  ParallelPlannerTestContext(ShapeMap* shape_map) : SequentialPlannerTestContext(shape_map) {\r\n  }\r\n  bool IsParallelExecutionEnabled() const override { return true; }\r\n  ExecutionOrder GetExecutionOrder() const override { return ExecutionOrder::DEFAULT; }\r\n  bool GetEnableMemoryReuse() const override { return false; }\r\n};\r\n\r\nclass PlannerTest : public ::testing::Test {\r\n private:\r\n  void index(const std::string& name, int& out) {\r\n    ASSERT_TRUE(state_->GetOrtValueNameIdxMap().GetIdx(name, out).IsOK());\r\n  }\r\n\r\n  onnxruntime::Model model_;\r\n  onnxruntime::Graph& graph_;\r\n\r\n  // some standard components used to build test-cases:\r\n  Type float_type_;\r\n\r\n  std::unique_ptr<::onnxruntime::KernelDef> std_kernel_;               // a unary kernel with no-aliasing and no-in-place\r\n  std::unique_ptr<::onnxruntime::KernelDef> in_place_kernel_;          // a unary kernel with in-place\r\n  std::unique_ptr<::onnxruntime::KernelDef> external_outputs_kernel_;  // an unary kernel with external outputs\r\n#ifdef ENABLE_STRIDED_TENSORS\r\n  std::unique_ptr<::onnxruntime::KernelDef> may_strided_input_kernel_;   // an uinary kernel with may_strided_input\r\n  std::unique_ptr<::onnxruntime::KernelDef> may_strided_output_kernel_;  // an unary kernel with may_strided_output\r\n#endif\r\n\r\n  std::unordered_map<std::string, onnxruntime::NodeArg*> name_to_arg_;\r\n  std::vector<std::unique_ptr<UnaryNode>> nodes_;\r\n  std::vector<std::unique_ptr<OpKernelInfo>> op_kernel_infos_;\r\n  std::vector<std::pair<onnxruntime::Node*, KernelDef&>> kernel_bindings_;\r\n  ExecutionProviders execution_providers_;\r\n  std::unique_ptr<concurrency::ThreadPool> tp_;\r\n  DataTransferManager dtm_;\r\n  ExternalDataLoaderManager edlm_;\r\n  profiling::Profiler profiler_;\r\n  std::unique_ptr<SessionOptions> sess_options_;\r\n  std::unique_ptr<SessionState> state_;\r\n  ShapeMap shape_map_;\r\n  std::optional<SequentialExecutionPlan> plan_;\r\n\r\n public:\r\n  PlannerTest()\r\n      : model_(\"test\", false, ModelMetaData(), PathString(), IOnnxRuntimeOpSchemaRegistryList(), {{kOnnxDomain, 10}}, {}, DefaultLoggingManager().DefaultLogger()),\r\n        graph_(model_.MainGraph()),\r\n        tp_(concurrency::CreateThreadPool(&onnxruntime::Env::Default(), OrtThreadPoolParams(),\r\n                                          concurrency::ThreadPoolType::INTRA_OP)) {\r\n    std_kernel_ = KernelDefBuilder().SetName(\"Transpose\").Provider(kCpuExecutionProvider).SinceVersion(1, 10).Build();\r\n    in_place_kernel_ =\r\n        KernelDefBuilder().SetName(\"Relu\").Provider(kCpuExecutionProvider).SinceVersion(1, 10).MayInplace(0, 0).Build();\r\n    external_outputs_kernel_ =\r\n        KernelDefBuilder().SetName(\"Tanh\").Provider(kCpuExecutionProvider).SinceVersion(1, 10).ExternalOutputs().Build();\r\n#ifdef ENABLE_STRIDED_TENSORS\r\n    may_strided_input_kernel_ = KernelDefBuilder()\r\n                                    .SetName(\"Abs\")\r\n                                    .Provider(kCpuExecutionProvider)\r\n                                    .SinceVersion(1, 10)\r\n                                    .MayStridedInput(0)\r\n                                    .Build();\r\n    may_strided_output_kernel_ = KernelDefBuilder()\r\n                                     .SetName(\"Neg\")\r\n                                     .Provider(kCpuExecutionProvider)\r\n                                     .SinceVersion(1, 10)\r\n                                     .MayStridedOutput(0, 0)\r\n                                     .Build();\r\n#endif\r\n    CPUExecutionProviderInfo epi;\r\n    auto execution_provider = std::make_unique<CPUExecutionProvider>(epi);\r\n    ORT_THROW_IF_ERROR(execution_providers_.Add(\"CPUExecutionProvider\", std::move(execution_provider)));\r\n    sess_options_ = std::make_unique<SessionOptions>();\r\n    sess_options_->enable_mem_pattern = false;\r\n    sess_options_->use_deterministic_compute = false;\r\n    sess_options_->enable_mem_reuse = true;\r\n    state_.reset(new SessionState(graph_, execution_providers_, tp_.get(), nullptr, dtm_, edlm_,\r\n                                  DefaultLoggingManager().DefaultLogger(), profiler_, *sess_options_));\r\n  }\r\n\r\n  onnxruntime::NodeArg* Arg(const std::string& name) {\r\n    auto iter = name_to_arg_.find(name);\r\n    if (name_to_arg_.end() != iter) return iter->second;\r\n    return (name_to_arg_[name] = &graph_.GetOrCreateNodeArg(name, &float_type_.value));\r\n  }\r\n\r\n  onnxruntime::Node* AddNode(::onnxruntime::KernelDef& kernel_def, std::string& input, std::string& output) {\r\n    auto node = std::make_unique<UnaryNode>(graph_, kernel_def.OpName(), Arg(input), Arg(output));\r\n    auto* p_node = node->p_node;\r\n    p_node->SetExecutionProviderType(kernel_def.Provider());\r\n    nodes_.push_back(std::move(node));\r\n    kernel_bindings_.emplace_back(p_node, kernel_def);\r\n    return p_node;\r\n  }\r\n\r\n  onnxruntime::Node* AddNode(::onnxruntime::KernelDef& kernel_def, std::string& node_name, std::vector<onnxruntime::NodeArg*>& input, std::vector<onnxruntime::NodeArg*>& output) {\r\n    auto node = std::make_unique<UnaryNode>(graph_, node_name, kernel_def.OpName(), input, output);\r\n    auto* p_node = node->p_node;\r\n    p_node->SetExecutionProviderType(kernel_def.Provider());\r\n    nodes_.push_back(std::move(node));\r\n    kernel_bindings_.emplace_back(p_node, kernel_def);\r\n    return p_node;\r\n  }\r\n\r\n  onnxruntime::Node* AddNormalNode(std::string& input, std::string& output) {\r\n    return AddNode(*std_kernel_, input, output);\r\n  }\r\n\r\n  onnxruntime::Node* AddInplaceNode(std::string& input, std::string& output) {\r\n    return AddNode(*in_place_kernel_, input, output);\r\n  }\r\n\r\n  onnxruntime::Node* AddExternalOutputsNode(std::string& input, std::string& output) {\r\n    return AddNode(*external_outputs_kernel_, input, output);\r\n  }\r\n\r\n#ifdef ENABLE_STRIDED_TENSORS\r\n  onnxruntime::Node* AddMayStridedInputNode(std::string& input, std::string& output) {\r\n    return AddNode(*may_strided_input_kernel_, input, output);\r\n  }\r\n\r\n  onnxruntime::Node* AddMayStridedOutputNode(std::string& input, std::string& output) {\r\n    return AddNode(*may_strided_output_kernel_, input, output);\r\n  }\r\n#endif\r\n\r\n  void BindKernel(onnxruntime::Node* p_node, ::onnxruntime::KernelDef& kernel_def, KernelRegistry* reg,\r\n                  std::unordered_map<NodeIndex, gsl::not_null<const KernelCreateInfo*>>& kernel_create_info_map) {\r\n    const auto& logger = DefaultLoggingManager().DefaultLogger();\r\n    const IExecutionProvider* ep = execution_providers_.Get(*p_node);\r\n    ASSERT_NE(ep, nullptr);\r\n    auto info = std::make_unique<OpKernelInfo>(\r\n        *p_node, kernel_def, *ep, state_->GetInitializedTensors(), state_->GetOrtValueNameIdxMap(),\r\n        state_->GetDataTransferMgr(), state_->GetAllocators(), state_->GetSessionOptions().config_options);\r\n\r\n    op_kernel_infos_.push_back(std::move(info));\r\n    const auto kernel_type_str_resolver = OpSchemaKernelTypeStrResolver{};\r\n    if (!KernelRegistry::HasImplementationOf(*reg, *p_node, onnxruntime::kCpuExecutionProvider,\r\n                                             kernel_type_str_resolver, logger)) {\r\n      ASSERT_STATUS_OK(reg->Register(\r\n          KernelCreateInfo(std::make_unique<KernelDef>(kernel_def),\r\n                           [](FuncManager&, const OpKernelInfo& info, std::unique_ptr<OpKernel>& out) -> Status {\r\n                             out = std::make_unique<DummyOpKernel>(info);\r\n                             return Status::OK();\r\n                           })));\r\n    }\r\n\r\n    const KernelCreateInfo* kci;\r\n    ASSERT_STATUS_OK(reg->TryFindKernel(*p_node, \"\", kernel_type_str_resolver, logger, &kci));\r\n    kernel_create_info_map.insert({p_node->Index(), gsl::not_null<const KernelCreateInfo*>(kci)});\r\n  }\r\n\r\n  void SetShape(std::string& name, TensorShapeProto* shape) { shape_map_[Arg(name)] = shape; }\r\n\r\n  void SetShape(std::initializer_list<std::pair<std::string&, TensorShapeProto*>> shapes) {\r\n    for (auto& pair : shapes) {\r\n      SetShape(pair.first, pair.second);\r\n    }\r\n  }\r\n\r\n  void CreatePlan(const std::vector<const NodeArg*>& outer_scope_node_args = {},\r\n                  bool invoke_createPlan_explicityly = true) {\r\n    state_.reset(new SessionState(graph_, execution_providers_, tp_.get(), nullptr, dtm_, edlm_,\r\n                                  DefaultLoggingManager().DefaultLogger(), profiler_, *sess_options_));\r\n    EXPECT_EQ(graph_.Resolve(), Status::OK());\r\n\r\n    std::shared_ptr<KernelRegistry> reg = std::make_shared<KernelRegistry>();\r\n    std::unordered_map<NodeIndex, gsl::not_null<const KernelCreateInfo*>> kernel_create_info_map;\r\n\r\n    for (auto& binding : kernel_bindings_) {\r\n      BindKernel(binding.first, binding.second, reg.get(), kernel_create_info_map);\r\n    }\r\n\r\n    auto cpu_execution_provider = std::make_unique<CPUExecutionProvider>(CPUExecutionProviderInfo());\r\n    KernelRegistryManager kernel_registry_manager;\r\n    kernel_registry_manager.RegisterKernelRegistry(reg);\r\n    auto status = kernel_registry_manager.RegisterKernels(execution_providers_);\r\n    EXPECT_TRUE(status.IsOK()) << status.ErrorMessage();\r\n\r\n    // CreatePlan is called inside FinalizeSessionState and usually the initializers are removed following that.\r\n    // Leave initializers so we can duplicate the call to CreatePlan from here to validate.\r\n    constexpr bool remove_initializers = false;\r\n    status = state_->FinalizeSessionState(ORT_TSTR(\"\"), kernel_registry_manager, {}, remove_initializers);\r\n\r\n    EXPECT_TRUE(status.IsOK()) << status.ErrorMessage();\r\n    SequentialPlannerTestContext test_context(&shape_map_);\r\n    plan_.emplace();\r\n\r\n    class MockStreamHandleRegsitry : public IStreamCommandHandleRegistry {\r\n     public:\r\n      // Wait is a little special as we need to consider the source stream the notification generated, and the stream we are waiting.\r\n      // i.e., for an cuda event what notify the memory copy, it could be wait on a CPU stream, or on another cuda stream.\r\n      virtual WaitNotificationFn GetWaitHandle(const OrtDevice& /*notification_owner_device*/,\r\n                                               const OrtDevice& /*executor_device*/) const override {\r\n        return nullptr;\r\n      }\r\n\r\n      virtual CreateStreamFn GetCreateStreamFn(const OrtDevice::DeviceType /*execution_provider_type*/) const override {\r\n        return nullptr;\r\n      }\r\n\r\n      virtual void RegisterWaitFn(const OrtDevice::DeviceType /*notification_ep_type*/, const OrtDevice::DeviceType /*ep_type*/, WaitNotificationFn /*fn*/) override {}\r\n\r\n      virtual void RegisterCreateStreamFn(const OrtDevice::DeviceType /*ep_type*/, CreateStreamFn /*f*/) override {}\r\n    };\r\n\r\n    if (invoke_createPlan_explicityly) {\r\n      onnxruntime::GraphViewer graph_viewer{graph_};\r\n      status = SequentialPlanner::CreatePlan(\r\n          nullptr,\r\n          graph_viewer,\r\n          outer_scope_node_args,\r\n          execution_providers_,\r\n          kernel_create_info_map,\r\n          {},\r\n          {},\r\n          state_->GetOrtValueNameIdxMap(),\r\n          test_context,\r\n#ifdef ORT_ENABLE_STREAM\r\n          MockStreamHandleRegsitry(),\r\n#endif\r\n          /* {{kCpuExecutionProvider, 1}}, {},*/\r\n          ORT_TSTR(\"\"),\r\n          DefaultLoggingManager().DefaultLogger(),\r\n          plan_);\r\n\r\n      EXPECT_TRUE(status.IsOK()) << status.ErrorMessage();\r\n      // AllocationPlanTestUtility::BasicIntegrityCheck(*plan_, name_to_arg_.size());\r\n    }\r\n  }\r\n\r\n  void CheckAllocKind(const std::string& name, AllocKind kind) {\r\n    int id;\r\n    index(name, id);\r\n    EXPECT_EQ(plan_->allocation_plan[id].alloc_kind, kind) << \"Error in allocation kind for \" << name;\r\n  }\r\n\r\n  void CheckFreed(int step_number, std::initializer_list<std::string> freed_items) {\r\n    // TODO: add the checker for new implementation of release plan\r\n    //// create set and check equality\r\n    std::unordered_set<int> expected;\r\n    for (auto& name : freed_items) {\r\n      int id;\r\n      index(name, id);\r\n      expected.insert(id);\r\n    }\r\n    std::unordered_set<int> plan_result;\r\n    // todo - support multi-stream\r\n    EXPECT_EQ(plan_->execution_plan.size(), 1U);\r\n    int list_size = static_cast<int>(plan_->node_release_list.size());\r\n    EXPECT_GT(list_size, step_number);\r\n    for (auto action_idx : plan_->node_release_list[step_number]) {\r\n      const size_t ortvalue_id = plan_->release_actions[action_idx].value_index;\r\n      plan_result.insert(static_cast<int>(ortvalue_id));\r\n    }\r\n    EXPECT_EQ(plan_result, expected) << \"Freed items incorrect for step \" << step_number;\r\n  }\r\n\r\n protected:\r\n  Graph& GetGraph() { return graph_; }\r\n  const SequentialExecutionPlan& GetPlan() const { return *plan_; }\r\n  const SessionState& GetState() const { return *state_; }\r\n  ExecutionProviders& GetExecutionProviders() { return execution_providers_; }\r\n  void SetNodePartitionConfigFilePath(const char* config_file_path) {\r\n    ORT_THROW_IF_ERROR(sess_options_->config_options.AddConfigEntry(kNodePartitionConfigFile, config_file_path));\r\n  }\r\n  std::unique_ptr<::onnxruntime::KernelDef>& GetStdKernel() { return std_kernel_; }\r\n#ifdef USE_CUDA\r\n  void MemcpyToHostInCuda_TransposeInCudaAndCpu(const char* partitionConfigFile = nullptr) {\r\n    std::unique_ptr<::onnxruntime::KernelDef> cudaKernel = KernelDefBuilder().SetName(\"MemcpyToHost\").Provider(kCudaExecutionProvider).SetDefaultOutputMemoryType(OrtMemTypeCPUOutput).Build();\r\n    std::unique_ptr<::onnxruntime::KernelDef> cudaKernelTrans = KernelDefBuilder().SetName(\"Transpose\").Provider(kCudaExecutionProvider).SinceVersion(1, 10).Build();\r\n    std::string Graph_input(\"Graph_input\"), Arg1(\"Arg1\"), Arg2(\"Arg2\"), Arg3(\"Arg3\"), node1(\"node1\"), node2(\"node2\"), node3(\"node3\");\r\n    std::vector<onnxruntime::NodeArg*> input1{Arg(Graph_input)}, output1{Arg(Arg1)}, output2{Arg(Arg2)}, output3{Arg(Arg3)};\r\n    AddNode(*cudaKernel, node1, input1, output1);\r\n    AddNode(*GetStdKernel(), node2, output1, output2);\r\n    AddNode(*cudaKernelTrans, node3, output1, output3);\r\n\r\n    CUDAExecutionProviderInfo epi;\r\n    onnxruntime::ProviderInfo_CUDA& ep = onnxruntime::GetProviderInfo_CUDA();\r\n    auto epFactory = ep.CreateExecutionProviderFactory(epi);\r\n    std::unique_ptr<IExecutionProvider> execution_provider = epFactory->CreateProvider();\r\n    ORT_THROW_IF_ERROR(GetExecutionProviders().Add(\"CUDAExecutionProvider\", std::move(execution_provider)));\r\n\r\n    if (partitionConfigFile != nullptr) SetNodePartitionConfigFilePath(partitionConfigFile);\r\n    CreatePlan({}, false);\r\n  }\r\n#endif  // USE_CUDA\r\n};\r\n\r\nTEST_F(PlannerTest, ChainTest) {\r\n  // tensor variables:\r\n  std::string W(\"W\"), X(\"X\"), B(\"B\"), Y(\"Y\"), Z(\"Z\");\r\n\r\n  // graph structure:\r\n\r\n  ONNX_NAMESPACE::TensorProto tensor;\r\n  tensor.add_dims(1);\r\n  tensor.add_float_data(1.0f);\r\n  tensor.set_data_type(TensorProto_DataType_FLOAT);\r\n  tensor.set_name(\"W\");\r\n  GetGraph().AddInitializedTensor(tensor);\r\n\r\n  AddNormalNode(W, X);\r\n  AddNormalNode(X, B);\r\n  AddNormalNode(B, Y);\r\n  AddNormalNode(Y, Z);\r\n\r\n  // simulate shape-inference results:\r\n  Shape shape1{50, 100};\r\n  auto shape = &shape1.value;\r\n  SetShape({{X, shape}, {B, shape}, {Y, shape}, {Z, shape}});\r\n\r\n  CreatePlan();\r\n\r\n  // Expected plan:\r\n  //   W: kAllocateStatically; X: kAllocate; B: kAllocate; Y: kReuse (X); post-node3: free(B); Z is returned output\r\n  CheckAllocKind(W, AllocKind::kAllocateStatically);\r\n  CheckAllocKind(X, AllocKind::kAllocate);\r\n  CheckAllocKind(B, AllocKind::kAllocate);\r\n  CheckAllocKind(Y, AllocKind::kReuse);\r\n  CheckAllocKind(Z, AllocKind::kAllocateOutput);\r\n\r\n  CheckFreed(0, {});\r\n  CheckFreed(1, {});\r\n  CheckFreed(2, {B});\r\n  CheckFreed(3, {X});\r\n}\r\n\r\n/* InputOutputTest: Test that:\r\n(a) All inputs are classified as kPreExisting,\r\n(b) All outer scope node args are classified as kPreExisting,\r\n(c) All outputs are classified as kAllocate (in this example),\r\n(d) Neither input nor outputs are freed.\r\n*/\r\nTEST_F(PlannerTest, InputOutputTest) {\r\n  // tensor variables:\r\n  std::string X1(\"X1\"), X2(\"X2\"), Y1(\"Y1\"), Y2(\"Y2\"), Outer1(\"Outer1\"), Y3(\"Y3\");\r\n\r\n  // graph structure:\r\n  AddNormalNode(X1, Y1);\r\n  AddNormalNode(X2, Y2);\r\n\r\n  // add node that consumes an outer scope node arg\r\n  auto outer_node = AddNormalNode(Outer1, Y3);\r\n  const NodeArg* outer_scope_node_arg = outer_node->InputDefs().at(0);\r\n  GetGraph().AddOuterScopeNodeArg(Outer1);\r\n\r\n  // simulate no shape-inference:\r\n\r\n  CreatePlan({outer_scope_node_arg});\r\n\r\n  // X1: kPreExisting, X2: kPreExisting, Outer1: kPreExisting, Y1: kAllocate, Y2: kAllocate, Y3: kAllocate\r\n  CheckAllocKind(X1, AllocKind::kPreExisting);\r\n  CheckAllocKind(X2, AllocKind::kPreExisting);\r\n  CheckAllocKind(Outer1, AllocKind::kPreExisting);\r\n  CheckAllocKind(Y1, AllocKind::kAllocateOutput);\r\n  CheckAllocKind(Y2, AllocKind::kAllocateOutput);\r\n  CheckAllocKind(Y3, AllocKind::kAllocateOutput);\r\n\r\n  // Nothing should be freed (since they are either inputs or outputs)\r\n  CheckFreed(0, {});\r\n  CheckFreed(1, {});\r\n  CheckFreed(2, {});\r\n}\r\n\r\n// InPlaceTest: Check that we reuse when Inplace allows us to.\r\n\r\nTEST_F(PlannerTest, InPlaceTest) {\r\n  // tensor variables:\r\n  std::string X1(\"X1\"), X2(\"X2\"), X3(\"X3\"), X4(\"X4\");\r\n\r\n  // graph structure:\r\n  AddNormalNode(X1, X2);   // no in-place operator; X1: input; X2: temporary\r\n  AddInplaceNode(X2, X3);  // may-in-place operator; X3: temporary\r\n  AddNormalNode(X3, X4);   // no in-place operator; X4: output\r\n\r\n  // simulate shape-inference results:\r\n  Shape shape1{\"M\", \"N\"};\r\n  auto shape = &shape1.value;\r\n  SetShape({{X1, shape}, {X2, shape}, {X3, shape}, {X4, shape}});\r\n\r\n  CreatePlan();\r\n\r\n  // check allocation kind:\r\n  CheckAllocKind(X1, AllocKind::kPreExisting);\r\n  CheckAllocKind(X2, AllocKind::kAllocate);\r\n  CheckAllocKind(X3, AllocKind::kReuse);\r\n  CheckAllocKind(X4, AllocKind::kAllocateOutput);\r\n\r\n  // check each ml-value is freed at appropriate step\r\n  CheckFreed(0, {});\r\n  CheckFreed(1, {});\r\n  CheckFreed(2, {X2});\r\n}\r\n\r\nTEST_F(PlannerTest, ExternalOutputsTest) {\r\n  // tensor variables:\r\n  std::string X1(\"X1\"), X2(\"X2\"), X3(\"X3\"), X4(\"X4\");\r\n\r\n  // graph structure:\r\n  AddExternalOutputsNode(X1, X2);  // external-outputs operator; X1: input; X2: temporary\r\n  AddNormalNode(X2, X3);           // normal operator; X3: temporary\r\n  AddNormalNode(X3, X4);           // normal operator; X4: output\r\n\r\n  // simulate shape-inference results:\r\n  Shape shape1{\"M\", \"N\"};\r\n  auto shape = &shape1.value;\r\n  SetShape({{X1, shape}, {X2, shape}, {X3, shape}, {X4, shape}});\r\n\r\n  CreatePlan();\r\n\r\n  // check allocation kind:\r\n  CheckAllocKind(X1, AllocKind::kPreExisting);\r\n  CheckAllocKind(X2, AllocKind::kAllocatedExternally);\r\n  CheckAllocKind(X3, AllocKind::kAllocate);\r\n  CheckAllocKind(X4, AllocKind::kAllocateOutput);\r\n\r\n  // check each ml-value is freed at appropriate step\r\n  // X2 will not be reused but will be freed (to release the current reference). X3 will be allocated and will be freed.\r\n  CheckFreed(0, {});\r\n  CheckFreed(1, {X2});\r\n  CheckFreed(2, {X3});\r\n}\r\n\r\nTEST_F(PlannerTest, ExternalOutputsNoReuseTest) {\r\n  // tensor variables:\r\n  std::string X1(\"X1\"), X2(\"X2\"), X3(\"X3\"), X4(\"X4\"), X5(\"X5\");\r\n\r\n  // graph structure:\r\n  AddExternalOutputsNode(X1, X2);  // external-outputs operator; X1: input; X2: temporary\r\n  AddInplaceNode(X2, X3);          // may-in-place operator; X3: temporary\r\n  AddNormalNode(X3, X4);           // normal operator; X4: temporary\r\n  AddNormalNode(X4, X5);           // normal operator; X5: output\r\n\r\n  // simulate shape-inference results:\r\n  Shape shape1{\"M\", \"N\"};\r\n  auto shape = &shape1.value;\r\n  SetShape({{X1, shape}, {X2, shape}, {X3, shape}, {X4, shape}, {X5, shape}});\r\n\r\n  CreatePlan();\r\n\r\n  // check allocation kind:\r\n  CheckAllocKind(X1, AllocKind::kPreExisting);\r\n  CheckAllocKind(X2, AllocKind::kAllocatedExternally);\r\n  CheckAllocKind(X3, AllocKind::kAllocate);  // Should not be Reused.\r\n  CheckAllocKind(X4, AllocKind::kAllocate);\r\n  CheckAllocKind(X5, AllocKind::kAllocateOutput);\r\n\r\n  // check each ml-value is freed at appropriate step\r\n  // X2 will not be reused. X3 will be allocated and will be freed.\r\n  CheckFreed(0, {});\r\n  CheckFreed(1, {X2});\r\n  CheckFreed(2, {X3});\r\n  CheckFreed(3, {X4});\r\n}\r\n\r\n#ifdef ENABLE_STRIDED_TENSORS\r\nTEST_F(PlannerTest, MayStridedTest1) {\r\n  // tensor variables:\r\n  std::string X1(\"X1\"), X2(\"X2\"), X3(\"X3\");\r\n\r\n  // graph structure:\r\n  AddNormalNode(X1, X2);\r\n  AddMayStridedOutputNode(X2, X3);  // may_strided_output as graph output.\r\n\r\n  // simulate shape-inference results:\r\n  Shape shape1{\"M\", \"N\"};\r\n  auto shape = &shape1.value;\r\n  SetShape({{X1, shape}, {X2, shape}, {X3, shape}});\r\n\r\n  CreatePlan();\r\n\r\n  // check allocation kind:\r\n  CheckAllocKind(X1, AllocKind::kPreExisting);\r\n  CheckAllocKind(X2, AllocKind::kAllocate);\r\n  CheckAllocKind(X3, AllocKind::kAllocateOutput);\r\n\r\n  // check each ml-value is freed at appropriate step\r\n  // X2 will not be reused because X3 is a graph output. X3 will be allocated and will be freed.\r\n  CheckFreed(0, {});\r\n  CheckFreed(1, {X2});\r\n}\r\n\r\nTEST_F(PlannerTest, MayStridedTest2) {\r\n  // tensor variables:\r\n  std::string X1(\"X1\"), X2(\"X2\"), X3(\"X3\"), X4(\"X4\");\r\n\r\n  // graph structure:\r\n  AddMayStridedOutputNode(X1, X2);  // X2 can reuse X1, and is a strided output.\r\n  AddMayStridedInputNode(X2, X3);   // X3 is a graph output, cannot reuse.\r\n  AddMayStridedInputNode(X2, X4);   // X4 is a graph output, cannot reuse.\r\n\r\n  // simulate shape-inference results:\r\n  Shape shape1{\"M\", \"N\"};\r\n  auto shape = &shape1.value;\r\n  SetShape({{X1, shape}, {X2, shape}, {X3, shape}, {X4, shape}});\r\n\r\n  CreatePlan();\r\n\r\n  // check allocation kind:\r\n  CheckAllocKind(X1, AllocKind::kPreExisting);\r\n  CheckAllocKind(X2, AllocKind::kReuse);\r\n  CheckAllocKind(X3, AllocKind::kAllocateOutput);\r\n  CheckAllocKind(X4, AllocKind::kAllocateOutput);\r\n\r\n  // check each ml-value is freed at appropriate step\r\n  // X2 will not be reused and will not be freed. X3 will be allocated and will be freed.\r\n  CheckFreed(0, {});\r\n  CheckFreed(1, {});\r\n  CheckFreed(2, {});\r\n}\r\n\r\nTEST_F(PlannerTest, MayStridedTest3) {\r\n  // tensor variables:\r\n  std::string X1(\"X1\"), X2(\"X2\"), X3(\"X3\"), X4(\"X4\");\r\n\r\n  // graph structure:\r\n  AddMayStridedOutputNode(X1, X2);  // X2 cannot strided reuse X1 because,\r\n  // one of X2's consumers is a node not supporting strided input. So X2 is a allocate.\r\n  AddMayStridedInputNode(X2, X3);  // X3 is a graph output, cannot reuse.\r\n  AddNormalNode(X2, X4);\r\n\r\n  // simulate shape-inference results:\r\n  Shape shape1{\"M\", \"N\"};\r\n  auto shape = &shape1.value;\r\n  SetShape({{X1, shape}, {X2, shape}, {X3, shape}, {X4, shape}});\r\n\r\n  CreatePlan();\r\n\r\n  // check allocation kind:\r\n  CheckAllocKind(X1, AllocKind::kPreExisting);\r\n  CheckAllocKind(X2, AllocKind::kAllocate);\r\n  CheckAllocKind(X3, AllocKind::kAllocateOutput);\r\n  CheckAllocKind(X4, AllocKind::kAllocateOutput);\r\n\r\n  // Be noted: the last two nodes added can run in two different orders, we need figure out the exact order\r\n  // we planned then we know how to check the free order.\r\n  const GraphViewer& graph_viewer = GetState().GetGraphViewer();\r\n  // Normal node index is 2.\r\n  bool does_normal_node_run_at_last = graph_viewer.GetNodesInTopologicalOrder()[2] == 2;\r\n\r\n  // check each ml-value is freed at appropriate step\r\n\r\n  CheckFreed(0, {});\r\n\r\n  if (does_normal_node_run_at_last) {\r\n    // Normal node has node index to be 2, but it is possible that the normal node is executed after the strided node.\r\n    // Then X2 will released once the normal node is executed.\r\n    CheckFreed(1, {});\r\n    CheckFreed(2, {X2});\r\n  } else {\r\n    // Normal node has node index to be 2, and is executed before the strided node.\r\n    // So X2 will be released after the strided node (node index to be 1) is executed.\r\n    CheckFreed(2, {});\r\n    CheckFreed(1, {X2});\r\n  }\r\n}\r\n#endif\r\n\r\n// InPlaceSizeMismatchTest: Check that Inplace reuse is not allowed when sizes don't match.\r\n// Also tests reuse of disjoint lifetime tensors.\r\nTEST_F(PlannerTest, InPlaceSizeMismatchTest) {\r\n  // tensor variables:\r\n  std::string X1(\"X1\"), X2(\"X2\"), X3(\"X3\"), X4(\"X4\"), X5(\"X5\");\r\n\r\n  // graph structure:\r\n  AddNormalNode(X1, X2);   // no in-place operator; X1: input; X2: temporary\r\n  AddInplaceNode(X2, X3);  // may-in-place operator; X3: temporary\r\n  AddNormalNode(X3, X4);   // no in-place operator; X4: temporary (reuse X2)\r\n  AddInplaceNode(X4, X5);  // may-in-place operator; X5 output\r\n\r\n  // simulate shape-inference results:\r\n  Shape shape1w{\"M\", \"N\"};\r\n  auto shape1 = &shape1w.value;\r\n  Shape shape2w{\"M\", \"K\"};\r\n  auto shape2 = &shape2w.value;\r\n  SetShape({{X1, shape1}, {X2, shape1}, {X3, shape2}, {X4, shape1}, {X5, shape1}});\r\n\r\n  CreatePlan();\r\n\r\n  // check allocation kind:\r\n  CheckAllocKind(X1, AllocKind::kPreExisting);\r\n  CheckAllocKind(X2, AllocKind::kAllocate);\r\n  CheckAllocKind(X3, AllocKind::kAllocate);\r\n  CheckAllocKind(X4, AllocKind::kReuse);\r\n  CheckAllocKind(X5, AllocKind::kAllocateOutput);\r\n\r\n  // check each ml-value is freed at appropriate step\r\n  CheckFreed(0, {});\r\n  CheckFreed(1, {});\r\n  CheckFreed(2, {X3});\r\n  CheckFreed(3, {X2});\r\n}\r\n\r\n// Test operator<< to output details of an allocation & execution plan.\r\nTEST_F(PlannerTest, PlanOutputTest) {\r\n  // tensor variables:\r\n  std::string X1(\"X1\"), X2(\"X2\"), X3(\"X3\"), X4(\"X4\");\r\n\r\n  // graph structure:\r\n  AddNormalNode(X1, X2);   // no in-place operator; X1: input; X2: temporary\r\n  AddInplaceNode(X2, X3);  // may-in-place operator; X3: temporary\r\n  AddNormalNode(X3, X4);   // no in-place operator; X4: output\r\n\r\n  // simulate shape-inference results:\r\n  Shape shape1{\"M\", \"N\"};\r\n  auto shape = &shape1.value;\r\n  SetShape({{X1, shape}, {X2, shape}, {X3, shape}, {X4, shape}});\r\n\r\n  CreatePlan();\r\n\r\n  ORT_TRY {\r\n    std::ostringstream output;\r\n    output << std::make_pair(&GetPlan(), &GetState());\r\n    auto output_size = output.str().size();\r\n    // Currently, we don't check details of the output, as it may change over time.\r\n    EXPECT_GT(output_size, 0u);\r\n  }\r\n  ORT_CATCH(const std::exception& ex) {\r\n    ORT_HANDLE_EXCEPTION([&ex]() {\r\n      EXPECT_TRUE(false) << \"Exception in producing output: \" << ex.what();\r\n    });\r\n  }\r\n}\r\n\r\n#ifdef USE_CUDA\r\nTEST_F(PlannerTest, LocationPlanningForPassThroughExplicitAndImplicitSubgraphInputs) {\r\n  // Types\r\n  TypeProto float_tensor;\r\n  float_tensor.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  float_tensor.mutable_tensor_type()->mutable_shape()->add_dim()->set_dim_param(\"dim_param\");\r\n\r\n  TypeProto int64_scalar;\r\n  int64_scalar.mutable_tensor_type()->set_elem_type(TensorProto_DataType_INT64);\r\n  int64_scalar.mutable_tensor_type()->mutable_shape()->add_dim()->set_dim_value(1);\r\n\r\n  TypeProto bool_scalar;\r\n  bool_scalar.mutable_tensor_type()->set_elem_type(TensorProto_DataType_BOOL);\r\n  bool_scalar.mutable_tensor_type()->mutable_shape()->add_dim()->set_dim_value(1);\r\n\r\n  // The model has a main graph and 2 levels of nested subgraphs\r\n  // Main graph: 2 Abs nodes + one Loop node\r\n  // First level (Loop) subgraph: Identity (condition pass-through) + If node\r\n  // Second level subgraph(s): Then and Else branches: Both have an Add node\r\n  // The Add node adds 2 values:\r\n  // One value from the main graph (\"abs_data_0_out\") that is \"implicitly\"\r\n  // consumed by the Loop node and \"passed through\" to the If subgraphs.\r\n  // Another value from the main graph (\"abs_data_1_out\") that is \"explicitly\"\r\n  // consumed by the Loop node as a loop carried dependency and its name in\r\n  // the scope of the Loop node is \"loop_state_var\".\r\n\r\n  // In the Loop subgraph, there are no explicit consumers of \"abs_data_0_out\"\r\n  // and \"loop_state_var\", there is only one implicit consumer - \"If\".\r\n  // We want to ensure that since there are no explicit consumers, the planned locations\r\n  // for these values in this subgraph are the same locations as their corresponding\r\n  // values in the outer scope, thus deferring any copies (if required) till the actual\r\n  // subgraph(s) they are explicitly consumed in.\r\n  auto create_model = [&float_tensor, &int64_scalar, &bool_scalar]() -> Model {\r\n    auto create_if_subgraph = [&float_tensor](bool is_then) -> GraphProto {\r\n      Model model(\"if_branch_subgraph\", true, DefaultLoggingManager().DefaultLogger());\r\n      auto& graph = model.MainGraph();\r\n\r\n      auto& outer_scope_0 = graph.GetOrCreateNodeArg(\"loop_state_var\", &float_tensor);\r\n      graph.AddOuterScopeNodeArg(\"loop_state_var\");\r\n\r\n      auto& outer_scope_1 = graph.GetOrCreateNodeArg(\"abs_data_0_out\", &float_tensor);\r\n      graph.AddOuterScopeNodeArg(\"abs_data_0_out\");\r\n\r\n      auto& if_out = graph.GetOrCreateNodeArg(is_then ? \"if_then_out\" : \"if_else_out\", &float_tensor);\r\n      graph.AddNode(\"if_out\", \"Add\", \"add\", {&outer_scope_0, &outer_scope_1}, {&if_out});\r\n\r\n      auto status = graph.Resolve();\r\n      EXPECT_EQ(status, Status::OK());\r\n\r\n      return graph.ToGraphProto();\r\n    };\r\n\r\n    auto create_loop_subgraph = [&create_if_subgraph, &float_tensor, &int64_scalar, &bool_scalar]() -> GraphProto {\r\n      Model model(\"loop_subgraph\", true, DefaultLoggingManager().DefaultLogger());\r\n      auto& graph = model.MainGraph();\r\n\r\n      std::vector<NodeArg*> inputs;\r\n      std::vector<NodeArg*> outputs;\r\n\r\n      /*  Inputs: iter_num, cond_in, loop carried state variables.\r\n         iter_num_in    cond_in     [loop_state_var]\r\n           (unused)        |               |\r\n                       [Identity]         [If]\r\n                           |               |\r\n                        cond_out     loop_state_var_out\r\n    */\r\n\r\n      // graph inputs\r\n      auto& iter_num_in = graph.GetOrCreateNodeArg(\"iter_num_in\", &int64_scalar);\r\n      auto& cond_in = graph.GetOrCreateNodeArg(\"cond_in\", &bool_scalar);\r\n      auto& loop_state_var = graph.GetOrCreateNodeArg(\"loop_state_var\", &float_tensor);\r\n\r\n      // graph outputs\r\n      auto& cond_out = graph.GetOrCreateNodeArg(\"cond_out\", &bool_scalar);\r\n      auto& loop_state_var_out = graph.GetOrCreateNodeArg(\"loop_state_var_out\", &float_tensor);\r\n\r\n      // outer scope args\r\n      ORT_IGNORE_RETURN_VALUE(graph.GetOrCreateNodeArg(\"abs_data_0_out\", &float_tensor));\r\n      graph.AddOuterScopeNodeArg(\"abs_data_0_out\");\r\n\r\n      // cond_in -> cond_out\r\n      {\r\n        inputs = {&cond_in};\r\n        outputs = {&cond_out};\r\n\r\n        graph.AddNode(\"cond_in_identity\", \"Identity\", \"Forward cond_in to cond_out\", inputs, outputs);\r\n      }\r\n\r\n      // loop_state_var -> If(cond_in) -> loop_state_var_out\r\n      {\r\n        inputs = {&cond_in};\r\n        outputs = {&loop_state_var_out};\r\n\r\n        auto& node = graph.AddNode(\"loop_var_out\", \"If\", \"If with loop_state_var as implicit_input\", inputs, outputs);\r\n        node.AddAttribute(\"then_branch\", create_if_subgraph(true));\r\n        node.AddAttribute(\"else_branch\", create_if_subgraph(false));\r\n      }\r\n\r\n      graph.SetInputs({&iter_num_in, &cond_in, &loop_state_var});\r\n      graph.SetOutputs({&cond_out, &loop_state_var_out});\r\n\r\n      auto status = graph.Resolve();\r\n      EXPECT_EQ(status, Status::OK());\r\n\r\n      return graph.ToGraphProto();\r\n    };\r\n\r\n    onnxruntime::Model model(\"main_graph\", false, ModelMetaData(),\r\n                             PathString(), IOnnxRuntimeOpSchemaRegistryList(),\r\n                             {{kOnnxDomain, 12}}, {}, DefaultLoggingManager().DefaultLogger());\r\n    auto& main_graph = model.MainGraph();\r\n\r\n    // Abs-0\r\n    auto& abs_data_0_in = main_graph.GetOrCreateNodeArg(\"abs_data_0_in\", &float_tensor);\r\n    auto& abs_data_0_out = main_graph.GetOrCreateNodeArg(\"abs_data_0_out\", &float_tensor);\r\n    std::vector<onnxruntime::NodeArg*> abs_0_inputs = {&abs_data_0_in};\r\n    std::vector<onnxruntime::NodeArg*> abs_0_outputs = {&abs_data_0_out};\r\n    main_graph.AddNode(\"abs_0\", \"Abs\", \"node abs\", abs_0_inputs, abs_0_outputs);\r\n\r\n    // Abs-1\r\n    auto& abs_data_1_in = main_graph.GetOrCreateNodeArg(\"abs_data_1_in\", &float_tensor);\r\n    auto& abs_data_1_out = main_graph.GetOrCreateNodeArg(\"abs_data_1_out\", &float_tensor);\r\n    const std::array<onnxruntime::NodeArg*, 1> abs_1_inputs = {&abs_data_1_in};\r\n    const std::array<onnxruntime::NodeArg*, 1> abs_1_outputs = {&abs_data_1_out};\r\n    main_graph.AddNode(\"abs_1\", \"Abs\", \"node abs\", abs_1_inputs, abs_1_outputs);\r\n\r\n    // Loop\r\n    auto& iter_num_in = main_graph.GetOrCreateNodeArg(\"iter_num_in\", &int64_scalar);\r\n    auto& cond_in = main_graph.GetOrCreateNodeArg(\"cond_in\", &bool_scalar);\r\n    auto& loop_state_out_var = main_graph.GetOrCreateNodeArg(\"loop_state_out_var\", &float_tensor);\r\n\r\n    auto& loop_node = main_graph.AddNode(\"loop\", \"Loop\", \"Loop node\",\r\n                                         {&iter_num_in, &cond_in, &abs_data_1_out},\r\n                                         {&loop_state_out_var});\r\n    loop_node.AddAttribute(\"body\", create_loop_subgraph());\r\n\r\n    main_graph.SetInputs({&abs_data_0_in, &abs_data_1_in, &iter_num_in, &cond_in});\r\n    main_graph.SetOutputs({&loop_state_out_var});\r\n\r\n    auto status = main_graph.Resolve();\r\n    EXPECT_EQ(status, Status::OK());\r\n\r\n    return model;\r\n  };\r\n\r\n  // Create and load session\r\n  SessionOptions so;\r\n  InferenceSession sess{so, GetEnvironment()};\r\n\r\n  auto status = sess.RegisterExecutionProvider(DefaultCudaExecutionProvider());\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  std::string s1;\r\n  const bool rc = create_model().ToProto().SerializeToString(&s1);\r\n  EXPECT_EQ(rc, true);\r\n  std::stringstream sstr(s1);\r\n\r\n  status = sess.Load(sstr);\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  status = sess.Initialize();\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  // Check planned locations of values in the main graph that are implicit subgraph inputs\r\n  // and explicit subgraph inputs to the Loop node\r\n\r\n  // Main graph (L0 graph)\r\n  const auto& main_graph_session_state = sess.GetSessionState();\r\n\r\n  {\r\n    const auto& main_graph_ort_value_index_map = main_graph_session_state.GetOrtValueNameIdxMap();\r\n    const auto* main_graph_plan = main_graph_session_state.GetExecutionPlan();\r\n\r\n    OrtValueIndex abs_data_0_out_index;\r\n    ASSERT_STATUS_OK(main_graph_ort_value_index_map.GetIdx(\"abs_data_0_out\", abs_data_0_out_index));\r\n\r\n    OrtValueIndex abs_data_1_out_index;\r\n    ASSERT_STATUS_OK(main_graph_ort_value_index_map.GetIdx(\"abs_data_1_out\", abs_data_1_out_index));\r\n\r\n    EXPECT_EQ(main_graph_plan->allocation_plan[abs_data_0_out_index].location.Type(), OrtDevice::GPU);\r\n    EXPECT_EQ(main_graph_plan->allocation_plan[abs_data_1_out_index].location.Type(), OrtDevice::GPU);\r\n  }\r\n\r\n  // First subgraph (Loop) (L1 graph)\r\n  // There are 3 nodes in the main level- Only one of them has a subgraph (Loop).\r\n  // Find that.\r\n  const SessionState* find_first_subgraph_session_state = nullptr;\r\n  for (size_t i = 0; i < 3; ++i) {\r\n    find_first_subgraph_session_state = main_graph_session_state.GetSubgraphSessionState(i, \"body\");\r\n    if (find_first_subgraph_session_state) {\r\n      break;\r\n    }\r\n  }\r\n\r\n  const auto& first_subgraph_session_state = *find_first_subgraph_session_state;\r\n\r\n  {\r\n    const auto& first_subgraph_ort_value_index_map = first_subgraph_session_state.GetOrtValueNameIdxMap();\r\n    const auto* first_subgraph_plan = first_subgraph_session_state.GetExecutionPlan();\r\n\r\n    OrtValueIndex abs_data_0_out_index;\r\n    ASSERT_STATUS_OK(first_subgraph_ort_value_index_map.GetIdx(\"abs_data_0_out\", abs_data_0_out_index));\r\n\r\n    // \"abs_data_1_out\" is \"loop_state_var\" in this scope as it was consumed as an explicit subgraph input\r\n    // to Loop's body subgraph\r\n    OrtValueIndex abs_data_1_out_index;\r\n    ASSERT_STATUS_OK(first_subgraph_ort_value_index_map.GetIdx(\"loop_state_var\", abs_data_1_out_index));\r\n\r\n    // There are no explicit consumers of \"abs_data_0_out\" and \"loop_state_var (abs_data_1_out)\" in this scope.\r\n    // There is only one implicit consumer \"If\". Hence, check that we are preserving the locations of these values\r\n    // from the outer scope, thus deferring any copies till the actual nested subgraph these values are used in.\r\n    EXPECT_EQ(first_subgraph_plan->allocation_plan[abs_data_0_out_index].location.Type(), OrtDevice::GPU);\r\n    EXPECT_EQ(first_subgraph_plan->allocation_plan[abs_data_1_out_index].location.Type(), OrtDevice::GPU);\r\n  }\r\n}\r\n\r\nTEST_F(PlannerTest, LocationPlanningForInitializersOnlyUsedInANestedSubgraph) {\r\n  // This a simple model that has one outer scope initializer and an `If` node\r\n  // and that initializer is ONLY used in nested subgraphs (both the `If` subgraphs).\r\n  // We want to test that the location planned for this initializer accounts for\r\n  // its usage in the nested subgraphs and statically determines the right location\r\n  // for it (without defaulting to CPU).\r\n\r\n  // Types\r\n  TypeProto float_tensor;\r\n  float_tensor.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  float_tensor.mutable_tensor_type()->mutable_shape()->add_dim()->set_dim_param(\"dim_param\");\r\n\r\n  TypeProto bool_scalar;\r\n  bool_scalar.mutable_tensor_type()->set_elem_type(TensorProto_DataType_BOOL);\r\n  bool_scalar.mutable_tensor_type()->mutable_shape()->add_dim()->set_dim_value(1);\r\n\r\n  auto create_model = [&float_tensor, &bool_scalar]() -> Model {\r\n    auto create_if_subgraph = [&float_tensor](bool is_then) -> GraphProto {\r\n      Model model(\"if_branch_subgraph\", true, DefaultLoggingManager().DefaultLogger());\r\n      auto& graph = model.MainGraph();\r\n\r\n      auto& outer_scope_0 = graph.GetOrCreateNodeArg(\"abs_data_out\", &float_tensor);\r\n      graph.AddOuterScopeNodeArg(\"abs_data_out\");\r\n\r\n      auto& outer_scope_1 = graph.GetOrCreateNodeArg(\"init_data\", &float_tensor);\r\n      graph.AddOuterScopeNodeArg(\"init_data\");\r\n\r\n      auto& if_out = graph.GetOrCreateNodeArg(is_then ? \"if_then_out\" : \"if_else_out\", &float_tensor);\r\n      graph.AddNode(\"if_out\", \"Add\", \"add\", {&outer_scope_0, &outer_scope_1}, {&if_out});\r\n\r\n      auto status = graph.Resolve();\r\n      EXPECT_EQ(status, Status::OK());\r\n\r\n      return graph.ToGraphProto();\r\n    };\r\n\r\n    onnxruntime::Model model(\"main_graph\", false, ModelMetaData(),\r\n                             PathString(), IOnnxRuntimeOpSchemaRegistryList(),\r\n                             {{kOnnxDomain, 12}}, {}, DefaultLoggingManager().DefaultLogger());\r\n    auto& main_graph = model.MainGraph();\r\n\r\n    // Abs-0\r\n    auto& abs_data_in = main_graph.GetOrCreateNodeArg(\"abs_data_in\", &float_tensor);\r\n    auto& abs_data_out = main_graph.GetOrCreateNodeArg(\"abs_data_out\", &float_tensor);\r\n    main_graph.AddNode(\"abs_0\", \"Abs\", \"node abs\", {&abs_data_in}, {&abs_data_out});\r\n\r\n    // If\r\n    auto& if_in = main_graph.GetOrCreateNodeArg(\"if_in\", &bool_scalar);\r\n    auto& if_out = main_graph.GetOrCreateNodeArg(\"if_out\", &float_tensor);\r\n    auto& node = main_graph.AddNode(\"if_out\", \"If\", \"If\", {&if_in}, {&if_out});\r\n    node.AddAttribute(\"then_branch\", create_if_subgraph(true));\r\n    node.AddAttribute(\"else_branch\", create_if_subgraph(false));\r\n\r\n    // Add initializer to the graph\r\n    ONNX_NAMESPACE::TensorProto tensor;\r\n    tensor.add_dims(1);\r\n    tensor.add_float_data(1.0f);\r\n    tensor.set_data_type(TensorProto_DataType_FLOAT);\r\n    tensor.set_name(\"init_data\");\r\n    graph_utils::AddInitializerWithOrtValue(main_graph, tensor);\r\n\r\n    // Main graph's inputs/outputs\r\n    main_graph.SetInputs({&abs_data_in, &if_in});\r\n    main_graph.SetOutputs({&if_out});\r\n\r\n    auto status = main_graph.Resolve();\r\n    EXPECT_EQ(status, Status::OK());\r\n\r\n    return model;\r\n  };\r\n\r\n  // Create and load session\r\n  SessionOptions so;\r\n  InferenceSession sess{so, GetEnvironment()};\r\n\r\n  auto status = sess.RegisterExecutionProvider(DefaultCudaExecutionProvider());\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  std::string s1;\r\n  const bool rc = create_model().ToProto().SerializeToString(&s1);\r\n  EXPECT_EQ(rc, true);\r\n  std::stringstream sstr(s1);\r\n\r\n  status = sess.Load(sstr);\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  status = sess.Initialize();\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  // Check planned locations for the initializer\r\n  const auto& main_graph_session_state = sess.GetSessionState();\r\n  const auto& main_graph_ort_value_index_map = main_graph_session_state.GetOrtValueNameIdxMap();\r\n  const auto* main_graph_plan = main_graph_session_state.GetExecutionPlan();\r\n\r\n  OrtValueIndex init_data_index;\r\n  ASSERT_STATUS_OK(main_graph_ort_value_index_map.GetIdx(\"init_data\", init_data_index));\r\n\r\n  EXPECT_EQ(main_graph_plan->allocation_plan[init_data_index].location.Type(), OrtDevice::GPU);\r\n}\r\n\r\nTEST_F(PlannerTest, LocationPlanningForInitializersUsedOnDifferentDevicesInMainGraphAndSubgraph) {\r\n  // This a simple model that has one outer scope initializer, an `If` node followed\r\n  // by a `TopK` node. The initializer is used in both nested subgraphs(`Add` consumes it\r\n  // and requires it on GPU) and main graph(the second input of `TopK` is required on CPU).\r\n  // The right location for the initializer should be CPU as no Memcpy will be inserted\r\n  // for a node in main graph that requires the input(initializer) on CPU if that initializer\r\n  // is placed on GPU by allocation planner.\r\n  TypeProto int_tensor;\r\n  int_tensor.mutable_tensor_type()->set_elem_type(TensorProto_DataType_INT64);\r\n  int_tensor.mutable_tensor_type()->mutable_shape()->add_dim()->set_dim_param(\"dim_param\");\r\n\r\n  TypeProto bool_scalar;\r\n  bool_scalar.mutable_tensor_type()->set_elem_type(TensorProto_DataType_BOOL);\r\n  bool_scalar.mutable_tensor_type()->mutable_shape()->add_dim()->set_dim_value(1);\r\n\r\n  auto create_model = [&int_tensor, &bool_scalar]() -> Model {\r\n    auto create_if_subgraph = [&int_tensor](bool is_then) -> GraphProto {\r\n      Model model(\"if_branch_subgraph\", true, DefaultLoggingManager().DefaultLogger());\r\n      auto& graph = model.MainGraph();\r\n\r\n      auto& outer_scope_0 = graph.GetOrCreateNodeArg(\"abs_data_out\", &int_tensor);\r\n      graph.AddOuterScopeNodeArg(\"abs_data_out\");\r\n\r\n      auto& outer_scope_1 = graph.GetOrCreateNodeArg(\"init_data\", &int_tensor);\r\n      graph.AddOuterScopeNodeArg(\"init_data\");\r\n\r\n      auto& if_out = graph.GetOrCreateNodeArg(is_then ? \"if_then_out\" : \"if_else_out\", &int_tensor);\r\n      graph.AddNode(\"if_out\", \"Add\", \"add\", {&outer_scope_0, &outer_scope_1}, {&if_out});\r\n\r\n      auto status = graph.Resolve();\r\n      EXPECT_EQ(status, Status::OK());\r\n\r\n      return graph.ToGraphProto();\r\n    };\r\n\r\n    onnxruntime::Model model(\"main_graph\", false, ModelMetaData(),\r\n                             PathString(), IOnnxRuntimeOpSchemaRegistryList(),\r\n                             {{kOnnxDomain, 12}}, {}, DefaultLoggingManager().DefaultLogger());\r\n    auto& main_graph = model.MainGraph();\r\n\r\n    // Abs-0\r\n    auto& abs_data_in = main_graph.GetOrCreateNodeArg(\"abs_data_in\", &int_tensor);\r\n    auto& abs_data_out = main_graph.GetOrCreateNodeArg(\"abs_data_out\", &int_tensor);\r\n    main_graph.AddNode(\"abs_0\", \"Abs\", \"node abs\", {&abs_data_in}, {&abs_data_out});\r\n\r\n    // If\r\n    auto& if_in = main_graph.GetOrCreateNodeArg(\"if_in\", &bool_scalar);\r\n    auto& if_out = main_graph.GetOrCreateNodeArg(\"if_out\", &int_tensor);\r\n    auto& node = main_graph.AddNode(\"if_out\", \"If\", \"If\", {&if_in}, {&if_out});\r\n    node.AddAttribute(\"then_branch\", create_if_subgraph(true));\r\n    node.AddAttribute(\"else_branch\", create_if_subgraph(false));\r\n\r\n    // TopK\r\n    auto& topk_data_in_0 = main_graph.GetOrCreateNodeArg(\"if_out\", &int_tensor);\r\n    auto& topk_data_in_1 = main_graph.GetOrCreateNodeArg(\"init_data\", &int_tensor);\r\n    auto& topk_data_out_0 = main_graph.GetOrCreateNodeArg(\"topk_data_out_0\", &int_tensor);\r\n    auto& topk_data_out_1 = main_graph.GetOrCreateNodeArg(\"topk_data_out_1\", &int_tensor);\r\n    main_graph.AddNode(\"topk_0\", \"TopK\", \"node topk\", {&topk_data_in_0, &topk_data_in_1},\r\n                       {&topk_data_out_0, &topk_data_out_1});\r\n\r\n    // Add initializer to the graph\r\n    ONNX_NAMESPACE::TensorProto tensor;\r\n    tensor.add_dims(1);\r\n    tensor.add_int64_data(1);\r\n    tensor.set_data_type(TensorProto_DataType_INT64);\r\n    tensor.set_name(\"init_data\");\r\n    graph_utils::AddInitializerWithOrtValue(main_graph, tensor);\r\n\r\n    // Main graph's inputs/outputs\r\n    main_graph.SetInputs({&abs_data_in, &if_in});\r\n    main_graph.SetOutputs({&topk_data_out_0, &topk_data_out_1});\r\n\r\n    auto status = main_graph.Resolve();\r\n    EXPECT_EQ(status, Status::OK());\r\n\r\n    return model;\r\n  };\r\n\r\n  // Create and load session\r\n  SessionOptions so;\r\n  InferenceSession sess{so, GetEnvironment()};\r\n\r\n  auto status = sess.RegisterExecutionProvider(DefaultCudaExecutionProvider());\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  std::string s1;\r\n  const bool rc = create_model().ToProto().SerializeToString(&s1);\r\n  EXPECT_EQ(rc, true);\r\n  std::stringstream sstr(s1);\r\n\r\n  status = sess.Load(sstr);\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  status = sess.Initialize();\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  // Check planned locations for the initializer\r\n  const auto& main_graph_session_state = sess.GetSessionState();\r\n  const auto& main_graph_ort_value_index_map = main_graph_session_state.GetOrtValueNameIdxMap();\r\n  const auto* main_graph_plan = main_graph_session_state.GetExecutionPlan();\r\n\r\n  OrtValueIndex init_data_index;\r\n  ASSERT_STATUS_OK(main_graph_ort_value_index_map.GetIdx(\"init_data\", init_data_index));\r\n\r\n  EXPECT_EQ(main_graph_plan->allocation_plan[init_data_index].location.Type(), OrtDevice::CPU);\r\n\r\n  // TODO: test para exe plan on subgraph supported\r\n  // const auto* para_graph_plan = const_cast<SessionState&>(main_graph_session_state).GetParallelExecutionPlan();\r\n  // EXPECT_EQ(para_graph_plan->allocation_plan[init_data_index].location.device.Type(), OrtDevice::GPU);\r\n}\r\n\r\nTEST_F(PlannerTest, LocationPlanningForImplicitInputsWithoutExplicitConsumersInMainGraph) {\r\n  // This a simple model that has two inputs and an `If` node.\r\n  // The first input is the condition for the `If` node and the second input\r\n  // is an input consumed implicitly by the `If` node to be used in its subgraphs.\r\n  // Note that there are no other explicit consumers of this input in the main graph.\r\n\r\n  // We want to test that the location planned for this implicit input is the default device\r\n  // of the EP that the `If` node is partitioned to (which will be CUDA)\r\n  // and that it doesn't default to CPU.\r\n\r\n  // Types\r\n  TypeProto float_tensor;\r\n  float_tensor.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  float_tensor.mutable_tensor_type()->mutable_shape()->add_dim()->set_dim_param(\"dim_param\");\r\n\r\n  TypeProto bool_scalar;\r\n  bool_scalar.mutable_tensor_type()->set_elem_type(TensorProto_DataType_BOOL);\r\n  bool_scalar.mutable_tensor_type()->mutable_shape()->add_dim()->set_dim_value(1);\r\n\r\n  auto create_model = [&float_tensor, &bool_scalar]() -> Model {\r\n    auto create_if_subgraph = [&float_tensor](bool is_then) -> GraphProto {\r\n      Model model(\"if_branch_subgraph\", true, DefaultLoggingManager().DefaultLogger());\r\n      auto& graph = model.MainGraph();\r\n\r\n      auto& outer_scope_0 = graph.GetOrCreateNodeArg(\"image_data_in\", &float_tensor);\r\n      graph.AddOuterScopeNodeArg(\"image_data_in\");\r\n\r\n      auto& if_out = graph.GetOrCreateNodeArg(is_then ? \"if_then_out\" : \"if_else_out\", &float_tensor);\r\n      graph.AddNode(\"if_out\", \"Relu\", \"relu\", {&outer_scope_0}, {&if_out});\r\n\r\n      auto status = graph.Resolve();\r\n      EXPECT_EQ(status, Status::OK());\r\n\r\n      return graph.ToGraphProto();\r\n    };\r\n\r\n    onnxruntime::Model model(\"main_graph\", false, ModelMetaData(),\r\n                             PathString(), IOnnxRuntimeOpSchemaRegistryList(),\r\n                             {{kOnnxDomain, 12}}, {}, DefaultLoggingManager().DefaultLogger());\r\n    auto& main_graph = model.MainGraph();\r\n    auto& image_data_in = main_graph.GetOrCreateNodeArg(\"image_data_in\", &float_tensor);\r\n\r\n    // If\r\n    auto& if_in = main_graph.GetOrCreateNodeArg(\"if_in\", &bool_scalar);\r\n    auto& if_out = main_graph.GetOrCreateNodeArg(\"if_out\", &float_tensor);\r\n    auto& node = main_graph.AddNode(\"if_out\", \"If\", \"If\", {&if_in}, {&if_out});\r\n    node.AddAttribute(\"then_branch\", create_if_subgraph(true));\r\n    node.AddAttribute(\"else_branch\", create_if_subgraph(false));\r\n\r\n    // Main graph's inputs/outputs\r\n    main_graph.SetInputs({&image_data_in, &if_in});\r\n    main_graph.SetOutputs({&if_out});\r\n\r\n    auto status = main_graph.Resolve();\r\n    EXPECT_EQ(status, Status::OK());\r\n\r\n    return model;\r\n  };\r\n\r\n  // Create and load session\r\n  SessionOptions so;\r\n  InferenceSession sess{so, GetEnvironment()};\r\n\r\n  auto status = sess.RegisterExecutionProvider(DefaultCudaExecutionProvider());\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  std::string s1;\r\n  const bool rc = create_model().ToProto().SerializeToString(&s1);\r\n  EXPECT_EQ(rc, true);\r\n  std::stringstream sstr(s1);\r\n\r\n  status = sess.Load(sstr);\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  status = sess.Initialize();\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  // Check planned locations for the implicit input\r\n  const auto& main_graph_session_state = sess.GetSessionState();\r\n  const auto& main_graph_ort_value_index_map = main_graph_session_state.GetOrtValueNameIdxMap();\r\n  const auto* main_graph_plan = main_graph_session_state.GetExecutionPlan();\r\n\r\n  OrtValueIndex input_data_index;\r\n  ASSERT_STATUS_OK(main_graph_ort_value_index_map.GetIdx(\"image_data_in\", input_data_index));\r\n\r\n  EXPECT_EQ(main_graph_plan->allocation_plan[input_data_index].location.Type(), OrtDevice::GPU);\r\n\r\n  // TODO: test para exe plan on subgraph supported\r\n  // const auto* para_graph_plan = const_cast<SessionState&>(main_graph_session_state).GetParallelExecutionPlan();\r\n  // EXPECT_EQ(para_graph_plan->allocation_plan[input_data_index].location.device.Type(), OrtDevice::GPU);\r\n}\r\n\r\n// Test MultiStream scenario for the graph:\r\n// node1(CPU ep)->node2(CPU ep)->node3(CUDA ep)->node4(CPU ep)\r\nTEST_F(PlannerTest, MultiStream) {\r\n  ONNX_NAMESPACE::TensorProto tensor;\r\n  tensor.add_dims(1);\r\n  tensor.add_float_data(1.0f);\r\n  tensor.set_data_type(TensorProto_DataType_FLOAT);\r\n  tensor.set_name(\"Graph_input\");\r\n  GetGraph().AddInitializedTensor(tensor);\r\n\r\n  std::string Graph_input(\"Graph_input\"), Arg1(\"Arg1\"), Arg2(\"Arg2\"), Arg3(\"Arg3\"), Arg4(\"Arg4\");\r\n  AddNormalNode(Graph_input, Arg1);\r\n  AddNormalNode(Arg1, Arg2);\r\n  std::unique_ptr<::onnxruntime::KernelDef> cudaKernel = KernelDefBuilder().SetName(\"Transpose\").Provider(kCudaExecutionProvider).SinceVersion(1, 10).Build();\r\n  AddNode(*cudaKernel, Arg2, Arg3);\r\n  AddNormalNode(Arg3, Arg4);\r\n\r\n  CUDAExecutionProviderInfo epi;\r\n  onnxruntime::ProviderInfo_CUDA& ep = onnxruntime::GetProviderInfo_CUDA();\r\n  auto epFactory = ep.CreateExecutionProviderFactory(epi);\r\n  std::unique_ptr<IExecutionProvider> execution_provider = epFactory->CreateProvider();\r\n  ORT_THROW_IF_ERROR(GetExecutionProviders().Add(\"CUDAExecutionProvider\", std::move(execution_provider)));\r\n\r\n  CreatePlan({}, false);\r\n\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan.size(), 2) << \"2 logic streams for CPU and CUDA separately\";\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan[0]->steps_.size(), 6) << \"CPU stream has 6 steps\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[0]).name(), \"LaunchKernelStep\"), nullptr) << \"0th step: LaunchKernelStep for node 1\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[1]).name(), \"LaunchKernelStep\"), nullptr) << \"1st step: LaunchKernelStep for node 2\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[2]).name(), \"TriggerDownstreamStep\"), nullptr) << \"2nd step: TriggerDownstreamStep for node 3, no Activate/Wait step between node 2 and node 3\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[3]).name(), \"BarrierStep\"), nullptr) << \"3rd step: BarrierStep for node 4\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[4]).name(), \"WaitOnEPStep\"), nullptr) << \"4th step: WaitOnEPStep for node 4\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[5]).name(), \"LaunchKernelStep\"), nullptr) << \"5th step: LaunchKernelStep for node 4\";\r\n\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan[1]->steps_.size(), 4) << \"CUDA stream has 4 steps\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[0]).name(), \"BarrierStep\"), nullptr) << \"0th step: BarrierStep for node 3\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[1]).name(), \"LaunchKernelStep\"), nullptr) << \"1st step: LaunchKernelStep for node 3\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[2]).name(), \"ActivateNotificationStep\"), nullptr) << \"2nd step: ActivateNofiticationStep by node 3\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[3]).name(), \"TriggerDownstreamStep\"), nullptr) << \"3rd step: TriggerDownstreamStep for node 4\";\r\n}\r\n\r\n// Test execution plan for the graph:\r\n// node1   node2\r\n//   \\       /\r\n//    \\     /\r\n//      node3\r\n// All 3 nodes are CUDA EP, node1 is in stream0, node2 is in stream1, node3 is in stream2\r\nTEST_F(PlannerTest, MultiStream1StreamWaitFor2Streams) {\r\n  std::unique_ptr<::onnxruntime::KernelDef> cudaKernel = KernelDefBuilder().SetName(\"Transpose\").Provider(kCudaExecutionProvider).SinceVersion(1, 10).Build();\r\n  std::unique_ptr<::onnxruntime::KernelDef> cudaKernelAdd = KernelDefBuilder().SetName(\"Add\").Provider(kCudaExecutionProvider).SinceVersion(1, 10).Build();\r\n  std::string Graph_input(\"Graph_input\"), Arg1(\"Arg1\"), Arg2(\"Arg2\"), Arg3(\"Arg3\"), node1(\"node1\"), node2(\"node2\"), node3(\"node3\");\r\n  std::vector<onnxruntime::NodeArg*> input1{Arg(Graph_input)}, output1{Arg(Arg1)}, output2{Arg(Arg2)}, input3{Arg(Arg1), Arg(Arg2)}, output3{Arg(Arg3)};\r\n  AddNode(*cudaKernel, node1, input1, output1);\r\n  AddNode(*cudaKernel, node2, input1, output2);\r\n  AddNode(*cudaKernelAdd, node3, input3, output3);\r\n\r\n  CUDAExecutionProviderInfo epi;\r\n  onnxruntime::ProviderInfo_CUDA& ep = onnxruntime::GetProviderInfo_CUDA();\r\n  auto epFactory = ep.CreateExecutionProviderFactory(epi);\r\n  std::unique_ptr<IExecutionProvider> execution_provider = epFactory->CreateProvider();\r\n  ORT_THROW_IF_ERROR(GetExecutionProviders().Add(\"CUDAExecutionProvider\", std::move(execution_provider)));\r\n\r\n  SetNodePartitionConfigFilePath(\"./testdata/multi_stream_models/3_gpu_streams.json\");\r\n  CreatePlan({}, false);\r\n\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan.size(), 3) << \"3 logic streams\";\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan[0]->steps_.size(), 3) << \"stream 0 has 3 steps\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[0]).name(), \"LaunchKernelStep\"), nullptr) << \"0th step: LaunchKernelStep for node 1\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[1]).name(), \"ActivateNotificationStep\"), nullptr) << \"1st step: ActivateNofiticationStep by node 1\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[2]).name(), \"TriggerDownstreamStep\"), nullptr) << \"2nd step: TriggerDownstreamStep for node 3\";\r\n\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan[1]->steps_.size(), 3) << \"stream 1 has 3 steps\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[0]).name(), \"LaunchKernelStep\"), nullptr) << \"0th step: LaunchKernelStep for node 2\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[1]).name(), \"ActivateNotificationStep\"), nullptr) << \"1st step: ActivateNofiticationStep by node 2\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[2]).name(), \"TriggerDownstreamStep\"), nullptr) << \"2nd step: TriggerDownstreamStep for node 3\";\r\n\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan[2]->steps_.size(), 5) << \"stream 2 has 5 steps\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[2]->steps_[0]).name(), \"BarrierStep\"), nullptr) << \"0th step: BarrierStep for node 3, for TriggerDownstreamStep in stream 1\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[2]->steps_[1]).name(), \"BarrierStep\"), nullptr) << \"1st step: BarrierStep for node 3, for TriggerDownstreamStep in stream 2\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[2]->steps_[2]).name(), \"WaitOnEPStep\"), nullptr) << \"2nd step: WaitOnEPStep for node 3, for ActivateNotificationStep in stream 1\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[2]->steps_[3]).name(), \"WaitOnEPStep\"), nullptr) << \"3rd step: WaitOnEPStep for node 3, for ActivateNotificationStep in stream 2\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[2]->steps_[4]).name(), \"LaunchKernelStep\"), nullptr) << \"4th step: LaunchKernelStep for node 3\";\r\n}\r\n\r\n// Test execution plan for the graph:\r\n// stream 0: node1 (MemcpyToHost, CUDA EP) -> node3 (Transpose, CUDA EP)\r\n// stream 1: node2 (CPU EP)\r\n// node1's output, which is consumed by both node2 and node3, is in CPU.\r\nTEST_F(PlannerTest, MultiStreamCudaEPNodeCPUOutput) {\r\n  MemcpyToHostInCuda_TransposeInCudaAndCpu(\"./testdata/multi_stream_models/memcpyToHost_same_stream_with_transpose.json\");\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan.size(), 2) << \"2 logic streams\";\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan[0]->steps_.size(), 5) << \"stream 0 has 5 steps\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[0]).name(), \"LaunchKernelStep\"), nullptr) << \"0th step: LaunchKernelStep for node 1\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[1]).name(), \"ActivateNotificationStep\"), nullptr) << \"1st step: ActivateNofiticationStep by node 1\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[2]).name(), \"TriggerDownstreamStep\"), nullptr) << \"2nd step: TriggerDownstreamStep for node 3\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[3]).name(), \"WaitOnEPStep\"), nullptr) << \"3rd step: WaitOnEPStep for node 3 in the same stream, as node 1's output is to CPU\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[4]).name(), \"LaunchKernelStep\"), nullptr) << \"4th step: LaunchKernelStep for node 3\";\r\n\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan[1]->steps_.size(), 3) << \"stream 1 has 3 steps\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[0]).name(), \"BarrierStep\"), nullptr) << \"0th step: BarrierStep for node 2, for TriggerDownstreamStep in stream 0\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[1]).name(), \"WaitOnEPStep\"), nullptr) << \"1st step: WaitOnEPStep for node 2, for ActivateNotificationStep in stream 0\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[2]).name(), \"LaunchKernelStep\"), nullptr) << \"2nd step: LaunchKernelStep for node 2\";\r\n}\r\n\r\n// Test execution plan for the graph:\r\n// node1 has 2 outputs which are both consumed by node2, node1 and node2 are in different streams\r\n// Only 1 WaitOnEPStep is expected before launching node2\r\n// TODO(leca): there is a bug in the corresponding graph that node2 will be visited twice when traversing node1's output nodes\r\n// (see: for (auto it = node->OutputNodesBegin(); it != node->OutputNodesEnd(); ++it) in BuildExecutionPlan()). We can just break the loop and don't need the extra variables once it is fixed\r\nTEST_F(PlannerTest, MultiStreamMultiOutput) {\r\n  std::unique_ptr<::onnxruntime::KernelDef> cudaKernel = KernelDefBuilder().SetName(\"RNN\").Provider(kCudaExecutionProvider).SinceVersion(7).Build();\r\n  std::string Graph_input1(\"Graph_input1\"), Graph_input2(\"Graph_input2\"), Graph_input3(\"Graph_input3\"), Arg1(\"Arg1\"), Arg2(\"Arg2\"), Arg3(\"Arg3\"), node1(\"node1\"), node2(\"node2\");\r\n  std::vector<onnxruntime::NodeArg*> input1{Arg(Graph_input1), Arg(Graph_input2), Arg(Graph_input3)}, output1{Arg(Arg1), Arg(Arg2)}, input2{Arg(Arg1), Arg(Arg2)}, output2{Arg(Arg3)};\r\n  AddNode(*cudaKernel, node1, input1, output1);\r\n\r\n  std::unique_ptr<::onnxruntime::KernelDef> cpuKernel = KernelDefBuilder().SetName(\"Add\").Provider(kCpuExecutionProvider).SinceVersion(7, 12).Build();\r\n  AddNode(*cpuKernel, node2, input2, output2);\r\n\r\n  CUDAExecutionProviderInfo epi;\r\n  onnxruntime::ProviderInfo_CUDA& ep = onnxruntime::GetProviderInfo_CUDA();\r\n  auto epFactory = ep.CreateExecutionProviderFactory(epi);\r\n  std::unique_ptr<IExecutionProvider> execution_provider = epFactory->CreateProvider();\r\n  ORT_THROW_IF_ERROR(GetExecutionProviders().Add(\"CUDAExecutionProvider\", std::move(execution_provider)));\r\n\r\n  CreatePlan({}, false);\r\n\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan.size(), 2) << \"2 logic streams\";\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan[0]->steps_.size(), 3) << \"stream 0 has 3 steps\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[0]).name(), \"LaunchKernelStep\"), nullptr) << \"0th step: LaunchKernelStep for node 1\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[1]).name(), \"ActivateNotificationStep\"), nullptr) << \"1st step: ActivateNofiticationStep by node 1\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[2]).name(), \"TriggerDownstreamStep\"), nullptr) << \"2nd step: TriggerDownstreamStep for node 2\";\r\n\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan[1]->steps_.size(), 3) << \"stream 1 has 3 steps\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[0]).name(), \"BarrierStep\"), nullptr) << \"0th step: BarrierStep for node 2, for TriggerDownstreamStep in stream 0\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[1]).name(), \"WaitOnEPStep\"), nullptr) << \"1st step: WaitOnEPStep for node 2, for ActivateNotificationStep in stream 0\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[2]).name(), \"LaunchKernelStep\"), nullptr) << \"2nd step: LaunchKernelStep for node 2\";\r\n}\r\n\r\n// Test execution plan for the graph:\r\n// node1   node2\r\n//   \\       /\r\n//    \\     /\r\n//      node3\r\n// node1 and node2 are in the same stream, both has an output which will be consumed by node3 in a different stream\r\n// TODO(leca): the ideal case is there is only 1 wait step before launching node3,\r\n// as there is a specific order between node1 and node2 if they are in the same stream, thus node3 will only need to wait the latter one\r\nTEST_F(PlannerTest, MultiStream2NodesSameStreamConsumedBy1NodeInDifferentStream) {\r\n  std::unique_ptr<::onnxruntime::KernelDef> cudaKernel = KernelDefBuilder().SetName(\"Transpose\").Provider(kCudaExecutionProvider).SinceVersion(1, 10).Build();\r\n  std::string Graph_input1(\"Graph_input1\"), Graph_input2(\"Graph_input2\"), Graph_input3(\"Graph_input3\"), Arg1(\"Arg1\"), Arg2(\"Arg2\"), Arg3(\"Arg3\"), node1(\"node1\"), node2(\"node2\"), node3(\"node3\");\r\n  std::vector<onnxruntime::NodeArg*> input1{Arg(Graph_input1)}, input2{Arg(Graph_input2)}, output1{Arg(Arg1)}, output2{Arg(Arg2)}, input3{Arg(Arg1), Arg(Arg2)}, output3{Arg(Arg3)};\r\n  AddNode(*cudaKernel, node1, input1, output1);\r\n  AddNode(*cudaKernel, node2, input2, output2);\r\n\r\n  std::unique_ptr<::onnxruntime::KernelDef> cpuKernel = KernelDefBuilder().SetName(\"Add\").Provider(kCpuExecutionProvider).SinceVersion(7, 12).Build();\r\n  AddNode(*cpuKernel, node3, input3, output3);\r\n\r\n  CUDAExecutionProviderInfo epi;\r\n  onnxruntime::ProviderInfo_CUDA& ep = onnxruntime::GetProviderInfo_CUDA();\r\n  auto epFactory = ep.CreateExecutionProviderFactory(epi);\r\n  std::unique_ptr<IExecutionProvider> execution_provider = epFactory->CreateProvider();\r\n  ORT_THROW_IF_ERROR(GetExecutionProviders().Add(\"CUDAExecutionProvider\", std::move(execution_provider)));\r\n\r\n  CreatePlan({}, false);\r\n\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan.size(), 2) << \"2 logic streams\";\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan[0]->steps_.size(), 6) << \"stream 0 has 6 steps\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[0]).name(), \"LaunchKernelStep\"), nullptr) << \"0th step: LaunchKernelStep for node 1\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[1]).name(), \"ActivateNotificationStep\"), nullptr) << \"1st step: ActivateNofiticationStep by node 1\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[2]).name(), \"TriggerDownstreamStep\"), nullptr) << \"2nd step: TriggerDownstreamStep for node 3\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[3]).name(), \"LaunchKernelStep\"), nullptr) << \"3rd step: LaunchKernelStep for node 2\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[4]).name(), \"ActivateNotificationStep\"), nullptr) << \"4th step: ActivateNofiticationStep by node 2\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[0]->steps_[5]).name(), \"TriggerDownstreamStep\"), nullptr) << \"5th step: TriggerDownstreamStep for node 3\";\r\n\r\n  EXPECT_EQ(GetState().GetExecutionPlan()->execution_plan[1]->steps_.size(), 5) << \"stream 1 has 5 steps\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[0]).name(), \"BarrierStep\"), nullptr) << \"0th step: BarrierStep for node 1, for TriggerDownstreamStep in stream 0\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[1]).name(), \"BarrierStep\"), nullptr) << \"1st step: BarrierStep for node 2, for TriggerDownstreamStep in stream 0\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[2]).name(), \"WaitOnEPStep\"), nullptr) << \"2nd step: WaitOnEPStep for node 1, for ActivateNotificationStep in stream 0\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[3]).name(), \"WaitOnEPStep\"), nullptr) << \"3rd step: WaitOnEPStep for node 2, for ActivateNotificationStep in stream 0\";\r\n  EXPECT_NE(strstr(typeid(*GetState().GetExecutionPlan()->execution_plan[1]->steps_[4]).name(), \"LaunchKernelStep\"), nullptr) << \"4th step: LaunchKernelStep for node 3\";\r\n}\r\n#endif\r\n\r\n#if !defined(__wasm__) && defined(ORT_ENABLE_STREAM)\r\nTEST_F(PlannerTest, ParaPlanCreation) {\r\n  TypeProto graph_in_type;\r\n  graph_in_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  auto* graph_in_shape = graph_in_type.mutable_tensor_type()->mutable_shape();\r\n  graph_in_shape->add_dim()->set_dim_value(3L);\r\n  graph_in_shape->add_dim()->set_dim_value(3L);\r\n  graph_in_shape->add_dim()->set_dim_value(300L);\r\n  graph_in_shape->add_dim()->set_dim_value(300L);\r\n\r\n  TypeProto relu_0_out_type, relu_1_out_type, relu_2_out_type;\r\n  relu_0_out_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  relu_1_out_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  relu_2_out_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n\r\n  TypeProto maxpool_0_out_type;\r\n  maxpool_0_out_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n\r\n  TypeProto conv_0_weight_type, conv_1_weight_type, conv_2_weight_type, conv_3_weight_type, conv_4_weight_type;\r\n\r\n  conv_0_weight_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  auto* conv_0_weight_shape = conv_0_weight_type.mutable_tensor_type()->mutable_shape();\r\n  conv_0_weight_shape->add_dim()->set_dim_value(64L);\r\n  conv_0_weight_shape->add_dim()->set_dim_value(3L);\r\n  conv_0_weight_shape->add_dim()->set_dim_value(7L);\r\n  conv_0_weight_shape->add_dim()->set_dim_value(7L);\r\n\r\n  conv_1_weight_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  auto* conv_1_weight_shape = conv_1_weight_type.mutable_tensor_type()->mutable_shape();\r\n  conv_1_weight_shape->add_dim()->set_dim_value(64L);\r\n  conv_1_weight_shape->add_dim()->set_dim_value(64L);\r\n  conv_1_weight_shape->add_dim()->set_dim_value(1L);\r\n  conv_1_weight_shape->add_dim()->set_dim_value(1L);\r\n\r\n  conv_2_weight_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  auto* conv_2_weight_shape = conv_2_weight_type.mutable_tensor_type()->mutable_shape();\r\n  conv_2_weight_shape->add_dim()->set_dim_value(64L);\r\n  conv_2_weight_shape->add_dim()->set_dim_value(64L);\r\n  conv_2_weight_shape->add_dim()->set_dim_value(3L);\r\n  conv_2_weight_shape->add_dim()->set_dim_value(3L);\r\n\r\n  conv_3_weight_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  auto* conv_3_weight_shape = conv_3_weight_type.mutable_tensor_type()->mutable_shape();\r\n  conv_3_weight_shape->add_dim()->set_dim_value(256L);\r\n  conv_3_weight_shape->add_dim()->set_dim_value(64L);\r\n  conv_3_weight_shape->add_dim()->set_dim_value(1L);\r\n  conv_3_weight_shape->add_dim()->set_dim_value(1L);\r\n\r\n  conv_4_weight_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  auto* conv_4_weight_shape = conv_4_weight_type.mutable_tensor_type()->mutable_shape();\r\n  conv_4_weight_shape->add_dim()->set_dim_value(256L);\r\n  conv_4_weight_shape->add_dim()->set_dim_value(64L);\r\n  conv_4_weight_shape->add_dim()->set_dim_value(1L);\r\n  conv_4_weight_shape->add_dim()->set_dim_value(1L);\r\n\r\n  TypeProto conv_0_bias_type, conv_1_bias_type, conv_2_bias_type, conv_3_bias_type, conv_4_bias_type;\r\n  conv_0_bias_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  auto* conv_0_bias_shape = conv_0_bias_type.mutable_tensor_type()->mutable_shape();\r\n  conv_0_bias_shape->add_dim()->set_dim_value(64L);\r\n\r\n  conv_1_bias_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  auto* conv_1_bias_shape = conv_1_bias_type.mutable_tensor_type()->mutable_shape();\r\n  conv_1_bias_shape->add_dim()->set_dim_value(64L);\r\n\r\n  conv_2_bias_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  auto* conv_2_bias_shape = conv_2_bias_type.mutable_tensor_type()->mutable_shape();\r\n  conv_2_bias_shape->add_dim()->set_dim_value(64L);\r\n\r\n  conv_3_bias_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  auto* conv_3_bias_shape = conv_3_bias_type.mutable_tensor_type()->mutable_shape();\r\n  conv_3_bias_shape->add_dim()->set_dim_value(256L);\r\n\r\n  conv_4_bias_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  auto* conv_4_bias_shape = conv_4_bias_type.mutable_tensor_type()->mutable_shape();\r\n  conv_4_bias_shape->add_dim()->set_dim_value(256L);\r\n\r\n  TypeProto conv_0_out_type, conv_1_out_type, conv_2_out_type, conv_3_out_type, conv_4_out_type;\r\n  conv_0_out_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  conv_1_out_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  conv_2_out_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  conv_3_out_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n  conv_4_out_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n\r\n  TypeProto graph_out_type;\r\n  graph_out_type.mutable_tensor_type()->set_elem_type(TensorProto_DataType_FLOAT);\r\n\r\n  onnxruntime::Model model(\"main_graph\", false, ModelMetaData(),\r\n                           PathString(), IOnnxRuntimeOpSchemaRegistryList(),\r\n                           {{kOnnxDomain, 14}}, {}, DefaultLoggingManager().DefaultLogger());\r\n  auto& main_graph = model.MainGraph();\r\n\r\n  auto& graph_in = main_graph.GetOrCreateNodeArg(\"graph_in\", &graph_in_type);\r\n\r\n  auto& maxpool_0_out = main_graph.GetOrCreateNodeArg(\"maxpool_out\", &maxpool_0_out_type);\r\n  auto& relu_0_out = main_graph.GetOrCreateNodeArg(\"relu_0_out\", &relu_0_out_type);\r\n  auto& relu_1_out = main_graph.GetOrCreateNodeArg(\"relu_1_out\", &relu_1_out_type);\r\n  auto& relu_2_out = main_graph.GetOrCreateNodeArg(\"relu_2_out\", &relu_2_out_type);\r\n\r\n  ONNX_NAMESPACE::TensorProto conv_0_weight_tensor;\r\n  conv_0_weight_tensor.add_dims(64L);\r\n  conv_0_weight_tensor.add_dims(3L);\r\n  conv_0_weight_tensor.add_dims(7L);\r\n  conv_0_weight_tensor.add_dims(7L);\r\n  for (int i = 0; i < 64 * 3 * 7 * 7; ++i) conv_0_weight_tensor.add_float_data(0.234f);\r\n  conv_0_weight_tensor.set_data_type(TensorProto_DataType_FLOAT);\r\n  conv_0_weight_tensor.set_name(\"conv_0_weight\");\r\n  graph_utils::AddInitializerWithOrtValue(main_graph, conv_0_weight_tensor);\r\n\r\n  ONNX_NAMESPACE::TensorProto conv_1_weight_tensor;\r\n  conv_1_weight_tensor.add_dims(64L);\r\n  conv_1_weight_tensor.add_dims(64L);\r\n  conv_1_weight_tensor.add_dims(1L);\r\n  conv_1_weight_tensor.add_dims(1L);\r\n  conv_1_weight_tensor.set_data_type(TensorProto_DataType_FLOAT);\r\n  for (int i = 0; i < 64 * 64; ++i) conv_1_weight_tensor.add_float_data(1.017f);\r\n  conv_1_weight_tensor.set_name(\"conv_1_weight\");\r\n  graph_utils::AddInitializerWithOrtValue(main_graph, conv_1_weight_tensor);\r\n\r\n  ONNX_NAMESPACE::TensorProto conv_2_weight_tensor;\r\n  conv_2_weight_tensor.add_dims(64L);\r\n  conv_2_weight_tensor.add_dims(64L);\r\n  conv_2_weight_tensor.add_dims(3L);\r\n  conv_2_weight_tensor.add_dims(3L);\r\n  for (int i = 0; i < 64 * 64 * 3 * 3; ++i) conv_2_weight_tensor.add_float_data(2.317f);\r\n  conv_2_weight_tensor.set_data_type(TensorProto_DataType_FLOAT);\r\n  conv_2_weight_tensor.set_name(\"conv_2_weight\");\r\n  graph_utils::AddInitializerWithOrtValue(main_graph, conv_2_weight_tensor);\r\n\r\n  ONNX_NAMESPACE::TensorProto conv_3_weight_tensor;\r\n  conv_3_weight_tensor.add_dims(256L);\r\n  conv_3_weight_tensor.add_dims(64L);\r\n  conv_3_weight_tensor.add_dims(1L);\r\n  conv_3_weight_tensor.add_dims(1L);\r\n  for (int i = 0; i < 256 * 64; ++i) conv_3_weight_tensor.add_float_data(1.256f);\r\n  conv_3_weight_tensor.set_data_type(TensorProto_DataType_FLOAT);\r\n  conv_3_weight_tensor.set_name(\"conv_3_weight\");\r\n  graph_utils::AddInitializerWithOrtValue(main_graph, conv_3_weight_tensor);\r\n\r\n  ONNX_NAMESPACE::TensorProto conv_4_weight_tensor;\r\n  conv_4_weight_tensor.add_dims(256L);\r\n  conv_4_weight_tensor.add_dims(64L);\r\n  conv_4_weight_tensor.add_dims(1L);\r\n  conv_4_weight_tensor.add_dims(1L);\r\n  for (int i = 0; i < 256 * 64; ++i) conv_4_weight_tensor.add_float_data(1.913f);\r\n  conv_4_weight_tensor.set_data_type(TensorProto_DataType_FLOAT);\r\n  conv_4_weight_tensor.set_name(\"conv_4_weight\");\r\n  graph_utils::AddInitializerWithOrtValue(main_graph, conv_4_weight_tensor);\r\n\r\n  auto& conv_0_weight = main_graph.GetOrCreateNodeArg(\"conv_0_weight\", &conv_0_weight_type);\r\n  auto& conv_1_weight = main_graph.GetOrCreateNodeArg(\"conv_1_weight\", &conv_1_weight_type);\r\n  auto& conv_2_weight = main_graph.GetOrCreateNodeArg(\"conv_2_weight\", &conv_2_weight_type);\r\n  auto& conv_3_weight = main_graph.GetOrCreateNodeArg(\"conv_3_weight\", &conv_3_weight_type);\r\n  auto& conv_4_weight = main_graph.GetOrCreateNodeArg(\"conv_4_weight\", &conv_4_weight_type);\r\n\r\n  ONNX_NAMESPACE::TensorProto conv_0_bias_tensor;\r\n  conv_0_bias_tensor.add_dims(64L);\r\n  conv_0_bias_tensor.set_data_type(TensorProto_DataType_FLOAT);\r\n  conv_0_bias_tensor.set_name(\"conv_0_bias\");\r\n  for (int i = 0; i < 64; ++i) conv_0_bias_tensor.add_float_data(1.123f);\r\n  graph_utils::AddInitializerWithOrtValue(main_graph, conv_0_bias_tensor);\r\n\r\n  ONNX_NAMESPACE::TensorProto conv_1_bias_tensor;\r\n  conv_1_bias_tensor.add_dims(64L);\r\n  for (int i = 0; i < 64; ++i) conv_1_bias_tensor.add_float_data(2.234f);\r\n  conv_1_bias_tensor.set_data_type(TensorProto_DataType_FLOAT);\r\n  conv_1_bias_tensor.set_name(\"conv_1_bias\");\r\n  graph_utils::AddInitializerWithOrtValue(main_graph, conv_1_bias_tensor);\r\n\r\n  ONNX_NAMESPACE::TensorProto conv_2_bias_tensor;\r\n  conv_2_bias_tensor.add_dims(64L);\r\n  for (int i = 0; i < 64; ++i) conv_2_bias_tensor.add_float_data(0.121f);\r\n  conv_2_bias_tensor.set_data_type(TensorProto_DataType_FLOAT);\r\n  conv_2_bias_tensor.set_name(\"conv_2_bias\");\r\n  graph_utils::AddInitializerWithOrtValue(main_graph, conv_2_bias_tensor);\r\n\r\n  ONNX_NAMESPACE::TensorProto conv_3_bias_tensor;\r\n  conv_3_bias_tensor.add_dims(256L);\r\n  for (int i = 0; i < 256; ++i) conv_3_bias_tensor.add_float_data(1.201f);\r\n  conv_3_bias_tensor.set_data_type(TensorProto_DataType_FLOAT);\r\n  conv_3_bias_tensor.set_name(\"conv_3_bias\");\r\n  graph_utils::AddInitializerWithOrtValue(main_graph, conv_3_bias_tensor);\r\n\r\n  ONNX_NAMESPACE::TensorProto conv_4_bias_tensor;\r\n  conv_4_bias_tensor.add_dims(256L);\r\n  for (int i = 0; i < 256; ++i) conv_4_bias_tensor.add_float_data(0.897f);\r\n  conv_4_bias_tensor.set_data_type(TensorProto_DataType_FLOAT);\r\n  conv_4_bias_tensor.set_name(\"conv_4_bias\");\r\n  graph_utils::AddInitializerWithOrtValue(main_graph, conv_4_bias_tensor);\r\n\r\n  auto& conv_0_bias = main_graph.GetOrCreateNodeArg(\"conv_0_bias\", &conv_0_bias_type);\r\n  auto& conv_1_bias = main_graph.GetOrCreateNodeArg(\"conv_1_bias\", &conv_1_bias_type);\r\n  auto& conv_2_bias = main_graph.GetOrCreateNodeArg(\"conv_2_bias\", &conv_2_bias_type);\r\n  auto& conv_3_bias = main_graph.GetOrCreateNodeArg(\"conv_3_bias\", &conv_3_bias_type);\r\n  auto& conv_4_bias = main_graph.GetOrCreateNodeArg(\"conv_4_bias\", &conv_4_bias_type);\r\n\r\n  auto& conv_0_out = main_graph.GetOrCreateNodeArg(\"conv_0_out\", &conv_0_out_type);\r\n  auto& conv_1_out = main_graph.GetOrCreateNodeArg(\"conv_1_out\", &conv_1_out_type);\r\n  auto& conv_2_out = main_graph.GetOrCreateNodeArg(\"conv_2_out\", &conv_2_out_type);\r\n  auto& conv_3_out = main_graph.GetOrCreateNodeArg(\"conv_3_out\", &conv_3_out_type);\r\n  auto& conv_4_out = main_graph.GetOrCreateNodeArg(\"conv_4_out\", &conv_4_out_type);\r\n\r\n  auto& graph_out = main_graph.GetOrCreateNodeArg(\"graph_out\", &graph_out_type);\r\n\r\n  NodeAttributes conv_0_attributes;\r\n\r\n  ONNX_NAMESPACE::AttributeProto dilation;\r\n  dilation.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  dilation.add_ints(1);\r\n  dilation.add_ints(1);\r\n  dilation.set_name(\"dilations\");\r\n  conv_0_attributes[\"dilations\"] = dilation;\r\n\r\n  ONNX_NAMESPACE::AttributeProto group;\r\n  group.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INT);\r\n  group.set_i(1);\r\n  group.set_name(\"group\");\r\n  conv_0_attributes[\"group\"] = group;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_0_kernel_shape;\r\n  conv_0_kernel_shape.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_0_kernel_shape.add_ints(7);\r\n  conv_0_kernel_shape.add_ints(7);\r\n  conv_0_kernel_shape.set_name(\"kernel_shape\");\r\n  conv_0_attributes[\"kernel_shape\"] = conv_0_kernel_shape;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_0_pads;\r\n  conv_0_pads.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_0_pads.add_ints(3);\r\n  conv_0_pads.add_ints(3);\r\n  conv_0_pads.add_ints(3);\r\n  conv_0_pads.add_ints(3);\r\n  conv_0_pads.set_name(\"pads\");\r\n  conv_0_attributes[\"pads\"] = conv_0_pads;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_0_strides;\r\n  conv_0_strides.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_0_strides.add_ints(2);\r\n  conv_0_strides.add_ints(2);\r\n  conv_0_strides.set_name(\"strides\");\r\n  conv_0_attributes[\"strides\"] = conv_0_strides;\r\n\r\n  main_graph.AddNode(\"conv_0\", \"Conv\", \"\", {&graph_in, &conv_0_weight, &conv_0_bias}, {&conv_0_out}, &conv_0_attributes);\r\n  main_graph.AddNode(\"relu_0\", \"Relu\", \"\", {&conv_0_out}, {&relu_0_out});\r\n\r\n  NodeAttributes maxpool_0_attributes;\r\n  ONNX_NAMESPACE::AttributeProto ceil_mode;\r\n  ceil_mode.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INT);\r\n  ceil_mode.set_i(0);\r\n  ceil_mode.set_name(\"ceil_mode\");\r\n  maxpool_0_attributes[\"ceil_mode\"] = ceil_mode;\r\n\r\n  ONNX_NAMESPACE::AttributeProto maxpool_0_kernel_shape;\r\n  maxpool_0_kernel_shape.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  maxpool_0_kernel_shape.add_ints(3);\r\n  maxpool_0_kernel_shape.add_ints(3);\r\n  maxpool_0_kernel_shape.set_name(\"kernel_shape\");\r\n  maxpool_0_attributes[\"kernel_shape\"] = maxpool_0_kernel_shape;\r\n\r\n  ONNX_NAMESPACE::AttributeProto maxpool_0_pads;\r\n  maxpool_0_pads.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  maxpool_0_pads.add_ints(1);\r\n  maxpool_0_pads.add_ints(1);\r\n  maxpool_0_pads.add_ints(1);\r\n  maxpool_0_pads.add_ints(1);\r\n  maxpool_0_pads.set_name(\"pads\");\r\n  maxpool_0_attributes[\"pads\"] = maxpool_0_pads;\r\n\r\n  ONNX_NAMESPACE::AttributeProto maxpool_0_strides;\r\n  maxpool_0_strides.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  maxpool_0_strides.add_ints(1);\r\n  maxpool_0_strides.add_ints(1);\r\n  maxpool_0_strides.set_name(\"strides\");\r\n  maxpool_0_attributes[\"strides\"] = maxpool_0_strides;\r\n\r\n  main_graph.AddNode(\"maxpool_0\", \"MaxPool\", \"\", {&relu_0_out}, {&maxpool_0_out}, &maxpool_0_attributes);\r\n\r\n  NodeAttributes conv_1_attributes;\r\n  conv_1_attributes[\"dilations\"] = dilation;\r\n  conv_1_attributes[\"group\"] = group;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_1_kernel_shape;\r\n  conv_1_kernel_shape.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_1_kernel_shape.add_ints(1);\r\n  conv_1_kernel_shape.add_ints(1);\r\n  conv_1_kernel_shape.set_name(\"kernel_shape\");\r\n  conv_1_attributes[\"kernel_shape\"] = conv_1_kernel_shape;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_1_pads;\r\n  conv_1_pads.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_1_pads.add_ints(0);\r\n  conv_1_pads.add_ints(0);\r\n  conv_1_pads.add_ints(0);\r\n  conv_1_pads.add_ints(0);\r\n  conv_1_pads.set_name(\"pads\");\r\n  conv_1_attributes[\"pads\"] = conv_1_pads;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_1_strides;\r\n  conv_1_strides.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_1_strides.add_ints(1);\r\n  conv_1_strides.add_ints(1);\r\n  conv_1_strides.set_name(\"strides\");\r\n  conv_1_attributes[\"strides\"] = conv_1_strides;\r\n\r\n  main_graph.AddNode(\"conv_1\", \"Conv\", \"\", {&maxpool_0_out, &conv_1_weight, &conv_1_bias}, {&conv_1_out}, &conv_1_attributes);\r\n  main_graph.AddNode(\"relu_1\", \"Relu\", \"\", {&conv_1_out}, {&relu_1_out});\r\n\r\n  NodeAttributes conv_2_attributes;\r\n  conv_2_attributes[\"dilations\"] = dilation;\r\n  conv_2_attributes[\"group\"] = group;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_2_kernel_shape;\r\n  conv_2_kernel_shape.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_2_kernel_shape.add_ints(3);\r\n  conv_2_kernel_shape.add_ints(3);\r\n  conv_2_kernel_shape.set_name(\"kernel_shape\");\r\n  conv_2_attributes[\"kernel_shape\"] = conv_2_kernel_shape;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_2_pads;\r\n  conv_2_pads.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_2_pads.add_ints(1);\r\n  conv_2_pads.add_ints(1);\r\n  conv_2_pads.add_ints(1);\r\n  conv_2_pads.add_ints(1);\r\n  conv_2_pads.set_name(\"pads\");\r\n  conv_2_attributes[\"pads\"] = conv_2_pads;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_2_strides;\r\n  conv_2_strides.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_2_strides.add_ints(1);\r\n  conv_2_strides.add_ints(1);\r\n  conv_2_strides.set_name(\"strides\");\r\n  conv_2_attributes[\"strides\"] = conv_2_strides;\r\n\r\n  main_graph.AddNode(\"conv_2\", \"Conv\", \"\", {&relu_1_out, &conv_2_weight, &conv_2_bias}, {&conv_2_out}, &conv_2_attributes);\r\n  main_graph.AddNode(\"relu_2\", \"Relu\", \"\", {&conv_2_out}, {&relu_2_out});\r\n\r\n  NodeAttributes conv_3_attributes;\r\n  conv_3_attributes[\"dilations\"] = dilation;\r\n  conv_3_attributes[\"group\"] = group;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_3_kernel_shape;\r\n  conv_3_kernel_shape.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_3_kernel_shape.add_ints(1);\r\n  conv_3_kernel_shape.add_ints(1);\r\n  conv_3_kernel_shape.set_name(\"kernel_shape\");\r\n  conv_3_attributes[\"kernel_shape\"] = conv_3_kernel_shape;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_3_pads;\r\n  conv_3_pads.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_3_pads.add_ints(0);\r\n  conv_3_pads.add_ints(0);\r\n  conv_3_pads.add_ints(0);\r\n  conv_3_pads.add_ints(0);\r\n  conv_3_pads.set_name(\"pads\");\r\n  conv_3_attributes[\"pads\"] = conv_3_pads;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_3_strides;\r\n  conv_3_strides.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_3_strides.add_ints(1);\r\n  conv_3_strides.add_ints(1);\r\n  conv_3_strides.set_name(\"strides\");\r\n  conv_3_attributes[\"strides\"] = conv_3_strides;\r\n\r\n  main_graph.AddNode(\"conv_3\", \"Conv\", \"\", {&relu_2_out, &conv_3_weight, &conv_3_bias}, {&conv_3_out}, &conv_3_attributes);\r\n\r\n  NodeAttributes conv_4_attributes;\r\n  conv_4_attributes[\"dilations\"] = dilation;\r\n  conv_4_attributes[\"group\"] = group;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_4_kernel_shape;\r\n  conv_4_kernel_shape.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_4_kernel_shape.add_ints(1);\r\n  conv_4_kernel_shape.add_ints(1);\r\n  conv_4_kernel_shape.set_name(\"kernel_shape\");\r\n  conv_4_attributes[\"kernel_shape\"] = conv_4_kernel_shape;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_4_pads;\r\n  conv_4_pads.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_4_pads.add_ints(0);\r\n  conv_4_pads.add_ints(0);\r\n  conv_4_pads.add_ints(0);\r\n  conv_4_pads.add_ints(0);\r\n  conv_4_pads.set_name(\"pads\");\r\n  conv_4_attributes[\"pads\"] = conv_4_pads;\r\n\r\n  ONNX_NAMESPACE::AttributeProto conv_4_strides;\r\n  conv_4_strides.set_type(ONNX_NAMESPACE::AttributeProto_AttributeType::AttributeProto_AttributeType_INTS);\r\n  conv_4_strides.add_ints(1);\r\n  conv_4_strides.add_ints(1);\r\n  conv_4_strides.set_name(\"strides\");\r\n  conv_4_attributes[\"strides\"] = conv_4_strides;\r\n\r\n  main_graph.AddNode(\"conv_4\", \"Conv\", \"\", {&maxpool_0_out, &conv_4_weight, &conv_4_bias}, {&conv_4_out}, &conv_4_attributes);\r\n  main_graph.AddNode(\"add_0\", \"Add\", \"\", {&conv_3_out, &conv_4_out}, {&graph_out});\r\n\r\n  main_graph.SetInputs({&graph_in});\r\n  main_graph.SetOutputs({&graph_out});\r\n\r\n  auto status = main_graph.Resolve();\r\n  EXPECT_EQ(status, Status::OK());\r\n\r\n  SessionOptions so;\r\n  so.graph_optimization_level = TransformerLevel::Default;\r\n  ASSERT_TRUE(so.config_options.AddConfigEntry(kNodePartitionConfigFile,\r\n                                               \"./testdata/multi_stream_models/simplified_ssd_cpu.json\")\r\n                  .IsOK());\r\n  InferenceSession sess{so, GetEnvironment()};\r\n\r\n  status = sess.RegisterExecutionProvider(DefaultCpuExecutionProvider());\r\n  ASSERT_TRUE(status.IsOK());\r\n  ASSERT_TRUE(model.Save(model, ORT_TSTR(\"./simplified_ssd.onnx\")).IsOK());\r\n\r\n  std::string s1;\r\n  const bool rc = model.ToProto().SerializeToString(&s1);\r\n  EXPECT_EQ(rc, true);\r\n  std::stringstream sstr(s1);\r\n\r\n  status = sess.Load(sstr);\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  status = sess.Initialize();\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  const auto& main_graph_session_state = sess.GetSessionState();\r\n  const auto& main_graph_ort_value_index_map = main_graph_session_state.GetOrtValueNameIdxMap();\r\n  auto* exe_plan = const_cast<onnxruntime::SessionState&>(main_graph_session_state).GetExecutionPlan();\r\n  auto& per_value_plans = exe_plan->GetAllocationPlan();\r\n  InlinedHashMap<std::string, std::string> reuse_pairs;\r\n  reuse_pairs.emplace(\"conv_0_out\", \"relu_0_out\");  // conv_0_out is reused by relu_0_out\r\n  reuse_pairs.emplace(\"conv_1_out\", \"relu_1_out\");  // conv_1_out is reused by relu_1_out\r\n  reuse_pairs.emplace(\"conv_2_out\", \"relu_2_out\");  // conv_2_out is reused by relu_2_out\r\n  for (size_t i = 0; i < per_value_plans.size(); ++i) {\r\n    auto& per_value_plan = per_value_plans[i];\r\n    if (per_value_plan.alloc_kind == AllocKind::kReuse) {\r\n      std::string reused;\r\n      ORT_ENFORCE(main_graph_ort_value_index_map.GetName(per_value_plan.reused_buffer, reused).IsOK());\r\n      reuse_pairs.erase(reused);\r\n    }  // if\r\n  }    // for\r\n  ASSERT_TRUE(reuse_pairs.empty());\r\n}\r\n\r\nTEST_F(PlannerTest, TestMultiStreamConfig) {\r\n  const char* type = \"DeviceBasedPartitioner\";\r\n  constexpr size_t type_len = 22;\r\n\r\n  auto graph_partitioner_cpu = IGraphPartitioner::CreateGraphPartitioner(\r\n      DefaultLoggingManager().DefaultLogger(),\r\n      ORT_TSTR(\"./testdata/multi_stream_models/multi_stream_single_stream.json\"));\r\n\r\n  ASSERT_TRUE(graph_partitioner_cpu &&\r\n              strncmp(graph_partitioner_cpu->Type(), type, type_len) == 0 &&\r\n              graph_partitioner_cpu->Streams() == 1);\r\n\r\n  auto graph_partitioner_cpu_gpu = IGraphPartitioner::CreateGraphPartitioner(\r\n      DefaultLoggingManager().DefaultLogger(),\r\n      ORT_TSTR(\"./testdata/multi_stream_models/multi_stream_double_stream.json\"));\r\n\r\n  ASSERT_TRUE(graph_partitioner_cpu_gpu &&\r\n              strncmp(graph_partitioner_cpu_gpu->Type(), type, type_len) == 0 &&\r\n              graph_partitioner_cpu_gpu->Streams() == 2);\r\n}\r\n\r\n// Save partition config to a file and check its completeness\r\nTEST_F(PlannerTest, TestMultiStreamSaveConfig) {\r\n  const char* config_file_path = \"./testdata/multi_stream_models/conv_add_relu_single_stream.json\";\r\n  {\r\n    SessionOptions sess_opt;\r\n    sess_opt.graph_optimization_level = TransformerLevel::Default;\r\n    ASSERT_TRUE(sess_opt.config_options.AddConfigEntry(kNodePartitionConfigFile,\r\n                                                       config_file_path)\r\n                    .IsOK());\r\n\r\n    InferenceSession sess(sess_opt, GetEnvironment(), ORT_TSTR(\"./testdata/multi_stream_models/conv_add_relu.onnx\"));\r\n    auto status = sess.RegisterExecutionProvider(DefaultCpuExecutionProvider());\r\n    ASSERT_TRUE(status.IsOK());\r\n\r\n    status = sess.Load();\r\n    ASSERT_TRUE(status.IsOK());\r\n\r\n    status = sess.Initialize();\r\n    ASSERT_TRUE(status.IsOK());\r\n  }\r\n\r\n  std::ifstream if_stream(config_file_path);\r\n  ASSERT_TRUE(if_stream.is_open());\r\n  std::set<std::string> node_set{\"model_41/conv2d_34/Conv2D__2321\",\r\n                                 \"model_41/conv2d_34/Conv2D\",\r\n                                 \"model_41/lambda_9/add\",\r\n                                 \"model_41/activation_27/Relu\",\r\n                                 \"Transpose__2331\"};\r\n\r\n  try {\r\n    json json_config = json::parse(if_stream);\r\n    ASSERT_TRUE(json_config[\"type\"] == \"DeviceBasedPartitioner\");\r\n\r\n    for (const auto& node_stream : json_config[\"streams\"]) {\r\n      ASSERT_TRUE(node_stream.is_array());\r\n\r\n      for (const auto& node_name : node_stream) {\r\n        ASSERT_TRUE(node_name.is_string());\r\n        auto iter = node_set.find(node_name);\r\n\r\n        ASSERT_TRUE(iter != node_set.end());\r\n        node_set.erase(iter);\r\n      }\r\n    }\r\n  } catch (...) {\r\n    ASSERT_TRUE(false);\r\n  }\r\n  if_stream.close();\r\n  ASSERT_TRUE(node_set.empty());\r\n}\r\n\r\n// Load with partition config where a node is missing, session load expected to fail.\r\nTEST_F(PlannerTest, TestMultiStreamMissingNodeConfig) {\r\n  const char* config_file_path = \"./testdata/multi_stream_models/conv_add_relu_single_stream_missing_node.json\";\r\n  SessionOptions sess_opt;\r\n  sess_opt.graph_optimization_level = TransformerLevel::Default;\r\n  ASSERT_TRUE(sess_opt.config_options.AddConfigEntry(kNodePartitionConfigFile,\r\n                                                     config_file_path)\r\n                  .IsOK());\r\n\r\n  InferenceSession sess(sess_opt, GetEnvironment(), ORT_TSTR(\"./testdata/multi_stream_models/conv_add_relu.onnx\"));\r\n  auto status = sess.RegisterExecutionProvider(DefaultCpuExecutionProvider());\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  status = sess.Load();\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  status = sess.Initialize();\r\n  ASSERT_TRUE(!status.IsOK());\r\n}\r\n\r\n// Load with partition config where streams and devices has mismatch\r\nTEST_F(PlannerTest, TestMultiStreamMismatchDevice) {\r\n  const char* config_file_path = \"./testdata/multi_stream_models/conv_add_relu_single_stream_mismatch_device.json\";\r\n  SessionOptions sess_opt;\r\n  sess_opt.graph_optimization_level = TransformerLevel::Default;\r\n  ASSERT_TRUE(sess_opt.config_options.AddConfigEntry(kNodePartitionConfigFile,\r\n                                                     config_file_path)\r\n                  .IsOK());\r\n\r\n  InferenceSession sess(sess_opt, GetEnvironment(), ORT_TSTR(\"./testdata/multi_stream_models/conv_add_relu.onnx\"));\r\n  auto status = sess.RegisterExecutionProvider(DefaultCpuExecutionProvider());\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  status = sess.Load();\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  status = sess.Initialize();\r\n  ASSERT_TRUE(!status.IsOK());\r\n}\r\n#endif\r\n\r\n#if defined(USE_CUDA) && defined(ORT_ENABLE_STREAM)\r\nTEST_F(PlannerTest, TestCpuIf) {\r\n  SessionOptions sess_opt;\r\n  sess_opt.graph_optimization_level = TransformerLevel::Default;\r\n\r\n  InferenceSession sess(sess_opt, GetEnvironment(), ORT_TSTR(\"./testdata/multi_stream_models/cpu_if.onnx\"));\r\n  ASSERT_STATUS_OK(sess.RegisterExecutionProvider(DefaultCudaExecutionProvider()));\r\n  ASSERT_STATUS_OK(sess.Load());\r\n  ASSERT_STATUS_OK(sess.Initialize());\r\n\r\n  auto& sess_state = const_cast<onnxruntime::SessionState&>(sess.GetSessionState());\r\n  const auto& exe_plan = sess_state.GetExecutionPlan()->execution_plan;\r\n  if (exe_plan.size() == 2 &&\r\n      exe_plan[1]->device_.Type() == OrtDevice::CPU &&\r\n      exe_plan[1]->steps_.size() == 9 &&\r\n      exe_plan[1]->steps_[7]->GetNodeIndex() == 7) {\r\n    // there must be a wait before cpu If node\r\n    static const std::string WaitOnEPStep = \"WaitOnEPStep\";\r\n    ASSERT_EQ(exe_plan[1]->steps_[6]->ToString().substr(0, WaitOnEPStep.size()), WaitOnEPStep);\r\n  }\r\n}\r\n\r\n// model looks like:\r\n//                                                 |-----------> Gather\r\n//                                                 |-----------> Gather\r\n//                                                 |-----------> Gather\r\n//                                                 |-----------> Gather\r\n// Shape ----------------> Reshape --> Shape ------------------> Reshape\r\n//                           ^                                     ^\r\n// InstanceNormalization ----|         InstanceNormalization ------|\r\n//\r\n// Python script to create this model:\r\n// def CreateModelFor19480():\r\n//    #shape->reshape->shape->reshape, 4 gather\r\n//    graphNodes = []\r\n//    graphNodes.append(h.make_node('Shape', inputs=['shape_input'], outputs=['9']))\r\n//    graphNodes.append(h.make_node('InstanceNormalization', inputs=['in0_input', 'scale0', 'B0'], outputs=['8']))\r\n//    graphNodes.append(h.make_node('Reshape', inputs=['8', '9'], outputs=['Reshape15_output']))\r\n//    graphNodes.append(h.make_node('Shape', inputs=['Reshape15_output'], outputs=['281']))\r\n//    graphNodes.append(h.make_node('InstanceNormalization', inputs=['in1_input', 'scale1', 'B1'], outputs=['293']))\r\n//    graphNodes.append(h.make_node('Reshape', inputs=['293', '281'], outputs=['output0']))\r\n//    graphNodes.append(h.make_node('Gather', inputs=['281', 'indices1'], outputs=['output1']))\r\n//    graphNodes.append(h.make_node('Gather', inputs=['281', 'indices2'], outputs=['output2']))\r\n//    graphNodes.append(h.make_node('Gather', inputs=['281', 'indices3'], outputs=['output3']))\r\n//    graphNodes.append(h.make_node('Gather', inputs=['281', 'indices4'], outputs=['output4']))\r\n//    g = h.make_graph(graphNodes, 'issue_19480',\r\n//                     [h.make_tensor_value_info('shape_input', tp.FLOAT, ['batch', 128, None, None]),\r\n//                      h.make_tensor_value_info('in0_input', tp.FLOAT, ['batch', 32, None]),\r\n//                      h.make_tensor_value_info('scale0', tp.FLOAT, [32]),\r\n//                      h.make_tensor_value_info('B0', tp.FLOAT, [32]),\r\n//                      h.make_tensor_value_info('in1_input', tp.FLOAT, ['batch', 32, None]),\r\n//                      h.make_tensor_value_info('scale1', tp.FLOAT, [32]),\r\n//                      h.make_tensor_value_info('B1', tp.FLOAT, [32]),\r\n//                      h.make_tensor_value_info('indices1', tp.INT32, []),\r\n//                      h.make_tensor_value_info('indices2', tp.INT32, []),\r\n//                      h.make_tensor_value_info('indices3', tp.INT32, []),\r\n//                      h.make_tensor_value_info('indices4', tp.INT32, [])],\r\n//                     [h.make_tensor_value_info('output0', tp.FLOAT, None),\r\n//                      h.make_tensor_value_info('output1', tp.INT64, None),\r\n//                      h.make_tensor_value_info('output2', tp.INT64, None),\r\n//                      h.make_tensor_value_info('output3', tp.INT64, None),\r\n//                      h.make_tensor_value_info('output4', tp.INT64, None)])\r\n//    model = h.make_model(g, opset_imports=[h.make_operatorsetid(\"\", 17)], producer_name='producer_name')\r\n//    onnx.save(model, 'issue_19480.onnx')\r\n//\r\nTEST(AllocationPlannerTest, ReusedInputCrossDifferentStreams) {\r\n  SessionOptions sess_opt;\r\n  sess_opt.graph_optimization_level = TransformerLevel::Default;\r\n\r\n  InferenceSession sess(sess_opt, GetEnvironment(), ORT_TSTR(\"./testdata/multi_stream_models/issue_19480.onnx\"));\r\n  auto status = sess.RegisterExecutionProvider(DefaultCudaExecutionProvider());\r\n  status = sess.Load();\r\n  status = sess.Initialize();\r\n  ASSERT_TRUE(status.IsOK()) << \"No crash\";\r\n  const SequentialExecutionPlan* plan = sess.GetSessionState().GetExecutionPlan();\r\n  ASSERT_EQ(plan->allocation_plan[14].alloc_kind, AllocKind::kReuse) << \"The input of reshape and gather will reuse the output of shape\";\r\n\r\n  int gather_count = 0;\r\n  ASSERT_GT(plan->execution_plan.size(), 1) << \"Number of execution plans should be greater than 1\";\r\n  for (size_t i = 0; i < plan->execution_plan[1]->steps_.size(); i++) {\r\n    if (strstr(typeid(*(plan->execution_plan[1]->steps_[i])).name(), \"LaunchKernelStep\")) {\r\n      const Node* node = sess.GetSessionState().GetGraphViewer().GetNode(plan->execution_plan[1]->steps_[i]->GetNodeIndex());\r\n      if (node->OpType() == \"Gather\")\r\n        gather_count++;\r\n      else\r\n        FAIL() << \"CPU stream should contain only gather ops\";\r\n    }\r\n  }\r\n  ASSERT_EQ(gather_count, 4) << \"4 gather ops are all placed in CPU stream\";\r\n}\r\n#endif\r\n\r\n#ifdef ENABLE_TRAINING_OPS\r\n// use a carefully constructed model to re-produce a customer reported issue where a model produced invalid output.\r\n// this issue required:\r\n// - buffer A that is re-used later in the model\r\n//   - output of the first Shape node\r\n//   - first usage completes after the following Cast node\r\n// - buffer B which has the same size requirement and is used after the first usage of A is complete\r\n//   - buffer B is used for the output from `squeeze2` and a number of other nodes in that part of the model.\r\n// - re-use of buffer A for an output of a node that has no consumers whilst buffer B is still in use\r\n//   - this is the `per_input_length` output of the ConcatTraining node\r\n//\r\n// Because the logic to determine when a buffer can be freed is based on consumers, buffer A gets freed after the\r\n// Cast node. It is then re-used as buffer B because the memory pattern planner believes that block to be available.\r\n// When we re-use buffer A for the ConcatTraining output we are using the same address for two different node output\r\n// buffers, leading to corruption of the output.\r\n// This tests that the change in allocation planner to not re-use a buffer for outputs with no consumers prevents this.\r\nTEST(AllocationPlannerTest, AvoidReuseOfBufferForNodeOutputWithNoConsumers) {\r\n  SessionOptions sess_opt;\r\n  sess_opt.graph_optimization_level = TransformerLevel::Default;\r\n\r\n  InferenceSession sess(sess_opt, GetEnvironment(), ORT_TSTR(\"./testdata/avoid_reuse_of_buffer_for_node_output_with_no_consumers.onnx\"));\r\n  auto status = sess.Load();\r\n  status = sess.Initialize();\r\n  ASSERT_TRUE(status.IsOK());\r\n\r\n  const auto& session_state = sess.GetSessionState();\r\n  const auto& ort_value_index_map = session_state.GetOrtValueNameIdxMap();\r\n  const SequentialExecutionPlan* plan = session_state.GetExecutionPlan();\r\n\r\n  OrtValueIndex concat_training_unused_out_index;\r\n  // Here per_input_length output of the ConcatTraining node has no consumers, so it should not reuse the buffer.\r\n  ASSERT_STATUS_OK(ort_value_index_map.GetIdx(\"per_input_length\", concat_training_unused_out_index));\r\n  EXPECT_EQ(plan->allocation_plan[concat_training_unused_out_index].alloc_kind, AllocKind::kAllocate);\r\n}\r\n#endif\r\n\r\n}  // namespace test\r\n}  // namespace onnxruntime\r\n"}
{"path":"\\\\?\\C:\\Users\\Naomi\\Desktop\\ONNX\\onnxruntime\\onnxruntime\\test\\python\\quantization\\test_op_matmul_4bits.py","line":8,"char":1,"code":"RUFF","severity":"warning","name":"I001","description":"Import block is un-sorted or un-formatted.\nSee https://docs.astral.sh/ruff/rules/unsorted-imports"}
{"path":"\\\\?\\C:\\Users\\Naomi\\Desktop\\ONNX\\onnxruntime\\onnxruntime\\test\\python\\quantization\\test_op_matmul_4bits.py","line":null,"char":null,"code":"RUFF","severity":"warning","name":"format","description":"Run `lintrunner -a` to apply this patch.","original":"#!/usr/bin/env python\r\n# -------------------------------------------------------------------------\r\n# Copyright (c) Microsoft Corporation. All rights reserved.\r\n# Licensed under the MIT License. See License.txt in the project root for\r\n# license information.\r\n# --------------------------------------------------------------------------\r\n\r\nimport tempfile\r\nimport unittest\r\nfrom importlib.util import find_spec\r\nfrom pathlib import Path\r\n\r\nimport numpy as np\r\nimport onnx\r\nfrom onnx import TensorProto, helper\r\nfrom op_test_utils import TestDataFeeds, check_model_correctness, check_op_type_count, check_qtype_by_node_type\r\n\r\nfrom onnxruntime.quantization import quant_utils\r\n\r\n\r\nclass TestOpMatMul4Bits(unittest.TestCase):\r\n    @classmethod\r\n    def setUpClass(cls):\r\n        cls._tmp_model_dir = tempfile.TemporaryDirectory(prefix=\"test_matmul4bits.\")\r\n\r\n    @classmethod\r\n    def tearDownClass(cls):\r\n        cls._tmp_model_dir.cleanup()\r\n\r\n    def fill_int4_data(self, shape: int | tuple[int, ...], symmetric: bool) -> np.ndarray:\r\n        line = np.zeros(shape)\r\n        line = line.reshape(-1)\r\n\r\n        if symmetric:\r\n            v = -2.0\r\n            for i in range(line.shape[0]):\r\n                if v == 0 or v == -3 or v == 3:\r\n                    v += 1\r\n                line[i] = v\r\n                v += 1\r\n                if v >= 8:\r\n                    v = -8\r\n        else:\r\n            v = 0.0\r\n            for i in range(line.shape[0]):\r\n                line[i] = v\r\n                v += 1\r\n                if v >= 16:\r\n                    v = 0\r\n\r\n        return line.reshape(shape)\r\n\r\n    def input_feeds(\r\n        self,\r\n        n: int,\r\n        name2shape: dict[str, int | tuple[int, ...]],\r\n        low: int = -1,\r\n        high: int = 2,\r\n        dtype: type = np.float32,\r\n    ) -> TestDataFeeds:\r\n        input_data_list = []\r\n        for _i in range(n):\r\n            inputs = {}\r\n            for name, shape in name2shape.items():\r\n                inputs.update({name: np.random.randint(low, high, shape).astype(dtype)})\r\n            input_data_list.extend([inputs])\r\n        dr = TestDataFeeds(input_data_list)\r\n        return dr\r\n\r\n    def construct_model_matmul(self, output_model_path: str, symmetric: bool) -> None:\r\n        #      (input)\r\n        #         |\r\n        #       MatMul\r\n        #         |\r\n        #      (output)\r\n        input_name = \"input\"\r\n        output_name = \"output\"\r\n        initializers = []\r\n\r\n        def make_matmul(\r\n            input_name, weight_shape: int | tuple[int, ...], weight_name: str, output_name: str, node_name: str\r\n        ):\r\n            weight_data = self.fill_int4_data(weight_shape, symmetric).astype(np.float32)\r\n            initializers.append(onnx.numpy_helper.from_array(weight_data, name=weight_name))\r\n            return onnx.helper.make_node(\r\n                \"MatMul\",\r\n                [input_name, weight_name],\r\n                [output_name],\r\n                node_name,\r\n            )\r\n\r\n        in_features = 52\r\n        out_features = 288\r\n        # make MatMul node\r\n        matmul_node = make_matmul(\r\n            input_name,\r\n            [in_features, out_features],\r\n            \"linear1.weight\",\r\n            output_name,\r\n            \"MatMul_0\",\r\n        )\r\n\r\n        # make graph\r\n        input_tensor = helper.make_tensor_value_info(input_name, TensorProto.FLOAT, [-1, in_features])\r\n        output_tensor = helper.make_tensor_value_info(output_name, TensorProto.FLOAT, [-1, out_features])\r\n        graph_name = \"matmul_4bits_test\"\r\n        graph = helper.make_graph(\r\n            [matmul_node],\r\n            graph_name,\r\n            [input_tensor],\r\n            [output_tensor],\r\n            initializer=initializers,\r\n        )\r\n        # blocked quantization requires DQ op set >= 21\r\n        model = helper.make_model(graph, opset_imports=[helper.make_opsetid(\"\", 21)])\r\n        model.ir_version = 10  # use stable onnx ir version\r\n\r\n        onnx.save(model, output_model_path)\r\n\r\n    def construct_model_gather(\r\n        self,\r\n        output_model_path: str,\r\n        symmetric: bool,\r\n        tdata: TensorProto.DataType,\r\n        tind: TensorProto.DataType,\r\n        vocab_size: int = 545,\r\n        embedding_len: int = 228,\r\n    ) -> None:\r\n        #      (input)\r\n        #         |\r\n        #       Gather\r\n        #         |\r\n        #      (output)\r\n        indices_name = \"input\"\r\n        output_name = \"output\"\r\n        initializers = []\r\n\r\n        def make_gather(\r\n            indices_name, data_shape: int | tuple[int, ...], data_name: str, output_name: str, node_name: str\r\n        ):\r\n            weight_data = self.fill_int4_data(data_shape, symmetric).astype(\r\n                np.float32 if tdata == TensorProto.FLOAT else np.float16\r\n            )\r\n            initializers.append(onnx.numpy_helper.from_array(weight_data, name=data_name))\r\n            kwargs = {\"axis\": 0}\r\n            return onnx.helper.make_node(\r\n                \"Gather\",\r\n                [data_name, indices_name],\r\n                [output_name],\r\n                node_name,\r\n                **kwargs,\r\n            )\r\n\r\n        gather_node = make_gather(\r\n            indices_name,\r\n            (vocab_size, embedding_len),\r\n            \"linear1.weight\",\r\n            output_name,\r\n            \"Gather_0\",\r\n        )\r\n\r\n        # make graph\r\n        input_tensor = helper.make_tensor_value_info(indices_name, tind, [-1, 1000])\r\n        output_tensor = helper.make_tensor_value_info(output_name, tdata, [-1, 1000, embedding_len])\r\n        graph_name = \"gather_4bits_test\"\r\n        graph = helper.make_graph(\r\n            [gather_node],\r\n            graph_name,\r\n            [input_tensor],\r\n            [output_tensor],\r\n            initializer=initializers,\r\n        )\r\n        # QDQ and gather requires op set >= 21. The tool should automatically update the opset.\r\n        model = helper.make_model(graph, opset_imports=[helper.make_opsetid(\"\", 19)])\r\n        model.ir_version = 9  # use stable onnx ir version\r\n\r\n        onnx.save(model, output_model_path)\r\n\r\n    def quant_test(\r\n        self,\r\n        model_fp32_path: str,\r\n        data_reader: TestDataFeeds,\r\n        block_size: int,\r\n        is_symmetric: bool,\r\n        quant_format: quant_utils.QuantFormat = quant_utils.QuantFormat.QOperator,\r\n        op_types_to_quantize: tuple[str, ...] = (\"MatMul\",),\r\n        quant_axes: tuple[tuple[str, int], ...] = ((\"MatMul\", 0), (\"Gather\", 1)),\r\n        rtol: float = 0.01,\r\n        atol: float = 0.05,\r\n    ):\r\n        use_qdq = quant_format == quant_utils.QuantFormat.QDQ\r\n        name_prefix = \"QDQ\" if use_qdq else \"QOperator\"\r\n        model_int4_path = str(\r\n            Path(self._tmp_model_dir.name).joinpath(f\"{name_prefix}_{block_size}_{is_symmetric}.onnx\").absolute()\r\n        )\r\n\r\n        # Quantize fp32 model to int4 model\r\n        from onnxruntime.quantization import matmul_nbits_quantizer  # noqa: PLC0415\r\n\r\n        model = quant_utils.load_model_with_shape_infer(Path(model_fp32_path))\r\n        quant_config = matmul_nbits_quantizer.DefaultWeightOnlyQuantConfig(\r\n            block_size=block_size,\r\n            is_symmetric=is_symmetric,\r\n            quant_format=quant_format,\r\n            op_types_to_quantize=op_types_to_quantize,\r\n            quant_axes=quant_axes,\r\n        )\r\n        quant = matmul_nbits_quantizer.MatMulNBitsQuantizer(model, algo_config=quant_config)\r\n        quant.process()\r\n        quant.model.save_model_to_file(model_int4_path, False)\r\n\r\n        if \"Gather\" in op_types_to_quantize:\r\n            quant_nodes = {\"GatherBlockQuantized\": 1}\r\n        else:\r\n            quant_nodes = {\"DequantizeLinear\": 1, \"MatMul\": 1} if use_qdq else {\"MatMulNBits\": 1}\r\n        check_op_type_count(self, model_int4_path, **quant_nodes)\r\n\r\n        if use_qdq:\r\n            dq_qtype = TensorProto.INT4 if is_symmetric else TensorProto.UINT4\r\n            dqnode_io_qtypes = (\r\n                {\r\n                    \"DequantizeLinear\": [\r\n                        [\"i\", 0, dq_qtype],\r\n                    ]\r\n                }\r\n                if is_symmetric\r\n                else {\r\n                    \"DequantizeLinear\": [\r\n                        [\"i\", 0, dq_qtype],\r\n                        [\"i\", 2, dq_qtype],\r\n                    ]\r\n                }\r\n            )\r\n            check_qtype_by_node_type(self, model_int4_path, dqnode_io_qtypes)\r\n            for op in quant.model.opset_import():\r\n                if op.domain in [None, \"\", \"ai.onnx\"] and op.version < 21:\r\n                    self.fail(f\"In QDQ format {op.domain} opset should be >= 21\")\r\n\r\n        data_reader.rewind()\r\n\r\n        try:\r\n            check_model_correctness(self, model_fp32_path, model_int4_path, data_reader.get_next(), rtol, atol)\r\n        except Exception as exception:\r\n            if \"4b quantization not yet supported on this hardware platform!\" in exception.args[0]:\r\n                # Currently we don't have int4 quantization support on all platforms, has to tolerate this exception\r\n                pass\r\n            else:\r\n                raise exception\r\n\r\n    def quant_test_with_algo(\r\n        self,\r\n        algorithm: str,\r\n        model_fp32_path: str,\r\n        data_reader: TestDataFeeds,\r\n        block_size: int,\r\n        is_symmetric: bool,\r\n    ):\r\n        model_int4_path = str(\r\n            Path(self._tmp_model_dir.name).joinpath(f\"MatMulNBits_{block_size}_{is_symmetric}.onnx\").absolute()\r\n        )\r\n\r\n        # Quantize fp32 model to int4 model\r\n        from onnxruntime.quantization import matmul_nbits_quantizer  # noqa: PLC0415\r\n\r\n        algo_config = None\r\n        if algorithm == \"RTN\":\r\n            # test RTN algorithm\r\n            algo_config = matmul_nbits_quantizer.RTNWeightOnlyQuantConfig()\r\n        elif algorithm == \"GPTQ\":\r\n            # test GPTQ algorithm\r\n            algo_config = matmul_nbits_quantizer.GPTQWeightOnlyQuantConfig(calibration_data_reader=data_reader)\r\n        elif algorithm == \"HQQ\":\r\n            # test HQQ algorithm\r\n            algo_config = matmul_nbits_quantizer.HQQWeightOnlyQuantConfig(block_size=block_size)\r\n\r\n        model = quant_utils.load_model_with_shape_infer(Path(model_fp32_path))\r\n        bits = 4\r\n        quant = matmul_nbits_quantizer.MatMulNBitsQuantizer(\r\n            model, bits, block_size, is_symmetric, algo_config=algo_config\r\n        )\r\n        quant.process()\r\n        quant.model.save_model_to_file(model_int4_path, False)\r\n\r\n        quant_nodes = {\"MatMulNBits\": 1}\r\n        check_op_type_count(self, model_int4_path, **quant_nodes)\r\n\r\n        data_reader.rewind()\r\n\r\n        try:\r\n            check_model_correctness(self, model_fp32_path, model_int4_path, data_reader.get_next())\r\n        except Exception as exception:\r\n            if \"4b quantization not yet supported on this hardware platform!\" in exception.args[0]:\r\n                # Currently we don't have int4 quantization support on all platforms, has to tolerate this exception\r\n                pass\r\n            else:\r\n                raise exception\r\n\r\n    def test_quantize_matmul_int4_symmetric(self):\r\n        np.random.seed(13)\r\n\r\n        model_fp32_path = str(Path(self._tmp_model_dir.name).joinpath(\"matmul_fp32_symmetric.onnx\").absolute())\r\n        self.construct_model_matmul(model_fp32_path, symmetric=True)\r\n        data_reader = self.input_feeds(1, {\"input\": (100, 52)})\r\n        self.quant_test(model_fp32_path, data_reader, 32, True)\r\n\r\n    def test_quantize_matmul_int4_offsets(self):\r\n        model_fp32_path = str(Path(self._tmp_model_dir.name).joinpath(\"matmul_fp32_offset.onnx\").absolute())\r\n        self.construct_model_matmul(model_fp32_path, symmetric=False)\r\n        data_reader = self.input_feeds(1, {\"input\": (100, 52)})\r\n        self.quant_test(model_fp32_path, data_reader, 32, False)\r\n\r\n    def test_quantize_gather_int4_symmetric(self):\r\n        np.random.seed(13)\r\n\r\n        model_fp32_path = str(Path(self._tmp_model_dir.name).joinpath(\"gather_fp32_symmetric.onnx\").absolute())\r\n        self.construct_model_gather(model_fp32_path, True, TensorProto.FLOAT, TensorProto.INT32)\r\n        data_reader = self.input_feeds(1, {\"input\": (100, 1000)}, -545, 535, np.int32)\r\n        # cover rounding error\r\n        self.quant_test(model_fp32_path, data_reader, 32, True, op_types_to_quantize=(\"Gather\",), rtol=0.2, atol=0.5)\r\n\r\n    def test_quantize_gather_int4_offsets(self):\r\n        model_fp32_path = str(Path(self._tmp_model_dir.name).joinpath(\"gather_fp32_offset.onnx\").absolute())\r\n        self.construct_model_gather(model_fp32_path, False, TensorProto.FLOAT16, TensorProto.INT64)\r\n        data_reader = self.input_feeds(1, {\"input\": (100, 1000)}, -545, 535, np.int64)\r\n        # cover rounding error\r\n        self.quant_test(model_fp32_path, data_reader, 32, False, op_types_to_quantize=(\"Gather\",), rtol=0.2, atol=0.5)\r\n\r\n    def test_quantize_matmul_int4_symmetric_qdq(self):\r\n        np.random.seed(13)\r\n\r\n        model_fp32_path = str(Path(self._tmp_model_dir.name).joinpath(\"matmul_fp32_symmetric.onnx\").absolute())\r\n        self.construct_model_matmul(model_fp32_path, symmetric=True)\r\n        data_reader = self.input_feeds(1, {\"input\": (100, 52)})\r\n        self.quant_test(model_fp32_path, data_reader, 32, True, quant_utils.QuantFormat.QDQ)\r\n\r\n    def test_quantize_matmul_int4_offsets_qdq(self):\r\n        model_fp32_path = str(Path(self._tmp_model_dir.name).joinpath(\"matmul_fp32_offset.onnx\").absolute())\r\n        self.construct_model_matmul(model_fp32_path, symmetric=False)\r\n        data_reader = self.input_feeds(1, {\"input\": (100, 52)})\r\n        self.quant_test(model_fp32_path, data_reader, 32, False, quant_utils.QuantFormat.QDQ)\r\n\r\n    def test_quantize_matmul_int4_using_rtn_algo(self):\r\n        if not find_spec(\"neural_compressor\"):\r\n            self.skipTest(\"skip test_smooth_quant since neural_compressor is not installed\")\r\n        if not find_spec(\"torch\"):\r\n            self.skipTest(\"skip test_quantize_matmul_int4_using_rtn_algo since torch is not installed\")\r\n        model_fp32_path = str(Path(self._tmp_model_dir.name).joinpath(\"matmul_fp32_offset.onnx\").absolute())\r\n        self.construct_model_matmul(model_fp32_path, symmetric=False)\r\n        data_reader = self.input_feeds(1, {\"input\": (100, 52)})\r\n        self.quant_test_with_algo(\"RTN\", model_fp32_path, data_reader, 32, False)\r\n\r\n    def test_quantize_matmul_int4_using_gptq_algo(self):\r\n        if not find_spec(\"neural_compressor\"):\r\n            self.skipTest(\"skip test_smooth_quant since neural_compressor is not installed\")\r\n        if not find_spec(\"torch\"):\r\n            self.skipTest(\"skip test_quantize_matmul_int4_using_gptq_algo since torch is not installed\")\r\n        model_fp32_path = str(Path(self._tmp_model_dir.name).joinpath(\"matmul_fp32_offset.onnx\").absolute())\r\n        self.construct_model_matmul(model_fp32_path, symmetric=False)\r\n        data_reader = self.input_feeds(1, {\"input\": (100, 52)})\r\n        self.quant_test_with_algo(\"GPTQ\", model_fp32_path, data_reader, 32, False)\r\n\r\n    def test_quantize_matmul_int4_using_hqq_algo(self):\r\n        if not find_spec(\"torch\"):\r\n            self.skipTest(\"skip test_hqq_quant since torch is not installed\")\r\n        model_fp32_path = str(Path(self._tmp_model_dir.name).joinpath(\"matmul_fp32_offset.onnx\").absolute())\r\n        self.construct_model_matmul(model_fp32_path, symmetric=False)\r\n        data_reader = self.input_feeds(1, {\"input\": (100, 52)})\r\n        self.quant_test_with_algo(\"HQQ\", model_fp32_path, data_reader, 32, False)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    unittest.main()\r\n    # TODO(fajin): add 8bit quantization test after enabling kenrels\r\n","replacement":"#!/usr/bin/env python\r\n# -------------------------------------------------------------------------\r\n# Copyright (c) Microsoft Corporation. All rights reserved.\r\n# Licensed under the MIT License. See License.txt in the project root for\r\n# license information.\r\n# --------------------------------------------------------------------------\r\n\r\nimport tempfile\r\nimport unittest\r\nfrom importlib.util import find_spec\r\nfrom pathlib import Path\r\n\r\nimport numpy as np\r\nimport onnx\r\nfrom onnx import TensorProto, helper\r\nfrom onnxruntime.quantization import quant_utils\r\nfrom op_test_utils import TestDataFeeds, check_model_correctness, check_op_type_count, check_qtype_by_node_type\r\n\r\n\r\nclass TestOpMatMul4Bits(unittest.TestCase):\r\n    @classmethod\r\n    def setUpClass(cls):\r\n        cls._tmp_model_dir = tempfile.TemporaryDirectory(prefix=\"test_matmul4bits.\")\r\n\r\n    @classmethod\r\n    def tearDownClass(cls):\r\n        cls._tmp_model_dir.cleanup()\r\n\r\n    def fill_int4_data(self, shape: int | tuple[int, ...], symmetric: bool) -> np.ndarray:\r\n        line = np.zeros(shape)\r\n        line = line.reshape(-1)\r\n\r\n        if symmetric:\r\n            v = -2.0\r\n            for i in range(line.shape[0]):\r\n                if v == 0 or v == -3 or v == 3:\r\n                    v += 1\r\n                line[i] = v\r\n                v += 1\r\n                if v >= 8:\r\n                    v = -8\r\n        else:\r\n            v = 0.0\r\n            for i in range(line.shape[0]):\r\n                line[i] = v\r\n                v += 1\r\n                if v >= 16:\r\n                    v = 0\r\n\r\n        return line.reshape(shape)\r\n\r\n    def input_feeds(\r\n        self,\r\n        n: int,\r\n        name2shape: dict[str, int | tuple[int, ...]],\r\n        low: int = -1,\r\n        high: int = 2,\r\n        dtype: type = np.float32,\r\n    ) -> TestDataFeeds:\r\n        input_data_list = []\r\n        for _i in range(n):\r\n            inputs = {}\r\n            for name, shape in name2shape.items():\r\n                inputs.update({name: np.random.randint(low, high, shape).astype(dtype)})\r\n            input_data_list.extend([inputs])\r\n        dr = TestDataFeeds(input_data_list)\r\n        return dr\r\n\r\n    def construct_model_matmul(self, output_model_path: str, symmetric: bool) -> None:\r\n        #      (input)\r\n        #         |\r\n        #       MatMul\r\n        #         |\r\n        #      (output)\r\n        input_name = \"input\"\r\n        output_name = \"output\"\r\n        initializers = []\r\n\r\n        def make_matmul(\r\n            input_name, weight_shape: int | tuple[int, ...], weight_name: str, output_name: str, node_name: str\r\n        ):\r\n            weight_data = self.fill_int4_data(weight_shape, symmetric).astype(np.float32)\r\n            initializers.append(onnx.numpy_helper.from_array(weight_data, name=weight_name))\r\n            return onnx.helper.make_node(\r\n                \"MatMul\",\r\n                [input_name, weight_name],\r\n                [output_name],\r\n                node_name,\r\n            )\r\n\r\n        in_features = 52\r\n        out_features = 288\r\n        # make MatMul node\r\n        matmul_node = make_matmul(\r\n            input_name,\r\n            [in_features, out_features],\r\n            \"linear1.weight\",\r\n            output_name,\r\n            \"MatMul_0\",\r\n        )\r\n\r\n        # make graph\r\n        input_tensor = helper.make_tensor_value_info(input_name, TensorProto.FLOAT, [-1, in_features])\r\n        output_tensor = helper.make_tensor_value_info(output_name, TensorProto.FLOAT, [-1, out_features])\r\n        graph_name = \"matmul_4bits_test\"\r\n        graph = helper.make_graph(\r\n            [matmul_node],\r\n            graph_name,\r\n            [input_tensor],\r\n            [output_tensor],\r\n            initializer=initializers,\r\n        )\r\n        # blocked quantization requires DQ op set >= 21\r\n        model = helper.make_model(graph, opset_imports=[helper.make_opsetid(\"\", 21)])\r\n        model.ir_version = 10  # use stable onnx ir version\r\n\r\n        onnx.save(model, output_model_path)\r\n\r\n    def construct_model_gather(\r\n        self,\r\n        output_model_path: str,\r\n        symmetric: bool,\r\n        tdata: TensorProto.DataType,\r\n        tind: TensorProto.DataType,\r\n        vocab_size: int = 545,\r\n        embedding_len: int = 228,\r\n    ) -> None:\r\n        #      (input)\r\n        #         |\r\n        #       Gather\r\n        #         |\r\n        #      (output)\r\n        indices_name = \"input\"\r\n        output_name = \"output\"\r\n        initializers = []\r\n\r\n        def make_gather(\r\n            indices_name, data_shape: int | tuple[int, ...], data_name: str, output_name: str, node_name: str\r\n        ):\r\n            weight_data = self.fill_int4_data(data_shape, symmetric).astype(\r\n                np.float32 if tdata == TensorProto.FLOAT else np.float16\r\n            )\r\n            initializers.append(onnx.numpy_helper.from_array(weight_data, name=data_name))\r\n            kwargs = {\"axis\": 0}\r\n            return onnx.helper.make_node(\r\n                \"Gather\",\r\n                [data_name, indices_name],\r\n                [output_name],\r\n                node_name,\r\n                **kwargs,\r\n            )\r\n\r\n        gather_node = make_gather(\r\n            indices_name,\r\n            (vocab_size, embedding_len),\r\n            \"linear1.weight\",\r\n            output_name,\r\n            \"Gather_0\",\r\n        )\r\n\r\n        # make graph\r\n        input_tensor = helper.make_tensor_value_info(indices_name, tind, [-1, 1000])\r\n        output_tensor = helper.make_tensor_value_info(output_name, tdata, [-1, 1000, embedding_len])\r\n        graph_name = \"gather_4bits_test\"\r\n        graph = helper.make_graph(\r\n            [gather_node],\r\n            graph_name,\r\n            [input_tensor],\r\n            [output_tensor],\r\n            initializer=initializers,\r\n        )\r\n        # QDQ and gather requires op set >= 21. The tool should automatically update the opset.\r\n        model = helper.make_model(graph, opset_imports=[helper.make_opsetid(\"\", 19)])\r\n        model.ir_version = 9  # use stable onnx ir version\r\n\r\n        onnx.save(model, output_model_path)\r\n\r\n    def quant_test(\r\n        self,\r\n        model_fp32_path: str,\r\n        data_reader: TestDataFeeds,\r\n        block_size: int,\r\n        is_symmetric: bool,\r\n        quant_format: quant_utils.QuantFormat = quant_utils.QuantFormat.QOperator,\r\n        op_types_to_quantize: tuple[str, ...] = (\"MatMul\",),\r\n        quant_axes: tuple[tuple[str, int], ...] = ((\"MatMul\", 0), (\"Gather\", 1)),\r\n        rtol: float = 0.01,\r\n        atol: float = 0.05,\r\n    ):\r\n        use_qdq = quant_format == quant_utils.QuantFormat.QDQ\r\n        name_prefix = \"QDQ\" if use_qdq else \"QOperator\"\r\n        model_int4_path = str(\r\n            Path(self._tmp_model_dir.name).joinpath(f\"{name_prefix}_{block_size}_{is_symmetric}.onnx\").absolute()\r\n        )\r\n\r\n        # Quantize fp32 model to int4 model\r\n        from onnxruntime.quantization import matmul_nbits_quantizer  # noqa: PLC0415\r\n\r\n        model = quant_utils.load_model_with_shape_infer(Path(model_fp32_path))\r\n        quant_config = matmul_nbits_quantizer.DefaultWeightOnlyQuantConfig(\r\n            block_size=block_size,\r\n            is_symmetric=is_symmetric,\r\n            quant_format=quant_format,\r\n            op_types_to_quantize=op_types_to_quantize,\r\n            quant_axes=quant_axes,\r\n        )\r\n        quant = matmul_nbits_quantizer.MatMulNBitsQuantizer(model, algo_config=quant_config)\r\n        quant.process()\r\n        quant.model.save_model_to_file(model_int4_path, False)\r\n\r\n        if \"Gather\" in op_types_to_quantize:\r\n            quant_nodes = {\"GatherBlockQuantized\": 1}\r\n        else:\r\n            quant_nodes = {\"DequantizeLinear\": 1, \"MatMul\": 1} if use_qdq else {\"MatMulNBits\": 1}\r\n        check_op_type_count(self, model_int4_path, **quant_nodes)\r\n\r\n        if use_qdq:\r\n            dq_qtype = TensorProto.INT4 if is_symmetric else TensorProto.UINT4\r\n            dqnode_io_qtypes = (\r\n                {\r\n                    \"DequantizeLinear\": [\r\n                        [\"i\", 0, dq_qtype],\r\n                    ]\r\n                }\r\n                if is_symmetric\r\n                else {\r\n                    \"DequantizeLinear\": [\r\n                        [\"i\", 0, dq_qtype],\r\n                        [\"i\", 2, dq_qtype],\r\n                    ]\r\n                }\r\n            )\r\n            check_qtype_by_node_type(self, model_int4_path, dqnode_io_qtypes)\r\n            for op in quant.model.opset_import():\r\n                if op.domain in [None, \"\", \"ai.onnx\"] and op.version < 21:\r\n                    self.fail(f\"In QDQ format {op.domain} opset should be >= 21\")\r\n\r\n        data_reader.rewind()\r\n\r\n        try:\r\n            check_model_correctness(self, model_fp32_path, model_int4_path, data_reader.get_next(), rtol, atol)\r\n        except Exception as exception:\r\n            if \"4b quantization not yet supported on this hardware platform!\" in exception.args[0]:\r\n                # Currently we don't have int4 quantization support on all platforms, has to tolerate this exception\r\n                pass\r\n            else:\r\n                raise exception\r\n\r\n    def quant_test_with_algo(\r\n        self,\r\n        algorithm: str,\r\n        model_fp32_path: str,\r\n        data_reader: TestDataFeeds,\r\n        block_size: int,\r\n        is_symmetric: bool,\r\n    ):\r\n        model_int4_path = str(\r\n            Path(self._tmp_model_dir.name).joinpath(f\"MatMulNBits_{block_size}_{is_symmetric}.onnx\").absolute()\r\n        )\r\n\r\n        # Quantize fp32 model to int4 model\r\n        from onnxruntime.quantization import matmul_nbits_quantizer  # noqa: PLC0415\r\n\r\n        algo_config = None\r\n        if algorithm == \"RTN\":\r\n            # test RTN algorithm\r\n            algo_config = matmul_nbits_quantizer.RTNWeightOnlyQuantConfig()\r\n        elif algorithm == \"GPTQ\":\r\n            # test GPTQ algorithm\r\n            algo_config = matmul_nbits_quantizer.GPTQWeightOnlyQuantConfig(calibration_data_reader=data_reader)\r\n        elif algorithm == \"HQQ\":\r\n            # test HQQ algorithm\r\n            algo_config = matmul_nbits_quantizer.HQQWeightOnlyQuantConfig(block_size=block_size)\r\n\r\n        model = quant_utils.load_model_with_shape_infer(Path(model_fp32_path))\r\n        bits = 4\r\n        quant = matmul_nbits_quantizer.MatMulNBitsQuantizer(\r\n            model, bits, block_size, is_symmetric, algo_config=algo_config\r\n        )\r\n        quant.process()\r\n        quant.model.save_model_to_file(model_int4_path, False)\r\n\r\n        quant_nodes = {\"MatMulNBits\": 1}\r\n        check_op_type_count(self, model_int4_path, **quant_nodes)\r\n\r\n        data_reader.rewind()\r\n\r\n        try:\r\n            check_model_correctness(self, model_fp32_path, model_int4_path, data_reader.get_next())\r\n        except Exception as exception:\r\n            if \"4b quantization not yet supported on this hardware platform!\" in exception.args[0]:\r\n                # Currently we don't have int4 quantization support on all platforms, has to tolerate this exception\r\n                pass\r\n            else:\r\n                raise exception\r\n\r\n    def test_quantize_matmul_int4_symmetric(self):\r\n        np.random.seed(13)\r\n\r\n        model_fp32_path = str(Path(self._tmp_model_dir.name).joinpath(\"matmul_fp32_symmetric.onnx\").absolute())\r\n        self.construct_model_matmul(model_fp32_path, symmetric=True)\r\n        data_reader = self.input_feeds(1, {\"input\": (100, 52)})\r\n        self.quant_test(model_fp32_path, data_reader, 32, True)\r\n\r\n    def test_quantize_matmul_int4_offsets(self):\r\n        model_fp32_path = str(Path(self._tmp_model_dir.name).joinpath(\"matmul_fp32_offset.onnx\").absolute())\r\n        self.construct_model_matmul(model_fp32_path, symmetric=False)\r\n        data_reader = self.input_feeds(1, {\"input\": (100, 52)})\r\n        self.quant_test(model_fp32_path, data_reader, 32, False)\r\n\r\n    def test_quantize_gather_int4_symmetric(self):\r\n        np.random.seed(13)\r\n\r\n        model_fp32_path = str(Path(self._tmp_model_dir.name).joinpath(\"gather_fp32_symmetric.onnx\").absolute())\r\n        self.construct_model_gather(model_fp32_path, True, TensorProto.FLOAT, TensorProto.INT32)\r\n        data_reader = self.input_feeds(1, {\"input\": (100, 1000)}, -545, 535, np.int32)\r\n        # cover rounding error\r\n        self.quant_test(model_fp32_path, data_reader, 32, True, op_types_to_quantize=(\"Gather\",), rtol=0.2, atol=0.5)\r\n\r\n    def test_quantize_gather_int4_offsets(self):\r\n        model_fp32_path = str(Path(self._tmp_model_dir.name).joinpath(\"gather_fp32_offset.onnx\").absolute())\r\n        self.construct_model_gather(model_fp32_path, False, TensorProto.FLOAT16, TensorProto.INT64)\r\n        data_reader = self.input_feeds(1, {\"input\": (100, 1000)}, -545, 535, np.int64)\r\n        # cover rounding error\r\n        self.quant_test(model_fp32_path, data_reader, 32, False, op_types_to_quantize=(\"Gather\",), rtol=0.2, atol=0.5)\r\n\r\n    def test_quantize_matmul_int4_symmetric_qdq(self):\r\n        np.random.seed(13)\r\n\r\n        model_fp32_path = str(Path(self._tmp_model_dir.name).joinpath(\"matmul_fp32_symmetric.onnx\").absolute())\r\n        self.construct_model_matmul(model_fp32_path, symmetric=True)\r\n        data_reader = self.input_feeds(1, {\"input\": (100, 52)})\r\n        self.quant_test(model_fp32_path, data_reader, 32, True, quant_utils.QuantFormat.QDQ)\r\n\r\n    def test_quantize_matmul_int4_offsets_qdq(self):\r\n        model_fp32_path = str(Path(self._tmp_model_dir.name).joinpath(\"matmul_fp32_offset.onnx\").absolute())\r\n        self.construct_model_matmul(model_fp32_path, symmetric=False)\r\n        data_reader = self.input_feeds(1, {\"input\": (100, 52)})\r\n        self.quant_test(model_fp32_path, data_reader, 32, False, quant_utils.QuantFormat.QDQ)\r\n\r\n    def test_quantize_matmul_int4_using_rtn_algo(self):\r\n        if not find_spec(\"neural_compressor\"):\r\n            self.skipTest(\"skip test_smooth_quant since neural_compressor is not installed\")\r\n        if not find_spec(\"torch\"):\r\n            self.skipTest(\"skip test_quantize_matmul_int4_using_rtn_algo since torch is not installed\")\r\n        model_fp32_path = str(Path(self._tmp_model_dir.name).joinpath(\"matmul_fp32_offset.onnx\").absolute())\r\n        self.construct_model_matmul(model_fp32_path, symmetric=False)\r\n        data_reader = self.input_feeds(1, {\"input\": (100, 52)})\r\n        self.quant_test_with_algo(\"RTN\", model_fp32_path, data_reader, 32, False)\r\n\r\n    def test_quantize_matmul_int4_using_gptq_algo(self):\r\n        if not find_spec(\"neural_compressor\"):\r\n            self.skipTest(\"skip test_smooth_quant since neural_compressor is not installed\")\r\n        if not find_spec(\"torch\"):\r\n            self.skipTest(\"skip test_quantize_matmul_int4_using_gptq_algo since torch is not installed\")\r\n        model_fp32_path = str(Path(self._tmp_model_dir.name).joinpath(\"matmul_fp32_offset.onnx\").absolute())\r\n        self.construct_model_matmul(model_fp32_path, symmetric=False)\r\n        data_reader = self.input_feeds(1, {\"input\": (100, 52)})\r\n        self.quant_test_with_algo(\"GPTQ\", model_fp32_path, data_reader, 32, False)\r\n\r\n    def test_quantize_matmul_int4_using_hqq_algo(self):\r\n        if not find_spec(\"torch\"):\r\n            self.skipTest(\"skip test_hqq_quant since torch is not installed\")\r\n        model_fp32_path = str(Path(self._tmp_model_dir.name).joinpath(\"matmul_fp32_offset.onnx\").absolute())\r\n        self.construct_model_matmul(model_fp32_path, symmetric=False)\r\n        data_reader = self.input_feeds(1, {\"input\": (100, 52)})\r\n        self.quant_test_with_algo(\"HQQ\", model_fp32_path, data_reader, 32, False)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    unittest.main()\r\n    # TODO(fajin): add 8bit quantization test after enabling kenrels\r\n"}
{"path":"\\\\?\\C:\\Users\\Naomi\\Desktop\\ONNX\\onnxruntime\\onnxruntime\\core\\providers\\cpu\\cpu_execution_provider.cc","line":null,"char":null,"code":"CLANGFORMAT","severity":"warning","name":"format","description":"See https://clang.llvm.org/docs/ClangFormat.html.\nRun `lintrunner -a` to apply this patch.","original":"// Copyright (c) Microsoft Corporation. All rights reserved.\r\n// Licensed under the MIT License.\r\n\r\n#include \"core/providers/cpu/cpu_execution_provider.h\"\r\n\r\n#include \"core/framework/allocator_utils.h\"\r\n#include \"core/framework/memcpy.h\"\r\n#include \"core/framework/op_kernel.h\"\r\n#include \"core/framework/kernel_registry.h\"\r\n#include \"core/framework/int4.h\"\r\n#include \"core/mlas/inc/mlas.h\"\r\n\r\n#ifndef DISABLE_CONTRIB_OPS\r\n#include \"contrib_ops/cpu/cpu_contrib_kernels.h\"\r\n#endif\r\n\r\n#if defined(ENABLE_TRAINING_OPS)\r\n#include \"orttraining/training_ops/cpu/cpu_training_kernels.h\"\r\n#endif\r\n\r\n#include \"core/framework/compute_capability.h\"\r\n\r\nnamespace {\r\nstruct KernelRegistryAndStatus {\r\n  std::shared_ptr<onnxruntime::KernelRegistry> kernel_registry = std::make_shared<onnxruntime::KernelRegistry>();\r\n  onnxruntime::Status st;\r\n};\r\n}  // namespace\r\n\r\nnamespace onnxruntime {\r\n\r\n// The MemcpyFromHost and MemcpyToHost kernels registered for the CPU EP are generic memcpy kernels.\r\n// Other EPs may provide their own memcpy kernels.\r\n// For a memcpy between host (CPU) and device of some other EP:\r\n// - If the EP provides the corresponding memcpy kernel, it will be used.\r\n// - Otherwise, one of these generic memcpy kernels will be used.\r\n\r\nONNX_OPERATOR_KERNEL_EX(\r\n    MemcpyFromHost,\r\n    kOnnxDomain,\r\n    1,\r\n    kCpuExecutionProvider,\r\n    (*KernelDefBuilder::Create())\r\n        .InputMemoryType(OrtMemTypeCPUInput, 0)\r\n        .TypeConstraint(\"T\", DataTypeImpl::AllFixedSizeTensorAndSequenceTensorTypesIRv9()),\r\n    Memcpy);\r\n\r\nONNX_OPERATOR_KERNEL_EX(\r\n    MemcpyToHost,\r\n    kOnnxDomain,\r\n    1,\r\n    kCpuExecutionProvider,\r\n    (*KernelDefBuilder::Create())\r\n        .OutputMemoryType(OrtMemTypeCPUOutput, 0)\r\n        .TypeConstraint(\"T\", DataTypeImpl::AllFixedSizeTensorAndSequenceTensorTypesIRv9()),\r\n    Memcpy);\r\n\r\nCPUExecutionProvider::CPUExecutionProvider(const CPUExecutionProviderInfo& info)\r\n    : IExecutionProvider{onnxruntime::kCpuExecutionProvider}, info_{info} {}\r\n\r\nstd::vector<AllocatorPtr> CPUExecutionProvider::CreatePreferredAllocators() {\r\n  const bool create_arena = DoesCpuAllocatorSupportArenaUsage() ? info_.create_arena : false;\r\n  AllocatorCreationInfo device_info_cpu{[](int) { return std::make_unique<CPUAllocator>(); },\r\n                                        DEFAULT_CPU_ALLOCATOR_DEVICE_ID, create_arena};\r\n\r\n  return std::vector<AllocatorPtr>{CreateAllocator(device_info_cpu)};\r\n}\r\n\r\n// Forward declarations of op kernels\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, MemcpyFromHost);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, MemcpyToHost);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 10, Clip);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 21, Elu);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 21, HardSigmoid);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 15, LeakyRelu);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, float, Relu);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, double, Relu);\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, MLFloat16, Relu);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 15, MLFloat16, LeakyRelu);\r\n#endif\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 21, Selu);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, float, Sigmoid);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, double, Sigmoid);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 21, Softplus);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 21, Softsign);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, float, Tanh);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, double, Tanh);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8, PRelu);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, RandomNormal);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, RandomUniform);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, RandomNormalLike);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, RandomUniformLike);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, Multinomial);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, float, Abs);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, double, Abs);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, int8_t, Abs);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, int16_t, Abs);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, int32_t, Abs);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, int64_t, Abs);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, uint8_t, Abs);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, uint16_t, Abs);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, uint32_t, Abs);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, uint64_t, Abs);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, float, Floor);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, double, Floor);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, float, Ceil);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, double, Ceil);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, float, Reciprocal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, double, Reciprocal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, float, Sqrt);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, double, Sqrt);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, float, Add);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, double, Add);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, int32_t, Add);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, int64_t, Add);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, uint32_t, Add);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, uint64_t, Add);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, float, Sub);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, double, Sub);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, int32_t, Sub);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, int64_t, Sub);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, uint32_t, Sub);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, uint64_t, Sub);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, float, Mul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, double, Mul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, int32_t, Mul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, int64_t, Mul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, uint32_t, Mul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, uint64_t, Mul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, float, Div);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, double, Div);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, int32_t, Div);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, int64_t, Div);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, uint32_t, Div);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, uint64_t, Div);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, float, Neg);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, double, Neg);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, int8_t, Neg);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, int16_t, Neg);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, int32_t, Neg);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, int64_t, Neg);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 11, Pow);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, float, Exp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, double, Exp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, float, Log);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, double, Log);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 7, float, Sum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 7, double, Sum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, float, Sum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, double, Sum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 7, float, Min);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 11, Min);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 7, float, Max);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 11, Max);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, Not);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, And);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, Or);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, Xor);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8, float, Less);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8, double, Less);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8, float, Greater);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8, double, Greater);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 10, bool, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 10, int32_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 10, int64_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 10, float, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 10, double, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 7, float, Mean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, float, Mean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 21, float, Sin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 21, double, Sin);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 21, Cos);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 21, Tan);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 21, Asin);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 21, Acos);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 21, Atan);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8, float, Gemm);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8, double, Gemm);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, Hardmax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, float, LogSoftmax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, double, LogSoftmax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 8, float, MatMul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 8, double, MatMul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, float, Softmax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, double, Softmax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 9, float, TopK);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 9, double, TopK);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8, float,\r\n                                                      BatchNormalization);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8, double,\r\n                                                      BatchNormalization);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, Conv);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, ConvTranspose);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 8, Flatten);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 21, InstanceNormalization);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, float, LpNormalization);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, double, LpNormalization);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 12, LRN);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 9, AveragePool);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 7, MaxPool);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 11, MaxPool);\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 11, MLFloat16, MaxPool);\r\n#endif\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 2, 10, LpPool);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 2, GlobalLpPool);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 21, GlobalAveragePool);\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 21, MLFloat16, GlobalAveragePool);\r\n#endif\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 21, GlobalMaxPool);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, MaxRoiPool);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, float, ReduceL1);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, double, ReduceL1);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int32_t, ReduceL1);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int64_t, ReduceL1);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, float, ReduceL2);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, double, ReduceL2);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int32_t, ReduceL2);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int64_t, ReduceL2);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, float, ReduceLogSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, double, ReduceLogSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int32_t, ReduceLogSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int64_t, ReduceLogSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, float,\r\n                                                      ReduceLogSumExp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, double,\r\n                                                      ReduceLogSumExp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int32_t,\r\n                                                      ReduceLogSumExp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int64_t,\r\n                                                      ReduceLogSumExp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, float, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, double, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int32_t, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int64_t, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, float, ReduceMean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, double, ReduceMean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int32_t, ReduceMean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int64_t, ReduceMean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, float, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, double, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int32_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int64_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, float, ReduceProd);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, double, ReduceProd);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int32_t, ReduceProd);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int64_t, ReduceProd);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, float, ReduceSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int32_t, ReduceSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, double, ReduceSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int64_t, ReduceSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, float,\r\n                                                      ReduceSumSquare);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int32_t,\r\n                                                      ReduceSumSquare);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, double,\r\n                                                      ReduceSumSquare);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int64_t,\r\n                                                      ReduceSumSquare);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, float, ArgMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, double, ArgMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int8_t, ArgMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, uint8_t, ArgMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int32_t, ArgMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int64_t, ArgMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, float, ArgMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, double, ArgMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int8_t, ArgMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, uint8_t, ArgMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int64_t, ArgMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int32_t, ArgMin);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 13, GRU);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 13, LSTM);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 13, RNN);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, Cast);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 4, 10, Concat);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, Gather);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 9, Dropout);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 12, Identity);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 2, 10, Pad);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 4, Reshape);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 5, 12, Reshape);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 12, Shape);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 12, Size);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 9, Slice);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 12, SpaceToDepth);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, DepthToSpace);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 2, 10, Split);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, Squeeze);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, Tile);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 12, Transpose);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, Unsqueeze);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8, float, Upsample);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8, int32_t, Upsample);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8, int8_t, Upsample);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8, uint8_t, Upsample);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, float, Expand);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, double, Expand);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, int8_t, Expand);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, int16_t, Expand);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, int32_t, Expand);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, int64_t, Expand);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, uint8_t, Expand);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, uint16_t, Expand);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, uint32_t, Expand);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, uint64_t, Expand);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, bool, Expand);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, MLFloat16, Expand);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, string, Expand);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 8, Scan);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, If);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, Loop);\r\n\r\n// Opset 9\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, Compress);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 19, ConstantOfShape);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, MeanVarianceNormalization);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, float, Greater);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, double, Greater);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, int8_t, Greater);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, int16_t, Greater);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, int32_t, Greater);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, int64_t, Greater);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, uint8_t, Greater);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, uint16_t, Greater);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, uint32_t, Greater);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, uint64_t, Greater);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, float, Less);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, double, Less);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, int8_t, Less);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, int16_t, Less);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, int32_t, Less);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, int64_t, Less);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, uint8_t, Less);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, uint16_t, Less);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, uint32_t, Less);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, uint64_t, Less);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 21, EyeLike);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, float, IsNaN);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, double, IsNaN);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, MLFloat16, IsNaN);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, Sign);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, Shrink);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, float, Erf);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                      int64_t_int64_t_int64_t, OneHot);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, float_int64_t_int64_t,\r\n                                                      OneHot);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, int64_t_string_int64_t,\r\n                                                      OneHot);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, float_string_int64_t,\r\n                                                      OneHot);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, float_float_float,\r\n                                                      OneHot);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, int64_t_int32_t_float,\r\n                                                      OneHot);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, int64_t_float_int64_t,\r\n                                                      OneHot);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, int32_t_float_int32_t,\r\n                                                      OneHot);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, int32_t_float_float,\r\n                                                      OneHot);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, int64_t_float_float,\r\n                                                      OneHot);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, int64_t_float_int32_t,\r\n                                                      OneHot);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, MaxUnpool);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 21, Sinh);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 21, Cosh);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 21, Asinh);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 21, Acosh);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 21, Atanh);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, Scan);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, Scatter);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, TfIdfVectorizer);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, bool, NonZero);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, float, NonZero);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, int32_t, NonZero);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, int64_t, NonZero);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, uint8_t, NonZero);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 15, string, Where);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 15, float, Where);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 15, double, Where);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 15, int32_t, Where);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 15, int64_t, Where);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 15, uint8_t, Where);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, Flatten);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, float, Gemm);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, double, Gemm);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, float, MatMul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, double, MatMul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, int32_t, MatMul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, int64_t, MatMul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 13, float,\r\n                                                      BatchNormalization);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 13, double,\r\n                                                      BatchNormalization);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 15, PRelu);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 9, float, Upsample);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 9, int32_t, Upsample);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 9, int8_t, Upsample);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 9, uint8_t, Upsample);\r\n\r\n// Opset 10\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, StringNormalizer);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10, float, TopK);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10, double, TopK);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10, AveragePool);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 12, Mod);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10, float, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10, int32_t, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10, int8_t, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10, uint8_t, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 21, ThresholdedRelu);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 12, uint8_t,\r\n                                                      DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 12, int8_t,\r\n                                                      DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 12, int32_t,\r\n                                                      DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 12, uint8_t,\r\n                                                      QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 12, int8_t,\r\n                                                      QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 20, uint8_t,\r\n                                                      QLinearMatMul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 20, int8_t,\r\n                                                      QLinearMatMul);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, uint8_t, MatMulInteger);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, int8_t, MatMulInteger);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, ConvInteger);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, uint8_t, QLinearConv);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, int8_t, QLinearConv);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10, Slice);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 11, Dropout);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10, NonMaxSuppression);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 19, IsInf);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 15, float, RoiAlign);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 15, double, RoiAlign);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, ReverseSequence);\r\n\r\n// opset 11\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11, Clip);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 13, float, CumSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 13, double, CumSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 13, int32_t, CumSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 13, int64_t, CumSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, bool, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int8_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int16_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int32_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int64_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, uint8_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, uint16_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, uint32_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, uint64_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, double, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, float, Round);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, double, Round);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, MLFloat16, Round);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, uint8_t, DynamicQuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float, ArgMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, double, ArgMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int8_t, ArgMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, uint8_t, ArgMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int32_t, ArgMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int64_t, ArgMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float, ArgMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, double, ArgMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int8_t, ArgMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, uint8_t, ArgMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int32_t, ArgMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int64_t, ArgMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float, ReduceL1);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, double, ReduceL1);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int32_t, ReduceL1);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int64_t, ReduceL1);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float, ReduceL2);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, double, ReduceL2);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int32_t, ReduceL2);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int64_t, ReduceL2);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float, ReduceLogSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, double, ReduceLogSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int32_t,\r\n                                                      ReduceLogSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int64_t,\r\n                                                      ReduceLogSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float,\r\n                                                      ReduceLogSumExp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, double,\r\n                                                      ReduceLogSumExp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int32_t,\r\n                                                      ReduceLogSumExp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int64_t,\r\n                                                      ReduceLogSumExp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11, float, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11, double, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11, int32_t, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11, int64_t, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float, ReduceMean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, double, ReduceMean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int32_t, ReduceMean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int64_t, ReduceMean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11, float, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11, double, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11, int32_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11, int64_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float, ReduceProd);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, double, ReduceProd);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int32_t, ReduceProd);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int64_t, ReduceProd);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float, ReduceSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, double, ReduceSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int32_t, ReduceSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int64_t, ReduceSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float,\r\n                                                      ReduceSumSquare);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, double,\r\n                                                      ReduceSumSquare);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int32_t,\r\n                                                      ReduceSumSquare);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int64_t,\r\n                                                      ReduceSumSquare);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, Hardmax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float, LogSoftmax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, double, LogSoftmax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float, Softmax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, double, Softmax);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, Loop);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, DepthToSpace);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 15, Scan);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, Flatten);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, Compress);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, Concat);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, Gather);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, Slice);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, Split);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, Squeeze);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, Unsqueeze);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, Det);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, ScatterElements);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, NonMaxSuppression);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 18, AveragePool);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, MaxUnpool);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 17, LpPool);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, Conv);\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, MLFloat16, Conv);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 18, MLFloat16,\r\n                                                      AveragePool);\r\n#endif\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, ConvTranspose);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, If);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, SequenceLength);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, SequenceAt);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, SequenceEmpty);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, SequenceInsert);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, SequenceErase);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, SequenceConstruct);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, ConcatFromSequence);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 23, SplitToSequence);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, ScatterND);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float, Gemm);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, double, Gemm);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, GatherElements);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, uint8_t, BitShift);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, uint32_t, BitShift);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, uint64_t, BitShift);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, Pad);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11, GatherND);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, Range);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, Unique);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 23, float, TopK);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 23, double, TopK);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 23, int64_t, TopK);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 23, int32_t, TopK);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, int64_t_int64_t_int64_t, OneHot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, float_int64_t_int64_t, OneHot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, int64_t_string_int64_t, OneHot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, float_string_int64_t, OneHot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, float_float_float, OneHot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, int64_t_int32_t_float, OneHot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, int64_t_float_int64_t, OneHot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, int32_t_float_int32_t, OneHot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, int32_t_float_float, OneHot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, int64_t_float_float, OneHot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, int64_t_float_int32_t, OneHot);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int32_t, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int8_t, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, uint8_t, Resize);\r\n\r\n// opset 12\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, Clip);\r\n\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, Min);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, Max);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 21, MaxPool);\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 21, MLFloat16, MaxPool);\r\n#endif\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, Pow);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, float, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, double, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, int32_t, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, int64_t, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, int8_t, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, uint8_t, ReduceMax);\r\n\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, float, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, double, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, int32_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, int64_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, int8_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, uint8_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, GatherND);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, Einsum);\r\n\r\n// REVIEW(codemzs): ConstEigenVectorArrayMap.cast<MLFLoat16) does not seem to be supported.\r\n// However these types work on GPU implementation.\r\n// class ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, MLFloat16_MLFloat16, Dropout);\r\n// class ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, MLFloat16_float, Dropout);\r\n// class ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, MLFloat16_double, Dropout);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, float_float, Dropout);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, float_double,\r\n                                                      Dropout);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, double_float,\r\n                                                      Dropout);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, double_double,\r\n                                                      Dropout);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, Celu);\r\n\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, float,\r\n                                                      GreaterOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, double,\r\n                                                      GreaterOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, int8_t,\r\n                                                      GreaterOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, int16_t,\r\n                                                      GreaterOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, int32_t,\r\n                                                      GreaterOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, int64_t,\r\n                                                      GreaterOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, uint8_t,\r\n                                                      GreaterOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, uint16_t,\r\n                                                      GreaterOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, uint32_t,\r\n                                                      GreaterOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, uint64_t,\r\n                                                      GreaterOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, float, LessOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, double, LessOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, int8_t, LessOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, int16_t, LessOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, int32_t, LessOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, int64_t, LessOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, uint8_t, LessOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, uint16_t, LessOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, uint32_t, LessOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, uint64_t, LessOrEqual);\r\n\r\n// opset 13\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, MLFloat16, Erf);\r\n#endif\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Erf);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, Cast);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Clip);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, uint8_t,\r\n                                                      DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, int8_t,\r\n                                                      DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, int32_t,\r\n                                                      DequantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Expand);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Expand);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int8_t, Expand);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int16_t, Expand);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t, Expand);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t, Expand);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint8_t, Expand);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint16_t, Expand);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint32_t, Expand);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint64_t, Expand);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, bool, Expand);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, MLFloat16, Expand);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, string, Expand);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Gemm);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Gemm);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, MatMul);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, MatMul);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t, MatMul);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t, MatMul);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Min);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Max);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Mean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, uint8_t,\r\n                                                      QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, int8_t,\r\n                                                      QuantizeLinear);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Sigmoid);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Sign);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, Size);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Sum);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Sum);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 20, Flatten);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, LRN);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, MeanVarianceNormalization);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 21, float_float, Dropout);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 21, float_double, Dropout);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 21, double_float, Dropout);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 21, double_double, Dropout);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, ArgMax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, ArgMax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int8_t, ArgMax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint8_t, ArgMax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t, ArgMax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t, ArgMax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, ArgMin);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, ArgMin);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int8_t, ArgMin);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint8_t, ArgMin);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t, ArgMin);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t, ArgMin);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, Reshape);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 14, Shape);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Concat);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Less);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Less);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int8_t, Less);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int16_t, Less);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t, Less);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t, Less);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint8_t, Less);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint16_t, Less);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint32_t, Less);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint64_t, Less);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Greater);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Greater);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int8_t, Greater);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int16_t, Greater);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t, Greater);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t, Greater);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint8_t, Greater);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint16_t, Greater);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint32_t, Greater);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint64_t, Greater);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, bool, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, int8_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, int16_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, int32_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, int64_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, uint8_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, uint16_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, uint32_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, uint64_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, float, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, double, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, float, Add);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, double, Add);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, int32_t, Add);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, int64_t, Add);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, uint32_t, Add);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, uint64_t, Add);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, float, Sub);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, double, Sub);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, int32_t, Sub);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, int64_t, Sub);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, uint32_t, Sub);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, uint64_t, Sub);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, float, Mul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, double, Mul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, int32_t, Mul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, int64_t, Mul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, uint32_t, Mul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, uint64_t, Mul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, float, Div);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, double, Div);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, int32_t, Div);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, int64_t, Div);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, uint32_t, Div);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, uint64_t, Div);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Neg);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Neg);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int8_t, Neg);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int16_t, Neg);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t, Neg);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t, Neg);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Mod);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Abs);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Abs);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int8_t, Abs);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int16_t, Abs);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t, Abs);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t, Abs);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint8_t, Abs);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint16_t, Abs);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint32_t, Abs);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint64_t, Abs);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Reciprocal);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Reciprocal);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Floor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Floor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Ceil);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Ceil);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Sqrt);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Sqrt);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, float, Relu);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, double, Relu);\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, MLFloat16, Relu);\r\n#endif\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Sigmoid);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Sigmoid);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Tanh);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Tanh);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Exp);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Exp);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Log);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Log);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 14, Pow);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, DepthToSpace);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, SpaceToDepth);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Slice);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, Split);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 20, Unsqueeze);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 20, Squeeze);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 20, Transpose);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Tile);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Gather);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, GatherElements);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 15, ScatterND);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 15, ScatterElements);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, Identity);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 19, float, IsNaN);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 19, double, IsNaN);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 19, MLFloat16, IsNaN);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 19, BFloat16, IsNaN);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, bool, NonZero);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, NonZero);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t, NonZero);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t, NonZero);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint8_t, NonZero);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, GatherND);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, Pad);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, float, ReduceL1);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, double, ReduceL1);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int32_t, ReduceL1);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int64_t, ReduceL1);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, float, ReduceL2);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, double, ReduceL2);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int32_t, ReduceL2);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int64_t, ReduceL2);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, float, ReduceLogSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, double, ReduceLogSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int32_t,\r\n                                                      ReduceLogSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int64_t,\r\n                                                      ReduceLogSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, float,\r\n                                                      ReduceLogSumExp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, double,\r\n                                                      ReduceLogSumExp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int32_t,\r\n                                                      ReduceLogSumExp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int64_t,\r\n                                                      ReduceLogSumExp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, float, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, double, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int32_t, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int64_t, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int8_t, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, uint8_t, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, float, ReduceMean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, double, ReduceMean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int32_t, ReduceMean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int64_t, ReduceMean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, float, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, double, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int32_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int64_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int8_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, uint8_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, float, ReduceProd);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, double, ReduceProd);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int32_t, ReduceProd);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int64_t, ReduceProd);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, float,\r\n                                                      ReduceSumSquare);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, double,\r\n                                                      ReduceSumSquare);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int32_t,\r\n                                                      ReduceSumSquare);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int64_t,\r\n                                                      ReduceSumSquare);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, ReduceSum);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, ReduceSum);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t, ReduceSum);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t, ReduceSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, float, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int32_t, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int8_t, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, uint8_t, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 15, Loop);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 15, If);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Hardmax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, LogSoftmax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, LogSoftmax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Softmax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Softmax);\r\n\r\n// Opset 14\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, float, CumSum);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, double, CumSum);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int32_t, CumSum);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int64_t, CumSum);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, float, Relu);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, double, Relu);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int8_t, Relu);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int32_t, Relu);\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, MLFloat16, Relu);\r\n#endif\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, Trilu);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, float, Add);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, double, Add);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int8_t, Add);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int16_t, Add);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int32_t, Add);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int64_t, Add);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint8_t, Add);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint16_t, Add);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint32_t, Add);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint64_t, Add);\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, MLFloat16, Add);\r\n#endif\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, float, Sub);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, double, Sub);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int8_t, Sub);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int16_t, Sub);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int32_t, Sub);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int64_t, Sub);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint8_t, Sub);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint16_t, Sub);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint32_t, Sub);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint64_t, Sub);\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, MLFloat16, Sub);\r\n#endif\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, float, Mul);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, double, Mul);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int8_t, Mul);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int16_t, Mul);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int32_t, Mul);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int64_t, Mul);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint8_t, Mul);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint16_t, Mul);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint32_t, Mul);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint64_t, Mul);\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, MLFloat16, Mul);\r\n#endif\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, float, Div);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, double, Div);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int8_t, Div);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int16_t, Div);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int32_t, Div);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int64_t, Div);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint8_t, Div);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint16_t, Div);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint32_t, Div);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint64_t, Div);\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, MLFloat16, Div);\r\n#endif\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, 18, Reshape);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, 15, Identity);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, 14, float,\r\n                                                      BatchNormalization);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, 14, double,\r\n                                                      BatchNormalization);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, 21, GRU);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, 21, LSTM);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, 21, RNN);\r\n\r\n// Opset 15\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 15, Pow);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 15, float, BatchNormalization);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 15, double, BatchNormalization);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 15, 18, Shape);\r\n\r\n#if !defined(DISABLE_OPTIONAL_TYPE)\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 15, 17, OptionalHasElement);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 15, 17, OptionalGetElement);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 15, Optional);\r\n#endif\r\n\r\n// Opset 16\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, 18, Identity);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, 18, Loop);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, 18, If);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, float, RoiAlign);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, double, RoiAlign);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, 19, float, GridSample);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, 17, ScatterElements);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, 17, ScatterND);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, string, Where);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, float, Where);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, double, Where);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int32_t, Where);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int64_t, Where);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint8_t, Where);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, LeakyRelu);\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, MLFloat16, LeakyRelu);\r\n#endif\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, PRelu);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, 18, Scan);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, float, GreaterOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, double, GreaterOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int8_t, GreaterOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int16_t, GreaterOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int32_t, GreaterOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int64_t, GreaterOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint8_t, GreaterOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint16_t, GreaterOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint32_t, GreaterOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint64_t, GreaterOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, float, LessOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, double, LessOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int8_t, LessOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int16_t, LessOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int32_t, LessOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int64_t, LessOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint8_t, LessOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint16_t, LessOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint32_t, LessOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint64_t, LessOrEqual);\r\n\r\n// Opset 17\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, 19, DFT);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, BlackmanWindow);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, HammingWindow);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, HannWindow);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, MelWeightMatrix);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, STFT);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, float, LayerNormalization);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, double, LayerNormalization);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, MLFloat16, LayerNormalization);\r\n// Opset 18\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 18, float, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 18, int32_t, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 18, int8_t, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 18, uint8_t, Resize);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, float, ReduceL1);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, double, ReduceL1);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t, ReduceL1);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t, ReduceL1);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, float, ReduceL2);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, double, ReduceL2);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t, ReduceL2);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t, ReduceL2);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, float, ReduceLogSum);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, double, ReduceLogSum);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t, ReduceLogSum);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t, ReduceLogSum);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, float, ReduceLogSumExp);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, double, ReduceLogSumExp);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t, ReduceLogSumExp);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t, ReduceLogSumExp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, float, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, double, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, int32_t, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, int64_t, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, int8_t, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, uint8_t, ReduceMax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, float, ReduceMean);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, double, ReduceMean);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t, ReduceMean);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t, ReduceMean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, float, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, double, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, int32_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, int64_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, int8_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, uint8_t, ReduceMin);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, float, ReduceProd);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, double, ReduceProd);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t, ReduceProd);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t, ReduceProd);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, float, ReduceSumSquare);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, double, ReduceSumSquare);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t, ReduceSumSquare);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t, ReduceSumSquare);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 21, LpPool);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, Col2Im);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int8_t, BitwiseAnd);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int16_t, BitwiseAnd);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t, BitwiseAnd);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t, BitwiseAnd);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint8_t, BitwiseAnd);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint16_t, BitwiseAnd);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint32_t, BitwiseAnd);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint64_t, BitwiseAnd);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int8_t, BitwiseNot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int16_t, BitwiseNot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t, BitwiseNot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t, BitwiseNot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint8_t, BitwiseNot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint16_t, BitwiseNot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint32_t, BitwiseNot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint64_t, BitwiseNot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int8_t, BitwiseOr);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int16_t, BitwiseOr);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t, BitwiseOr);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t, BitwiseOr);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint8_t, BitwiseOr);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint16_t, BitwiseOr);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint32_t, BitwiseOr);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint64_t, BitwiseOr);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int8_t, BitwiseXor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int16_t, BitwiseXor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t, BitwiseXor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t, BitwiseXor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint8_t, BitwiseXor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint16_t, BitwiseXor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint32_t, BitwiseXor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint64_t, BitwiseXor);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 18, Pad);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, ScatterND);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, ScatterElements);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, Split);\r\n#if !defined(DISABLE_OPTIONAL_TYPE)\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, OptionalHasElement);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, OptionalGetElement);\r\n\r\n#endif\r\n\r\n// Opset 19\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Size);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 21, AveragePool);\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 21, MLFloat16, AveragePool);\r\n#endif\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Cast);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, int32_t,\r\n                                                      DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, uint8_t,\r\n                                                      DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, int8_t,\r\n                                                      DequantizeLinear);\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Float8E4M3FN,\r\n                                                      DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Float8E4M3FNUZ,\r\n                                                      DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Float8E5M2,\r\n                                                      DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Float8E5M2FNUZ,\r\n                                                      DequantizeLinear);\r\n#endif\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, bool, Equal);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, int8_t, Equal);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, int16_t, Equal);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, int32_t, Equal);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, int64_t, Equal);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, uint8_t, Equal);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, uint16_t, Equal);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, uint32_t, Equal);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, uint64_t, Equal);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, float, Equal);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, double, Equal);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, string, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Identity);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, If);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Loop);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Pad);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, uint8_t,\r\n                                                      QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, int8_t,\r\n                                                      QuantizeLinear);\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Float8E4M3FN,\r\n                                                      QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Float8E4M3FNUZ,\r\n                                                      QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Float8E5M2,\r\n                                                      QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Float8E5M2FNUZ,\r\n                                                      QuantizeLinear);\r\n#endif\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Reshape);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, float, Resize);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, int32_t, Resize);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, int8_t, Resize);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, uint8_t, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Scan);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Shape);\r\n\r\n// Opset 20\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, 20, ConstantOfShape);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, bool, ReduceMax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, float, ReduceMax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, double, ReduceMax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, int32_t, ReduceMax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, int64_t, ReduceMax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, int8_t, ReduceMax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, uint8_t, ReduceMax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, bool, ReduceMin);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, float, ReduceMin);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, double, ReduceMin);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, int32_t, ReduceMin);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, int64_t, ReduceMin);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, int8_t, ReduceMin);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, uint8_t, ReduceMin);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, DFT);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, 21, float, GridSample);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, 21, double, GridSample);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, float, AffineGrid);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, double, AffineGrid);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, float, IsNaN);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, double, IsNaN);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, MLFloat16, IsNaN);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, BFloat16, IsNaN);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, Gelu);\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, Float8E4M3FN, IsNaN);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, Float8E4M3FNUZ, IsNaN);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, Float8E5M2, IsNaN);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, Float8E5M2FNUZ, IsNaN);\r\n#endif\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, IsInf);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, StringConcat);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, RegexFullMatch);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, StringSplit);\r\n\r\n// Opset 21\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Cast);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, ConstantOfShape);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Identity);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Reshape);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Scan);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Shape);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Size);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Squeeze);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Transpose);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Unsqueeze);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, If);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Loop);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Flatten);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Pad);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, int32_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, uint8_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, int8_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, uint16_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, int16_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Int4x2, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, UInt4x2, DequantizeLinear);\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E4M3FN, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E4M3FNUZ, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E5M2, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E5M2FNUZ, DequantizeLinear);\r\n#endif\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, uint8_t, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, int8_t, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, uint16_t, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, int16_t, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Int4x2, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, UInt4x2, QuantizeLinear);\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E4M3FN, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E4M3FNUZ, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E5M2, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E5M2FNUZ, QuantizeLinear);\r\n#endif\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, uint8_t, QLinearMatMul);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, int8_t, QLinearMatMul);\r\n\r\n// Opset 22\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Acos);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Cos);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Tan);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Asin);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Atan);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Sinh);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Cosh);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Asinh);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Acosh);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Atanh);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Conv);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, ConvTranspose);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Det);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, float_float, Dropout);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, float_double, Dropout);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, double_float, Dropout);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, double_double, Dropout);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, float, GridSample);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, double, GridSample);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Elu);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, EyeLike);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, GlobalAveragePool);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, GlobalMaxPool);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, GRU);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, LSTM);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, RNN);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, HardSigmoid);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, InstanceNormalization);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, LpPool);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, MaxPool);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, MaxUnpool);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Softplus);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, float, Round);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, double, Round);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, MLFloat16, Round);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Selu);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, float, Sin);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, double, Sin);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Softsign);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, ThresholdedRelu);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, AveragePool);\r\n\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, MLFloat16, Conv);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, MLFloat16, GlobalAveragePool);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, MLFloat16, MaxPool);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, MLFloat16, AveragePool);\r\n#endif\r\n\r\n// Opset 23\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, float, Attention);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, MLFloat16, Attention);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Cast);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, ConstantOfShape);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, int32_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, uint8_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, int8_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, uint16_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, int16_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Int4x2, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, UInt4x2, DequantizeLinear);\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E4M3FN, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E4M3FNUZ, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E5M2, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E5M2FNUZ, DequantizeLinear);\r\n#endif\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, uint8_t, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, int8_t, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, uint16_t, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, int16_t, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Int4x2, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, UInt4x2, QuantizeLinear);\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E4M3FN, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E4M3FNUZ, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E5M2, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E5M2FNUZ, QuantizeLinear);\r\n#endif\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Flatten);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Identity);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Reshape);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Scan);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Shape);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Size);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Squeeze);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Transpose);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Unsqueeze);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, If);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Loop);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Pad);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, float, RMSNormalization);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, double, RMSNormalization);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, MLFloat16, RMSNormalization);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, float, RotaryEmbedding);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, MLFloat16, RotaryEmbedding);\r\n\r\n// Opset 24\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, float, Attention);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, MLFloat16, Attention);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Cast);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, ConstantOfShape);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, int32_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, uint8_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, int8_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, uint16_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, int16_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Int4x2, DequantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, UInt4x2, DequantizeLinear);\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E4M3FN, DequantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E4M3FNUZ, DequantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E5M2, DequantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E5M2FNUZ, DequantizeLinear);\r\n#endif\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, uint8_t, QuantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, int8_t, QuantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, uint16_t, QuantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, int16_t, QuantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Int4x2, QuantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, UInt4x2, QuantizeLinear);\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E4M3FN, QuantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E4M3FNUZ, QuantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E5M2, QuantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E5M2FNUZ, QuantizeLinear);\r\n#endif\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Flatten);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Identity);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Pad);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, If);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Loop);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Reshape);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Shape);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Squeeze);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Transpose);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Unsqueeze);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Scan);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Size);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, SplitToSequence);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, float, TopK);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, double, TopK);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, int64_t, TopK);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, int32_t, TopK);\r\n\r\n// !!PLEASE READ BELOW!! Following that, add new entries above this comment\r\n\r\n/*  *** IMPORTANT! ***\r\n NEVER update a versioned entry to change the start or end version. These MUST be treated as immutable.\r\n   i.e. if the macro has 'VERSIONED' in it, do not modify that entry\r\n\r\n When updating a declaration to add a new version of an operator there are 2 simple steps:\r\n\r\n   1. There should be a non-versioned entry for that latest version. Update this to be versioned.\r\n      Note that the end version is inclusive, so the end value will be one less than the operator's new opset version.\r\n   2. Add a new non-versioned entry for the new opset.\r\n\r\n e.g. Say opset 13 is being added, and we need to update Add. The most recent change to Add was in opset 7 so it\r\n      should have an un-versioned registration in the opset 7 section like this:\r\n\r\n     class ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, float, Add);\r\n\r\n   Step 1 is to change that to add 'VERSIONED_' to the macro and add an end version of 12 as the new opset is 13:\r\n     class ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, float, Add);\r\n\r\n   Step 2 is to create a new un-versioned entry in the opset 13 sections:\r\n     class ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Add);\r\n\r\n The process is the same for TYPED and untyped kernels - just repeat for each type when updating the typed entries.\r\n\r\n The changes below in the registrations using BuildKernelCreateInfo are essentially the same. Update existing\r\n registration to use the VERSIONED_ macro, add end version, add new un-versioned entry in the section for the new\r\n opset.\r\n\r\n To double-check what versions an operator should have registrations for see\r\n https://github.com/onnx/onnx/blob/main/docs/Operators.md\r\n*****/\r\n\r\ntemplate <>\r\nKernelCreateInfo BuildKernelCreateInfo<void>() {\r\n  KernelCreateInfo info;\r\n  return info;\r\n}\r\n\r\nStatus RegisterOnnxOperatorKernels(KernelRegistry& kernel_registry) {\r\n  static const BuildKernelCreateInfoFn function_table[] = {\r\n      BuildKernelCreateInfo<void>,  // default entry to avoid the list become empty after ops-reducing\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, MemcpyFromHost)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, MemcpyToHost)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 10, Clip)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 21, Elu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 21, HardSigmoid)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 15,\r\n                                                                      LeakyRelu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            float, Relu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            double, Relu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 21, Selu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            float, Sigmoid)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            double, Sigmoid)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 21, Softplus)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 21, Softsign)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            float, Tanh)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            double, Tanh)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8, PRelu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, RandomNormal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, RandomUniform)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, RandomNormalLike)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, RandomUniformLike)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, Multinomial)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                            float, Add)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                            double, Add)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                            int32_t, Add)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                            int64_t, Add)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                            uint32_t, Add)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                            uint64_t, Add)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                            float, Sub)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                            double, Sub)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                            int32_t, Sub)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                            int64_t, Sub)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                            uint32_t, Sub)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                            uint64_t, Sub)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                            float, Mul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                            double, Mul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                            int32_t, Mul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                            int64_t, Mul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                            uint32_t, Mul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                            uint64_t, Mul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                            float, Div)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                            double, Div)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                            int32_t, Div)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                            int64_t, Div)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                            uint32_t, Div)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                            uint64_t, Div)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            float, Abs)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            double, Abs)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            int8_t, Abs)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            int16_t, Abs)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            int32_t, Abs)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            int64_t, Abs)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            uint8_t, Abs)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            uint16_t, Abs)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            uint32_t, Abs)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            uint64_t, Abs)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            float, Floor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            double, Floor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            float, Ceil)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            double, Ceil)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            float, Reciprocal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            double, Reciprocal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            float, Sqrt)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            double, Sqrt)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            float, Neg)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            double, Neg)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            int8_t, Neg)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            int16_t, Neg)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            int32_t, Neg)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            int64_t, Neg)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 11, Pow)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            float, Exp)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            double, Exp)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            float, Log)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            double, Log)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 7,\r\n                                                                            float, Sum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 7,\r\n                                                                            double, Sum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                            float, Sum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                            double, Sum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 7,\r\n                                                                            float, Min)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 11, Min)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 7,\r\n                                                                            float, Max)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 11, Max)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, Not)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, And)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, Or)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, Xor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8,\r\n                                                                            float, Less)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8,\r\n                                                                            double, Less)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8,\r\n                                                                            float, Greater)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8,\r\n                                                                            double, Greater)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 10,\r\n                                                                            bool, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 10,\r\n                                                                            int32_t, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 10,\r\n                                                                            int64_t, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 10,\r\n                                                                            float, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 10,\r\n                                                                            double, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 7,\r\n                                                                            float, Mean)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                            float, Mean)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 21, float, Sin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 21, double, Sin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 21, Cos)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 21, Tan)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 21, Asin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 21, Acos)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 21, Atan)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8,\r\n                                                                            float, Gemm)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8,\r\n                                                                            double, Gemm)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                      Hardmax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            float, LogSoftmax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            double, LogSoftmax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 8,\r\n                                                                            float, MatMul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 8,\r\n                                                                            double, MatMul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            float, Softmax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            double, Softmax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 9,\r\n                                                                            float, TopK)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 9,\r\n                                                                            double, TopK)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8,\r\n                                                                            float, BatchNormalization)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8,\r\n                                                                            double, BatchNormalization)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, Conv)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                      ConvTranspose)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 8, Flatten)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 21,\r\n                                                                      InstanceNormalization)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, float,\r\n                                                                  LpNormalization)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, double,\r\n                                                                  LpNormalization)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 12, LRN)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 9,\r\n                                                                      AveragePool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 7, MaxPool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 11,\r\n                                                                      MaxPool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 2, 10, LpPool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 2, GlobalLpPool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 21, GlobalAveragePool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 21, GlobalMaxPool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, MaxRoiPool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            float, ReduceL1)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            double, ReduceL1)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            int32_t, ReduceL1)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            int64_t, ReduceL1)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            float, ReduceL2)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            double, ReduceL2)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            int32_t, ReduceL2)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            int64_t, ReduceL2)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            float, ReduceLogSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            double, ReduceLogSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            int32_t, ReduceLogSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            int64_t, ReduceLogSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            float, ReduceLogSumExp)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            double, ReduceLogSumExp)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            int32_t, ReduceLogSumExp)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            int64_t, ReduceLogSumExp)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            float, ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            double, ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            int32_t, ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            int64_t, ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            float, ReduceMean)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            double, ReduceMean)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            int32_t, ReduceMean)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            int64_t, ReduceMean)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            float, ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            double, ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            int32_t, ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            int64_t, ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            float, ReduceProd)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            double, ReduceProd)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            int32_t, ReduceProd)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            int64_t, ReduceProd)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            float, ReduceSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            int32_t, ReduceSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            double, ReduceSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            int64_t, ReduceSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            float, ReduceSumSquare)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            int32_t, ReduceSumSquare)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            double, ReduceSumSquare)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            int64_t, ReduceSumSquare)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            float, ArgMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            double, ArgMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            int8_t, ArgMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            uint8_t, ArgMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            int32_t, ArgMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            int64_t, ArgMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            float, ArgMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            double, ArgMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            int8_t, ArgMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            uint8_t, ArgMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            int32_t, ArgMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                            int64_t, ArgMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 13, GRU)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 13, LSTM)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 13, RNN)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, Cast)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 4, 10, Concat)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, Gather)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 9, Dropout)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 12,\r\n                                                                      Identity)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 2, 10, Pad)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 4, Reshape)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 5, 12,\r\n                                                                      Reshape)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 12, Shape)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 12, Size)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 9, Slice)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 12,\r\n                                                                      SpaceToDepth)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                      DepthToSpace)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 2, 10, Split)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                      Squeeze)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, Tile)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 12,\r\n                                                                      Transpose)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                      Unsqueeze)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8,\r\n                                                                            float, Upsample)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8,\r\n                                                                            int32_t, Upsample)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8,\r\n                                                                            int8_t, Upsample)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8,\r\n                                                                            uint8_t, Upsample)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                            float, Expand)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                            double, Expand)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                            int8_t, Expand)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                            int16_t, Expand)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                            int32_t, Expand)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                            int64_t, Expand)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                            uint8_t, Expand)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                            uint16_t, Expand)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                            uint32_t, Expand)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                            uint64_t, Expand)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                            bool, Expand)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                            MLFloat16, Expand)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                            string, Expand)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 8, Scan)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, If)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, Loop)>,\r\n\r\n      // Opset 9\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                      Compress)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 19,\r\n                                                                      ConstantOfShape)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                      MeanVarianceNormalization)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            float, Greater)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            double, Greater)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            int8_t, Greater)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            int16_t, Greater)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            int32_t, Greater)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            int64_t, Greater)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            uint8_t, Greater)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            uint16_t, Greater)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            uint32_t, Greater)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            uint64_t, Greater)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            float, Less)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            double, Less)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            int8_t, Less)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            int16_t, Less)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            int32_t, Less)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            int64_t, Less)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            uint8_t, Less)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            uint16_t, Less)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            uint32_t, Less)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            uint64_t, Less)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 21, EyeLike)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            float, IsNaN)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            double, IsNaN)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            MLFloat16, IsNaN)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, Sign)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, Shrink)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            float, Erf)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                            int64_t_int64_t_int64_t, OneHot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                            float_int64_t_int64_t, OneHot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                            int64_t_string_int64_t, OneHot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                            float_string_int64_t, OneHot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                            float_float_float, OneHot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                            int64_t_int32_t_float, OneHot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                            int64_t_float_int64_t, OneHot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                            int32_t_float_int32_t, OneHot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                            int32_t_float_float, OneHot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                            int64_t_float_float, OneHot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                            int64_t_float_int32_t, OneHot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                      MaxUnpool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 21, Sinh)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 21, Cosh)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 21, Asinh)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 21, Acosh)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 21, Atanh)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, Scan)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                      Scatter)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, TfIdfVectorizer)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            bool, NonZero)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            float, NonZero)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            int32_t, NonZero)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            int64_t, NonZero)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            uint8_t, NonZero)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 15,\r\n                                                                            string, Where)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 15,\r\n                                                                            float, Where)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 15,\r\n                                                                            double, Where)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 15,\r\n                                                                            int32_t, Where)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 15,\r\n                                                                            int64_t, Where)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 15,\r\n                                                                            uint8_t, Where)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                      Flatten)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                            float, Gemm)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                            double, Gemm)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            float, MatMul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            double, MatMul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            int32_t, MatMul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                            int64_t, MatMul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 13,\r\n                                                                            float, BatchNormalization)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 13,\r\n                                                                            double, BatchNormalization)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 15, PRelu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 9,\r\n                                                                            float, Upsample)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 9,\r\n                                                                            int32_t, Upsample)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 9,\r\n                                                                            int8_t, Upsample)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 9,\r\n                                                                            uint8_t, Upsample)>,\r\n\r\n      // Opset 10\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, StringNormalizer)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10,\r\n                                                                            float, TopK)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10,\r\n                                                                            double, TopK)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10,\r\n                                                                      AveragePool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 12, Mod)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10,\r\n                                                                            float, Resize)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10,\r\n                                                                            int32_t, Resize)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10,\r\n                                                                            int8_t, Resize)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10,\r\n                                                                            uint8_t, Resize)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 21, ThresholdedRelu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 12,\r\n                                                                            uint8_t, DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 12,\r\n                                                                            int8_t, DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 12,\r\n                                                                            int32_t, DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 12,\r\n                                                                            uint8_t, QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 12,\r\n                                                                            int8_t, QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 20,\r\n                                                                            uint8_t, QLinearMatMul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 20,\r\n                                                                            int8_t, QLinearMatMul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, uint8_t,\r\n                                                                  MatMulInteger)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, int8_t,\r\n                                                                  MatMulInteger)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, ConvInteger)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, uint8_t,\r\n                                                                  QLinearConv)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, int8_t,\r\n                                                                  QLinearConv)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10, Slice)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 11,\r\n                                                                      Dropout)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10,\r\n                                                                      NonMaxSuppression)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 19, IsInf)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 15,\r\n                                                                            float, RoiAlign)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 15,\r\n                                                                            double, RoiAlign)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, ReverseSequence)>,\r\n      // opset 11\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11, Clip)>,\r\n\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 13,\r\n                                                                            float, CumSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 13,\r\n                                                                            double, CumSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 13,\r\n                                                                            int32_t, CumSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 13,\r\n                                                                            int64_t, CumSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            bool, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            int8_t, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            int16_t, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            int32_t, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            int64_t, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            uint8_t, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            uint16_t, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            uint32_t, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            uint64_t, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            float, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            double, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, float, Round)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, double, Round)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, MLFloat16,\r\n                                                                            Round)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, uint8_t,\r\n                                                                  DynamicQuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            float, ArgMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            double, ArgMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            int8_t, ArgMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            uint8_t, ArgMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            int32_t, ArgMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            int64_t, ArgMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            float, ArgMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            double, ArgMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            int8_t, ArgMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            uint8_t, ArgMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            int32_t, ArgMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            int64_t, ArgMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, Loop)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                      Hardmax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            float, LogSoftmax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            double, LogSoftmax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            double, Softmax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            float, Softmax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                      DepthToSpace)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 15, Scan)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                      Flatten)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, Compress)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                      Concat)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                      Gather)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, Slice)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, Split)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                      Squeeze)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                      Unsqueeze)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, Det)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                      ScatterElements)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, NonMaxSuppression)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 18,\r\n                                                                      AveragePool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, MaxUnpool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 17,\r\n                                                                      LpPool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, Conv)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, ConvTranspose)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, If)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, SequenceLength)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, SequenceAt)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, SequenceEmpty)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, SequenceInsert)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, SequenceErase)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, SequenceConstruct)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, ConcatFromSequence)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 23,\r\n                                                                      SplitToSequence)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                      ScatterND)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            float, Gemm)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            double, Gemm)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                      GatherElements)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, uint8_t,\r\n                                                                  BitShift)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, uint32_t,\r\n                                                                  BitShift)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, uint64_t,\r\n                                                                  BitShift)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, Pad)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11,\r\n                                                                      GatherND)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, Range)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, Unique)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 23,\r\n                                                                            float, TopK)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 23,\r\n                                                                            double, TopK)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 23,\r\n                                                                            int64_t, TopK)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 23,\r\n                                                                            int32_t, TopK)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11,\r\n                                                                  int64_t_int64_t_int64_t, OneHot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11,\r\n                                                                  float_int64_t_int64_t, OneHot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11,\r\n                                                                  int64_t_string_int64_t, OneHot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11,\r\n                                                                  float_string_int64_t, OneHot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11,\r\n                                                                  float_float_float, OneHot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11,\r\n                                                                  int64_t_int32_t_float, OneHot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11,\r\n                                                                  int64_t_float_int64_t, OneHot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11,\r\n                                                                  int32_t_float_int32_t, OneHot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11,\r\n                                                                  int32_t_float_float, OneHot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11,\r\n                                                                  int64_t_float_float, OneHot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11,\r\n                                                                  int64_t_float_int32_t, OneHot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            float, Resize)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            int32_t, Resize)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            int8_t, Resize)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            uint8_t, Resize)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11,\r\n                                                                            float, ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11,\r\n                                                                            double, ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11,\r\n                                                                            int32_t, ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11,\r\n                                                                            int64_t, ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11,\r\n                                                                            float, ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11,\r\n                                                                            double, ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11,\r\n                                                                            int32_t, ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11,\r\n                                                                            int64_t, ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            float, ReduceL1)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            double, ReduceL1)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            int32_t, ReduceL1)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            int64_t, ReduceL1)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            float, ReduceL2)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            double, ReduceL2)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            int32_t, ReduceL2)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            int64_t, ReduceL2)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            float, ReduceLogSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            double, ReduceLogSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            int32_t, ReduceLogSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            int64_t, ReduceLogSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            float, ReduceLogSumExp)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            double, ReduceLogSumExp)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            int32_t, ReduceLogSumExp)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            int64_t, ReduceLogSumExp)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            float, ReduceMean)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            double, ReduceMean)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            int32_t, ReduceMean)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            int64_t, ReduceMean)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            float, ReduceProd)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            double, ReduceProd)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            int32_t, ReduceProd)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            int64_t, ReduceProd)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            float, ReduceSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            int32_t, ReduceSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            double, ReduceSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            int64_t, ReduceSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            float, ReduceSumSquare)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            double, ReduceSumSquare)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            int32_t, ReduceSumSquare)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                            int64_t, ReduceSumSquare)>,\r\n\r\n      // OpSet 12\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, Clip)>,\r\n\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, Min)>,\r\n\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, Max)>,\r\n\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, Pow)>,\r\n\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 21, MaxPool)>,\r\n\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                            float, ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                            double, ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                            int32_t, ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                            int64_t, ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                            int8_t, ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                            uint8_t, ReduceMax)>,\r\n\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                            float, ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                            double, ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                            int32_t, ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                            int64_t, ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                            int8_t, ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                            uint8_t, ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                      GatherND)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, Einsum)>,\r\n\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Expand)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double,\r\n                                                                  Expand)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int8_t,\r\n                                                                  Expand)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int16_t,\r\n                                                                  Expand)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t,\r\n                                                                  Expand)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t,\r\n                                                                  Expand)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint8_t,\r\n                                                                  Expand)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint16_t,\r\n                                                                  Expand)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint32_t,\r\n                                                                  Expand)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint64_t,\r\n                                                                  Expand)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, bool, Expand)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, MLFloat16,\r\n                                                                  Expand)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, string,\r\n                                                                  Expand)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Erf)>,\r\n      // REVIEW(codemzs): ConstEigenVectorArrayMap.cast<MLFLoat16) does not seem to be supported.\r\n      // However these types work on GPU implementation.\r\n      // BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12,\r\n      // MLFloat16_MLFloat16, Dropout)>,\r\n      // BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12,\r\n      // MLFloat16_float, Dropout)>, BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider,\r\n      // kOnnxDomain, 12, MLFloat16_double, Dropout)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                            float_float, Dropout)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                            float_double, Dropout)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                            double_float, Dropout)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                            double_double, Dropout)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, Celu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                            float, GreaterOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                            double, GreaterOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                            int8_t, GreaterOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                            int16_t, GreaterOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                            int32_t, GreaterOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                            int64_t, GreaterOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                            uint8_t, GreaterOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                            uint16_t, GreaterOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                            uint32_t, GreaterOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                            uint64_t, GreaterOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                            float, LessOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                            double, LessOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                            int8_t, LessOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                            int16_t, LessOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                            int32_t, LessOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                            int64_t, LessOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                            uint8_t, LessOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                            uint16_t, LessOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                            uint32_t, LessOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                            uint64_t, LessOrEqual)>,\r\n\r\n      // opset 13\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, Cast)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Clip)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, MatMul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double,\r\n                                                                  MatMul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t,\r\n                                                                  MatMul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t,\r\n                                                                  MatMul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Min)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Max)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Mean)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Gemm)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Gemm)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Sign)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, Size)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Sum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Sum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float,\r\n                                                                  Sigmoid)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double,\r\n                                                                  Sigmoid)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                            uint8_t, DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                            int8_t, DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                            int32_t, DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                            uint8_t, QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                            int8_t, QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 20,\r\n                                                                      Flatten)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, LRN)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13,\r\n                                                            MeanVarianceNormalization)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 21, float_float,\r\n                                                                            Dropout)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 21, float_double,\r\n                                                                            Dropout)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 21, double_float,\r\n                                                                            Dropout)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 21, double_double,\r\n                                                                            Dropout)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, ArgMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double,\r\n                                                                  ArgMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int8_t,\r\n                                                                  ArgMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint8_t,\r\n                                                                  ArgMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t,\r\n                                                                  ArgMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t,\r\n                                                                  ArgMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, ArgMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double,\r\n                                                                  ArgMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t,\r\n                                                                  ArgMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t,\r\n                                                                  ArgMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int8_t,\r\n                                                                  ArgMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint8_t,\r\n                                                                  ArgMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                      Reshape)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 14, Shape)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Concat)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                            bool, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                            int8_t, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                            int16_t, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                            int32_t, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                            int64_t, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                            uint8_t, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                            uint16_t, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                            uint32_t, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                            uint64_t, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                            float, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                            double, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float,\r\n                                                                  Greater)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double,\r\n                                                                  Greater)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int8_t,\r\n                                                                  Greater)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int16_t,\r\n                                                                  Greater)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t,\r\n                                                                  Greater)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t,\r\n                                                                  Greater)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint8_t,\r\n                                                                  Greater)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint16_t,\r\n                                                                  Greater)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint32_t,\r\n                                                                  Greater)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint64_t,\r\n                                                                  Greater)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Less)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Less)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13,\r\n                                                                  int8_t, Less)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13,\r\n                                                                  int16_t, Less)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13,\r\n                                                                  int32_t, Less)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13,\r\n                                                                  int64_t, Less)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13,\r\n                                                                  uint8_t, Less)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13,\r\n                                                                  uint16_t, Less)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13,\r\n                                                                  uint32_t, Less)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13,\r\n                                                                  uint64_t, Less)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                            float, Add)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                            double, Add)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                            int32_t, Add)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                            int64_t, Add)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                            uint32_t, Add)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                            uint64_t, Add)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                            float, Sub)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                            double, Sub)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                            int32_t, Sub)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                            int64_t, Sub)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                            uint32_t, Sub)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                            uint64_t, Sub)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                            float, Mul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                            double, Mul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                            int32_t, Mul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                            int64_t, Mul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                            uint32_t, Mul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                            uint64_t, Mul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                            float, Div)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                            double, Div)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                            int32_t, Div)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                            int64_t, Div)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                            uint32_t, Div)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                            uint64_t, Div)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Neg)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Neg)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int8_t, Neg)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int16_t, Neg)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t, Neg)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t, Neg)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Mod)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Abs)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Abs)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int8_t, Abs)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int16_t, Abs)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t, Abs)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t, Abs)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint8_t, Abs)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint16_t, Abs)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint32_t, Abs)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint64_t, Abs)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float,\r\n                                                                  Reciprocal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double,\r\n                                                                  Reciprocal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Floor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Floor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Ceil)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Ceil)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Sqrt)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Sqrt)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                            float, Relu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                            double, Relu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Tanh)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Tanh)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Exp)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Exp)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Log)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Log)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 14, Pow)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Slice)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, Split)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 20,\r\n                                                                      Unsqueeze)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 20,\r\n                                                                      Squeeze)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 20,\r\n                                                                      Transpose)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Tile)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Gather)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, GatherElements)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, DepthToSpace)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, SpaceToDepth)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 15,\r\n                                                                      ScatterElements)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 15,\r\n                                                                      ScatterND)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                      Identity)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 19,\r\n                                                                            float, IsNaN)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 19,\r\n                                                                            double, IsNaN)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 19,\r\n                                                                            MLFloat16, IsNaN)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, bool, NonZero)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float,\r\n                                                                  NonZero)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t,\r\n                                                                  NonZero)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t,\r\n                                                                  NonZero)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint8_t,\r\n                                                                  NonZero)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, GatherND)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, Pad)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            float, ReduceL1)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            double, ReduceL1)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            int32_t, ReduceL1)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            int64_t, ReduceL1)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            float, ReduceL2)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            double, ReduceL2)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            int32_t, ReduceL2)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            int64_t, ReduceL2)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            float, ReduceLogSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            double, ReduceLogSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            int32_t, ReduceLogSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            int64_t, ReduceLogSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            float, ReduceLogSumExp)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            double, ReduceLogSumExp)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            int32_t, ReduceLogSumExp)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            int64_t, ReduceLogSumExp)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            float, ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            double, ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            int32_t, ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            int64_t, ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            int8_t, ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            uint8_t, ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            float, ReduceMean)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            double, ReduceMean)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            int32_t, ReduceMean)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            int64_t, ReduceMean)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            float, ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            double, ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            int32_t, ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            int64_t, ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            int8_t, ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            uint8_t, ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            float, ReduceProd)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            double, ReduceProd)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            int32_t, ReduceProd)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            int64_t, ReduceProd)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            float, ReduceSumSquare)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            int32_t, ReduceSumSquare)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            double, ReduceSumSquare)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            int64_t, ReduceSumSquare)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float,\r\n                                                                  ReduceSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t,\r\n                                                                  ReduceSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double,\r\n                                                                  ReduceSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t,\r\n                                                                  ReduceSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            float, Resize)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            int32_t, Resize)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            int8_t, Resize)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                            uint8_t, Resize)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 15, Loop)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 15, If)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Hardmax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float,\r\n                                                                  LogSoftmax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double,\r\n                                                                  LogSoftmax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double,\r\n                                                                  Softmax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float,\r\n                                                                  Softmax)>,\r\n\r\n      // OpSet 14\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, float, CumSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, double,\r\n                                                                  CumSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int32_t,\r\n                                                                  CumSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int64_t,\r\n                                                                  CumSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, float, Relu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, double, Relu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int8_t, Relu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int32_t, Relu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, Trilu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, float, Add)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, double, Add)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int8_t, Add)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int16_t, Add)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int32_t, Add)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int64_t, Add)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint8_t, Add)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint16_t, Add)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint32_t, Add)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint64_t, Add)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, float, Sub)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, double, Sub)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int8_t, Sub)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int16_t, Sub)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int32_t, Sub)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int64_t, Sub)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint8_t, Sub)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint16_t, Sub)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint32_t, Sub)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint64_t, Sub)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, float, Mul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, double, Mul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int8_t, Mul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int16_t, Mul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int32_t, Mul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int64_t, Mul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint8_t, Mul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint16_t, Mul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint32_t, Mul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint64_t, Mul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, float, Div)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, double, Div)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int8_t, Div)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int16_t, Div)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int32_t, Div)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int64_t, Div)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint8_t, Div)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint16_t, Div)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint32_t, Div)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint64_t, Div)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, 18,\r\n                                                                      Reshape)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, 15,\r\n                                                                      Identity)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, 14,\r\n                                                                            float, BatchNormalization)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, 14,\r\n                                                                            double, BatchNormalization)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, 21, GRU)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, 21, LSTM)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, 21, RNN)>,\r\n\r\n      // Opset 15\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 15, Pow)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 15, float,\r\n                                                                  BatchNormalization)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 15, double,\r\n                                                                  BatchNormalization)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 15, 18, Shape)>,\r\n\r\n#if !defined(DISABLE_OPTIONAL_TYPE)\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 15, 17,\r\n                                                                      OptionalHasElement)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 15, 17,\r\n                                                                      OptionalGetElement)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 15, Optional)>,\r\n#endif\r\n\r\n      // Opset 16\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, 18,\r\n                                                                      Identity)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, 18, If)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, 18, Loop)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, float,\r\n                                                                  RoiAlign)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, double,\r\n                                                                  RoiAlign)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, 19,\r\n                                                                            float, GridSample)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, 17,\r\n                                                                      ScatterElements)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, 17,\r\n                                                                      ScatterND)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, string, Where)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, float, Where)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, double, Where)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int32_t,\r\n                                                                  Where)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int64_t,\r\n                                                                  Where)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint8_t,\r\n                                                                  Where)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, LeakyRelu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, PRelu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, 18, Scan)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, float,\r\n                                                                  GreaterOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, double,\r\n                                                                  GreaterOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int8_t,\r\n                                                                  GreaterOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int16_t,\r\n                                                                  GreaterOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int32_t,\r\n                                                                  GreaterOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int64_t,\r\n                                                                  GreaterOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint8_t,\r\n                                                                  GreaterOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint16_t,\r\n                                                                  GreaterOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint32_t,\r\n                                                                  GreaterOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint64_t,\r\n                                                                  GreaterOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, float,\r\n                                                                  LessOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, double,\r\n                                                                  LessOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int8_t,\r\n                                                                  LessOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int16_t,\r\n                                                                  LessOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int32_t,\r\n                                                                  LessOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int64_t,\r\n                                                                  LessOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint8_t,\r\n                                                                  LessOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint16_t,\r\n                                                                  LessOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint32_t,\r\n                                                                  LessOrEqual)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint64_t,\r\n                                                                  LessOrEqual)>,\r\n\r\n      // Opset 17\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, BlackmanWindow)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, 19, DFT)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, HammingWindow)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, HannWindow)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, MelWeightMatrix)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, STFT)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, float,\r\n                                                                  LayerNormalization)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, double,\r\n                                                                  LayerNormalization)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, MLFloat16,\r\n                                                                  LayerNormalization)>,\r\n\r\n      // Opset 18\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 18,\r\n                                                                            float, Resize)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 18,\r\n                                                                            int32_t, Resize)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 18,\r\n                                                                            int8_t, Resize)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 18,\r\n                                                                            uint8_t, Resize)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, float,\r\n                                                                  ReduceL1)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, double,\r\n                                                                  ReduceL1)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t,\r\n                                                                  ReduceL1)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t,\r\n                                                                  ReduceL1)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, float,\r\n                                                                  ReduceL2)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, double,\r\n                                                                  ReduceL2)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t,\r\n                                                                  ReduceL2)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t,\r\n                                                                  ReduceL2)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, float,\r\n                                                                  ReduceLogSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, double,\r\n                                                                  ReduceLogSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t,\r\n                                                                  ReduceLogSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t,\r\n                                                                  ReduceLogSum)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, float,\r\n                                                                  ReduceLogSumExp)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, double,\r\n                                                                  ReduceLogSumExp)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t,\r\n                                                                  ReduceLogSumExp)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t,\r\n                                                                  ReduceLogSumExp)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, float,\r\n                                                                            ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, double,\r\n                                                                            ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, int32_t,\r\n                                                                            ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, int64_t,\r\n                                                                            ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, int8_t,\r\n                                                                            ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, uint8_t,\r\n                                                                            ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, float,\r\n                                                                  ReduceMean)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, double,\r\n                                                                  ReduceMean)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t,\r\n                                                                  ReduceMean)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t,\r\n                                                                  ReduceMean)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, float,\r\n                                                                            ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, double,\r\n                                                                            ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, int32_t,\r\n                                                                            ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, int64_t,\r\n                                                                            ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, int8_t,\r\n                                                                            ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, uint8_t,\r\n                                                                            ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, float,\r\n                                                                  ReduceProd)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, double,\r\n                                                                  ReduceProd)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t,\r\n                                                                  ReduceProd)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t,\r\n                                                                  ReduceProd)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, float,\r\n                                                                  ReduceSumSquare)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t,\r\n                                                                  ReduceSumSquare)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, double,\r\n                                                                  ReduceSumSquare)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t,\r\n                                                                  ReduceSumSquare)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 21, LpPool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, Col2Im)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int8_t,\r\n                                                                  BitwiseAnd)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int16_t,\r\n                                                                  BitwiseAnd)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t,\r\n                                                                  BitwiseAnd)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t,\r\n                                                                  BitwiseAnd)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint8_t,\r\n                                                                  BitwiseAnd)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint16_t,\r\n                                                                  BitwiseAnd)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint32_t,\r\n                                                                  BitwiseAnd)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint64_t,\r\n                                                                  BitwiseAnd)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int8_t,\r\n                                                                  BitwiseNot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int16_t,\r\n                                                                  BitwiseNot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t,\r\n                                                                  BitwiseNot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t,\r\n                                                                  BitwiseNot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint8_t,\r\n                                                                  BitwiseNot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint16_t,\r\n                                                                  BitwiseNot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint32_t,\r\n                                                                  BitwiseNot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint64_t,\r\n                                                                  BitwiseNot)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int8_t,\r\n                                                                  BitwiseOr)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int16_t,\r\n                                                                  BitwiseOr)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t,\r\n                                                                  BitwiseOr)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t,\r\n                                                                  BitwiseOr)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint8_t,\r\n                                                                  BitwiseOr)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint16_t,\r\n                                                                  BitwiseOr)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint32_t,\r\n                                                                  BitwiseOr)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint64_t,\r\n                                                                  BitwiseOr)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int8_t,\r\n                                                                  BitwiseXor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int16_t,\r\n                                                                  BitwiseXor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t,\r\n                                                                  BitwiseXor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t,\r\n                                                                  BitwiseXor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint8_t,\r\n                                                                  BitwiseXor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint16_t,\r\n                                                                  BitwiseXor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint32_t,\r\n                                                                  BitwiseXor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint64_t,\r\n                                                                  BitwiseXor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 18, Pad)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, ScatterND)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, ScatterElements)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, Split)>,\r\n#if !defined(DISABLE_OPTIONAL_TYPE)\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, OptionalHasElement)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, OptionalGetElement)>,\r\n#endif\r\n\r\n      // Opset 19\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Size)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 21, AveragePool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Cast)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                            uint8_t, DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                            int8_t, DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                            int32_t, DequantizeLinear)>,\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                            Float8E4M3FN, DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                            Float8E4M3FNUZ, DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                            Float8E5M2, DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                            Float8E5M2FNUZ, DequantizeLinear)>,\r\n#endif\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, bool, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, int8_t,\r\n                                                                  Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, int16_t,\r\n                                                                  Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, int32_t,\r\n                                                                  Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, int64_t,\r\n                                                                  Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, uint8_t,\r\n                                                                  Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, uint16_t,\r\n                                                                  Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, uint32_t,\r\n                                                                  Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, uint64_t,\r\n                                                                  Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, float, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, double, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, string, Equal)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                      Identity)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, If)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Loop)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Pad)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                            uint8_t, QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                            int8_t, QuantizeLinear)>,\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                            Float8E4M3FN, QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                            Float8E4M3FNUZ, QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                            Float8E5M2, QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                            Float8E5M2FNUZ, QuantizeLinear)>,\r\n#endif\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                      Reshape)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, float, Resize)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, int32_t,\r\n                                                                  Resize)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, int8_t,\r\n                                                                  Resize)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, uint8_t,\r\n                                                                  Resize)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Scan)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Shape)>,\r\n\r\n      // Opset 20\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, 20,\r\n                                                                      ConstantOfShape)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, bool, ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, float, ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, double, ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, int32_t, ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, int64_t, ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, int8_t, ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, uint8_t, ReduceMax)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, bool, ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, float, ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, double, ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, int32_t, ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, int64_t, ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, int8_t, ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, uint8_t, ReduceMin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, DFT)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, 21, float,\r\n                                                                            GridSample)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, 21, double,\r\n                                                                            GridSample)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, float,\r\n                                                                  AffineGrid)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, double,\r\n                                                                  AffineGrid)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, float, IsNaN)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, double, IsNaN)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, MLFloat16,\r\n                                                                  IsNaN)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, BFloat16,\r\n                                                                  IsNaN)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, Gelu)>,\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, Float8E4M3FN,\r\n                                                                  IsNaN)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, Float8E4M3FNUZ,\r\n                                                                  IsNaN)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, Float8E5M2,\r\n                                                                  IsNaN)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, Float8E5M2FNUZ,\r\n                                                                  IsNaN)>,\r\n#endif\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, IsInf)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, StringConcat)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, RegexFullMatch)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, StringSplit)>,\r\n\r\n      // Opset 21\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Cast)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, ConstantOfShape)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Identity)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Reshape)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Scan)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Shape)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Size)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Flatten)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Squeeze)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Transpose)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Unsqueeze)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, If)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Loop)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Pad)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, uint8_t,\r\n                                                                            DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, int8_t,\r\n                                                                            DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, uint16_t,\r\n                                                                            DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, int16_t,\r\n                                                                            DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, int32_t,\r\n                                                                            DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Int4x2,\r\n                                                                            DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, UInt4x2,\r\n                                                                            DequantizeLinear)>,\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E4M3FN,\r\n                                                                            DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E4M3FNUZ,\r\n                                                                            DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E5M2,\r\n                                                                            DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E5M2FNUZ,\r\n                                                                            DequantizeLinear)>,\r\n#endif\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, uint8_t,\r\n                                                                            QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, int8_t,\r\n                                                                            QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, uint16_t,\r\n                                                                            QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, int16_t,\r\n                                                                            QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Int4x2,\r\n                                                                            QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, UInt4x2,\r\n                                                                            QuantizeLinear)>,\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E4M3FN,\r\n                                                                            QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E4M3FNUZ,\r\n                                                                            QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E5M2,\r\n                                                                            QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E5M2FNUZ,\r\n                                                                            QuantizeLinear)>,\r\n#endif\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, uint8_t,\r\n                                                                  QLinearMatMul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, int8_t,\r\n                                                                  QLinearMatMul)>,\r\n\r\n      // Opset 22\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Cos)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Tan)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Asin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Acos)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Atan)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Sinh)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Cosh)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Asinh)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Acosh)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Atanh)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Conv)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, ConvTranspose)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Det)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, EyeLike)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, GlobalAveragePool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, GlobalMaxPool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, float_float,\r\n                                                                  Dropout)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, float_double,\r\n                                                                  Dropout)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, double_float,\r\n                                                                  Dropout)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, double_double,\r\n                                                                  Dropout)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, float, GridSample)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, double, GridSample)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, GRU)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, LSTM)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, RNN)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Elu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, HardSigmoid)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, InstanceNormalization)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, LpPool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, MaxPool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, MaxUnpool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Softplus)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, float, Round)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, double, Round)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, MLFloat16,\r\n                                                                  Round)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Selu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, float, Sin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, double, Sin)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Softsign)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, ThresholdedRelu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, AveragePool)>,\r\n\r\n      // Opset 23\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23,\r\n                                                                            float, Attention)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23,\r\n                                                                            MLFloat16, Attention)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Cast)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, ConstantOfShape)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, uint8_t,\r\n                                                                            DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, int8_t,\r\n                                                                            DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, uint16_t,\r\n                                                                            DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, int16_t,\r\n                                                                            DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, int32_t,\r\n                                                                            DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Int4x2,\r\n                                                                            DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, UInt4x2,\r\n                                                                            DequantizeLinear)>,\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E4M3FN,\r\n                                                                            DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E4M3FNUZ,\r\n                                                                            DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E5M2,\r\n                                                                            DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E5M2FNUZ,\r\n                                                                            DequantizeLinear)>,\r\n#endif\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, uint8_t,\r\n                                                                            QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, int8_t,\r\n                                                                            QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, uint16_t,\r\n                                                                            QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, int16_t,\r\n                                                                            QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Int4x2,\r\n                                                                            QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, UInt4x2,\r\n                                                                            QuantizeLinear)>,\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E4M3FN,\r\n                                                                            QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E4M3FNUZ,\r\n                                                                            QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E5M2,\r\n                                                                            QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E5M2FNUZ,\r\n                                                                            QuantizeLinear)>,\r\n#endif\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Identity)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Reshape)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Scan)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Shape)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Size)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Flatten)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Squeeze)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Transpose)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Unsqueeze)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, If)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Loop)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Pad)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, float,\r\n                                                                  RMSNormalization)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, double,\r\n                                                                  RMSNormalization)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, MLFloat16,\r\n                                                                  RMSNormalization)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, float,\r\n                                                                  RotaryEmbedding)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, MLFloat16,\r\n                                                                  RotaryEmbedding)>,\r\n\r\n      // opset 24\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, float, Attention)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, MLFloat16, Attention)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Cast)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, ConstantOfShape)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, uint8_t,\r\n                                                                  DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, int8_t,\r\n                                                                  DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, uint16_t,\r\n                                                                  DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, int16_t,\r\n                                                                  DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, int32_t,\r\n                                                                  DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Int4x2,\r\n                                                                  DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, UInt4x2,\r\n                                                                  DequantizeLinear)>,\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E4M3FN,\r\n                                                                  DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E4M3FNUZ,\r\n                                                                  DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E5M2,\r\n                                                                  DequantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E5M2FNUZ,\r\n                                                                  DequantizeLinear)>,\r\n#endif\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, uint8_t,\r\n                                                                  QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, int8_t,\r\n                                                                  QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, uint16_t,\r\n                                                                  QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, int16_t,\r\n                                                                  QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Int4x2,\r\n                                                                  QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, UInt4x2,\r\n                                                                  QuantizeLinear)>,\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E4M3FN,\r\n                                                                  QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E4M3FNUZ,\r\n                                                                  QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E5M2,\r\n                                                                  QuantizeLinear)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E5M2FNUZ,\r\n                                                                  QuantizeLinear)>,\r\n#endif\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Flatten)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Identity)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, If)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Loop)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Pad)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Reshape)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Scan)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Shape)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Size)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Squeeze)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Transpose)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Unsqueeze)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, SplitToSequence)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, float, TopK)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, double, TopK)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, int64_t, TopK)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, int32_t, TopK)>,\r\n  };\r\n  for (auto& function_table_entry : function_table) {\r\n    KernelCreateInfo info = function_table_entry();\r\n    if (info.kernel_def != nullptr) {  // filter disabled entries where type is void\r\n      ORT_RETURN_IF_ERROR(kernel_registry.Register(std::move(info)));\r\n    }\r\n  }\r\n\r\n  return Status::OK();\r\n}\r\n\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nStatus RegisterFp16Kernels(KernelRegistry& kernel_registry) {\r\n  static const BuildKernelCreateInfoFn function_table[] = {\r\n      BuildKernelCreateInfo<void>,  // default entry to avoid the list become empty after ops-reducing\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 21, MLFloat16,\r\n                                                                            GlobalAveragePool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, MLFloat16,\r\n                                                                  GlobalAveragePool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, MLFloat16,\r\n                                                                            Conv)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, MLFloat16,\r\n                                                                  Conv)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 18,\r\n                                                                            MLFloat16, AveragePool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 21, MLFloat16,\r\n                                                                            AveragePool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, MLFloat16,\r\n                                                                  AveragePool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 11,\r\n                                                                            MLFloat16, MaxPool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 21, MLFloat16,\r\n                                                                            MaxPool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22,\r\n                                                                  MLFloat16, MaxPool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            MLFloat16, Relu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                            MLFloat16, Relu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, MLFloat16,\r\n                                                                  Relu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 15,\r\n                                                                            MLFloat16, LeakyRelu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, MLFloat16,\r\n                                                                  LeakyRelu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, MLFloat16, Erf)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, MLFloat16, Add)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, MLFloat16, Sub)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, MLFloat16, Mul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, MLFloat16, Div)>,\r\n  };\r\n\r\n  for (auto& function_table_entry : function_table) {\r\n    KernelCreateInfo info = function_table_entry();\r\n    if (info.kernel_def != nullptr) {  // filter disabled entries where type is void\r\n      ORT_RETURN_IF_ERROR(kernel_registry.Register(std::move(info)));\r\n    }\r\n  }\r\n\r\n  return Status::OK();\r\n}\r\n#endif\r\n\r\n// Forward declarations of ml op kernels\r\n#ifndef DISABLE_ML_OPS\r\nnamespace ml {\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, float, ArrayFeatureExtractor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, double, ArrayFeatureExtractor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int32_t, ArrayFeatureExtractor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int64_t, ArrayFeatureExtractor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, string, ArrayFeatureExtractor);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, Binarizer);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, CastMap);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, CategoryMapper);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, string_int64_t, DictVectorizer);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, string_float, DictVectorizer);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, string_double, DictVectorizer);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int64_t_string, DictVectorizer);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int64_t_float, DictVectorizer);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int64_t_double, DictVectorizer);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, FeatureVectorizer);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, Imputer);\r\n\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, 1, LabelEncoder);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, LinearClassifier);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, LinearRegressor);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, Normalizer);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int64_t, OneHotEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, float, OneHotEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, double, OneHotEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, string, OneHotEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, float, Scaler);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, double, Scaler);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int64_t, Scaler);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int32_t, Scaler);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, SVMClassifier);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, SVMRegressor);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, 2, float,\r\n                                                      TreeEnsembleClassifier);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, 2, double,\r\n                                                      TreeEnsembleClassifier);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, 2, int64_t,\r\n                                                      TreeEnsembleClassifier);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, 2, int32_t,\r\n                                                      TreeEnsembleClassifier);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, 2, float,\r\n                                                      TreeEnsembleRegressor);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, 2, double,\r\n                                                      TreeEnsembleRegressor);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, ZipMap);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3, float_string,\r\n                                                      LabelEncoder);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3, string_float,\r\n                                                      LabelEncoder);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3, int64_float,\r\n                                                      LabelEncoder);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3, float_int64,\r\n                                                      LabelEncoder);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3, int64_string,\r\n                                                      LabelEncoder);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3, string_int64,\r\n                                                      LabelEncoder);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3, int64_int64,\r\n                                                      LabelEncoder);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3, string_string,\r\n                                                      LabelEncoder);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3, float_float,\r\n                                                      LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 3, float, TreeEnsembleClassifier);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 3, double, TreeEnsembleClassifier);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 3, int64_t, TreeEnsembleClassifier);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 3, int32_t, TreeEnsembleClassifier);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 3, float, TreeEnsembleRegressor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 3, double, TreeEnsembleRegressor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 5, float, TreeEnsemble);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 5, double, TreeEnsemble);\r\n\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, float_string, LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, string_float, LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, int64_float, LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, float_int64, LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, int64_string, LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, string_int64, LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, int64_int64, LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, string_string, LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, float_float, LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, string_int16, LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, double_string, LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, string_double, LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, int64_double, LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, double_int64, LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, double_double, LabelEncoder);\r\n\r\ntemplate <>\r\nKernelCreateInfo BuildKernelCreateInfo<void>() {\r\n  KernelCreateInfo info;\r\n  return info;\r\n}\r\n\r\nStatus RegisterOnnxMLOperatorKernels(KernelRegistry& kernel_registry) {\r\n  static const BuildKernelCreateInfoFn function_table[] = {\r\n      BuildKernelCreateInfo<void>,  // default entry to avoid the list become empty after ops-reducing\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, float,\r\n                                                                  ArrayFeatureExtractor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, double,\r\n                                                                  ArrayFeatureExtractor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int32_t,\r\n                                                                  ArrayFeatureExtractor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int64_t,\r\n                                                                  ArrayFeatureExtractor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, string,\r\n                                                                  ArrayFeatureExtractor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, Binarizer)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, CastMap)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, CategoryMapper)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, string_int64_t,\r\n                                                                  DictVectorizer)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, string_float,\r\n                                                                  DictVectorizer)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, string_double,\r\n                                                                  DictVectorizer)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int64_t_string,\r\n                                                                  DictVectorizer)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int64_t_float,\r\n                                                                  DictVectorizer)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int64_t_double,\r\n                                                                  DictVectorizer)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, FeatureVectorizer)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, Imputer)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, 1,\r\n                                                                      LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, LinearClassifier)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, LinearRegressor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, Normalizer)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int64_t,\r\n                                                                  OneHotEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, float,\r\n                                                                  OneHotEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, double,\r\n                                                                  OneHotEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, string,\r\n                                                                  OneHotEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, float, Scaler)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, double, Scaler)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int64_t,\r\n                                                                  Scaler)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int32_t,\r\n                                                                  Scaler)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, SVMClassifier)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, SVMRegressor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, 2,\r\n                                                                            float, TreeEnsembleClassifier)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, 2,\r\n                                                                            double, TreeEnsembleClassifier)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, 2,\r\n                                                                            int64_t, TreeEnsembleClassifier)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, 2,\r\n                                                                            int32_t, TreeEnsembleClassifier)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, 2,\r\n                                                                            float, TreeEnsembleRegressor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, 2,\r\n                                                                            double, TreeEnsembleRegressor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, ZipMap)>,\r\n\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3,\r\n                                                                            float_string, LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3,\r\n                                                                            string_float, LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3,\r\n                                                                            int64_float, LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3,\r\n                                                                            float_int64, LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3,\r\n                                                                            int64_string, LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3,\r\n                                                                            string_int64, LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3,\r\n                                                                            int64_int64, LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3,\r\n                                                                            string_string, LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3,\r\n                                                                            float_float, LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 3, float,\r\n                                                                  TreeEnsembleClassifier)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 3, double,\r\n                                                                  TreeEnsembleClassifier)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 3, int64_t,\r\n                                                                  TreeEnsembleClassifier)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 3, int32_t,\r\n                                                                  TreeEnsembleClassifier)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 3, float,\r\n                                                                  TreeEnsembleRegressor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 3, double,\r\n                                                                  TreeEnsembleRegressor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 5, float,\r\n                                                                  TreeEnsemble)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 5, double,\r\n                                                                  TreeEnsemble)>,\r\n\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, float_string,\r\n                                                                  LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, string_float,\r\n                                                                  LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, int64_float,\r\n                                                                  LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, float_int64,\r\n                                                                  LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, int64_string,\r\n                                                                  LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, string_int64,\r\n                                                                  LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, int64_int64,\r\n                                                                  LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, string_string,\r\n                                                                  LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, float_float,\r\n                                                                  LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, string_int16,\r\n                                                                  LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, double_string,\r\n                                                                  LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, string_double,\r\n                                                                  LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, int64_double,\r\n                                                                  LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, double_int64,\r\n                                                                  LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, double_double,\r\n                                                                  LabelEncoder)>,\r\n  };\r\n\r\n  for (auto& function_table_entry : function_table) {\r\n    KernelCreateInfo info = function_table_entry();\r\n    if (info.kernel_def != nullptr) {  // filter disabled entries where type is void\r\n      ORT_RETURN_IF_ERROR(kernel_registry.Register(std::move(info)));\r\n    }\r\n  }\r\n\r\n  return Status::OK();\r\n}\r\n}  // namespace ml\r\n#endif\r\n\r\nStatus RegisterCPUKernels(KernelRegistry& kernel_registry) {\r\n  ORT_RETURN_IF_ERROR(RegisterOnnxOperatorKernels(kernel_registry));\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\n  if (MlasFp16AccelerationSupported()) {\r\n    ORT_RETURN_IF_ERROR(RegisterFp16Kernels(kernel_registry));\r\n  }\r\n#endif\r\n#ifndef DISABLE_ML_OPS\r\n  ORT_RETURN_IF_ERROR(::onnxruntime::ml::RegisterOnnxMLOperatorKernels(kernel_registry));\r\n#endif\r\n#ifndef DISABLE_CONTRIB_OPS\r\n  ORT_RETURN_IF_ERROR(::onnxruntime::contrib::RegisterCpuContribKernels(kernel_registry));\r\n#endif\r\n#if defined(ENABLE_TRAINING_OPS)\r\n  ORT_RETURN_IF_ERROR(::onnxruntime::contrib::RegisterCpuTrainingKernels(kernel_registry));\r\n#endif\r\n  return Status::OK();\r\n}\r\n\r\nKernelRegistryAndStatus GetCpuKernelRegistry() {\r\n  KernelRegistryAndStatus ret;\r\n  ret.st = RegisterCPUKernels(*ret.kernel_registry);\r\n  return ret;\r\n}\r\n\r\nstd::shared_ptr<KernelRegistry> CPUExecutionProvider::GetKernelRegistry() const {\r\n  static KernelRegistryAndStatus k = GetCpuKernelRegistry();\r\n  // throw if the registry failed to initialize\r\n  ORT_THROW_IF_ERROR(k.st);\r\n  return k.kernel_registry;\r\n}\r\n\r\nstd::unique_ptr<IDataTransfer> CPUExecutionProvider::GetDataTransfer() const {\r\n  return std::make_unique<CPUDataTransfer>();\r\n}\r\n\r\n}  // namespace onnxruntime\r\n","replacement":"// Copyright (c) Microsoft Corporation. All rights reserved.\r\n// Licensed under the MIT License.\r\n\r\n#include \"core/providers/cpu/cpu_execution_provider.h\"\r\n\r\n#include \"core/framework/allocator_utils.h\"\r\n#include \"core/framework/memcpy.h\"\r\n#include \"core/framework/op_kernel.h\"\r\n#include \"core/framework/kernel_registry.h\"\r\n#include \"core/framework/int4.h\"\r\n#include \"core/mlas/inc/mlas.h\"\r\n\r\n#ifndef DISABLE_CONTRIB_OPS\r\n#include \"contrib_ops/cpu/cpu_contrib_kernels.h\"\r\n#endif\r\n\r\n#if defined(ENABLE_TRAINING_OPS)\r\n#include \"orttraining/training_ops/cpu/cpu_training_kernels.h\"\r\n#endif\r\n\r\n#include \"core/framework/compute_capability.h\"\r\n\r\nnamespace {\r\nstruct KernelRegistryAndStatus {\r\n  std::shared_ptr<onnxruntime::KernelRegistry> kernel_registry = std::make_shared<onnxruntime::KernelRegistry>();\r\n  onnxruntime::Status st;\r\n};\r\n}  // namespace\r\n\r\nnamespace onnxruntime {\r\n\r\n// The MemcpyFromHost and MemcpyToHost kernels registered for the CPU EP are generic memcpy kernels.\r\n// Other EPs may provide their own memcpy kernels.\r\n// For a memcpy between host (CPU) and device of some other EP:\r\n// - If the EP provides the corresponding memcpy kernel, it will be used.\r\n// - Otherwise, one of these generic memcpy kernels will be used.\r\n\r\nONNX_OPERATOR_KERNEL_EX(\r\n    MemcpyFromHost,\r\n    kOnnxDomain,\r\n    1,\r\n    kCpuExecutionProvider,\r\n    (*KernelDefBuilder::Create())\r\n        .InputMemoryType(OrtMemTypeCPUInput, 0)\r\n        .TypeConstraint(\"T\", DataTypeImpl::AllFixedSizeTensorAndSequenceTensorTypesIRv9()),\r\n    Memcpy);\r\n\r\nONNX_OPERATOR_KERNEL_EX(\r\n    MemcpyToHost,\r\n    kOnnxDomain,\r\n    1,\r\n    kCpuExecutionProvider,\r\n    (*KernelDefBuilder::Create())\r\n        .OutputMemoryType(OrtMemTypeCPUOutput, 0)\r\n        .TypeConstraint(\"T\", DataTypeImpl::AllFixedSizeTensorAndSequenceTensorTypesIRv9()),\r\n    Memcpy);\r\n\r\nCPUExecutionProvider::CPUExecutionProvider(const CPUExecutionProviderInfo& info)\r\n    : IExecutionProvider{onnxruntime::kCpuExecutionProvider}, info_{info} {}\r\n\r\nstd::vector<AllocatorPtr> CPUExecutionProvider::CreatePreferredAllocators() {\r\n  const bool create_arena = DoesCpuAllocatorSupportArenaUsage() ? info_.create_arena : false;\r\n  AllocatorCreationInfo device_info_cpu{[](int) { return std::make_unique<CPUAllocator>(); },\r\n                                        DEFAULT_CPU_ALLOCATOR_DEVICE_ID, create_arena};\r\n\r\n  return std::vector<AllocatorPtr>{CreateAllocator(device_info_cpu)};\r\n}\r\n\r\n// Forward declarations of op kernels\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, MemcpyFromHost);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, MemcpyToHost);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 10, Clip);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 21, Elu);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 21, HardSigmoid);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 15, LeakyRelu);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, float, Relu);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, double, Relu);\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, MLFloat16, Relu);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 15, MLFloat16, LeakyRelu);\r\n#endif\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 21, Selu);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, float, Sigmoid);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, double, Sigmoid);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 21, Softplus);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 21, Softsign);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, float, Tanh);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, double, Tanh);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8, PRelu);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, RandomNormal);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, RandomUniform);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, RandomNormalLike);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, RandomUniformLike);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, Multinomial);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, float, Abs);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, double, Abs);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, int8_t, Abs);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, int16_t, Abs);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, int32_t, Abs);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, int64_t, Abs);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, uint8_t, Abs);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, uint16_t, Abs);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, uint32_t, Abs);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, uint64_t, Abs);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, float, Floor);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, double, Floor);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, float, Ceil);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, double, Ceil);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, float, Reciprocal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, double, Reciprocal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, float, Sqrt);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, double, Sqrt);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, float, Add);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, double, Add);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, int32_t, Add);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, int64_t, Add);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, uint32_t, Add);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, uint64_t, Add);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, float, Sub);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, double, Sub);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, int32_t, Sub);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, int64_t, Sub);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, uint32_t, Sub);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, uint64_t, Sub);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, float, Mul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, double, Mul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, int32_t, Mul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, int64_t, Mul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, uint32_t, Mul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, uint64_t, Mul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, float, Div);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, double, Div);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, int32_t, Div);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, int64_t, Div);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, uint32_t, Div);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, uint64_t, Div);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, float, Neg);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, double, Neg);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, int8_t, Neg);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, int16_t, Neg);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, int32_t, Neg);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, int64_t, Neg);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 11, Pow);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, float, Exp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, double, Exp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, float, Log);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, double, Log);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 7, float, Sum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 7, double, Sum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, float, Sum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, double, Sum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 7, float, Min);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 11, Min);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 7, float, Max);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 11, Max);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, Not);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, And);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, Or);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, Xor);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8, float, Less);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8, double, Less);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8, float, Greater);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8, double, Greater);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 10, bool, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 10, int32_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 10, int64_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 10, float, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 10, double, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 7, float, Mean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, float, Mean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 21, float, Sin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 21, double, Sin);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 21, Cos);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 21, Tan);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 21, Asin);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 21, Acos);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 21, Atan);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8, float, Gemm);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8, double, Gemm);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, Hardmax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, float, LogSoftmax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, double, LogSoftmax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 8, float, MatMul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 8, double, MatMul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, float, Softmax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, double, Softmax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 9, float, TopK);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 9, double, TopK);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8, float,\r\n                                                      BatchNormalization);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8, double,\r\n                                                      BatchNormalization);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, Conv);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, ConvTranspose);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 8, Flatten);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 21, InstanceNormalization);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, float, LpNormalization);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, double, LpNormalization);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 12, LRN);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 9, AveragePool);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 7, MaxPool);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 11, MaxPool);\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 11, MLFloat16, MaxPool);\r\n#endif\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 2, 10, LpPool);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 2, GlobalLpPool);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 21, GlobalAveragePool);\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 21, MLFloat16, GlobalAveragePool);\r\n#endif\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 21, GlobalMaxPool);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, MaxRoiPool);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, float, ReduceL1);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, double, ReduceL1);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int32_t, ReduceL1);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int64_t, ReduceL1);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, float, ReduceL2);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, double, ReduceL2);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int32_t, ReduceL2);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int64_t, ReduceL2);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, float, ReduceLogSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, double, ReduceLogSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int32_t, ReduceLogSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int64_t, ReduceLogSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, float,\r\n                                                      ReduceLogSumExp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, double,\r\n                                                      ReduceLogSumExp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int32_t,\r\n                                                      ReduceLogSumExp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int64_t,\r\n                                                      ReduceLogSumExp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, float, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, double, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int32_t, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int64_t, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, float, ReduceMean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, double, ReduceMean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int32_t, ReduceMean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int64_t, ReduceMean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, float, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, double, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int32_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int64_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, float, ReduceProd);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, double, ReduceProd);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int32_t, ReduceProd);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int64_t, ReduceProd);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, float, ReduceSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int32_t, ReduceSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, double, ReduceSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int64_t, ReduceSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, float,\r\n                                                      ReduceSumSquare);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int32_t,\r\n                                                      ReduceSumSquare);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, double,\r\n                                                      ReduceSumSquare);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int64_t,\r\n                                                      ReduceSumSquare);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, float, ArgMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, double, ArgMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int8_t, ArgMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, uint8_t, ArgMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int32_t, ArgMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int64_t, ArgMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, float, ArgMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, double, ArgMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int8_t, ArgMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, uint8_t, ArgMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int64_t, ArgMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, int32_t, ArgMin);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 13, GRU);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 13, LSTM);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 13, RNN);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, Cast);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 4, 10, Concat);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, Gather);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 9, Dropout);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 12, Identity);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 2, 10, Pad);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 4, Reshape);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 5, 12, Reshape);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 12, Shape);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 12, Size);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 9, Slice);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 12, SpaceToDepth);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, DepthToSpace);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 2, 10, Split);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, Squeeze);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, Tile);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 12, Transpose);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, Unsqueeze);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8, float, Upsample);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8, int32_t, Upsample);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8, int8_t, Upsample);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8, uint8_t, Upsample);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, float, Expand);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, double, Expand);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, int8_t, Expand);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, int16_t, Expand);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, int32_t, Expand);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, int64_t, Expand);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, uint8_t, Expand);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, uint16_t, Expand);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, uint32_t, Expand);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, uint64_t, Expand);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, bool, Expand);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, MLFloat16, Expand);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12, string, Expand);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 8, Scan);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, If);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, Loop);\r\n\r\n// Opset 9\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, Compress);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 19, ConstantOfShape);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, MeanVarianceNormalization);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, float, Greater);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, double, Greater);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, int8_t, Greater);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, int16_t, Greater);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, int32_t, Greater);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, int64_t, Greater);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, uint8_t, Greater);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, uint16_t, Greater);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, uint32_t, Greater);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, uint64_t, Greater);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, float, Less);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, double, Less);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, int8_t, Less);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, int16_t, Less);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, int32_t, Less);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, int64_t, Less);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, uint8_t, Less);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, uint16_t, Less);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, uint32_t, Less);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, uint64_t, Less);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 21, EyeLike);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, float, IsNaN);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, double, IsNaN);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, MLFloat16, IsNaN);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, Sign);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, Shrink);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, float, Erf);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                      int64_t_int64_t_int64_t, OneHot);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, float_int64_t_int64_t,\r\n                                                      OneHot);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, int64_t_string_int64_t,\r\n                                                      OneHot);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, float_string_int64_t,\r\n                                                      OneHot);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, float_float_float,\r\n                                                      OneHot);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, int64_t_int32_t_float,\r\n                                                      OneHot);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, int64_t_float_int64_t,\r\n                                                      OneHot);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, int32_t_float_int32_t,\r\n                                                      OneHot);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, int32_t_float_float,\r\n                                                      OneHot);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, int64_t_float_float,\r\n                                                      OneHot);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, int64_t_float_int32_t,\r\n                                                      OneHot);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, MaxUnpool);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 21, Sinh);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 21, Cosh);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 21, Asinh);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 21, Acosh);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 21, Atanh);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, Scan);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, Scatter);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, TfIdfVectorizer);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, bool, NonZero);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, float, NonZero);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, int32_t, NonZero);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, int64_t, NonZero);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, uint8_t, NonZero);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 15, string, Where);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 15, float, Where);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 15, double, Where);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 15, int32_t, Where);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 15, int64_t, Where);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 15, uint8_t, Where);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, Flatten);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, float, Gemm);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, double, Gemm);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, float, MatMul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, double, MatMul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, int32_t, MatMul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, int64_t, MatMul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 13, float,\r\n                                                      BatchNormalization);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 13, double,\r\n                                                      BatchNormalization);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 15, PRelu);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 9, float, Upsample);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 9, int32_t, Upsample);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 9, int8_t, Upsample);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 9, uint8_t, Upsample);\r\n\r\n// Opset 10\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, StringNormalizer);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10, float, TopK);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10, double, TopK);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10, AveragePool);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 12, Mod);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10, float, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10, int32_t, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10, int8_t, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10, uint8_t, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 21, ThresholdedRelu);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 12, uint8_t,\r\n                                                      DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 12, int8_t,\r\n                                                      DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 12, int32_t,\r\n                                                      DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 12, uint8_t,\r\n                                                      QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 12, int8_t,\r\n                                                      QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 20, uint8_t,\r\n                                                      QLinearMatMul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 20, int8_t,\r\n                                                      QLinearMatMul);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, uint8_t, MatMulInteger);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, int8_t, MatMulInteger);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, ConvInteger);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, uint8_t, QLinearConv);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, int8_t, QLinearConv);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10, Slice);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 11, Dropout);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10, NonMaxSuppression);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 19, IsInf);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 15, float, RoiAlign);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 15, double, RoiAlign);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, ReverseSequence);\r\n\r\n// opset 11\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11, Clip);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 13, float, CumSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 13, double, CumSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 13, int32_t, CumSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 13, int64_t, CumSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, bool, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int8_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int16_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int32_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int64_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, uint8_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, uint16_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, uint32_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, uint64_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, double, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, float, Round);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, double, Round);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, MLFloat16, Round);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, uint8_t, DynamicQuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float, ArgMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, double, ArgMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int8_t, ArgMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, uint8_t, ArgMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int32_t, ArgMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int64_t, ArgMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float, ArgMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, double, ArgMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int8_t, ArgMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, uint8_t, ArgMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int32_t, ArgMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int64_t, ArgMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float, ReduceL1);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, double, ReduceL1);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int32_t, ReduceL1);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int64_t, ReduceL1);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float, ReduceL2);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, double, ReduceL2);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int32_t, ReduceL2);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int64_t, ReduceL2);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float, ReduceLogSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, double, ReduceLogSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int32_t,\r\n                                                      ReduceLogSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int64_t,\r\n                                                      ReduceLogSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float,\r\n                                                      ReduceLogSumExp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, double,\r\n                                                      ReduceLogSumExp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int32_t,\r\n                                                      ReduceLogSumExp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int64_t,\r\n                                                      ReduceLogSumExp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11, float, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11, double, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11, int32_t, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11, int64_t, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float, ReduceMean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, double, ReduceMean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int32_t, ReduceMean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int64_t, ReduceMean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11, float, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11, double, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11, int32_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11, int64_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float, ReduceProd);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, double, ReduceProd);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int32_t, ReduceProd);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int64_t, ReduceProd);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float, ReduceSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, double, ReduceSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int32_t, ReduceSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int64_t, ReduceSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float,\r\n                                                      ReduceSumSquare);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, double,\r\n                                                      ReduceSumSquare);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int32_t,\r\n                                                      ReduceSumSquare);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int64_t,\r\n                                                      ReduceSumSquare);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, Hardmax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float, LogSoftmax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, double, LogSoftmax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float, Softmax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, double, Softmax);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, Loop);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, DepthToSpace);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 15, Scan);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, Flatten);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, Compress);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, Concat);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, Gather);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, Slice);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, Split);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, Squeeze);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, Unsqueeze);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, Det);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, ScatterElements);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, NonMaxSuppression);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 18, AveragePool);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, MaxUnpool);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 17, LpPool);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, Conv);\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, MLFloat16, Conv);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 18, MLFloat16,\r\n                                                      AveragePool);\r\n#endif\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, ConvTranspose);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, If);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, SequenceLength);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, SequenceAt);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, SequenceEmpty);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, SequenceInsert);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, SequenceErase);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, SequenceConstruct);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, ConcatFromSequence);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 23, SplitToSequence);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, ScatterND);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float, Gemm);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, double, Gemm);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, GatherElements);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, uint8_t, BitShift);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, uint32_t, BitShift);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, uint64_t, BitShift);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, Pad);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11, GatherND);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, Range);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, Unique);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 23, float, TopK);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 23, double, TopK);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 23, int64_t, TopK);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 23, int32_t, TopK);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, int64_t_int64_t_int64_t, OneHot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, float_int64_t_int64_t, OneHot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, int64_t_string_int64_t, OneHot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, float_string_int64_t, OneHot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, float_float_float, OneHot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, int64_t_int32_t_float, OneHot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, int64_t_float_int64_t, OneHot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, int32_t_float_int32_t, OneHot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, int32_t_float_float, OneHot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, int64_t_float_float, OneHot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, int64_t_float_int32_t, OneHot);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, float, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int32_t, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, int8_t, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, uint8_t, Resize);\r\n\r\n// opset 12\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, Clip);\r\n\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, Min);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, Max);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 21, MaxPool);\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 21, MLFloat16, MaxPool);\r\n#endif\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, Pow);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, float, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, double, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, int32_t, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, int64_t, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, int8_t, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, uint8_t, ReduceMax);\r\n\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, float, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, double, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, int32_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, int64_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, int8_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, uint8_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, GatherND);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, Einsum);\r\n\r\n// REVIEW(codemzs): ConstEigenVectorArrayMap.cast<MLFLoat16) does not seem to be supported.\r\n// However these types work on GPU implementation.\r\n// class ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, MLFloat16_MLFloat16, Dropout);\r\n// class ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, MLFloat16_float, Dropout);\r\n// class ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, MLFloat16_double, Dropout);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, float_float, Dropout);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, float_double,\r\n                                                      Dropout);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, double_float,\r\n                                                      Dropout);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, double_double,\r\n                                                      Dropout);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, Celu);\r\n\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, float,\r\n                                                      GreaterOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, double,\r\n                                                      GreaterOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, int8_t,\r\n                                                      GreaterOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, int16_t,\r\n                                                      GreaterOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, int32_t,\r\n                                                      GreaterOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, int64_t,\r\n                                                      GreaterOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, uint8_t,\r\n                                                      GreaterOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, uint16_t,\r\n                                                      GreaterOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, uint32_t,\r\n                                                      GreaterOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, uint64_t,\r\n                                                      GreaterOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, float, LessOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, double, LessOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, int8_t, LessOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, int16_t, LessOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, int32_t, LessOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, int64_t, LessOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, uint8_t, LessOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, uint16_t, LessOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, uint32_t, LessOrEqual);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15, uint64_t, LessOrEqual);\r\n\r\n// opset 13\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, MLFloat16, Erf);\r\n#endif\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Erf);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, Cast);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Clip);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, uint8_t,\r\n                                                      DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, int8_t,\r\n                                                      DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, int32_t,\r\n                                                      DequantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Expand);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Expand);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int8_t, Expand);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int16_t, Expand);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t, Expand);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t, Expand);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint8_t, Expand);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint16_t, Expand);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint32_t, Expand);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint64_t, Expand);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, bool, Expand);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, MLFloat16, Expand);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, string, Expand);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Gemm);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Gemm);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, MatMul);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, MatMul);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t, MatMul);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t, MatMul);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Min);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Max);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Mean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, uint8_t,\r\n                                                      QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, int8_t,\r\n                                                      QuantizeLinear);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Sigmoid);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Sign);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, Size);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Sum);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Sum);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 20, Flatten);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, LRN);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, MeanVarianceNormalization);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 21, float_float, Dropout);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 21, float_double, Dropout);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 21, double_float, Dropout);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 21, double_double, Dropout);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, ArgMax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, ArgMax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int8_t, ArgMax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint8_t, ArgMax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t, ArgMax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t, ArgMax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, ArgMin);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, ArgMin);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int8_t, ArgMin);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint8_t, ArgMin);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t, ArgMin);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t, ArgMin);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, Reshape);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 14, Shape);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Concat);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Less);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Less);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int8_t, Less);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int16_t, Less);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t, Less);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t, Less);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint8_t, Less);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint16_t, Less);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint32_t, Less);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint64_t, Less);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Greater);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Greater);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int8_t, Greater);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int16_t, Greater);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t, Greater);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t, Greater);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint8_t, Greater);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint16_t, Greater);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint32_t, Greater);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint64_t, Greater);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, bool, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, int8_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, int16_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, int32_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, int64_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, uint8_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, uint16_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, uint32_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, uint64_t, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, float, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, double, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, float, Add);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, double, Add);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, int32_t, Add);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, int64_t, Add);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, uint32_t, Add);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, uint64_t, Add);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, float, Sub);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, double, Sub);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, int32_t, Sub);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, int64_t, Sub);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, uint32_t, Sub);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, uint64_t, Sub);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, float, Mul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, double, Mul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, int32_t, Mul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, int64_t, Mul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, uint32_t, Mul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, uint64_t, Mul);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, float, Div);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, double, Div);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, int32_t, Div);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, int64_t, Div);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, uint32_t, Div);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, uint64_t, Div);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Neg);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Neg);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int8_t, Neg);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int16_t, Neg);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t, Neg);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t, Neg);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Mod);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Abs);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Abs);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int8_t, Abs);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int16_t, Abs);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t, Abs);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t, Abs);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint8_t, Abs);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint16_t, Abs);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint32_t, Abs);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint64_t, Abs);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Reciprocal);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Reciprocal);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Floor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Floor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Ceil);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Ceil);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Sqrt);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Sqrt);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, float, Relu);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, double, Relu);\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, MLFloat16, Relu);\r\n#endif\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Sigmoid);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Sigmoid);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Tanh);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Tanh);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Exp);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Exp);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Log);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Log);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 14, Pow);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, DepthToSpace);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, SpaceToDepth);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Slice);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, Split);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 20, Unsqueeze);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 20, Squeeze);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 20, Transpose);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Tile);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Gather);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, GatherElements);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 15, ScatterND);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 15, ScatterElements);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13, Identity);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 19, float, IsNaN);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 19, double, IsNaN);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 19, MLFloat16, IsNaN);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 19, BFloat16, IsNaN);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, bool, NonZero);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, NonZero);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t, NonZero);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t, NonZero);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint8_t, NonZero);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, GatherND);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, Pad);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, float, ReduceL1);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, double, ReduceL1);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int32_t, ReduceL1);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int64_t, ReduceL1);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, float, ReduceL2);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, double, ReduceL2);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int32_t, ReduceL2);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int64_t, ReduceL2);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, float, ReduceLogSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, double, ReduceLogSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int32_t,\r\n                                                      ReduceLogSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int64_t,\r\n                                                      ReduceLogSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, float,\r\n                                                      ReduceLogSumExp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, double,\r\n                                                      ReduceLogSumExp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int32_t,\r\n                                                      ReduceLogSumExp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int64_t,\r\n                                                      ReduceLogSumExp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, float, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, double, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int32_t, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int64_t, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int8_t, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, uint8_t, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, float, ReduceMean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, double, ReduceMean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int32_t, ReduceMean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int64_t, ReduceMean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, float, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, double, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int32_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int64_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int8_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, uint8_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, float, ReduceProd);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, double, ReduceProd);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int32_t, ReduceProd);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int64_t, ReduceProd);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, float,\r\n                                                      ReduceSumSquare);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, double,\r\n                                                      ReduceSumSquare);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int32_t,\r\n                                                      ReduceSumSquare);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int64_t,\r\n                                                      ReduceSumSquare);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, ReduceSum);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, ReduceSum);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t, ReduceSum);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t, ReduceSum);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, float, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int32_t, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, int8_t, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, uint8_t, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 15, Loop);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 15, If);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Hardmax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, LogSoftmax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, LogSoftmax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Softmax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Softmax);\r\n\r\n// Opset 14\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, float, CumSum);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, double, CumSum);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int32_t, CumSum);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int64_t, CumSum);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, float, Relu);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, double, Relu);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int8_t, Relu);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int32_t, Relu);\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, MLFloat16, Relu);\r\n#endif\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, Trilu);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, float, Add);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, double, Add);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int8_t, Add);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int16_t, Add);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int32_t, Add);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int64_t, Add);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint8_t, Add);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint16_t, Add);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint32_t, Add);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint64_t, Add);\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, MLFloat16, Add);\r\n#endif\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, float, Sub);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, double, Sub);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int8_t, Sub);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int16_t, Sub);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int32_t, Sub);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int64_t, Sub);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint8_t, Sub);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint16_t, Sub);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint32_t, Sub);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint64_t, Sub);\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, MLFloat16, Sub);\r\n#endif\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, float, Mul);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, double, Mul);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int8_t, Mul);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int16_t, Mul);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int32_t, Mul);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int64_t, Mul);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint8_t, Mul);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint16_t, Mul);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint32_t, Mul);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint64_t, Mul);\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, MLFloat16, Mul);\r\n#endif\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, float, Div);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, double, Div);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int8_t, Div);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int16_t, Div);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int32_t, Div);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int64_t, Div);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint8_t, Div);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint16_t, Div);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint32_t, Div);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint64_t, Div);\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, MLFloat16, Div);\r\n#endif\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, 18, Reshape);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, 15, Identity);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, 14, float,\r\n                                                      BatchNormalization);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, 14, double,\r\n                                                      BatchNormalization);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, 21, GRU);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, 21, LSTM);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, 21, RNN);\r\n\r\n// Opset 15\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 15, Pow);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 15, float, BatchNormalization);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 15, double, BatchNormalization);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 15, 18, Shape);\r\n\r\n#if !defined(DISABLE_OPTIONAL_TYPE)\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 15, 17, OptionalHasElement);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 15, 17, OptionalGetElement);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 15, Optional);\r\n#endif\r\n\r\n// Opset 16\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, 18, Identity);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, 18, Loop);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, 18, If);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, float, RoiAlign);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, double, RoiAlign);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, 19, float, GridSample);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, 17, ScatterElements);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, 17, ScatterND);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, string, Where);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, float, Where);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, double, Where);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int32_t, Where);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int64_t, Where);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint8_t, Where);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, LeakyRelu);\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, MLFloat16, LeakyRelu);\r\n#endif\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, PRelu);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, 18, Scan);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, float, GreaterOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, double, GreaterOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int8_t, GreaterOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int16_t, GreaterOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int32_t, GreaterOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int64_t, GreaterOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint8_t, GreaterOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint16_t, GreaterOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint32_t, GreaterOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint64_t, GreaterOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, float, LessOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, double, LessOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int8_t, LessOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int16_t, LessOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int32_t, LessOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int64_t, LessOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint8_t, LessOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint16_t, LessOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint32_t, LessOrEqual);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint64_t, LessOrEqual);\r\n\r\n// Opset 17\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, 19, DFT);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, BlackmanWindow);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, HammingWindow);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, HannWindow);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, MelWeightMatrix);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, STFT);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, float, LayerNormalization);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, double, LayerNormalization);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, MLFloat16, LayerNormalization);\r\n// Opset 18\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 18, float, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 18, int32_t, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 18, int8_t, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 18, uint8_t, Resize);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, float, ReduceL1);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, double, ReduceL1);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t, ReduceL1);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t, ReduceL1);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, float, ReduceL2);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, double, ReduceL2);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t, ReduceL2);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t, ReduceL2);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, float, ReduceLogSum);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, double, ReduceLogSum);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t, ReduceLogSum);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t, ReduceLogSum);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, float, ReduceLogSumExp);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, double, ReduceLogSumExp);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t, ReduceLogSumExp);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t, ReduceLogSumExp);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, float, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, double, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, int32_t, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, int64_t, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, int8_t, ReduceMax);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, uint8_t, ReduceMax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, float, ReduceMean);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, double, ReduceMean);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t, ReduceMean);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t, ReduceMean);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, float, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, double, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, int32_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, int64_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, int8_t, ReduceMin);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, uint8_t, ReduceMin);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, float, ReduceProd);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, double, ReduceProd);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t, ReduceProd);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t, ReduceProd);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, float, ReduceSumSquare);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, double, ReduceSumSquare);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t, ReduceSumSquare);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t, ReduceSumSquare);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 21, LpPool);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, Col2Im);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int8_t, BitwiseAnd);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int16_t, BitwiseAnd);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t, BitwiseAnd);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t, BitwiseAnd);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint8_t, BitwiseAnd);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint16_t, BitwiseAnd);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint32_t, BitwiseAnd);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint64_t, BitwiseAnd);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int8_t, BitwiseNot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int16_t, BitwiseNot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t, BitwiseNot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t, BitwiseNot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint8_t, BitwiseNot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint16_t, BitwiseNot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint32_t, BitwiseNot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint64_t, BitwiseNot);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int8_t, BitwiseOr);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int16_t, BitwiseOr);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t, BitwiseOr);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t, BitwiseOr);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint8_t, BitwiseOr);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint16_t, BitwiseOr);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint32_t, BitwiseOr);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint64_t, BitwiseOr);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int8_t, BitwiseXor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int16_t, BitwiseXor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t, BitwiseXor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t, BitwiseXor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint8_t, BitwiseXor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint16_t, BitwiseXor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint32_t, BitwiseXor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint64_t, BitwiseXor);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 18, Pad);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, ScatterND);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, ScatterElements);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, Split);\r\n#if !defined(DISABLE_OPTIONAL_TYPE)\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, OptionalHasElement);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, OptionalGetElement);\r\n\r\n#endif\r\n\r\n// Opset 19\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Size);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 21, AveragePool);\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 21, MLFloat16, AveragePool);\r\n#endif\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Cast);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, int32_t,\r\n                                                      DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, uint8_t,\r\n                                                      DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, int8_t,\r\n                                                      DequantizeLinear);\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Float8E4M3FN,\r\n                                                      DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Float8E4M3FNUZ,\r\n                                                      DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Float8E5M2,\r\n                                                      DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Float8E5M2FNUZ,\r\n                                                      DequantizeLinear);\r\n#endif\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, bool, Equal);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, int8_t, Equal);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, int16_t, Equal);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, int32_t, Equal);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, int64_t, Equal);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, uint8_t, Equal);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, uint16_t, Equal);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, uint32_t, Equal);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, uint64_t, Equal);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, float, Equal);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, double, Equal);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, string, Equal);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Identity);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, If);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Loop);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Pad);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, uint8_t,\r\n                                                      QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, int8_t,\r\n                                                      QuantizeLinear);\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Float8E4M3FN,\r\n                                                      QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Float8E4M3FNUZ,\r\n                                                      QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Float8E5M2,\r\n                                                      QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Float8E5M2FNUZ,\r\n                                                      QuantizeLinear);\r\n#endif\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Reshape);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, float, Resize);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, int32_t, Resize);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, int8_t, Resize);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, uint8_t, Resize);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Scan);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Shape);\r\n\r\n// Opset 20\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, 20, ConstantOfShape);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, bool, ReduceMax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, float, ReduceMax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, double, ReduceMax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, int32_t, ReduceMax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, int64_t, ReduceMax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, int8_t, ReduceMax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, uint8_t, ReduceMax);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, bool, ReduceMin);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, float, ReduceMin);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, double, ReduceMin);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, int32_t, ReduceMin);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, int64_t, ReduceMin);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, int8_t, ReduceMin);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, uint8_t, ReduceMin);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, DFT);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, 21, float, GridSample);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, 21, double, GridSample);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, float, AffineGrid);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, double, AffineGrid);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, float, IsNaN);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, double, IsNaN);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, MLFloat16, IsNaN);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, BFloat16, IsNaN);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, Gelu);\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, Float8E4M3FN, IsNaN);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, Float8E4M3FNUZ, IsNaN);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, Float8E5M2, IsNaN);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, Float8E5M2FNUZ, IsNaN);\r\n#endif\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, IsInf);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, StringConcat);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, RegexFullMatch);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, StringSplit);\r\n\r\n// Opset 21\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Cast);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, ConstantOfShape);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Identity);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Reshape);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Scan);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Shape);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Size);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Squeeze);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Transpose);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Unsqueeze);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, If);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Loop);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Flatten);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Pad);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, int32_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, uint8_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, int8_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, uint16_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, int16_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Int4x2, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, UInt4x2, DequantizeLinear);\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E4M3FN, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E4M3FNUZ, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E5M2, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E5M2FNUZ, DequantizeLinear);\r\n#endif\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, uint8_t, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, int8_t, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, uint16_t, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, int16_t, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Int4x2, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, UInt4x2, QuantizeLinear);\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E4M3FN, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E4M3FNUZ, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E5M2, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E5M2FNUZ, QuantizeLinear);\r\n#endif\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, uint8_t, QLinearMatMul);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, int8_t, QLinearMatMul);\r\n\r\n// Opset 22\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Acos);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Cos);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Tan);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Asin);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Atan);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Sinh);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Cosh);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Asinh);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Acosh);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Atanh);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Conv);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, ConvTranspose);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Det);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, float_float, Dropout);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, float_double, Dropout);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, double_float, Dropout);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, double_double, Dropout);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, float, GridSample);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, double, GridSample);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Elu);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, EyeLike);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, GlobalAveragePool);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, GlobalMaxPool);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, GRU);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, LSTM);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, RNN);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, HardSigmoid);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, InstanceNormalization);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, LpPool);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, MaxPool);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, MaxUnpool);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Softplus);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, float, Round);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, double, Round);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, MLFloat16, Round);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Selu);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, float, Sin);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, double, Sin);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Softsign);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, ThresholdedRelu);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, AveragePool);\r\n\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, MLFloat16, Conv);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, MLFloat16, GlobalAveragePool);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, MLFloat16, MaxPool);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, MLFloat16, AveragePool);\r\n#endif\r\n\r\n// Opset 23\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, float, Attention);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, MLFloat16, Attention);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Cast);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, ConstantOfShape);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, int32_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, uint8_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, int8_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, uint16_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, int16_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Int4x2, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, UInt4x2, DequantizeLinear);\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E4M3FN, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E4M3FNUZ, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E5M2, DequantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E5M2FNUZ, DequantizeLinear);\r\n#endif\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, uint8_t, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, int8_t, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, uint16_t, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, int16_t, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Int4x2, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, UInt4x2, QuantizeLinear);\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E4M3FN, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E4M3FNUZ, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E5M2, QuantizeLinear);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E5M2FNUZ, QuantizeLinear);\r\n#endif\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Flatten);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Identity);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Reshape);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Scan);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Shape);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Size);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Squeeze);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Transpose);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Unsqueeze);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, If);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Loop);\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Pad);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, float, RMSNormalization);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, double, RMSNormalization);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, MLFloat16, RMSNormalization);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, float, RotaryEmbedding);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, MLFloat16, RotaryEmbedding);\r\n\r\n// Opset 24\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, float, Attention);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, MLFloat16, Attention);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Cast);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, ConstantOfShape);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, int32_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, uint8_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, int8_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, uint16_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, int16_t, DequantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Int4x2, DequantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, UInt4x2, DequantizeLinear);\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E4M3FN, DequantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E4M3FNUZ, DequantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E5M2, DequantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E5M2FNUZ, DequantizeLinear);\r\n#endif\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, uint8_t, QuantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, int8_t, QuantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, uint16_t, QuantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, int16_t, QuantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Int4x2, QuantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, UInt4x2, QuantizeLinear);\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E4M3FN, QuantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E4M3FNUZ, QuantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E5M2, QuantizeLinear);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E5M2FNUZ, QuantizeLinear);\r\n#endif\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Flatten);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Identity);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Pad);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, If);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Loop);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Reshape);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Shape);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Squeeze);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Transpose);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Unsqueeze);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Scan);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Size);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, SplitToSequence);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, float, TopK);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, double, TopK);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, int64_t, TopK);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, int32_t, TopK);\r\n\r\n// !!PLEASE READ BELOW!! Following that, add new entries above this comment\r\n\r\n/*  *** IMPORTANT! ***\r\n NEVER update a versioned entry to change the start or end version. These MUST be treated as immutable.\r\n   i.e. if the macro has 'VERSIONED' in it, do not modify that entry\r\n\r\n When updating a declaration to add a new version of an operator there are 2 simple steps:\r\n\r\n   1. There should be a non-versioned entry for that latest version. Update this to be versioned.\r\n      Note that the end version is inclusive, so the end value will be one less than the operator's new opset version.\r\n   2. Add a new non-versioned entry for the new opset.\r\n\r\n e.g. Say opset 13 is being added, and we need to update Add. The most recent change to Add was in opset 7 so it\r\n      should have an un-versioned registration in the opset 7 section like this:\r\n\r\n     class ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, float, Add);\r\n\r\n   Step 1 is to change that to add 'VERSIONED_' to the macro and add an end version of 12 as the new opset is 13:\r\n     class ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12, float, Add);\r\n\r\n   Step 2 is to create a new un-versioned entry in the opset 13 sections:\r\n     class ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Add);\r\n\r\n The process is the same for TYPED and untyped kernels - just repeat for each type when updating the typed entries.\r\n\r\n The changes below in the registrations using BuildKernelCreateInfo are essentially the same. Update existing\r\n registration to use the VERSIONED_ macro, add end version, add new un-versioned entry in the section for the new\r\n opset.\r\n\r\n To double-check what versions an operator should have registrations for see\r\n https://github.com/onnx/onnx/blob/main/docs/Operators.md\r\n*****/\r\n\r\ntemplate <>\r\nKernelCreateInfo BuildKernelCreateInfo<void>() {\r\n  KernelCreateInfo info;\r\n  return info;\r\n}\r\n\r\nStatus RegisterOnnxOperatorKernels(KernelRegistry& kernel_registry) {\r\n  static const BuildKernelCreateInfoFn function_table[] = {\r\n    BuildKernelCreateInfo<void>,  // default entry to avoid the list become empty after ops-reducing\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, MemcpyFromHost)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, MemcpyToHost)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 10, Clip)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 21, Elu)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 21, HardSigmoid)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 15,\r\n                                                                    LeakyRelu)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          float, Relu)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          double, Relu)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 21, Selu)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          float, Sigmoid)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          double, Sigmoid)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 21, Softplus)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 21, Softsign)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          float, Tanh)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          double, Tanh)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8, PRelu)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, RandomNormal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, RandomUniform)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, RandomNormalLike)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, RandomUniformLike)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, Multinomial)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                          float, Add)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                          double, Add)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                          int32_t, Add)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                          int64_t, Add)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                          uint32_t, Add)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                          uint64_t, Add)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                          float, Sub)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                          double, Sub)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                          int32_t, Sub)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                          int64_t, Sub)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                          uint32_t, Sub)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                          uint64_t, Sub)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                          float, Mul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                          double, Mul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                          int32_t, Mul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                          int64_t, Mul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                          uint32_t, Mul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                          uint64_t, Mul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                          float, Div)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                          double, Div)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                          int32_t, Div)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                          int64_t, Div)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                          uint32_t, Div)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 12,\r\n                                                                          uint64_t, Div)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          float, Abs)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          double, Abs)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          int8_t, Abs)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          int16_t, Abs)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          int32_t, Abs)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          int64_t, Abs)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          uint8_t, Abs)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          uint16_t, Abs)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          uint32_t, Abs)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          uint64_t, Abs)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          float, Floor)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          double, Floor)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          float, Ceil)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          double, Ceil)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          float, Reciprocal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          double, Reciprocal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          float, Sqrt)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          double, Sqrt)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          float, Neg)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          double, Neg)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          int8_t, Neg)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          int16_t, Neg)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          int32_t, Neg)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          int64_t, Neg)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 11, Pow)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          float, Exp)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          double, Exp)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          float, Log)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                          double, Log)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 7,\r\n                                                                          float, Sum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 7,\r\n                                                                          double, Sum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                          float, Sum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                          double, Sum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 7,\r\n                                                                          float, Min)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 11, Min)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 7,\r\n                                                                          float, Max)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 11, Max)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, Not)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, And)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, Or)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, Xor)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8,\r\n                                                                          float, Less)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8,\r\n                                                                          double, Less)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8,\r\n                                                                          float, Greater)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8,\r\n                                                                          double, Greater)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 10,\r\n                                                                          bool, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 10,\r\n                                                                          int32_t, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 10,\r\n                                                                          int64_t, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 10,\r\n                                                                          float, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 10,\r\n                                                                          double, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 7,\r\n                                                                          float, Mean)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                          float, Mean)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 21, float, Sin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 21, double, Sin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 21, Cos)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 21, Tan)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 21, Asin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 21, Acos)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 21, Atan)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8,\r\n                                                                          float, Gemm)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8,\r\n                                                                          double, Gemm)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                    Hardmax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          float, LogSoftmax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          double, LogSoftmax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 8,\r\n                                                                          float, MatMul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 8,\r\n                                                                          double, MatMul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          float, Softmax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          double, Softmax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 9,\r\n                                                                          float, TopK)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 9,\r\n                                                                          double, TopK)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8,\r\n                                                                          float, BatchNormalization)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8,\r\n                                                                          double, BatchNormalization)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, Conv)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                    ConvTranspose)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 8, Flatten)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 21,\r\n                                                                    InstanceNormalization)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, float,\r\n                                                                LpNormalization)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, double,\r\n                                                                LpNormalization)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 12, LRN)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 9,\r\n                                                                    AveragePool)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 7, MaxPool)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 11,\r\n                                                                    MaxPool)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 2, 10, LpPool)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 2, GlobalLpPool)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 21, GlobalAveragePool)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 21, GlobalMaxPool)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, MaxRoiPool)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          float, ReduceL1)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          double, ReduceL1)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          int32_t, ReduceL1)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          int64_t, ReduceL1)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          float, ReduceL2)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          double, ReduceL2)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          int32_t, ReduceL2)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          int64_t, ReduceL2)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          float, ReduceLogSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          double, ReduceLogSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          int32_t, ReduceLogSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          int64_t, ReduceLogSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          float, ReduceLogSumExp)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          double, ReduceLogSumExp)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          int32_t, ReduceLogSumExp)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          int64_t, ReduceLogSumExp)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          float, ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          double, ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          int32_t, ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          int64_t, ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          float, ReduceMean)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          double, ReduceMean)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          int32_t, ReduceMean)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          int64_t, ReduceMean)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          float, ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          double, ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          int32_t, ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          int64_t, ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          float, ReduceProd)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          double, ReduceProd)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          int32_t, ReduceProd)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          int64_t, ReduceProd)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          float, ReduceSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          int32_t, ReduceSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          double, ReduceSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          int64_t, ReduceSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          float, ReduceSumSquare)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          int32_t, ReduceSumSquare)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          double, ReduceSumSquare)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          int64_t, ReduceSumSquare)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          float, ArgMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          double, ArgMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          int8_t, ArgMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          uint8_t, ArgMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          int32_t, ArgMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          int64_t, ArgMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          float, ArgMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          double, ArgMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          int8_t, ArgMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          uint8_t, ArgMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          int32_t, ArgMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                          int64_t, ArgMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 13, GRU)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 13, LSTM)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 13, RNN)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, Cast)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 4, 10, Concat)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, Gather)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 9, Dropout)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 12,\r\n                                                                    Identity)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 2, 10, Pad)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 4, Reshape)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 5, 12,\r\n                                                                    Reshape)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 12, Shape)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 12, Size)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 9, Slice)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 12,\r\n                                                                    SpaceToDepth)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                    DepthToSpace)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 2, 10, Split)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                    Squeeze)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12, Tile)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 12,\r\n                                                                    Transpose)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10,\r\n                                                                    Unsqueeze)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8,\r\n                                                                          float, Upsample)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8,\r\n                                                                          int32_t, Upsample)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8,\r\n                                                                          int8_t, Upsample)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 7, 8,\r\n                                                                          uint8_t, Upsample)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                          float, Expand)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                          double, Expand)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                          int8_t, Expand)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                          int16_t, Expand)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                          int32_t, Expand)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                          int64_t, Expand)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                          uint8_t, Expand)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                          uint16_t, Expand)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                          uint32_t, Expand)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                          uint64_t, Expand)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                          bool, Expand)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                          MLFloat16, Expand)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 12,\r\n                                                                          string, Expand)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 8, Scan)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, If)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 10, Loop)>,\r\n\r\n    // Opset 9\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                    Compress)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 19,\r\n                                                                    ConstantOfShape)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                    MeanVarianceNormalization)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          float, Greater)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          double, Greater)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          int8_t, Greater)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          int16_t, Greater)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          int32_t, Greater)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          int64_t, Greater)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          uint8_t, Greater)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          uint16_t, Greater)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          uint32_t, Greater)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          uint64_t, Greater)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          float, Less)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          double, Less)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          int8_t, Less)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          int16_t, Less)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          int32_t, Less)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          int64_t, Less)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          uint8_t, Less)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          uint16_t, Less)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          uint32_t, Less)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          uint64_t, Less)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 21, EyeLike)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          float, IsNaN)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          double, IsNaN)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          MLFloat16, IsNaN)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12, Sign)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, Shrink)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          float, Erf)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                          int64_t_int64_t_int64_t, OneHot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                          float_int64_t_int64_t, OneHot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                          int64_t_string_int64_t, OneHot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                          float_string_int64_t, OneHot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                          float_float_float, OneHot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                          int64_t_int32_t_float, OneHot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                          int64_t_float_int64_t, OneHot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                          int32_t_float_int32_t, OneHot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                          int32_t_float_float, OneHot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                          int64_t_float_float, OneHot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                          int64_t_float_int32_t, OneHot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                    MaxUnpool)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 21, Sinh)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 21, Cosh)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 21, Asinh)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 21, Acosh)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 21, Atanh)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10, Scan)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                    Scatter)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, TfIdfVectorizer)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          bool, NonZero)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          float, NonZero)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          int32_t, NonZero)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          int64_t, NonZero)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          uint8_t, NonZero)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 15,\r\n                                                                          string, Where)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 15,\r\n                                                                          float, Where)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 15,\r\n                                                                          double, Where)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 15,\r\n                                                                          int32_t, Where)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 15,\r\n                                                                          int64_t, Where)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 15,\r\n                                                                          uint8_t, Where)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                    Flatten)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                          float, Gemm)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 10,\r\n                                                                          double, Gemm)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          float, MatMul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          double, MatMul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          int32_t, MatMul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 12,\r\n                                                                          int64_t, MatMul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 13,\r\n                                                                          float, BatchNormalization)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 13,\r\n                                                                          double, BatchNormalization)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 15, PRelu)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 9,\r\n                                                                          float, Upsample)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 9,\r\n                                                                          int32_t, Upsample)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 9,\r\n                                                                          int8_t, Upsample)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 9, 9,\r\n                                                                          uint8_t, Upsample)>,\r\n\r\n    // Opset 10\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, StringNormalizer)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10,\r\n                                                                          float, TopK)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10,\r\n                                                                          double, TopK)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10,\r\n                                                                    AveragePool)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 12, Mod)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10,\r\n                                                                          float, Resize)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10,\r\n                                                                          int32_t, Resize)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10,\r\n                                                                          int8_t, Resize)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10,\r\n                                                                          uint8_t, Resize)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 21, ThresholdedRelu)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 12,\r\n                                                                          uint8_t, DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 12,\r\n                                                                          int8_t, DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 12,\r\n                                                                          int32_t, DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 12,\r\n                                                                          uint8_t, QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 12,\r\n                                                                          int8_t, QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 20,\r\n                                                                          uint8_t, QLinearMatMul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 20,\r\n                                                                          int8_t, QLinearMatMul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, uint8_t,\r\n                                                                MatMulInteger)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, int8_t,\r\n                                                                MatMulInteger)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, ConvInteger)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, uint8_t,\r\n                                                                QLinearConv)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, int8_t,\r\n                                                                QLinearConv)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10, Slice)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 11,\r\n                                                                    Dropout)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 10,\r\n                                                                    NonMaxSuppression)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 19, IsInf)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 15,\r\n                                                                          float, RoiAlign)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, 15,\r\n                                                                          double, RoiAlign)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 10, ReverseSequence)>,\r\n    // opset 11\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11, Clip)>,\r\n\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 13,\r\n                                                                          float, CumSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 13,\r\n                                                                          double, CumSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 13,\r\n                                                                          int32_t, CumSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 13,\r\n                                                                          int64_t, CumSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          bool, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          int8_t, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          int16_t, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          int32_t, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          int64_t, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          uint8_t, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          uint16_t, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          uint32_t, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          uint64_t, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          float, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          double, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, float, Round)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, double, Round)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, MLFloat16,\r\n                                                                          Round)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, uint8_t,\r\n                                                                DynamicQuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          float, ArgMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          double, ArgMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          int8_t, ArgMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          uint8_t, ArgMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          int32_t, ArgMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          int64_t, ArgMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          float, ArgMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          double, ArgMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          int8_t, ArgMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          uint8_t, ArgMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          int32_t, ArgMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          int64_t, ArgMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, Loop)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                    Hardmax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          float, LogSoftmax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          double, LogSoftmax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          double, Softmax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          float, Softmax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                    DepthToSpace)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 15, Scan)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                    Flatten)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, Compress)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                    Concat)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                    Gather)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, Slice)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, Split)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                    Squeeze)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                    Unsqueeze)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, Det)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                    ScatterElements)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, NonMaxSuppression)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 18,\r\n                                                                    AveragePool)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, MaxUnpool)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 17,\r\n                                                                    LpPool)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, Conv)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, ConvTranspose)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, If)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, SequenceLength)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, SequenceAt)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, SequenceEmpty)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, SequenceInsert)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, SequenceErase)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, SequenceConstruct)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, ConcatFromSequence)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 23,\r\n                                                                    SplitToSequence)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                    ScatterND)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          float, Gemm)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          double, Gemm)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                    GatherElements)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, uint8_t,\r\n                                                                BitShift)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, uint32_t,\r\n                                                                BitShift)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, uint64_t,\r\n                                                                BitShift)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12, Pad)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11,\r\n                                                                    GatherND)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, Range)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, Unique)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 23,\r\n                                                                          float, TopK)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 23,\r\n                                                                          double, TopK)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 23,\r\n                                                                          int64_t, TopK)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 23,\r\n                                                                          int32_t, TopK)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11,\r\n                                                                int64_t_int64_t_int64_t, OneHot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11,\r\n                                                                float_int64_t_int64_t, OneHot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11,\r\n                                                                int64_t_string_int64_t, OneHot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11,\r\n                                                                float_string_int64_t, OneHot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11,\r\n                                                                float_float_float, OneHot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11,\r\n                                                                int64_t_int32_t_float, OneHot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11,\r\n                                                                int64_t_float_int64_t, OneHot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11,\r\n                                                                int32_t_float_int32_t, OneHot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11,\r\n                                                                int32_t_float_float, OneHot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11,\r\n                                                                int64_t_float_float, OneHot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11,\r\n                                                                int64_t_float_int32_t, OneHot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          float, Resize)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          int32_t, Resize)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          int8_t, Resize)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          uint8_t, Resize)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11,\r\n                                                                          float, ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11,\r\n                                                                          double, ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11,\r\n                                                                          int32_t, ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11,\r\n                                                                          int64_t, ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11,\r\n                                                                          float, ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11,\r\n                                                                          double, ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11,\r\n                                                                          int32_t, ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 11,\r\n                                                                          int64_t, ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          float, ReduceL1)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          double, ReduceL1)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          int32_t, ReduceL1)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          int64_t, ReduceL1)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          float, ReduceL2)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          double, ReduceL2)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          int32_t, ReduceL2)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          int64_t, ReduceL2)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          float, ReduceLogSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          double, ReduceLogSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          int32_t, ReduceLogSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          int64_t, ReduceLogSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          float, ReduceLogSumExp)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          double, ReduceLogSumExp)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          int32_t, ReduceLogSumExp)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          int64_t, ReduceLogSumExp)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          float, ReduceMean)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          double, ReduceMean)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          int32_t, ReduceMean)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          int64_t, ReduceMean)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          float, ReduceProd)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          double, ReduceProd)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          int32_t, ReduceProd)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          int64_t, ReduceProd)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          float, ReduceSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          int32_t, ReduceSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          double, ReduceSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          int64_t, ReduceSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          float, ReduceSumSquare)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          double, ReduceSumSquare)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          int32_t, ReduceSumSquare)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 12,\r\n                                                                          int64_t, ReduceSumSquare)>,\r\n\r\n    // OpSet 12\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, Clip)>,\r\n\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, Min)>,\r\n\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, Max)>,\r\n\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12, Pow)>,\r\n\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 21, MaxPool)>,\r\n\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                          float, ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                          double, ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                          int32_t, ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                          int64_t, ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                          int8_t, ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                          uint8_t, ReduceMax)>,\r\n\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                          float, ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                          double, ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                          int32_t, ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                          int64_t, ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                          int8_t, ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                          uint8_t, ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                    GatherND)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, Einsum)>,\r\n\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Expand)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double,\r\n                                                                Expand)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int8_t,\r\n                                                                Expand)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int16_t,\r\n                                                                Expand)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t,\r\n                                                                Expand)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t,\r\n                                                                Expand)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint8_t,\r\n                                                                Expand)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint16_t,\r\n                                                                Expand)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint32_t,\r\n                                                                Expand)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint64_t,\r\n                                                                Expand)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, bool, Expand)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, MLFloat16,\r\n                                                                Expand)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, string,\r\n                                                                Expand)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Erf)>,\r\n    // REVIEW(codemzs): ConstEigenVectorArrayMap.cast<MLFLoat16) does not seem to be supported.\r\n    // However these types work on GPU implementation.\r\n    // BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12,\r\n    // MLFloat16_MLFloat16, Dropout)>,\r\n    // BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12,\r\n    // MLFloat16_float, Dropout)>, BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider,\r\n    // kOnnxDomain, 12, MLFloat16_double, Dropout)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                          float_float, Dropout)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                          float_double, Dropout)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                          double_float, Dropout)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 12,\r\n                                                                          double_double, Dropout)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, Celu)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                          float, GreaterOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                          double, GreaterOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                          int8_t, GreaterOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                          int16_t, GreaterOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                          int32_t, GreaterOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                          int64_t, GreaterOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                          uint8_t, GreaterOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                          uint16_t, GreaterOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                          uint32_t, GreaterOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                          uint64_t, GreaterOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                          float, LessOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                          double, LessOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                          int8_t, LessOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                          int16_t, LessOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                          int32_t, LessOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                          int64_t, LessOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                          uint8_t, LessOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                          uint16_t, LessOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                          uint32_t, LessOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 15,\r\n                                                                          uint64_t, LessOrEqual)>,\r\n\r\n    // opset 13\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, Cast)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Clip)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, MatMul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double,\r\n                                                                MatMul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t,\r\n                                                                MatMul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t,\r\n                                                                MatMul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Min)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Max)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Mean)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Gemm)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Gemm)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Sign)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18, Size)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Sum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Sum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float,\r\n                                                                Sigmoid)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double,\r\n                                                                Sigmoid)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                          uint8_t, DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                          int8_t, DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                          int32_t, DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                          uint8_t, QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                          int8_t, QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 20,\r\n                                                                    Flatten)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, LRN)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13,\r\n                                                          MeanVarianceNormalization)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 21, float_float,\r\n                                                                          Dropout)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 21, float_double,\r\n                                                                          Dropout)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 21, double_float,\r\n                                                                          Dropout)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 21, double_double,\r\n                                                                          Dropout)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, ArgMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double,\r\n                                                                ArgMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int8_t,\r\n                                                                ArgMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint8_t,\r\n                                                                ArgMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t,\r\n                                                                ArgMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t,\r\n                                                                ArgMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, ArgMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double,\r\n                                                                ArgMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t,\r\n                                                                ArgMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t,\r\n                                                                ArgMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int8_t,\r\n                                                                ArgMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint8_t,\r\n                                                                ArgMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                    Reshape)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 14, Shape)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Concat)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                          bool, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                          int8_t, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                          int16_t, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                          int32_t, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                          int64_t, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                          uint8_t, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                          uint16_t, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                          uint32_t, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                          uint64_t, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                          float, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 18,\r\n                                                                          double, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float,\r\n                                                                Greater)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double,\r\n                                                                Greater)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int8_t,\r\n                                                                Greater)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int16_t,\r\n                                                                Greater)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t,\r\n                                                                Greater)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t,\r\n                                                                Greater)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint8_t,\r\n                                                                Greater)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint16_t,\r\n                                                                Greater)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint32_t,\r\n                                                                Greater)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint64_t,\r\n                                                                Greater)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Less)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Less)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13,\r\n                                                                int8_t, Less)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13,\r\n                                                                int16_t, Less)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13,\r\n                                                                int32_t, Less)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13,\r\n                                                                int64_t, Less)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13,\r\n                                                                uint8_t, Less)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13,\r\n                                                                uint16_t, Less)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13,\r\n                                                                uint32_t, Less)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13,\r\n                                                                uint64_t, Less)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                          float, Add)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                          double, Add)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                          int32_t, Add)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                          int64_t, Add)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                          uint32_t, Add)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                          uint64_t, Add)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                          float, Sub)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                          double, Sub)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                          int32_t, Sub)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                          int64_t, Sub)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                          uint32_t, Sub)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                          uint64_t, Sub)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                          float, Mul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                          double, Mul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                          int32_t, Mul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                          int64_t, Mul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                          uint32_t, Mul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                          uint64_t, Mul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                          float, Div)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                          double, Div)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                          int32_t, Div)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                          int64_t, Div)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                          uint32_t, Div)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                          uint64_t, Div)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Neg)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Neg)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int8_t, Neg)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int16_t, Neg)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t, Neg)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t, Neg)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Mod)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Abs)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Abs)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int8_t, Abs)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int16_t, Abs)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t, Abs)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t, Abs)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint8_t, Abs)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint16_t, Abs)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint32_t, Abs)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint64_t, Abs)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float,\r\n                                                                Reciprocal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double,\r\n                                                                Reciprocal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Floor)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Floor)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Ceil)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Ceil)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Sqrt)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Sqrt)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                          float, Relu)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                          double, Relu)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Tanh)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Tanh)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Exp)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Exp)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float, Log)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double, Log)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 14, Pow)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Slice)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, Split)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 20,\r\n                                                                    Unsqueeze)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 20,\r\n                                                                    Squeeze)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 20,\r\n                                                                    Transpose)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Tile)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Gather)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, GatherElements)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, DepthToSpace)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, SpaceToDepth)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 15,\r\n                                                                    ScatterElements)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 15,\r\n                                                                    ScatterND)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                    Identity)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 19,\r\n                                                                          float, IsNaN)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 19,\r\n                                                                          double, IsNaN)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 19,\r\n                                                                          MLFloat16, IsNaN)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, bool, NonZero)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float,\r\n                                                                NonZero)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t,\r\n                                                                NonZero)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t,\r\n                                                                NonZero)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, uint8_t,\r\n                                                                NonZero)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, GatherND)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17, Pad)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          float, ReduceL1)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          double, ReduceL1)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          int32_t, ReduceL1)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          int64_t, ReduceL1)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          float, ReduceL2)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          double, ReduceL2)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          int32_t, ReduceL2)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          int64_t, ReduceL2)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          float, ReduceLogSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          double, ReduceLogSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          int32_t, ReduceLogSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          int64_t, ReduceLogSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          float, ReduceLogSumExp)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          double, ReduceLogSumExp)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          int32_t, ReduceLogSumExp)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          int64_t, ReduceLogSumExp)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          float, ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          double, ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          int32_t, ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          int64_t, ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          int8_t, ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          uint8_t, ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          float, ReduceMean)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          double, ReduceMean)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          int32_t, ReduceMean)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          int64_t, ReduceMean)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          float, ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          double, ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          int32_t, ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          int64_t, ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          int8_t, ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          uint8_t, ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          float, ReduceProd)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          double, ReduceProd)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          int32_t, ReduceProd)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          int64_t, ReduceProd)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          float, ReduceSumSquare)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          int32_t, ReduceSumSquare)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          double, ReduceSumSquare)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          int64_t, ReduceSumSquare)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float,\r\n                                                                ReduceSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int32_t,\r\n                                                                ReduceSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double,\r\n                                                                ReduceSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, int64_t,\r\n                                                                ReduceSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          float, Resize)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          int32_t, Resize)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          int8_t, Resize)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 17,\r\n                                                                          uint8_t, Resize)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 15, Loop)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 15, If)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, Hardmax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float,\r\n                                                                LogSoftmax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double,\r\n                                                                LogSoftmax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, double,\r\n                                                                Softmax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, float,\r\n                                                                Softmax)>,\r\n\r\n    // OpSet 14\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, float, CumSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, double,\r\n                                                                CumSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int32_t,\r\n                                                                CumSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int64_t,\r\n                                                                CumSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, float, Relu)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, double, Relu)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int8_t, Relu)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int32_t, Relu)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, Trilu)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, float, Add)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, double, Add)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int8_t, Add)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int16_t, Add)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int32_t, Add)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int64_t, Add)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint8_t, Add)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint16_t, Add)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint32_t, Add)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint64_t, Add)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, float, Sub)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, double, Sub)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int8_t, Sub)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int16_t, Sub)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int32_t, Sub)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int64_t, Sub)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint8_t, Sub)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint16_t, Sub)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint32_t, Sub)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint64_t, Sub)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, float, Mul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, double, Mul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int8_t, Mul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int16_t, Mul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int32_t, Mul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int64_t, Mul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint8_t, Mul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint16_t, Mul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint32_t, Mul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint64_t, Mul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, float, Div)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, double, Div)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int8_t, Div)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int16_t, Div)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int32_t, Div)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, int64_t, Div)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint8_t, Div)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint16_t, Div)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint32_t, Div)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, uint64_t, Div)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, 18,\r\n                                                                    Reshape)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, 15,\r\n                                                                    Identity)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, 14,\r\n                                                                          float, BatchNormalization)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, 14,\r\n                                                                          double, BatchNormalization)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, 21, GRU)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, 21, LSTM)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, 21, RNN)>,\r\n\r\n    // Opset 15\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 15, Pow)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 15, float,\r\n                                                                BatchNormalization)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 15, double,\r\n                                                                BatchNormalization)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 15, 18, Shape)>,\r\n\r\n#if !defined(DISABLE_OPTIONAL_TYPE)\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 15, 17,\r\n                                                                    OptionalHasElement)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 15, 17,\r\n                                                                    OptionalGetElement)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 15, Optional)>,\r\n#endif\r\n\r\n    // Opset 16\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, 18,\r\n                                                                    Identity)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, 18, If)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, 18, Loop)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, float,\r\n                                                                RoiAlign)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, double,\r\n                                                                RoiAlign)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, 19,\r\n                                                                          float, GridSample)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, 17,\r\n                                                                    ScatterElements)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, 17,\r\n                                                                    ScatterND)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, string, Where)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, float, Where)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, double, Where)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int32_t,\r\n                                                                Where)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int64_t,\r\n                                                                Where)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint8_t,\r\n                                                                Where)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, LeakyRelu)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, PRelu)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, 18, Scan)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, float,\r\n                                                                GreaterOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, double,\r\n                                                                GreaterOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int8_t,\r\n                                                                GreaterOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int16_t,\r\n                                                                GreaterOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int32_t,\r\n                                                                GreaterOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int64_t,\r\n                                                                GreaterOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint8_t,\r\n                                                                GreaterOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint16_t,\r\n                                                                GreaterOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint32_t,\r\n                                                                GreaterOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint64_t,\r\n                                                                GreaterOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, float,\r\n                                                                LessOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, double,\r\n                                                                LessOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int8_t,\r\n                                                                LessOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int16_t,\r\n                                                                LessOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int32_t,\r\n                                                                LessOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, int64_t,\r\n                                                                LessOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint8_t,\r\n                                                                LessOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint16_t,\r\n                                                                LessOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint32_t,\r\n                                                                LessOrEqual)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, uint64_t,\r\n                                                                LessOrEqual)>,\r\n\r\n    // Opset 17\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, BlackmanWindow)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, 19, DFT)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, HammingWindow)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, HannWindow)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, MelWeightMatrix)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, STFT)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, float,\r\n                                                                LayerNormalization)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, double,\r\n                                                                LayerNormalization)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 17, MLFloat16,\r\n                                                                LayerNormalization)>,\r\n\r\n    // Opset 18\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 18,\r\n                                                                          float, Resize)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 18,\r\n                                                                          int32_t, Resize)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 18,\r\n                                                                          int8_t, Resize)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 18,\r\n                                                                          uint8_t, Resize)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, float,\r\n                                                                ReduceL1)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, double,\r\n                                                                ReduceL1)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t,\r\n                                                                ReduceL1)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t,\r\n                                                                ReduceL1)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, float,\r\n                                                                ReduceL2)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, double,\r\n                                                                ReduceL2)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t,\r\n                                                                ReduceL2)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t,\r\n                                                                ReduceL2)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, float,\r\n                                                                ReduceLogSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, double,\r\n                                                                ReduceLogSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t,\r\n                                                                ReduceLogSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t,\r\n                                                                ReduceLogSum)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, float,\r\n                                                                ReduceLogSumExp)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, double,\r\n                                                                ReduceLogSumExp)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t,\r\n                                                                ReduceLogSumExp)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t,\r\n                                                                ReduceLogSumExp)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, float,\r\n                                                                          ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, double,\r\n                                                                          ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, int32_t,\r\n                                                                          ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, int64_t,\r\n                                                                          ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, int8_t,\r\n                                                                          ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, uint8_t,\r\n                                                                          ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, float,\r\n                                                                ReduceMean)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, double,\r\n                                                                ReduceMean)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t,\r\n                                                                ReduceMean)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t,\r\n                                                                ReduceMean)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, float,\r\n                                                                          ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, double,\r\n                                                                          ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, int32_t,\r\n                                                                          ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, int64_t,\r\n                                                                          ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, int8_t,\r\n                                                                          ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 19, uint8_t,\r\n                                                                          ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, float,\r\n                                                                ReduceProd)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, double,\r\n                                                                ReduceProd)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t,\r\n                                                                ReduceProd)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t,\r\n                                                                ReduceProd)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, float,\r\n                                                                ReduceSumSquare)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t,\r\n                                                                ReduceSumSquare)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, double,\r\n                                                                ReduceSumSquare)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t,\r\n                                                                ReduceSumSquare)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 21, LpPool)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, Col2Im)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int8_t,\r\n                                                                BitwiseAnd)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int16_t,\r\n                                                                BitwiseAnd)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t,\r\n                                                                BitwiseAnd)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t,\r\n                                                                BitwiseAnd)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint8_t,\r\n                                                                BitwiseAnd)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint16_t,\r\n                                                                BitwiseAnd)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint32_t,\r\n                                                                BitwiseAnd)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint64_t,\r\n                                                                BitwiseAnd)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int8_t,\r\n                                                                BitwiseNot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int16_t,\r\n                                                                BitwiseNot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t,\r\n                                                                BitwiseNot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t,\r\n                                                                BitwiseNot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint8_t,\r\n                                                                BitwiseNot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint16_t,\r\n                                                                BitwiseNot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint32_t,\r\n                                                                BitwiseNot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint64_t,\r\n                                                                BitwiseNot)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int8_t,\r\n                                                                BitwiseOr)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int16_t,\r\n                                                                BitwiseOr)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t,\r\n                                                                BitwiseOr)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t,\r\n                                                                BitwiseOr)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint8_t,\r\n                                                                BitwiseOr)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint16_t,\r\n                                                                BitwiseOr)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint32_t,\r\n                                                                BitwiseOr)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint64_t,\r\n                                                                BitwiseOr)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int8_t,\r\n                                                                BitwiseXor)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int16_t,\r\n                                                                BitwiseXor)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int32_t,\r\n                                                                BitwiseXor)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, int64_t,\r\n                                                                BitwiseXor)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint8_t,\r\n                                                                BitwiseXor)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint16_t,\r\n                                                                BitwiseXor)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint32_t,\r\n                                                                BitwiseXor)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, uint64_t,\r\n                                                                BitwiseXor)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, 18, Pad)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, ScatterND)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, ScatterElements)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, Split)>,\r\n#if !defined(DISABLE_OPTIONAL_TYPE)\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, OptionalHasElement)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 18, OptionalGetElement)>,\r\n#endif\r\n\r\n    // Opset 19\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Size)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 21, AveragePool)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Cast)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                          uint8_t, DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                          int8_t, DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                          int32_t, DequantizeLinear)>,\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                          Float8E4M3FN, DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                          Float8E4M3FNUZ, DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                          Float8E5M2, DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                          Float8E5M2FNUZ, DequantizeLinear)>,\r\n#endif\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, bool, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, int8_t,\r\n                                                                Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, int16_t,\r\n                                                                Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, int32_t,\r\n                                                                Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, int64_t,\r\n                                                                Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, uint8_t,\r\n                                                                Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, uint16_t,\r\n                                                                Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, uint32_t,\r\n                                                                Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, uint64_t,\r\n                                                                Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, float, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, double, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, string, Equal)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                    Identity)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, If)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Loop)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Pad)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                          uint8_t, QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                          int8_t, QuantizeLinear)>,\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                          Float8E4M3FN, QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                          Float8E4M3FNUZ, QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                          Float8E5M2, QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                          Float8E5M2FNUZ, QuantizeLinear)>,\r\n#endif\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20,\r\n                                                                    Reshape)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, float, Resize)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, int32_t,\r\n                                                                Resize)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, int8_t,\r\n                                                                Resize)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, uint8_t,\r\n                                                                Resize)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Scan)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 20, Shape)>,\r\n\r\n    // Opset 20\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, 20,\r\n                                                                    ConstantOfShape)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, bool, ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, float, ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, double, ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, int32_t, ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, int64_t, ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, int8_t, ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, uint8_t, ReduceMax)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, bool, ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, float, ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, double, ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, int32_t, ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, int64_t, ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, int8_t, ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, uint8_t, ReduceMin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, DFT)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, 21, float,\r\n                                                                          GridSample)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, 21, double,\r\n                                                                          GridSample)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, float,\r\n                                                                AffineGrid)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, double,\r\n                                                                AffineGrid)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, float, IsNaN)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, double, IsNaN)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, MLFloat16,\r\n                                                                IsNaN)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, BFloat16,\r\n                                                                IsNaN)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, Gelu)>,\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, Float8E4M3FN,\r\n                                                                IsNaN)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, Float8E4M3FNUZ,\r\n                                                                IsNaN)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, Float8E5M2,\r\n                                                                IsNaN)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, Float8E5M2FNUZ,\r\n                                                                IsNaN)>,\r\n#endif\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, IsInf)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, StringConcat)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, RegexFullMatch)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 20, StringSplit)>,\r\n\r\n    // Opset 21\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Cast)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, ConstantOfShape)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Identity)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Reshape)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Scan)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Shape)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Size)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Flatten)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Squeeze)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Transpose)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Unsqueeze)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, If)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Loop)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Pad)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, uint8_t,\r\n                                                                          DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, int8_t,\r\n                                                                          DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, uint16_t,\r\n                                                                          DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, int16_t,\r\n                                                                          DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, int32_t,\r\n                                                                          DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Int4x2,\r\n                                                                          DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, UInt4x2,\r\n                                                                          DequantizeLinear)>,\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E4M3FN,\r\n                                                                          DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E4M3FNUZ,\r\n                                                                          DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E5M2,\r\n                                                                          DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E5M2FNUZ,\r\n                                                                          DequantizeLinear)>,\r\n#endif\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, uint8_t,\r\n                                                                          QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, int8_t,\r\n                                                                          QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, uint16_t,\r\n                                                                          QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, int16_t,\r\n                                                                          QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Int4x2,\r\n                                                                          QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, UInt4x2,\r\n                                                                          QuantizeLinear)>,\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E4M3FN,\r\n                                                                          QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E4M3FNUZ,\r\n                                                                          QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E5M2,\r\n                                                                          QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, 22, Float8E5M2FNUZ,\r\n                                                                          QuantizeLinear)>,\r\n#endif\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, uint8_t,\r\n                                                                QLinearMatMul)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 21, int8_t,\r\n                                                                QLinearMatMul)>,\r\n\r\n    // Opset 22\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Cos)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Tan)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Asin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Acos)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Atan)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Sinh)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Cosh)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Asinh)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Acosh)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Atanh)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Conv)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, ConvTranspose)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Det)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, EyeLike)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, GlobalAveragePool)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, GlobalMaxPool)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, float_float,\r\n                                                                Dropout)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, float_double,\r\n                                                                Dropout)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, double_float,\r\n                                                                Dropout)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, double_double,\r\n                                                                Dropout)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, float, GridSample)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, double, GridSample)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, GRU)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, LSTM)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, RNN)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Elu)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, HardSigmoid)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, InstanceNormalization)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, LpPool)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, MaxPool)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, MaxUnpool)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Softplus)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, float, Round)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, double, Round)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, MLFloat16,\r\n                                                                Round)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Selu)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, float, Sin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, double, Sin)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, Softsign)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, ThresholdedRelu)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, AveragePool)>,\r\n\r\n    // Opset 23\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23,\r\n                                                                          float, Attention)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23,\r\n                                                                          MLFloat16, Attention)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Cast)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, ConstantOfShape)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, uint8_t,\r\n                                                                          DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, int8_t,\r\n                                                                          DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, uint16_t,\r\n                                                                          DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, int16_t,\r\n                                                                          DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, int32_t,\r\n                                                                          DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Int4x2,\r\n                                                                          DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, UInt4x2,\r\n                                                                          DequantizeLinear)>,\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E4M3FN,\r\n                                                                          DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E4M3FNUZ,\r\n                                                                          DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E5M2,\r\n                                                                          DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E5M2FNUZ,\r\n                                                                          DequantizeLinear)>,\r\n#endif\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, uint8_t,\r\n                                                                          QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, int8_t,\r\n                                                                          QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, uint16_t,\r\n                                                                          QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, int16_t,\r\n                                                                          QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Int4x2,\r\n                                                                          QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, UInt4x2,\r\n                                                                          QuantizeLinear)>,\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E4M3FN,\r\n                                                                          QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E4M3FNUZ,\r\n                                                                          QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E5M2,\r\n                                                                          QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Float8E5M2FNUZ,\r\n                                                                          QuantizeLinear)>,\r\n#endif\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Identity)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Reshape)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Scan)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Shape)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Size)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Flatten)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Squeeze)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Transpose)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Unsqueeze)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, If)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Loop)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, 23, Pad)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, float,\r\n                                                                RMSNormalization)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, double,\r\n                                                                RMSNormalization)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, MLFloat16,\r\n                                                                RMSNormalization)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, float,\r\n                                                                RotaryEmbedding)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 23, MLFloat16,\r\n                                                                RotaryEmbedding)>,\r\n\r\n    // opset 24\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, float, Attention)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, MLFloat16, Attention)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Cast)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, ConstantOfShape)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, uint8_t,\r\n                                                                DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, int8_t,\r\n                                                                DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, uint16_t,\r\n                                                                DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, int16_t,\r\n                                                                DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, int32_t,\r\n                                                                DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Int4x2,\r\n                                                                DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, UInt4x2,\r\n                                                                DequantizeLinear)>,\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E4M3FN,\r\n                                                                DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E4M3FNUZ,\r\n                                                                DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E5M2,\r\n                                                                DequantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E5M2FNUZ,\r\n                                                                DequantizeLinear)>,\r\n#endif\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, uint8_t,\r\n                                                                QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, int8_t,\r\n                                                                QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, uint16_t,\r\n                                                                QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, int16_t,\r\n                                                                QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Int4x2,\r\n                                                                QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, UInt4x2,\r\n                                                                QuantizeLinear)>,\r\n#if !defined(DISABLE_FLOAT8_TYPES)\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E4M3FN,\r\n                                                                QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E4M3FNUZ,\r\n                                                                QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E5M2,\r\n                                                                QuantizeLinear)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Float8E5M2FNUZ,\r\n                                                                QuantizeLinear)>,\r\n#endif\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Flatten)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Identity)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, If)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Loop)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Pad)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Reshape)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Scan)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Shape)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Size)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Squeeze)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Transpose)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, Unsqueeze)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, SplitToSequence)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, float, TopK)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, double, TopK)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, int64_t, TopK)>,\r\n    BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 24, int32_t, TopK)>,\r\n  };\r\n  for (auto& function_table_entry : function_table) {\r\n    KernelCreateInfo info = function_table_entry();\r\n    if (info.kernel_def != nullptr) {  // filter disabled entries where type is void\r\n      ORT_RETURN_IF_ERROR(kernel_registry.Register(std::move(info)));\r\n    }\r\n  }\r\n\r\n  return Status::OK();\r\n}\r\n\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\nStatus RegisterFp16Kernels(KernelRegistry& kernel_registry) {\r\n  static const BuildKernelCreateInfoFn function_table[] = {\r\n      BuildKernelCreateInfo<void>,  // default entry to avoid the list become empty after ops-reducing\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 1, 21, MLFloat16,\r\n                                                                            GlobalAveragePool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, MLFloat16,\r\n                                                                  GlobalAveragePool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 21, MLFloat16,\r\n                                                                            Conv)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, MLFloat16,\r\n                                                                  Conv)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 11, 18,\r\n                                                                            MLFloat16, AveragePool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 19, 21, MLFloat16,\r\n                                                                            AveragePool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22, MLFloat16,\r\n                                                                  AveragePool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 8, 11,\r\n                                                                            MLFloat16, MaxPool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 12, 21, MLFloat16,\r\n                                                                            MaxPool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 22,\r\n                                                                  MLFloat16, MaxPool)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 12,\r\n                                                                            MLFloat16, Relu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, 13,\r\n                                                                            MLFloat16, Relu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, MLFloat16,\r\n                                                                  Relu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 6, 15,\r\n                                                                            MLFloat16, LeakyRelu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 16, MLFloat16,\r\n                                                                  LeakyRelu)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 13, MLFloat16, Erf)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, MLFloat16, Add)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, MLFloat16, Sub)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, MLFloat16, Mul)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kOnnxDomain, 14, MLFloat16, Div)>,\r\n  };\r\n\r\n  for (auto& function_table_entry : function_table) {\r\n    KernelCreateInfo info = function_table_entry();\r\n    if (info.kernel_def != nullptr) {  // filter disabled entries where type is void\r\n      ORT_RETURN_IF_ERROR(kernel_registry.Register(std::move(info)));\r\n    }\r\n  }\r\n\r\n  return Status::OK();\r\n}\r\n#endif\r\n\r\n// Forward declarations of ml op kernels\r\n#ifndef DISABLE_ML_OPS\r\nnamespace ml {\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, float, ArrayFeatureExtractor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, double, ArrayFeatureExtractor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int32_t, ArrayFeatureExtractor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int64_t, ArrayFeatureExtractor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, string, ArrayFeatureExtractor);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, Binarizer);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, CastMap);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, CategoryMapper);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, string_int64_t, DictVectorizer);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, string_float, DictVectorizer);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, string_double, DictVectorizer);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int64_t_string, DictVectorizer);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int64_t_float, DictVectorizer);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int64_t_double, DictVectorizer);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, FeatureVectorizer);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, Imputer);\r\n\r\nclass ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, 1, LabelEncoder);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, LinearClassifier);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, LinearRegressor);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, Normalizer);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int64_t, OneHotEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, float, OneHotEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, double, OneHotEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, string, OneHotEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, float, Scaler);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, double, Scaler);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int64_t, Scaler);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int32_t, Scaler);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, SVMClassifier);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, SVMRegressor);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, 2, float,\r\n                                                      TreeEnsembleClassifier);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, 2, double,\r\n                                                      TreeEnsembleClassifier);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, 2, int64_t,\r\n                                                      TreeEnsembleClassifier);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, 2, int32_t,\r\n                                                      TreeEnsembleClassifier);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, 2, float,\r\n                                                      TreeEnsembleRegressor);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, 2, double,\r\n                                                      TreeEnsembleRegressor);\r\nclass ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, ZipMap);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3, float_string,\r\n                                                      LabelEncoder);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3, string_float,\r\n                                                      LabelEncoder);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3, int64_float,\r\n                                                      LabelEncoder);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3, float_int64,\r\n                                                      LabelEncoder);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3, int64_string,\r\n                                                      LabelEncoder);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3, string_int64,\r\n                                                      LabelEncoder);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3, int64_int64,\r\n                                                      LabelEncoder);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3, string_string,\r\n                                                      LabelEncoder);\r\nclass ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3, float_float,\r\n                                                      LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 3, float, TreeEnsembleClassifier);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 3, double, TreeEnsembleClassifier);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 3, int64_t, TreeEnsembleClassifier);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 3, int32_t, TreeEnsembleClassifier);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 3, float, TreeEnsembleRegressor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 3, double, TreeEnsembleRegressor);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 5, float, TreeEnsemble);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 5, double, TreeEnsemble);\r\n\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, float_string, LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, string_float, LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, int64_float, LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, float_int64, LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, int64_string, LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, string_int64, LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, int64_int64, LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, string_string, LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, float_float, LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, string_int16, LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, double_string, LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, string_double, LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, int64_double, LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, double_int64, LabelEncoder);\r\nclass ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, double_double, LabelEncoder);\r\n\r\ntemplate <>\r\nKernelCreateInfo BuildKernelCreateInfo<void>() {\r\n  KernelCreateInfo info;\r\n  return info;\r\n}\r\n\r\nStatus RegisterOnnxMLOperatorKernels(KernelRegistry& kernel_registry) {\r\n  static const BuildKernelCreateInfoFn function_table[] = {\r\n      BuildKernelCreateInfo<void>,  // default entry to avoid the list become empty after ops-reducing\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, float,\r\n                                                                  ArrayFeatureExtractor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, double,\r\n                                                                  ArrayFeatureExtractor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int32_t,\r\n                                                                  ArrayFeatureExtractor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int64_t,\r\n                                                                  ArrayFeatureExtractor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, string,\r\n                                                                  ArrayFeatureExtractor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, Binarizer)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, CastMap)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, CategoryMapper)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, string_int64_t,\r\n                                                                  DictVectorizer)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, string_float,\r\n                                                                  DictVectorizer)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, string_double,\r\n                                                                  DictVectorizer)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int64_t_string,\r\n                                                                  DictVectorizer)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int64_t_float,\r\n                                                                  DictVectorizer)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int64_t_double,\r\n                                                                  DictVectorizer)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, FeatureVectorizer)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, Imputer)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, 1,\r\n                                                                      LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, LinearClassifier)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, LinearRegressor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, Normalizer)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int64_t,\r\n                                                                  OneHotEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, float,\r\n                                                                  OneHotEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, double,\r\n                                                                  OneHotEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, string,\r\n                                                                  OneHotEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, float, Scaler)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, double, Scaler)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int64_t,\r\n                                                                  Scaler)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, int32_t,\r\n                                                                  Scaler)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, SVMClassifier)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, SVMRegressor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, 2,\r\n                                                                            float, TreeEnsembleClassifier)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, 2,\r\n                                                                            double, TreeEnsembleClassifier)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, 2,\r\n                                                                            int64_t, TreeEnsembleClassifier)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, 2,\r\n                                                                            int32_t, TreeEnsembleClassifier)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, 2,\r\n                                                                            float, TreeEnsembleRegressor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, 2,\r\n                                                                            double, TreeEnsembleRegressor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 1, ZipMap)>,\r\n\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3,\r\n                                                                            float_string, LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3,\r\n                                                                            string_float, LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3,\r\n                                                                            int64_float, LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3,\r\n                                                                            float_int64, LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3,\r\n                                                                            int64_string, LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3,\r\n                                                                            string_int64, LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3,\r\n                                                                            int64_int64, LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3,\r\n                                                                            string_string, LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_VERSIONED_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 2, 3,\r\n                                                                            float_float, LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 3, float,\r\n                                                                  TreeEnsembleClassifier)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 3, double,\r\n                                                                  TreeEnsembleClassifier)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 3, int64_t,\r\n                                                                  TreeEnsembleClassifier)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 3, int32_t,\r\n                                                                  TreeEnsembleClassifier)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 3, float,\r\n                                                                  TreeEnsembleRegressor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 3, double,\r\n                                                                  TreeEnsembleRegressor)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 5, float,\r\n                                                                  TreeEnsemble)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 5, double,\r\n                                                                  TreeEnsemble)>,\r\n\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, float_string,\r\n                                                                  LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, string_float,\r\n                                                                  LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, int64_float,\r\n                                                                  LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, float_int64,\r\n                                                                  LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, int64_string,\r\n                                                                  LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, string_int64,\r\n                                                                  LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, int64_int64,\r\n                                                                  LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, string_string,\r\n                                                                  LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, float_float,\r\n                                                                  LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, string_int16,\r\n                                                                  LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, double_string,\r\n                                                                  LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, string_double,\r\n                                                                  LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, int64_double,\r\n                                                                  LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, double_int64,\r\n                                                                  LabelEncoder)>,\r\n      BuildKernelCreateInfo<ONNX_OPERATOR_TYPED_KERNEL_CLASS_NAME(kCpuExecutionProvider, kMLDomain, 4, double_double,\r\n                                                                  LabelEncoder)>,\r\n  };\r\n\r\n  for (auto& function_table_entry : function_table) {\r\n    KernelCreateInfo info = function_table_entry();\r\n    if (info.kernel_def != nullptr) {  // filter disabled entries where type is void\r\n      ORT_RETURN_IF_ERROR(kernel_registry.Register(std::move(info)));\r\n    }\r\n  }\r\n\r\n  return Status::OK();\r\n}\r\n}  // namespace ml\r\n#endif\r\n\r\nStatus RegisterCPUKernels(KernelRegistry& kernel_registry) {\r\n  ORT_RETURN_IF_ERROR(RegisterOnnxOperatorKernels(kernel_registry));\r\n#ifdef MLAS_F16VEC_INTRINSICS_SUPPORTED\r\n  if (MlasFp16AccelerationSupported()) {\r\n    ORT_RETURN_IF_ERROR(RegisterFp16Kernels(kernel_registry));\r\n  }\r\n#endif\r\n#ifndef DISABLE_ML_OPS\r\n  ORT_RETURN_IF_ERROR(::onnxruntime::ml::RegisterOnnxMLOperatorKernels(kernel_registry));\r\n#endif\r\n#ifndef DISABLE_CONTRIB_OPS\r\n  ORT_RETURN_IF_ERROR(::onnxruntime::contrib::RegisterCpuContribKernels(kernel_registry));\r\n#endif\r\n#if defined(ENABLE_TRAINING_OPS)\r\n  ORT_RETURN_IF_ERROR(::onnxruntime::contrib::RegisterCpuTrainingKernels(kernel_registry));\r\n#endif\r\n  return Status::OK();\r\n}\r\n\r\nKernelRegistryAndStatus GetCpuKernelRegistry() {\r\n  KernelRegistryAndStatus ret;\r\n  ret.st = RegisterCPUKernels(*ret.kernel_registry);\r\n  return ret;\r\n}\r\n\r\nstd::shared_ptr<KernelRegistry> CPUExecutionProvider::GetKernelRegistry() const {\r\n  static KernelRegistryAndStatus k = GetCpuKernelRegistry();\r\n  // throw if the registry failed to initialize\r\n  ORT_THROW_IF_ERROR(k.st);\r\n  return k.kernel_registry;\r\n}\r\n\r\nstd::unique_ptr<IDataTransfer> CPUExecutionProvider::GetDataTransfer() const {\r\n  return std::make_unique<CPUDataTransfer>();\r\n}\r\n\r\n}  // namespace onnxruntime\r\n"}
{"path":"\\\\?\\C:\\Users\\Naomi\\Desktop\\ONNX\\onnxruntime\\onnxruntime\\core\\providers\\webgpu\\webgpu_context.cc","line":null,"char":null,"code":"CLANGFORMAT","severity":"warning","name":"format","description":"See https://clang.llvm.org/docs/ClangFormat.html.\nRun `lintrunner -a` to apply this patch.","original":"// Copyright (c) Microsoft Corporation. All rights reserved.\r\n// Licensed under the MIT License.\r\n\r\n#include <memory>\r\n#include <cmath>\r\n\r\n#if defined(__GNUC__)\r\n#pragma GCC diagnostic push\r\n#pragma GCC diagnostic ignored \"-Wstrict-aliasing\"\r\n#endif\r\n\r\n#if !defined(__wasm__)\r\n#if !defined(BUILD_DAWN_SHARED_LIBRARY)\r\n#include \"dawn/dawn_proc.h\"\r\n#endif\r\n#if !defined(USE_EXTERNAL_DAWN)\r\n#include \"dawn/native/DawnNative.h\"\r\n#endif\r\n#endif\r\n#if defined(__GNUC__)\r\n#pragma GCC diagnostic pop\r\n#endif\r\n\r\n#include \"core/common/common.h\"\r\n#include \"core/common/path_string.h\"\r\n#include \"core/platform/env.h\"\r\n\r\n#include \"core/providers/webgpu/compute_context.h\"\r\n#include \"core/providers/webgpu/webgpu_context.h\"\r\n#include \"core/providers/webgpu/buffer_manager.h\"\r\n#include \"core/providers/webgpu/webgpu_execution_provider.h\"\r\n#include \"core/providers/webgpu/program.h\"\r\n#include \"core/providers/webgpu/program_cache_key.h\"\r\n#include \"core/providers/webgpu/program_manager.h\"\r\n#include \"core/providers/webgpu/string_macros.h\"\r\n\r\nnamespace onnxruntime {\r\nnamespace webgpu {\r\n\r\nvoid WebGpuContext::Initialize(const WebGpuBufferCacheConfig& buffer_cache_config, int backend_type, bool enable_pix_capture) {\r\n  std::call_once(init_flag_, [this, &buffer_cache_config, backend_type, enable_pix_capture]() {\r\n    if (device_ == nullptr) {\r\n      // Create wgpu::Adapter\r\n      wgpu::RequestAdapterOptions req_adapter_options = {};\r\n      req_adapter_options.backendType = static_cast<wgpu::BackendType>(backend_type);\r\n      req_adapter_options.powerPreference = static_cast<wgpu::PowerPreference>(power_preference_);\r\n\r\n#if !defined(__wasm__)\r\n      auto enabled_adapter_toggles = GetEnabledAdapterToggles();\r\n\r\n      wgpu::DawnTogglesDescriptor adapter_toggles_desc = {};\r\n      adapter_toggles_desc.enabledToggleCount = enabled_adapter_toggles.size();\r\n      adapter_toggles_desc.enabledToggles = enabled_adapter_toggles.data();\r\n\r\n      req_adapter_options.nextInChain = &adapter_toggles_desc;\r\n#endif\r\n\r\n      wgpu::Adapter adapter;\r\n      ORT_ENFORCE(wgpu::WaitStatus::Success == instance_.WaitAny(instance_.RequestAdapter(\r\n                                                                     &req_adapter_options,\r\n                                                                     wgpu::CallbackMode::WaitAnyOnly,\r\n                                                                     [](wgpu::RequestAdapterStatus status, wgpu::Adapter adapter, wgpu::StringView message, wgpu::Adapter* ptr) {\r\n                                                                       ORT_ENFORCE(status == wgpu::RequestAdapterStatus::Success, \"Failed to get a WebGPU adapter: \", std::string_view{message});\r\n                                                                       *ptr = std::move(adapter);\r\n                                                                     },\r\n                                                                     &adapter),\r\n                                                                 UINT64_MAX));\r\n      ORT_ENFORCE(adapter != nullptr, \"Failed to get a WebGPU adapter.\");\r\n\r\n      // Create wgpu::Device\r\n      wgpu::DeviceDescriptor device_desc = {};\r\n\r\n#if !defined(__wasm__)\r\n      wgpu::DawnTogglesDescriptor device_toggles_desc = {};\r\n      device_desc.nextInChain = &device_toggles_desc;\r\n\r\n      auto enabled_device_toggles = GetEnabledDeviceToggles();\r\n      device_toggles_desc.enabledToggleCount = enabled_device_toggles.size();\r\n      device_toggles_desc.enabledToggles = enabled_device_toggles.data();\r\n\r\n      auto disabled_device_toggles = GetDisabledDeviceToggles();\r\n      device_toggles_desc.disabledToggleCount = disabled_device_toggles.size();\r\n      device_toggles_desc.disabledToggles = disabled_device_toggles.data();\r\n#endif\r\n\r\n      std::vector<wgpu::FeatureName> required_features = GetAvailableRequiredFeatures(adapter);\r\n      if (required_features.size() > 0) {\r\n        device_desc.requiredFeatures = required_features.data();\r\n        device_desc.requiredFeatureCount = required_features.size();\r\n      }\r\n      wgpu::Limits required_limits = GetRequiredLimits(adapter);\r\n      device_desc.requiredLimits = &required_limits;\r\n\r\n      // TODO: revise temporary error handling\r\n      device_desc.SetUncapturedErrorCallback([](const wgpu::Device& /*device*/, wgpu::ErrorType type, wgpu::StringView message) {\r\n        LOGS_DEFAULT(ERROR) << \"WebGPU device error(\" << int(type) << \"): \" << std::string_view{message};\r\n      });\r\n      // TODO: revise temporary device lost handling\r\n      device_desc.SetDeviceLostCallback(wgpu::CallbackMode::AllowSpontaneous, [](const wgpu::Device& /*device*/, wgpu::DeviceLostReason reason, wgpu::StringView message) {\r\n        LOGS_DEFAULT(INFO) << \"WebGPU device lost (\" << int(reason) << \"): \" << std::string_view{message};\r\n      });\r\n\r\n      ORT_ENFORCE(wgpu::WaitStatus::Success == instance_.WaitAny(adapter.RequestDevice(\r\n                                                                     &device_desc,\r\n                                                                     wgpu::CallbackMode::WaitAnyOnly,\r\n                                                                     [](wgpu::RequestDeviceStatus status, wgpu::Device device, wgpu::StringView message, wgpu::Device* ptr) {\r\n                                                                       ORT_ENFORCE(status == wgpu::RequestDeviceStatus::Success, \"Failed to get a WebGPU device: \", std::string_view{message});\r\n                                                                       *ptr = std::move(device);\r\n                                                                     },\r\n                                                                     &device_),\r\n                                                                 UINT64_MAX));\r\n      ORT_ENFORCE(device_ != nullptr, \"Failed to get a WebGPU device.\");\r\n    }\r\n\r\n    LOGS_DEFAULT(VERBOSE) << \"WebGPU EP Context is created for: Instance=\" << instance_.Get() << \", Device=\" << device_.Get() << \".\";\r\n\r\n    // cache device queue\r\n    device_queue_ = device_.GetQueue();\r\n    // cache device limits\r\n    ORT_ENFORCE(Device().GetLimits(&device_limits_));\r\n    // cache device features\r\n    wgpu::SupportedFeatures supported_features;\r\n    Device().GetFeatures(&supported_features);\r\n    for (size_t i = 0; i < supported_features.featureCount; i++) {\r\n      device_features_.insert(supported_features.features[i]);\r\n    }\r\n    // cache adapter info\r\n#if !defined(__wasm__)\r\n    if (DeviceHasFeature(wgpu::FeatureName::ChromiumExperimentalSubgroupMatrix)) {\r\n      adapter_info_.nextInChain = &subgroup_matrix_configs_;\r\n    }\r\n#endif\r\n    ORT_ENFORCE(Device().GetAdapterInfo(&adapter_info_));\r\n\r\n    // create buffer manager\r\n    buffer_mgr_ = BufferManagerFactory::Create(*this,\r\n                                               buffer_cache_config.storage.mode,\r\n                                               buffer_cache_config.uniform.mode,\r\n                                               buffer_cache_config.query_resolve.mode);\r\n\r\n    // create initializer buffer manager. cache is always disabled for initializer buffer manager\r\n    initializer_buffer_mgr_ = BufferManagerFactory::Create(*this,\r\n                                                           BufferCacheMode::Disabled,\r\n                                                           BufferCacheMode::Disabled,\r\n                                                           BufferCacheMode::Disabled);\r\n\r\n    // create program manager\r\n    program_mgr_ = std::make_unique<ProgramManager>(*this);\r\n\r\n    // set query type\r\n#if !defined(__wasm__)\r\n    if (device_.HasFeature(wgpu::FeatureName::ChromiumExperimentalTimestampQueryInsidePasses)) {\r\n      query_type_ = TimestampQueryType::InsidePasses;\r\n    } else\r\n#endif\r\n        if (device_.HasFeature(wgpu::FeatureName::TimestampQuery)) {\r\n      query_type_ = TimestampQueryType::AtPasses;\r\n    } else {\r\n      query_type_ = TimestampQueryType::None;\r\n    }\r\n    if (enable_pix_capture) {\r\n#if defined(ENABLE_PIX_FOR_WEBGPU_EP)\r\n      // set pix frame generator\r\n      pix_frame_generator_ = std::make_unique<WebGpuPIXFrameGenerator>(instance_,\r\n                                                                       Device());\r\n#else\r\n    ORT_THROW(\"Support PIX capture requires extra build flags (--enable_pix_capture)\");\r\n#endif  // ENABLE_PIX_FOR_WEBGPU_EP\r\n    }\r\n  });\r\n}\r\n\r\nStatus WebGpuContext::Wait(wgpu::Future f) {\r\n  auto status = instance_.WaitAny(f, UINT64_MAX);\r\n  if (status == wgpu::WaitStatus::Success) {\r\n    return Status::OK();\r\n  }\r\n  return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, \"Failed to wait for the operation:\", uint32_t(status));\r\n}\r\n\r\nStatus WebGpuContext::Run(ComputeContext& context, ProgramBase& program) {\r\n  // Finalize program inputs by adding the indirect buffer as the last input if needed.\r\n  if (program.IndirectDispatchTensor() != nullptr) {\r\n    program.AddInput({program.IndirectDispatchTensor(), ProgramTensorMetadataDependency::None});\r\n  }\r\n\r\n  const auto& inputs = program.Inputs();\r\n  const auto& outputs = program.Outputs();\r\n\r\n  if (outputs.size() == 0) {\r\n    return Status::OK();\r\n  }\r\n\r\n  if (ValidationMode() >= ValidationMode::Basic) {\r\n    ORT_ENFORCE(std::all_of(inputs.begin(), inputs.end(), [](const ProgramInput& input) {\r\n                  const auto* tensor = input.tensor;\r\n                  return tensor != nullptr &&\r\n                         tensor->Location().mem_type == OrtMemType::OrtMemTypeDefault &&\r\n                         tensor->Location().device.Type() == OrtDevice::GPU &&\r\n                         !strcmp(tensor->Location().name.c_str(), WEBGPU_BUFFER);\r\n                }),\r\n                \"All inputs must be tensors on WebGPU buffers.\");\r\n\r\n    ORT_ENFORCE(std::all_of(outputs.begin(), outputs.end(), [](const ProgramOutput& output) {\r\n                  const auto* tensor = output.tensor;\r\n                  return tensor != nullptr &&\r\n                         tensor->Location().mem_type == OrtMemType::OrtMemTypeDefault &&\r\n                         tensor->Location().device.Type() == OrtDevice::GPU &&\r\n                         !strcmp(tensor->Location().name.c_str(), WEBGPU_BUFFER);\r\n                }),\r\n                \"All outputs must be tensors on WebGPU buffers.\");\r\n  }\r\n\r\n  const ProgramMetadata& metadata = program.Metadata();\r\n\r\n  // validate program metadata\r\n  if (ValidationMode() >= ValidationMode::Basic) {\r\n    const auto& [constants, overridable_constants, uniform_variables] = metadata;\r\n\r\n    // check overridable constants\r\n    ORT_RETURN_IF(program.OverridableConstants().size() != overridable_constants.size(),\r\n                  \"Size of overridable constants mismatch in program \\\"\", program.Name(),\r\n                  \"\\\", Expected: \", overridable_constants.size(),\r\n                  \", Actual: \", program.OverridableConstants().size());\r\n\r\n    if (ValidationMode() >= ValidationMode::Full) {\r\n      size_t num_overridable_constants = program.OverridableConstants().size();\r\n      for (size_t i = 0; i < num_overridable_constants; ++i) {\r\n        const auto& override_value = program.OverridableConstants()[i];\r\n        const auto& definition = overridable_constants[i];\r\n        ORT_RETURN_IF(override_value.has_value && override_value.type != definition.type,\r\n                      \"Overridable override_value[\", i, \"] (\", definition.name, \") data type mismatch in program \\\"\", program.Name(),\r\n                      \"\\\", Expected: \", definition.type,\r\n                      \", Actual: \", override_value.type);\r\n        ORT_RETURN_IF(!override_value.has_value && !definition.has_default_value,\r\n                      \"Overridable override_value[\", i, \"] (\", definition.name, \") no override_value specified in program \\\"\", program.Name(),\r\n                      \"\\\"\");\r\n      }\r\n    }\r\n\r\n    // check uniform variables\r\n    ORT_RETURN_IF(program.UniformVariables().size() != uniform_variables.size(),\r\n                  \"Size of uniform_value variables mismatch in program \\\"\", program.Name(),\r\n                  \"\\\", Expected: \", uniform_variables.size(),\r\n                  \", Actual: \", program.UniformVariables().size());\r\n\r\n    if (ValidationMode() >= ValidationMode::Full) {\r\n      size_t num_uniform_variables = program.UniformVariables().size();\r\n      for (size_t i = 0; i < num_uniform_variables; ++i) {\r\n        const auto& uniform_value = program.UniformVariables()[i];\r\n        const auto& definition = uniform_variables[i];\r\n        ORT_RETURN_IF(uniform_value.length > 0 && uniform_value.data_type != definition.data_type,\r\n                      \"Uniform variable[\", i, \"] (\", definition.name, \") data type mismatch in program \\\"\", program.Name(),\r\n                      \"\\\", Expected: \", definition.data_type,\r\n                      \", Actual: \", uniform_value.data_type);\r\n      }\r\n    }\r\n  }\r\n\r\n  uint32_t x = program.DispatchGroupSizeX();\r\n  uint32_t y = program.DispatchGroupSizeY();\r\n  uint32_t z = program.DispatchGroupSizeZ();\r\n\r\n  // Skip normalization for indirect dispatch since dimensions are determined by the indirect buffer\r\n  if (program.IndirectDispatchTensor() == nullptr) {\r\n    ORT_RETURN_IF_ERROR(program_mgr_->NormalizeDispatchGroupSize(x, y, z));\r\n  } else {\r\n    ORT_ENFORCE(x == 0 && y == 0 && z == 0,\r\n                \"Only one of SetIndirectDispatchTensor and SetDispatchGroupSize should be called for program\", program.Name());\r\n  }\r\n  ORT_RETURN_IF_ERROR(program_mgr_->CalculateSegmentsForInputsAndOutputs(program));\r\n\r\n  bool is_1d_dispatch = (y == 1 && z == 1);\r\n\r\n  auto key = CalculateProgramCacheKey(program, is_1d_dispatch);\r\n\r\n  if (is_profiling_) {\r\n    PendingKernelInfo pending_kernel_info(context.KernelContext().GetNodeName(),\r\n                                          context.KernelContext().GetOpType(),\r\n                                          program.Name(),\r\n                                          key,\r\n                                          inputs,\r\n                                          outputs);\r\n    pending_kernels_.emplace_back(std::move(pending_kernel_info));\r\n  }\r\n\r\n  LOGS(context.Logger(), INFO) << \"Starting program \\\"\" << key << \"\\\" (\" << x << \", \" << y << \", \" << z << \")\";\r\n\r\n  const auto* program_artifact = program_mgr_->Get(key);\r\n  if (program_artifact == nullptr) {\r\n    wgpu::ComputePipeline compute_pipeline;\r\n    std::vector<int> shape_uniform_ranks;\r\n    auto status = program_mgr_->Build(program,\r\n                                      metadata,\r\n#ifndef NDEBUG  // if debug build\r\n                                      key,\r\n#endif\r\n                                      x,\r\n                                      y,\r\n                                      z,\r\n                                      compute_pipeline,\r\n                                      shape_uniform_ranks);\r\n    ORT_RETURN_IF_ERROR(status);\r\n    program_artifact = program_mgr_->Set(key, ProgramArtifact{program,\r\n                                                              std::move(compute_pipeline),\r\n                                                              std::move(shape_uniform_ranks)});\r\n#ifndef NDEBUG  // if debug build\r\n    ORT_ENFORCE(program_artifact != nullptr, \"Program artifact should not be nullptr.\");\r\n#endif\r\n  }\r\n\r\n  // prepare shape uniforms for shader variables (if any) and user defined uniforms\r\n  std::vector<ProgramUniformVariableValue> shape_uniforms;\r\n  shape_uniforms.reserve(program_artifact->shape_uniform_ranks.size() * 2);\r\n  if (ValidationMode() >= ValidationMode::Basic) {\r\n    ORT_RETURN_IF_NOT(program_artifact->shape_uniform_ranks.size() == inputs.size() + outputs.size() + program.Indices().size(),\r\n                      \"Invalid program artifact: variable size (\", program_artifact->shape_uniform_ranks.size(),\r\n                      \") does not match current program (input: \", inputs.size(),\r\n                      \", output: \", outputs.size(),\r\n                      \", indices: \", program.Indices().size(), \")\");\r\n  }\r\n\r\n  auto append_shape_uniforms = [&shape_uniforms, program_artifact](size_t i, const TensorShape& shape) {\r\n    if (program_artifact->shape_uniform_ranks[i] > 0) {\r\n      size_t expected_rank = static_cast<size_t>(program_artifact->shape_uniform_ranks[i]);\r\n      ORT_RETURN_IF(expected_rank != shape.NumDimensions(),\r\n                    \"Invalid program artifact: variable[\", i, \"] rank mismatch. Expected: \", expected_rank,\r\n                    \", Actual: \", shape.NumDimensions());\r\n\r\n      std::vector<uint32_t> dims(expected_rank);\r\n      std::vector<uint32_t> stride(expected_rank - 1);\r\n      for (size_t j = 0; j < expected_rank; ++j) {\r\n        dims[j] = onnxruntime::narrow<uint32_t>(shape[j]);\r\n        if (j < expected_rank - 1) {\r\n          stride[j] = onnxruntime::narrow<uint32_t>(shape.SizeFromDimension(j + 1));\r\n        }\r\n      }\r\n\r\n      shape_uniforms.emplace_back(gsl::make_span(dims));\r\n      if (expected_rank > 1) {\r\n        shape_uniforms.emplace_back(gsl::make_span(stride));\r\n      }\r\n    }\r\n    return Status::OK();\r\n  };\r\n\r\n  for (size_t i = 0; i < inputs.size(); i++) {\r\n    ORT_RETURN_IF_ERROR(append_shape_uniforms(i,\r\n                                              inputs[i].use_override_shape ? inputs[i].override_shape : inputs[i].tensor->Shape()));\r\n  }\r\n  for (size_t i = 0; i < outputs.size(); i++) {\r\n    ORT_RETURN_IF_ERROR(append_shape_uniforms(i + inputs.size(),\r\n                                              outputs[i].use_override_shape ? outputs[i].override_shape : outputs[i].tensor->Shape()));\r\n  }\r\n  for (size_t i = 0; i < program.Indices().size(); i++) {\r\n    ORT_RETURN_IF_ERROR(append_shape_uniforms(i + inputs.size() + outputs.size(), program.Indices()[i]));\r\n  }\r\n\r\n  const size_t uniform_count = shape_uniforms.size() + program.UniformVariables().size();\r\n  size_t current_offset = 0;\r\n  std::vector<std::tuple<const ProgramUniformVariableValue&, size_t>> uniform_and_offsets;\r\n  uniform_and_offsets.reserve(uniform_count);\r\n  for (size_t i = 0; i < uniform_count; i++) {\r\n    const auto& uniform = i < shape_uniforms.size() ? shape_uniforms[i]\r\n                                                    : program.UniformVariables()[i - shape_uniforms.size()];\r\n    size_t length = uniform.length;\r\n    if (length == 0) {  // skip zero-length uniform\r\n      continue;\r\n    }\r\n\r\n    // Calculate the size and alignment of the uniform variable.\r\n    //\r\n    // https://www.w3.org/TR/WGSL/#alignof\r\n    //\r\n    // For f16:\r\n    // - length > 8      : array<vec4<u32>, N>   (align 16) (size 16 * N, N = ceil(length / 8))\r\n    // - length == 7 or 8: vec4<u32>             (align 16) (size 16)\r\n    // - length == 5 or 6: vec3<u32>             (align 16) (size 12)\r\n    // - length == 3 or 4: vec2<u32>             (align 8)  (size 8)\r\n    // - length == 1 or 2: u32                   (align 4)  (size 4)\r\n    //\r\n    // For other types (i32, u32, f32):\r\n    // - length > 4      : array<vec4<T>, N>     (align 16) (size 16 * N, N = ceil(length / 4))\r\n    // - length == 4     : vec4<T>               (align 16) (size 16)\r\n    // - length == 3     : vec3<T>               (align 16) (size 12)\r\n    // - length == 2     : vec2<T>               (align 8)  (size 8)\r\n    // - length == 1     : T                     (align 4)  (size 4)\r\n    //\r\n\r\n    const bool is_f16 = uniform.data_type == ProgramUniformVariableDataType::Float16;\r\n\r\n    size_t variable_alignment = 4;  // default alignment for scalar types\r\n    size_t variable_size = 4;       // default size for scalar types\r\n\r\n    if (is_f16) {\r\n      if (length > 6) {\r\n        variable_alignment = 16;\r\n        variable_size = 16 * ((length + 7) / 8);\r\n      } else if (length > 4) {\r\n        variable_alignment = 16;\r\n        variable_size = 12;\r\n      } else if (length > 2) {\r\n        variable_alignment = 8;\r\n        variable_size = 8;\r\n      }\r\n    } else {\r\n      if (length > 3) {\r\n        variable_alignment = 16;\r\n        variable_size = 16 * ((length + 3) / 4);\r\n      } else if (length > 2) {\r\n        variable_alignment = 16;\r\n        variable_size = 12;\r\n      } else if (length > 1) {\r\n        variable_alignment = 8;\r\n        variable_size = 8;\r\n      }\r\n    }\r\n    current_offset = (current_offset + variable_alignment - 1) / variable_alignment * variable_alignment;\r\n    uniform_and_offsets.emplace_back(uniform, current_offset);\r\n\r\n    current_offset += variable_size;\r\n  }\r\n\r\n  // Meet alignment of struct here: https://www.w3.org/TR/WGSL/#alignment-and-size. For simplicity, set\r\n  // max_alignment_of_field to 16 since the underlying buffer has been rounded up to 16.\r\n  constexpr size_t max_alignment_of_field = 16;\r\n  const size_t uniform_buffer_total_size = (current_offset + max_alignment_of_field - 1) / max_alignment_of_field * max_alignment_of_field;\r\n\r\n  WGPUBuffer uniform_buffer = nullptr;\r\n  const webgpu::BufferManager& buffer_mgr = context.BufferManager();\r\n  if (uniform_buffer_total_size > 0) {\r\n    std::vector<uint8_t> uniform_data_buffer(uniform_buffer_total_size);\r\n\r\n    for (auto const& [uniform, offset] : uniform_and_offsets) {\r\n      memcpy(uniform_data_buffer.data() + offset, uniform.data.data(), uniform.data.size());\r\n    }\r\n\r\n    uniform_buffer = buffer_mgr.Create(uniform_buffer_total_size, wgpu::BufferUsage::CopyDst | wgpu::BufferUsage::Uniform);\r\n    device_queue_.WriteBuffer(uniform_buffer, 0, uniform_data_buffer.data(), uniform_buffer_total_size);\r\n  }\r\n\r\n  const auto& compute_pass_encoder = GetComputePassEncoder();\r\n\r\n  WriteTimestamp(num_pending_dispatches_ * 2);\r\n\r\n  const size_t total_buffer_count = inputs.size() + outputs.size() + (uniform_buffer ? 1 : 0);\r\n\r\n  std::vector<WGPUBuffer> bind_buffers;\r\n  std::vector<uint32_t> bind_buffers_segments;\r\n  bind_buffers.reserve(total_buffer_count);\r\n  bind_buffers_segments.reserve(total_buffer_count);\r\n  for (const auto& input : inputs) {\r\n    bind_buffers.push_back(reinterpret_cast<WGPUBuffer>(const_cast<void*>(input.tensor->DataRaw())));\r\n    bind_buffers_segments.push_back(input.segments);\r\n  }\r\n  for (const auto& output : outputs) {\r\n    bind_buffers.push_back(reinterpret_cast<WGPUBuffer>(output.tensor->MutableDataRaw()));\r\n    bind_buffers_segments.push_back(output.segments);\r\n  }\r\n  if (uniform_buffer) {\r\n    bind_buffers.push_back(uniform_buffer);\r\n    bind_buffers_segments.push_back(1);  // uniform buffer defaults to 1 segment\r\n  }\r\n\r\n  LaunchComputePipeline(compute_pass_encoder, bind_buffers, bind_buffers_segments, *program_artifact, x, y, z, program.IndirectDispatchTensor());\r\n  if (uniform_buffer) {\r\n    buffer_mgr.Release(uniform_buffer);\r\n  }\r\n\r\n  WriteTimestamp(num_pending_dispatches_ * 2 + 1);\r\n  ++num_pending_dispatches_;\r\n\r\n  if (num_pending_dispatches_ >= max_num_pending_dispatches_ ||\r\n      (is_profiling_ && query_type_ == TimestampQueryType::AtPasses)) {\r\n    EndComputePass();\r\n  }\r\n  if (num_pending_dispatches_ >= max_num_pending_dispatches_) {\r\n    Flush(buffer_mgr);\r\n    num_pending_dispatches_ = 0;\r\n  }\r\n\r\n  return Status::OK();\r\n}\r\n\r\nstd::vector<const char*> WebGpuContext::GetEnabledAdapterToggles() const {\r\n  // See the description of all the toggles in toggles.cpp\r\n  // \"use_dxc\" for Shader Model 6+ features (e.g. float16)\r\n  // \"allow_unsafe_apis\" for chromium experimental features\r\n  constexpr const char* toggles[] = {\r\n      \"use_dxc\",\r\n      \"allow_unsafe_apis\",\r\n#if defined(DAWN_ENABLE_VULKAN)\r\n      \"use_vulkan_memory_model\",\r\n#endif\r\n  };\r\n  return std::vector<const char*>(std::begin(toggles), std::end(toggles));\r\n}\r\n\r\nstd::vector<const char*> WebGpuContext::GetEnabledDeviceToggles() const {\r\n  // Enable / disable other toggles that may affect the performance.\r\n  // Other toggles that may be useful: \"dump_shaders\", \"disable_symbol_renaming\"\r\n  constexpr const char* toggles[] = {\r\n      \"skip_validation\",  // only use \"skip_validation\" when ValidationMode is set to \"Disabled\"\r\n      \"disable_robustness\",\r\n      \"d3d_disable_ieee_strictness\",\r\n  };\r\n  return std::vector<const char*>(ValidationMode() >= ValidationMode::WGPUOnly\r\n                                      ? std::begin(toggles) + 1\r\n                                      : std::begin(toggles),\r\n                                  std::end(toggles));\r\n}\r\n\r\nstd::vector<const char*> WebGpuContext::GetDisabledDeviceToggles() const {\r\n  constexpr const char* toggles[] = {\r\n      \"lazy_clear_resource_on_first_use\",\r\n      \"timestamp_quantization\",\r\n  };\r\n  return std::vector<const char*>(std::begin(toggles), std::end(toggles));\r\n}\r\n\r\nstd::vector<wgpu::FeatureName> WebGpuContext::GetAvailableRequiredFeatures(const wgpu::Adapter& adapter) const {\r\n  std::vector<wgpu::FeatureName> required_features;\r\n  constexpr wgpu::FeatureName features[]{\r\n#if !defined(__wasm__)\r\n      wgpu::FeatureName::ChromiumExperimentalTimestampQueryInsidePasses,\r\n      wgpu::FeatureName::ChromiumExperimentalSubgroupMatrix,\r\n#endif\r\n      wgpu::FeatureName::TimestampQuery,\r\n      wgpu::FeatureName::ShaderF16,\r\n      wgpu::FeatureName::Subgroups,\r\n#if !defined(__wasm__)\r\n      wgpu::FeatureName::BufferMapExtendedUsages,\r\n#endif\r\n  };\r\n  for (auto feature : features) {\r\n    if (adapter.HasFeature(feature)) {\r\n      required_features.push_back(feature);\r\n    }\r\n  }\r\n  return required_features;\r\n}\r\n\r\nwgpu::Limits WebGpuContext::GetRequiredLimits(const wgpu::Adapter& adapter) const {\r\n  wgpu::Limits required_limits{};\r\n  wgpu::Limits adapter_limits;\r\n  ORT_ENFORCE(adapter.GetLimits(&adapter_limits));\r\n\r\n  required_limits.maxBindGroups = adapter_limits.maxBindGroups;\r\n  required_limits.maxComputeWorkgroupStorageSize = adapter_limits.maxComputeWorkgroupStorageSize;\r\n  required_limits.maxComputeWorkgroupsPerDimension = adapter_limits.maxComputeWorkgroupsPerDimension;\r\n  required_limits.maxStorageBuffersPerShaderStage = adapter_limits.maxStorageBuffersPerShaderStage;\r\n\r\n  if (small_storage_buffer_binding_size_for_testing_) {\r\n    // No matter how small it is set, the minimum storage buffer binding size in WebGPU is 128 MB.\r\n    required_limits.maxStorageBufferBindingSize = 134217728;\r\n  } else {\r\n    required_limits.maxStorageBufferBindingSize = adapter_limits.maxStorageBufferBindingSize;\r\n  }\r\n\r\n  required_limits.maxBufferSize = adapter_limits.maxBufferSize;\r\n  required_limits.maxComputeInvocationsPerWorkgroup = adapter_limits.maxComputeInvocationsPerWorkgroup;\r\n  required_limits.maxComputeWorkgroupSizeX = adapter_limits.maxComputeWorkgroupSizeX;\r\n  required_limits.maxComputeWorkgroupSizeY = adapter_limits.maxComputeWorkgroupSizeY;\r\n  required_limits.maxComputeWorkgroupSizeZ = adapter_limits.maxComputeWorkgroupSizeZ;\r\n\r\n  return required_limits;\r\n}\r\n\r\nvoid WebGpuContext::WriteTimestamp(uint32_t query_index) {\r\n  if (!is_profiling_ || query_type_ != TimestampQueryType::InsidePasses) {\r\n    return;\r\n  }\r\n\r\n  const auto& compute_pass_encoder = GetComputePassEncoder();\r\n  compute_pass_encoder.WriteTimestamp(query_set_, query_index);\r\n}\r\n\r\nvoid WebGpuContext::StartProfiling() {\r\n  if (query_type_ == TimestampQueryType::None) {\r\n    return;\r\n  }\r\n\r\n  is_profiling_ = true;\r\n\r\n  const uint32_t query_count = max_num_pending_dispatches_ * 2;\r\n\r\n  if (!query_set_) {\r\n    // Create query set\r\n    wgpu::QuerySetDescriptor querySetDescriptor;\r\n    querySetDescriptor.count = query_count;\r\n    querySetDescriptor.type = wgpu::QueryType::Timestamp;\r\n    query_set_ = device_.CreateQuerySet(&querySetDescriptor);\r\n  }\r\n\r\n  if (!query_resolve_buffer_) {\r\n    // Create resolve buffer\r\n    wgpu::BufferDescriptor bufferDescriptor;\r\n    bufferDescriptor.size = query_count * sizeof(uint64_t);\r\n    bufferDescriptor.usage = wgpu::BufferUsage::QueryResolve | wgpu::BufferUsage::CopySrc |\r\n                             wgpu::BufferUsage::CopyDst;\r\n    query_resolve_buffer_ = device_.CreateBuffer(&bufferDescriptor);\r\n  }\r\n}\r\n\r\nvoid WebGpuContext::CollectProfilingData(profiling::Events& events) {\r\n  if (!pending_queries_.empty()) {\r\n    for (const auto& pending_query : pending_queries_) {\r\n      const auto& pending_kernels = pending_query.kernels;\r\n      const auto& query_read_buffer = pending_query.query_buffer;\r\n\r\n      ORT_ENFORCE(Wait(query_read_buffer.MapAsync(wgpu::MapMode::Read,\r\n                                                  0,\r\n                                                  static_cast<size_t>(query_read_buffer.GetSize()),\r\n                                                  wgpu::CallbackMode::WaitAnyOnly,\r\n                                                  [](wgpu::MapAsyncStatus status, wgpu::StringView message) {\r\n                                                    ORT_ENFORCE(status == wgpu::MapAsyncStatus::Success, \"Failed to download data from buffer: \", std::string_view{message});\r\n                                                  })) == Status::OK());\r\n      auto mapped_data = static_cast<const uint64_t*>(query_read_buffer.GetConstMappedRange());\r\n\r\n      for (size_t i = 0; i < pending_kernels.size(); i++) {\r\n        const PendingKernelInfo& pending_kernel_info = pending_kernels[i];\r\n        const auto& inputs = pending_kernel_info.inputs;\r\n        const auto& outputs = pending_kernel_info.outputs;\r\n\r\n        SS(shapes, 128);\r\n        for (size_t s = 0; s < inputs.size(); s++) {\r\n          const auto& input = inputs[s];\r\n          shapes << \"inputs[\" << s << \"] = \" << input.override_shape.ToString() << \" \";\r\n        }\r\n        for (size_t s = 0; s < outputs.size(); s++) {\r\n          const auto& output = outputs[s];\r\n          shapes << \"outputs[\" << s << \"] = \" << output.override_shape.ToString() << \" \";\r\n        }\r\n\r\n        if (gpu_timestamp_offset_ == 0) {\r\n          gpu_timestamp_offset_ = mapped_data[i * 2];\r\n          // TODO: apply CPU-GPU time offset so that timestamps are aligned\r\n        }\r\n        uint64_t start_time = mapped_data[i * 2] - gpu_timestamp_offset_;\r\n        uint64_t end_time = mapped_data[i * 2 + 1] - gpu_timestamp_offset_;\r\n\r\n        const std::unordered_map<std::string, std::string>& event_args = {\r\n            {\"shapes\", SS_GET(shapes)},\r\n            {\"cache_key\", pending_kernel_info.cache_key},\r\n        };\r\n\r\n        profiling::EventRecord event(profiling::API_EVENT,\r\n                                     -1,\r\n                                     -1,\r\n                                     pending_kernel_info.name,\r\n                                     static_cast<int64_t>(std::round(start_time / 1000.0)),\r\n                                     static_cast<int64_t>(std::round((end_time - start_time) / 1000.0)),\r\n                                     event_args);\r\n        events.emplace_back(std::move(event));\r\n      }\r\n\r\n      query_read_buffer.Unmap();\r\n      query_read_buffer.Destroy();\r\n    }\r\n\r\n    pending_queries_.clear();\r\n  }\r\n\r\n  is_profiling_ = false;\r\n}\r\n\r\nvoid WebGpuContext::EndProfiling(TimePoint /* tp */, profiling::Events& events, profiling::Events& cached_events) {\r\n  // This function is called when no active inference is ongoing.\r\n  ORT_ENFORCE(!is_profiling_, \"Profiling is ongoing in an inference run.\");\r\n\r\n  if (query_type_ != TimestampQueryType::None) {\r\n    // No pending kernels or queries should be present at this point. They should have been collected in CollectProfilingData.\r\n    ORT_ENFORCE(pending_kernels_.empty() && pending_queries_.empty(), \"Pending kernels or queries are not empty.\");\r\n\r\n    events.insert(events.end(),\r\n                  std::make_move_iterator(cached_events.begin()),\r\n                  std::make_move_iterator(cached_events.end()));\r\n\r\n    cached_events.clear();\r\n  } else {\r\n    LOGS_DEFAULT(WARNING) << \"TimestampQuery is not supported in this device.\";\r\n  }\r\n}\r\n\r\nvoid WebGpuContext::PushErrorScope() { device_.PushErrorScope(wgpu::ErrorFilter::Validation); }\r\n\r\nStatus WebGpuContext::PopErrorScope() {\r\n  Status status{};\r\n  ORT_RETURN_IF_ERROR(Wait(device_.PopErrorScope(\r\n      wgpu::CallbackMode::WaitAnyOnly,\r\n      [](wgpu::PopErrorScopeStatus pop_status, wgpu::ErrorType error_type, char const* message, Status* status) {\r\n        ORT_ENFORCE(pop_status == wgpu::PopErrorScopeStatus::Success, \"Instance dropped.\");\r\n        if (error_type == wgpu::ErrorType::NoError) {\r\n          return;\r\n        }\r\n        *status = ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, \"WebGPU validation failed. \", message);\r\n      },\r\n      &status)));\r\n  return status;\r\n}\r\n\r\nvoid WebGpuContext::Flush(const webgpu::BufferManager& buffer_mgr) {\r\n  if (!current_command_encoder_) {\r\n    return;\r\n  }\r\n\r\n  EndComputePass();\r\n\r\n  if (is_profiling_ && num_pending_dispatches_ > 0) {\r\n    uint32_t query_count = num_pending_dispatches_ * 2;\r\n    current_command_encoder_.ResolveQuerySet(\r\n        query_set_,\r\n        0,\r\n        query_count,\r\n        query_resolve_buffer_,\r\n        0);\r\n\r\n    wgpu::BufferDescriptor bufferDescriptor;\r\n    bufferDescriptor.size = query_count * sizeof(uint64_t);\r\n    bufferDescriptor.usage = wgpu::BufferUsage::MapRead | wgpu::BufferUsage::CopyDst;\r\n    wgpu::Buffer query_read_buffer = device_.CreateBuffer(&bufferDescriptor);\r\n\r\n    current_command_encoder_.CopyBufferToBuffer(\r\n        query_resolve_buffer_,\r\n        0,\r\n        query_read_buffer,\r\n        0,\r\n        query_count * sizeof(uint64_t));\r\n\r\n    pending_queries_.emplace_back(std::move(pending_kernels_), query_read_buffer);\r\n    pending_kernels_.clear();\r\n  }\r\n  auto command_buffer = current_command_encoder_.Finish();\r\n  device_queue_.Submit(1, &command_buffer);\r\n  if (graph_capture_state_ != GraphCaptureState::Replaying) {\r\n    buffer_mgr.RefreshPendingBuffers(graph_capture_state_);\r\n  }\r\n  current_command_encoder_ = nullptr;\r\n  num_pending_dispatches_ = 0;\r\n}\r\n\r\nvoid WebGpuContext::OnRunEnd() {\r\n#if defined(ENABLE_PIX_FOR_WEBGPU_EP)\r\n  if (pix_frame_generator_) {\r\n    pix_frame_generator_->GeneratePIXFrame();\r\n  }\r\n#endif  // ENABLE_PIX_FOR_WEBGPU_EP\r\n}\r\n\r\nvoid WebGpuContext::LaunchComputePipeline(const wgpu::ComputePassEncoder& compute_pass_encoder,\r\n                                          const std::vector<WGPUBuffer>& bind_buffers,\r\n                                          const std::vector<uint32_t>& bind_buffers_segments,\r\n                                          const ProgramArtifact& program_artifact,\r\n                                          uint32_t x, uint32_t y, uint32_t z,\r\n                                          const Tensor* indirect_dispatch_tensor) {\r\n  uint32_t entry_index = 0;\r\n  std::vector<WGPUBindGroupEntry> bind_group_entries;\r\n\r\n  for (size_t buffer_idx = 0; buffer_idx < bind_buffers.size(); ++buffer_idx) {\r\n    WGPUBuffer buffer = bind_buffers[buffer_idx];\r\n    uint64_t buffer_size = wgpuBufferGetSize(buffer);\r\n    const uint64_t kMaxBufferSize = device_limits_.maxStorageBufferBindingSize;\r\n    const uint32_t total_segments = bind_buffers_segments[buffer_idx];\r\n    // `total_segments` we used is calculated by tensor size, not actual buffer size. Because for bucketed buffer,\r\n    // the actual buffer size may be larger than the tensor size, an extreme case is that tensor size = 127MB, buffer size = 256MB,\r\n    // maxStorageBufferBindingSize = 128MB, in this case we only need to bind 1 segment instead of 2 segments because\r\n    // there is no data for the second segment.\r\n    if (total_segments > 1) {\r\n      uint64_t offset = 0;\r\n      for (uint32_t segment = 0; segment < total_segments; ++segment) {\r\n        uint64_t segment_size = std::min(kMaxBufferSize, buffer_size - offset);\r\n        bind_group_entries.push_back({nullptr, entry_index++, buffer, offset, segment_size, nullptr, nullptr});\r\n        offset += segment_size;\r\n      }\r\n    } else {\r\n      bind_group_entries.push_back({nullptr, entry_index++, buffer, 0, std::min(kMaxBufferSize, buffer_size), nullptr, nullptr});\r\n    }\r\n  }\r\n\r\n  ORT_ENFORCE(entry_index < device_limits_.maxBindingsPerBindGroup, \"Number of bind group entries (\", entry_index,\r\n              \") exceeds device limit (\", device_limits_.maxBindingsPerBindGroup, \").\");\r\n\r\n  WGPUBindGroupLayout bind_group_layout = program_artifact.compute_pipeline.GetBindGroupLayout(0).MoveToCHandle();\r\n  WGPUBindGroupDescriptor bind_group_desc{};\r\n  bind_group_desc.layout = bind_group_layout;\r\n  bind_group_desc.entryCount = bind_group_entries.size();\r\n  bind_group_desc.entries = bind_group_entries.data();\r\n  bind_group_desc.label = {program_artifact.name.data(), program_artifact.name.length()};\r\n\r\n  auto bind_group = wgpuDeviceCreateBindGroup(Device().Get(), &bind_group_desc);\r\n  if (graph_capture_state_ == GraphCaptureState::Capturing) {\r\n    WGPUBuffer indirect_buffer = nullptr;\r\n    if (indirect_dispatch_tensor != nullptr) {\r\n      indirect_buffer = reinterpret_cast<WGPUBuffer>(const_cast<void*>(indirect_dispatch_tensor->DataRaw()));\r\n    }\r\n    external_captured_commands_->push_back({program_artifact.compute_pipeline,\r\n                                            bind_group,\r\n                                            bind_group_layout,\r\n                                            {x, y, z},\r\n                                            indirect_buffer});\r\n  } else {\r\n    compute_pass_encoder.SetPipeline(program_artifact.compute_pipeline);\r\n    wgpuComputePassEncoderSetBindGroup(compute_pass_encoder.Get(), 0, bind_group, 0, nullptr);\r\n\r\n    if (indirect_dispatch_tensor != nullptr) {\r\n      // Use indirect dispatch\r\n      WGPUBuffer indirect_buffer = reinterpret_cast<WGPUBuffer>(const_cast<void*>(indirect_dispatch_tensor->DataRaw()));\r\n      compute_pass_encoder.DispatchWorkgroupsIndirect(indirect_buffer, 0);\r\n    } else {\r\n      // Use direct dispatch\r\n      compute_pass_encoder.DispatchWorkgroups(x, y, z);\r\n    }\r\n\r\n    wgpuBindGroupRelease(bind_group);\r\n    wgpuBindGroupLayoutRelease(bind_group_layout);\r\n  }\r\n}\r\n\r\nvoid WebGpuContext::CaptureBegin(std::vector<webgpu::CapturedCommandInfo>* captured_commands, const webgpu::BufferManager& buffer_manager) {\r\n  LOGS_DEFAULT(VERBOSE) << \"CaptureBegin with external storage\";\r\n  // Flush any pending commands before we change the status\r\n  Flush(buffer_manager);\r\n\r\n  external_captured_commands_ = captured_commands;\r\n\r\n  // Make sure the external vector is empty before we start capturing\r\n  if (external_captured_commands_) {\r\n    external_captured_commands_->clear();\r\n  }\r\n\r\n  // TODO: support profiling with graph capture.\r\n  ORT_ENFORCE(!is_profiling_, \"profiling is not supported yet under graph capture mode\");\r\n\r\n  graph_capture_state_ = GraphCaptureState::Capturing;\r\n}\r\n\r\nvoid WebGpuContext::Replay(const std::vector<webgpu::CapturedCommandInfo>& captured_commands, const webgpu::BufferManager& buffer_manager) {\r\n  LOGS_DEFAULT(VERBOSE) << \"Replay with external storage\";\r\n  graph_capture_state_ = GraphCaptureState::Replaying;\r\n  // Replay all captured commands from the provided vector\r\n  const size_t command_count = captured_commands.size();\r\n  for (size_t i = 0; i < command_count; ++i) {\r\n    auto& command = captured_commands[i];\r\n    const auto& compute_pass_encoder = GetComputePassEncoder();\r\n    WriteTimestamp(num_pending_dispatches_ * 2);\r\n    compute_pass_encoder.SetPipeline(command.compute_pipeline);\r\n    wgpuComputePassEncoderSetBindGroup(compute_pass_encoder.Get(), 0, command.bind_group, 0, nullptr);\r\n\r\n    if (command.indirect_buffer != nullptr) {\r\n      // Use indirect dispatch\r\n      compute_pass_encoder.DispatchWorkgroupsIndirect(command.indirect_buffer, 0);\r\n    } else {\r\n      // Use direct dispatch\r\n      compute_pass_encoder.DispatchWorkgroups(command.dispatch_group[0], command.dispatch_group[1], command.dispatch_group[2]);\r\n    }\r\n\r\n    WriteTimestamp(num_pending_dispatches_ * 2 + 1);\r\n    ++num_pending_dispatches_;\r\n    if (num_pending_dispatches_ >= max_num_pending_dispatches_ ||\r\n        (is_profiling_ && query_type_ == TimestampQueryType::AtPasses)) {\r\n      EndComputePass();\r\n    }\r\n    if (num_pending_dispatches_ >= max_num_pending_dispatches_) {\r\n      Flush(buffer_manager);\r\n      num_pending_dispatches_ = 0;\r\n    }\r\n  }\r\n\r\n  // Flush any remaining commands\r\n  Flush(buffer_manager);\r\n\r\n  graph_capture_state_ = GraphCaptureState::Default;\r\n}\r\n\r\nvoid WebGpuContext::CaptureEnd() {\r\n  LOGS_DEFAULT(VERBOSE) << \"CaptureEnd\";\r\n\r\n  graph_capture_state_ = GraphCaptureState::Default;\r\n  external_captured_commands_ = nullptr;\r\n}\r\n\r\nvoid WebGpuContext::ReleaseGraphResources(std::vector<webgpu::CapturedCommandInfo>& captured_commands) {\r\n  LOGS_DEFAULT(VERBOSE) << \"ReleaseGraphResources: Releasing \" << captured_commands.size() << \" captured command resources\";\r\n\r\n  for (auto& command : captured_commands) {\r\n    if (command.bind_group != nullptr) {\r\n      wgpuBindGroupRelease(command.bind_group);\r\n      command.bind_group = nullptr;\r\n    }\r\n\r\n    if (command.bind_group_layout != nullptr) {\r\n      wgpuBindGroupLayoutRelease(command.bind_group_layout);\r\n      command.bind_group_layout = nullptr;\r\n    }\r\n  }\r\n}\r\n\r\nstd::unordered_map<int32_t, WebGpuContextFactory::WebGpuContextInfo> WebGpuContextFactory::contexts_;\r\nstd::mutex WebGpuContextFactory::mutex_;\r\nstd::once_flag WebGpuContextFactory::init_default_flag_;\r\nwgpu::Instance WebGpuContextFactory::default_instance_;\r\n\r\nWebGpuContext& WebGpuContextFactory::CreateContext(const WebGpuContextConfig& config) {\r\n  const int context_id = config.context_id;\r\n  WGPUInstance instance = config.instance;\r\n  WGPUDevice device = config.device;\r\n\r\n  std::call_once(init_default_flag_, [\r\n#if !defined(__wasm__)\r\n                                         dawn_proc_table = config.dawn_proc_table\r\n#endif\r\n  ]() {\r\n  // Step.1 - setup dawn proc table (only for non-WASM build)\r\n\r\n#if !defined(__wasm__)\r\n    const DawnProcTable* dawn_procs = reinterpret_cast<const DawnProcTable*>(dawn_proc_table);\r\n#if defined(BUILD_DAWN_SHARED_LIBRARY)\r\n    ORT_ENFORCE(dawn_procs == nullptr, \"setting DawnProcTable is not allowed when dynamically linked to webgpu_dawn.\");\r\n#else\r\n#if !defined(USE_EXTERNAL_DAWN)\r\n    if (dawn_procs == nullptr) {\r\n      dawn_procs = &dawn::native::GetProcs();\r\n    }\r\n#else\r\n    ORT_ENFORCE(dawn_procs != nullptr, \"DawnProcTable must be provided.\");\r\n#endif\r\n    dawnProcSetProcs(dawn_procs);\r\n#endif\r\n#endif\r\n\r\n    // Step.2 - Create wgpu::Instance\r\n    wgpu::InstanceFeatureName required_instance_features[] = {wgpu::InstanceFeatureName::TimedWaitAny};\r\n    wgpu::InstanceDescriptor instance_desc{};\r\n    instance_desc.requiredFeatures = required_instance_features;\r\n    instance_desc.requiredFeatureCount = sizeof(required_instance_features) / sizeof(required_instance_features[0]);\r\n    default_instance_ = wgpu::CreateInstance(&instance_desc);\r\n\r\n    ORT_ENFORCE(default_instance_ != nullptr, \"Failed to create wgpu::Instance.\");\r\n  });\r\n\r\n  if (context_id == 0) {\r\n    // context ID is preserved for the default context. User cannot use context ID 0 as a custom context.\r\n    ORT_ENFORCE(instance == nullptr && device == nullptr,\r\n                \"WebGPU EP default context (contextId=0) must not have custom WebGPU instance or device.\");\r\n\r\n    instance = default_instance_.Get();\r\n  } else {\r\n    // for context ID > 0, user must provide custom WebGPU instance and device.\r\n    ORT_ENFORCE(instance != nullptr && device != nullptr,\r\n                \"WebGPU EP custom context (contextId>0) must have custom WebGPU instance and device.\");\r\n  }\r\n\r\n  std::lock_guard<std::mutex> lock(mutex_);\r\n\r\n  auto it = contexts_.find(context_id);\r\n  if (it == contexts_.end()) {\r\n    GSL_SUPPRESS(r.11)\r\n    auto context = std::unique_ptr<WebGpuContext>(new WebGpuContext(instance, device, config.validation_mode, config.preserve_device, config.small_storage_buffer_binding_size_for_testing, config.power_preference));\r\n    it = contexts_.emplace(context_id, WebGpuContextFactory::WebGpuContextInfo{std::move(context), 0}).first;\r\n  } else if (context_id != 0) {\r\n    ORT_ENFORCE(it->second.context->instance_.Get() == instance &&\r\n                    it->second.context->device_.Get() == device,\r\n                \"WebGPU EP context ID \", context_id, \" is already created with different WebGPU instance or device.\");\r\n  }\r\n  it->second.ref_count++;\r\n  return *it->second.context;\r\n}\r\n\r\nWebGpuContext& WebGpuContextFactory::GetContext(int context_id) {\r\n  std::lock_guard<std::mutex> lock(mutex_);\r\n\r\n  auto it = contexts_.find(context_id);\r\n  ORT_ENFORCE(it != contexts_.end(), \"WebGPU EP context ID \", context_id, \" is not found.\");\r\n\r\n  return *it->second.context;\r\n}\r\n\r\nvoid WebGpuContextFactory::ReleaseContext(int context_id) {\r\n  std::lock_guard<std::mutex> lock(mutex_);\r\n\r\n  auto it = contexts_.find(context_id);\r\n  ORT_ENFORCE(it != contexts_.end(), \"WebGPU EP context ID \", context_id, \" is not found.\");\r\n\r\n  if (--it->second.ref_count == 0 && !it->second.context->preserve_device_) {\r\n    contexts_.erase(it);\r\n  }\r\n}\r\n\r\nvoid WebGpuContextFactory::Cleanup() {\r\n  std::lock_guard<std::mutex> lock(mutex_);\r\n  contexts_.clear();\r\n  default_instance_ = nullptr;\r\n}\r\n\r\nvoid CleanupWebGpuContexts() {\r\n  WebGpuContextFactory::Cleanup();\r\n}\r\n\r\nWGPUDevice GetDevice(int context_id) {\r\n  return WebGpuContextFactory::GetContext(context_id).Device().Get();\r\n}\r\n\r\n}  // namespace webgpu\r\n}  // namespace onnxruntime\r\n","replacement":"// Copyright (c) Microsoft Corporation. All rights reserved.\r\n// Licensed under the MIT License.\r\n\r\n#include <memory>\r\n#include <cmath>\r\n\r\n#if defined(__GNUC__)\r\n#pragma GCC diagnostic push\r\n#pragma GCC diagnostic ignored \"-Wstrict-aliasing\"\r\n#endif\r\n\r\n#if !defined(__wasm__)\r\n#if !defined(BUILD_DAWN_SHARED_LIBRARY)\r\n#include \"dawn/dawn_proc.h\"\r\n#endif\r\n#if !defined(USE_EXTERNAL_DAWN)\r\n#include \"dawn/native/DawnNative.h\"\r\n#endif\r\n#endif\r\n#if defined(__GNUC__)\r\n#pragma GCC diagnostic pop\r\n#endif\r\n\r\n#include \"core/common/common.h\"\r\n#include \"core/common/path_string.h\"\r\n#include \"core/platform/env.h\"\r\n\r\n#include \"core/providers/webgpu/compute_context.h\"\r\n#include \"core/providers/webgpu/webgpu_context.h\"\r\n#include \"core/providers/webgpu/buffer_manager.h\"\r\n#include \"core/providers/webgpu/webgpu_execution_provider.h\"\r\n#include \"core/providers/webgpu/program.h\"\r\n#include \"core/providers/webgpu/program_cache_key.h\"\r\n#include \"core/providers/webgpu/program_manager.h\"\r\n#include \"core/providers/webgpu/string_macros.h\"\r\n\r\nnamespace onnxruntime {\r\nnamespace webgpu {\r\n\r\nvoid WebGpuContext::Initialize(const WebGpuBufferCacheConfig& buffer_cache_config, int backend_type, bool enable_pix_capture) {\r\n  std::call_once(init_flag_, [this, &buffer_cache_config, backend_type, enable_pix_capture]() {\r\n    if (device_ == nullptr) {\r\n      // Create wgpu::Adapter\r\n      wgpu::RequestAdapterOptions req_adapter_options = {};\r\n      req_adapter_options.backendType = static_cast<wgpu::BackendType>(backend_type);\r\n      req_adapter_options.powerPreference = static_cast<wgpu::PowerPreference>(power_preference_);\r\n\r\n#if !defined(__wasm__)\r\n      auto enabled_adapter_toggles = GetEnabledAdapterToggles();\r\n\r\n      wgpu::DawnTogglesDescriptor adapter_toggles_desc = {};\r\n      adapter_toggles_desc.enabledToggleCount = enabled_adapter_toggles.size();\r\n      adapter_toggles_desc.enabledToggles = enabled_adapter_toggles.data();\r\n\r\n      req_adapter_options.nextInChain = &adapter_toggles_desc;\r\n#endif\r\n\r\n      wgpu::Adapter adapter;\r\n      ORT_ENFORCE(wgpu::WaitStatus::Success == instance_.WaitAny(instance_.RequestAdapter(\r\n                                                                     &req_adapter_options,\r\n                                                                     wgpu::CallbackMode::WaitAnyOnly,\r\n                                                                     [](wgpu::RequestAdapterStatus status, wgpu::Adapter adapter, wgpu::StringView message, wgpu::Adapter* ptr) {\r\n                                                                       ORT_ENFORCE(status == wgpu::RequestAdapterStatus::Success, \"Failed to get a WebGPU adapter: \", std::string_view{message});\r\n                                                                       *ptr = std::move(adapter);\r\n                                                                     },\r\n                                                                     &adapter),\r\n                                                                 UINT64_MAX));\r\n      ORT_ENFORCE(adapter != nullptr, \"Failed to get a WebGPU adapter.\");\r\n\r\n      // Create wgpu::Device\r\n      wgpu::DeviceDescriptor device_desc = {};\r\n\r\n#if !defined(__wasm__)\r\n      wgpu::DawnTogglesDescriptor device_toggles_desc = {};\r\n      device_desc.nextInChain = &device_toggles_desc;\r\n\r\n      auto enabled_device_toggles = GetEnabledDeviceToggles();\r\n      device_toggles_desc.enabledToggleCount = enabled_device_toggles.size();\r\n      device_toggles_desc.enabledToggles = enabled_device_toggles.data();\r\n\r\n      auto disabled_device_toggles = GetDisabledDeviceToggles();\r\n      device_toggles_desc.disabledToggleCount = disabled_device_toggles.size();\r\n      device_toggles_desc.disabledToggles = disabled_device_toggles.data();\r\n#endif\r\n\r\n      std::vector<wgpu::FeatureName> required_features = GetAvailableRequiredFeatures(adapter);\r\n      if (required_features.size() > 0) {\r\n        device_desc.requiredFeatures = required_features.data();\r\n        device_desc.requiredFeatureCount = required_features.size();\r\n      }\r\n      wgpu::Limits required_limits = GetRequiredLimits(adapter);\r\n      device_desc.requiredLimits = &required_limits;\r\n\r\n      // TODO: revise temporary error handling\r\n      device_desc.SetUncapturedErrorCallback([](const wgpu::Device& /*device*/, wgpu::ErrorType type, wgpu::StringView message) {\r\n        LOGS_DEFAULT(ERROR) << \"WebGPU device error(\" << int(type) << \"): \" << std::string_view{message};\r\n      });\r\n      // TODO: revise temporary device lost handling\r\n      device_desc.SetDeviceLostCallback(wgpu::CallbackMode::AllowSpontaneous, [](const wgpu::Device& /*device*/, wgpu::DeviceLostReason reason, wgpu::StringView message) {\r\n        LOGS_DEFAULT(INFO) << \"WebGPU device lost (\" << int(reason) << \"): \" << std::string_view{message};\r\n      });\r\n\r\n      ORT_ENFORCE(wgpu::WaitStatus::Success == instance_.WaitAny(adapter.RequestDevice(\r\n                                                                     &device_desc,\r\n                                                                     wgpu::CallbackMode::WaitAnyOnly,\r\n                                                                     [](wgpu::RequestDeviceStatus status, wgpu::Device device, wgpu::StringView message, wgpu::Device* ptr) {\r\n                                                                       ORT_ENFORCE(status == wgpu::RequestDeviceStatus::Success, \"Failed to get a WebGPU device: \", std::string_view{message});\r\n                                                                       *ptr = std::move(device);\r\n                                                                     },\r\n                                                                     &device_),\r\n                                                                 UINT64_MAX));\r\n      ORT_ENFORCE(device_ != nullptr, \"Failed to get a WebGPU device.\");\r\n    }\r\n\r\n    LOGS_DEFAULT(VERBOSE) << \"WebGPU EP Context is created for: Instance=\" << instance_.Get() << \", Device=\" << device_.Get() << \".\";\r\n\r\n    // cache device queue\r\n    device_queue_ = device_.GetQueue();\r\n    // cache device limits\r\n    ORT_ENFORCE(Device().GetLimits(&device_limits_));\r\n    // cache device features\r\n    wgpu::SupportedFeatures supported_features;\r\n    Device().GetFeatures(&supported_features);\r\n    for (size_t i = 0; i < supported_features.featureCount; i++) {\r\n      device_features_.insert(supported_features.features[i]);\r\n    }\r\n    // cache adapter info\r\n#if !defined(__wasm__)\r\n    if (DeviceHasFeature(wgpu::FeatureName::ChromiumExperimentalSubgroupMatrix)) {\r\n      adapter_info_.nextInChain = &subgroup_matrix_configs_;\r\n    }\r\n#endif\r\n    ORT_ENFORCE(Device().GetAdapterInfo(&adapter_info_));\r\n\r\n    // create buffer manager\r\n    buffer_mgr_ = BufferManagerFactory::Create(*this,\r\n                                               buffer_cache_config.storage.mode,\r\n                                               buffer_cache_config.uniform.mode,\r\n                                               buffer_cache_config.query_resolve.mode);\r\n\r\n    // create initializer buffer manager. cache is always disabled for initializer buffer manager\r\n    initializer_buffer_mgr_ = BufferManagerFactory::Create(*this,\r\n                                                           BufferCacheMode::Disabled,\r\n                                                           BufferCacheMode::Disabled,\r\n                                                           BufferCacheMode::Disabled);\r\n\r\n    // create program manager\r\n    program_mgr_ = std::make_unique<ProgramManager>(*this);\r\n\r\n    // set query type\r\n#if !defined(__wasm__)\r\n    if (device_.HasFeature(wgpu::FeatureName::ChromiumExperimentalTimestampQueryInsidePasses)) {\r\n      query_type_ = TimestampQueryType::InsidePasses;\r\n    } else\r\n#endif\r\n        if (device_.HasFeature(wgpu::FeatureName::TimestampQuery)) {\r\n      query_type_ = TimestampQueryType::AtPasses;\r\n    } else {\r\n      query_type_ = TimestampQueryType::None;\r\n    }\r\n    if (enable_pix_capture) {\r\n#if defined(ENABLE_PIX_FOR_WEBGPU_EP)\r\n      // set pix frame generator\r\n      pix_frame_generator_ = std::make_unique<WebGpuPIXFrameGenerator>(instance_,\r\n                                                                       Device());\r\n#else\r\n    ORT_THROW(\"Support PIX capture requires extra build flags (--enable_pix_capture)\");\r\n#endif  // ENABLE_PIX_FOR_WEBGPU_EP\r\n    }\r\n  });\r\n}\r\n\r\nStatus WebGpuContext::Wait(wgpu::Future f) {\r\n  auto status = instance_.WaitAny(f, UINT64_MAX);\r\n  if (status == wgpu::WaitStatus::Success) {\r\n    return Status::OK();\r\n  }\r\n  return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, \"Failed to wait for the operation:\", uint32_t(status));\r\n}\r\n\r\nStatus WebGpuContext::Run(ComputeContext& context, ProgramBase& program) {\r\n  // Finalize program inputs by adding the indirect buffer as the last input if needed.\r\n  if (program.IndirectDispatchTensor() != nullptr) {\r\n    program.AddInput({program.IndirectDispatchTensor(), ProgramTensorMetadataDependency::None});\r\n  }\r\n\r\n  const auto& inputs = program.Inputs();\r\n  const auto& outputs = program.Outputs();\r\n\r\n  if (outputs.size() == 0) {\r\n    return Status::OK();\r\n  }\r\n\r\n  if (ValidationMode() >= ValidationMode::Basic) {\r\n    ORT_ENFORCE(std::all_of(inputs.begin(), inputs.end(), [](const ProgramInput& input) {\r\n                  const auto* tensor = input.tensor;\r\n                  return tensor != nullptr &&\r\n                         tensor->Location().mem_type == OrtMemType::OrtMemTypeDefault &&\r\n                         tensor->Location().device.Type() == OrtDevice::GPU &&\r\n                         !strcmp(tensor->Location().name.c_str(), WEBGPU_BUFFER);\r\n                }),\r\n                \"All inputs must be tensors on WebGPU buffers.\");\r\n\r\n    ORT_ENFORCE(std::all_of(outputs.begin(), outputs.end(), [](const ProgramOutput& output) {\r\n                  const auto* tensor = output.tensor;\r\n                  return tensor != nullptr &&\r\n                         tensor->Location().mem_type == OrtMemType::OrtMemTypeDefault &&\r\n                         tensor->Location().device.Type() == OrtDevice::GPU &&\r\n                         !strcmp(tensor->Location().name.c_str(), WEBGPU_BUFFER);\r\n                }),\r\n                \"All outputs must be tensors on WebGPU buffers.\");\r\n  }\r\n\r\n  const ProgramMetadata& metadata = program.Metadata();\r\n\r\n  // validate program metadata\r\n  if (ValidationMode() >= ValidationMode::Basic) {\r\n    const auto& [constants, overridable_constants, uniform_variables] = metadata;\r\n\r\n    // check overridable constants\r\n    ORT_RETURN_IF(program.OverridableConstants().size() != overridable_constants.size(),\r\n                  \"Size of overridable constants mismatch in program \\\"\", program.Name(),\r\n                  \"\\\", Expected: \", overridable_constants.size(),\r\n                  \", Actual: \", program.OverridableConstants().size());\r\n\r\n    if (ValidationMode() >= ValidationMode::Full) {\r\n      size_t num_overridable_constants = program.OverridableConstants().size();\r\n      for (size_t i = 0; i < num_overridable_constants; ++i) {\r\n        const auto& override_value = program.OverridableConstants()[i];\r\n        const auto& definition = overridable_constants[i];\r\n        ORT_RETURN_IF(override_value.has_value && override_value.type != definition.type,\r\n                      \"Overridable override_value[\", i, \"] (\", definition.name, \") data type mismatch in program \\\"\", program.Name(),\r\n                      \"\\\", Expected: \", definition.type,\r\n                      \", Actual: \", override_value.type);\r\n        ORT_RETURN_IF(!override_value.has_value && !definition.has_default_value,\r\n                      \"Overridable override_value[\", i, \"] (\", definition.name, \") no override_value specified in program \\\"\", program.Name(),\r\n                      \"\\\"\");\r\n      }\r\n    }\r\n\r\n    // check uniform variables\r\n    ORT_RETURN_IF(program.UniformVariables().size() != uniform_variables.size(),\r\n                  \"Size of uniform_value variables mismatch in program \\\"\", program.Name(),\r\n                  \"\\\", Expected: \", uniform_variables.size(),\r\n                  \", Actual: \", program.UniformVariables().size());\r\n\r\n    if (ValidationMode() >= ValidationMode::Full) {\r\n      size_t num_uniform_variables = program.UniformVariables().size();\r\n      for (size_t i = 0; i < num_uniform_variables; ++i) {\r\n        const auto& uniform_value = program.UniformVariables()[i];\r\n        const auto& definition = uniform_variables[i];\r\n        ORT_RETURN_IF(uniform_value.length > 0 && uniform_value.data_type != definition.data_type,\r\n                      \"Uniform variable[\", i, \"] (\", definition.name, \") data type mismatch in program \\\"\", program.Name(),\r\n                      \"\\\", Expected: \", definition.data_type,\r\n                      \", Actual: \", uniform_value.data_type);\r\n      }\r\n    }\r\n  }\r\n\r\n  uint32_t x = program.DispatchGroupSizeX();\r\n  uint32_t y = program.DispatchGroupSizeY();\r\n  uint32_t z = program.DispatchGroupSizeZ();\r\n\r\n  // Skip normalization for indirect dispatch since dimensions are determined by the indirect buffer\r\n  if (program.IndirectDispatchTensor() == nullptr) {\r\n    ORT_RETURN_IF_ERROR(program_mgr_->NormalizeDispatchGroupSize(x, y, z));\r\n  } else {\r\n    ORT_ENFORCE(x == 0 && y == 0 && z == 0,\r\n                \"Only one of SetIndirectDispatchTensor and SetDispatchGroupSize should be called for program\", program.Name());\r\n  }\r\n  ORT_RETURN_IF_ERROR(program_mgr_->CalculateSegmentsForInputsAndOutputs(program));\r\n\r\n  bool is_1d_dispatch = (y == 1 && z == 1);\r\n\r\n  auto key = CalculateProgramCacheKey(program, is_1d_dispatch);\r\n\r\n  if (is_profiling_) {\r\n    PendingKernelInfo pending_kernel_info(context.KernelContext().GetNodeName(),\r\n                                          context.KernelContext().GetOpType(),\r\n                                          program.Name(),\r\n                                          key,\r\n                                          inputs,\r\n                                          outputs);\r\n    pending_kernels_.emplace_back(std::move(pending_kernel_info));\r\n  }\r\n\r\n  LOGS(context.Logger(), INFO) << \"Starting program \\\"\" << key << \"\\\" (\" << x << \", \" << y << \", \" << z << \")\";\r\n\r\n  const auto* program_artifact = program_mgr_->Get(key);\r\n  if (program_artifact == nullptr) {\r\n    wgpu::ComputePipeline compute_pipeline;\r\n    std::vector<int> shape_uniform_ranks;\r\n    auto status = program_mgr_->Build(program,\r\n                                      metadata,\r\n#ifndef NDEBUG  // if debug build\r\n                                      key,\r\n#endif\r\n                                      x,\r\n                                      y,\r\n                                      z,\r\n                                      compute_pipeline,\r\n                                      shape_uniform_ranks);\r\n    ORT_RETURN_IF_ERROR(status);\r\n    program_artifact = program_mgr_->Set(key, ProgramArtifact{program,\r\n                                                              std::move(compute_pipeline),\r\n                                                              std::move(shape_uniform_ranks)});\r\n#ifndef NDEBUG  // if debug build\r\n    ORT_ENFORCE(program_artifact != nullptr, \"Program artifact should not be nullptr.\");\r\n#endif\r\n  }\r\n\r\n  // prepare shape uniforms for shader variables (if any) and user defined uniforms\r\n  std::vector<ProgramUniformVariableValue> shape_uniforms;\r\n  shape_uniforms.reserve(program_artifact->shape_uniform_ranks.size() * 2);\r\n  if (ValidationMode() >= ValidationMode::Basic) {\r\n    ORT_RETURN_IF_NOT(program_artifact->shape_uniform_ranks.size() == inputs.size() + outputs.size() + program.Indices().size(),\r\n                      \"Invalid program artifact: variable size (\", program_artifact->shape_uniform_ranks.size(),\r\n                      \") does not match current program (input: \", inputs.size(),\r\n                      \", output: \", outputs.size(),\r\n                      \", indices: \", program.Indices().size(), \")\");\r\n  }\r\n\r\n  auto append_shape_uniforms = [&shape_uniforms, program_artifact](size_t i, const TensorShape& shape) {\r\n    if (program_artifact->shape_uniform_ranks[i] > 0) {\r\n      size_t expected_rank = static_cast<size_t>(program_artifact->shape_uniform_ranks[i]);\r\n      ORT_RETURN_IF(expected_rank != shape.NumDimensions(),\r\n                    \"Invalid program artifact: variable[\", i, \"] rank mismatch. Expected: \", expected_rank,\r\n                    \", Actual: \", shape.NumDimensions());\r\n\r\n      std::vector<uint32_t> dims(expected_rank);\r\n      std::vector<uint32_t> stride(expected_rank - 1);\r\n      for (size_t j = 0; j < expected_rank; ++j) {\r\n        dims[j] = onnxruntime::narrow<uint32_t>(shape[j]);\r\n        if (j < expected_rank - 1) {\r\n          stride[j] = onnxruntime::narrow<uint32_t>(shape.SizeFromDimension(j + 1));\r\n        }\r\n      }\r\n\r\n      shape_uniforms.emplace_back(gsl::make_span(dims));\r\n      if (expected_rank > 1) {\r\n        shape_uniforms.emplace_back(gsl::make_span(stride));\r\n      }\r\n    }\r\n    return Status::OK();\r\n  };\r\n\r\n  for (size_t i = 0; i < inputs.size(); i++) {\r\n    ORT_RETURN_IF_ERROR(append_shape_uniforms(i,\r\n                                              inputs[i].use_override_shape ? inputs[i].override_shape : inputs[i].tensor->Shape()));\r\n  }\r\n  for (size_t i = 0; i < outputs.size(); i++) {\r\n    ORT_RETURN_IF_ERROR(append_shape_uniforms(i + inputs.size(),\r\n                                              outputs[i].use_override_shape ? outputs[i].override_shape : outputs[i].tensor->Shape()));\r\n  }\r\n  for (size_t i = 0; i < program.Indices().size(); i++) {\r\n    ORT_RETURN_IF_ERROR(append_shape_uniforms(i + inputs.size() + outputs.size(), program.Indices()[i]));\r\n  }\r\n\r\n  const size_t uniform_count = shape_uniforms.size() + program.UniformVariables().size();\r\n  size_t current_offset = 0;\r\n  std::vector<std::tuple<const ProgramUniformVariableValue&, size_t>> uniform_and_offsets;\r\n  uniform_and_offsets.reserve(uniform_count);\r\n  for (size_t i = 0; i < uniform_count; i++) {\r\n    const auto& uniform = i < shape_uniforms.size() ? shape_uniforms[i]\r\n                                                    : program.UniformVariables()[i - shape_uniforms.size()];\r\n    size_t length = uniform.length;\r\n    if (length == 0) {  // skip zero-length uniform\r\n      continue;\r\n    }\r\n\r\n    // Calculate the size and alignment of the uniform variable.\r\n    //\r\n    // https://www.w3.org/TR/WGSL/#alignof\r\n    //\r\n    // For f16:\r\n    // - length > 8      : array<vec4<u32>, N>   (align 16) (size 16 * N, N = ceil(length / 8))\r\n    // - length == 7 or 8: vec4<u32>             (align 16) (size 16)\r\n    // - length == 5 or 6: vec3<u32>             (align 16) (size 12)\r\n    // - length == 3 or 4: vec2<u32>             (align 8)  (size 8)\r\n    // - length == 1 or 2: u32                   (align 4)  (size 4)\r\n    //\r\n    // For other types (i32, u32, f32):\r\n    // - length > 4      : array<vec4<T>, N>     (align 16) (size 16 * N, N = ceil(length / 4))\r\n    // - length == 4     : vec4<T>               (align 16) (size 16)\r\n    // - length == 3     : vec3<T>               (align 16) (size 12)\r\n    // - length == 2     : vec2<T>               (align 8)  (size 8)\r\n    // - length == 1     : T                     (align 4)  (size 4)\r\n    //\r\n\r\n    const bool is_f16 = uniform.data_type == ProgramUniformVariableDataType::Float16;\r\n\r\n    size_t variable_alignment = 4;  // default alignment for scalar types\r\n    size_t variable_size = 4;       // default size for scalar types\r\n\r\n    if (is_f16) {\r\n      if (length > 6) {\r\n        variable_alignment = 16;\r\n        variable_size = 16 * ((length + 7) / 8);\r\n      } else if (length > 4) {\r\n        variable_alignment = 16;\r\n        variable_size = 12;\r\n      } else if (length > 2) {\r\n        variable_alignment = 8;\r\n        variable_size = 8;\r\n      }\r\n    } else {\r\n      if (length > 3) {\r\n        variable_alignment = 16;\r\n        variable_size = 16 * ((length + 3) / 4);\r\n      } else if (length > 2) {\r\n        variable_alignment = 16;\r\n        variable_size = 12;\r\n      } else if (length > 1) {\r\n        variable_alignment = 8;\r\n        variable_size = 8;\r\n      }\r\n    }\r\n    current_offset = (current_offset + variable_alignment - 1) / variable_alignment * variable_alignment;\r\n    uniform_and_offsets.emplace_back(uniform, current_offset);\r\n\r\n    current_offset += variable_size;\r\n  }\r\n\r\n  // Meet alignment of struct here: https://www.w3.org/TR/WGSL/#alignment-and-size. For simplicity, set\r\n  // max_alignment_of_field to 16 since the underlying buffer has been rounded up to 16.\r\n  constexpr size_t max_alignment_of_field = 16;\r\n  const size_t uniform_buffer_total_size = (current_offset + max_alignment_of_field - 1) / max_alignment_of_field * max_alignment_of_field;\r\n\r\n  WGPUBuffer uniform_buffer = nullptr;\r\n  const webgpu::BufferManager& buffer_mgr = context.BufferManager();\r\n  if (uniform_buffer_total_size > 0) {\r\n    std::vector<uint8_t> uniform_data_buffer(uniform_buffer_total_size);\r\n\r\n    for (auto const& [uniform, offset] : uniform_and_offsets) {\r\n      memcpy(uniform_data_buffer.data() + offset, uniform.data.data(), uniform.data.size());\r\n    }\r\n\r\n    uniform_buffer = buffer_mgr.Create(uniform_buffer_total_size, wgpu::BufferUsage::CopyDst | wgpu::BufferUsage::Uniform);\r\n    device_queue_.WriteBuffer(uniform_buffer, 0, uniform_data_buffer.data(), uniform_buffer_total_size);\r\n  }\r\n\r\n  const auto& compute_pass_encoder = GetComputePassEncoder();\r\n\r\n  WriteTimestamp(num_pending_dispatches_ * 2);\r\n\r\n  const size_t total_buffer_count = inputs.size() + outputs.size() + (uniform_buffer ? 1 : 0);\r\n\r\n  std::vector<WGPUBuffer> bind_buffers;\r\n  std::vector<uint32_t> bind_buffers_segments;\r\n  bind_buffers.reserve(total_buffer_count);\r\n  bind_buffers_segments.reserve(total_buffer_count);\r\n  for (const auto& input : inputs) {\r\n    bind_buffers.push_back(reinterpret_cast<WGPUBuffer>(const_cast<void*>(input.tensor->DataRaw())));\r\n    bind_buffers_segments.push_back(input.segments);\r\n  }\r\n  for (const auto& output : outputs) {\r\n    bind_buffers.push_back(reinterpret_cast<WGPUBuffer>(output.tensor->MutableDataRaw()));\r\n    bind_buffers_segments.push_back(output.segments);\r\n  }\r\n  if (uniform_buffer) {\r\n    bind_buffers.push_back(uniform_buffer);\r\n    bind_buffers_segments.push_back(1);  // uniform buffer defaults to 1 segment\r\n  }\r\n\r\n  LaunchComputePipeline(compute_pass_encoder, bind_buffers, bind_buffers_segments, *program_artifact, x, y, z, program.IndirectDispatchTensor());\r\n  if (uniform_buffer) {\r\n    buffer_mgr.Release(uniform_buffer);\r\n  }\r\n\r\n  WriteTimestamp(num_pending_dispatches_ * 2 + 1);\r\n  ++num_pending_dispatches_;\r\n\r\n  if (num_pending_dispatches_ >= max_num_pending_dispatches_ ||\r\n      (is_profiling_ && query_type_ == TimestampQueryType::AtPasses)) {\r\n    EndComputePass();\r\n  }\r\n  if (num_pending_dispatches_ >= max_num_pending_dispatches_) {\r\n    Flush(buffer_mgr);\r\n    num_pending_dispatches_ = 0;\r\n  }\r\n\r\n  return Status::OK();\r\n}\r\n\r\nstd::vector<const char*> WebGpuContext::GetEnabledAdapterToggles() const {\r\n  // See the description of all the toggles in toggles.cpp\r\n  // \"use_dxc\" for Shader Model 6+ features (e.g. float16)\r\n  // \"allow_unsafe_apis\" for chromium experimental features\r\n  constexpr const char* toggles[] = {\r\n    \"use_dxc\",\r\n    \"allow_unsafe_apis\",\r\n#if defined(DAWN_ENABLE_VULKAN)\r\n    \"use_vulkan_memory_model\",\r\n#endif\r\n  };\r\n  return std::vector<const char*>(std::begin(toggles), std::end(toggles));\r\n}\r\n\r\nstd::vector<const char*> WebGpuContext::GetEnabledDeviceToggles() const {\r\n  // Enable / disable other toggles that may affect the performance.\r\n  // Other toggles that may be useful: \"dump_shaders\", \"disable_symbol_renaming\"\r\n  constexpr const char* toggles[] = {\r\n      \"skip_validation\",  // only use \"skip_validation\" when ValidationMode is set to \"Disabled\"\r\n      \"disable_robustness\",\r\n      \"d3d_disable_ieee_strictness\",\r\n  };\r\n  return std::vector<const char*>(ValidationMode() >= ValidationMode::WGPUOnly\r\n                                      ? std::begin(toggles) + 1\r\n                                      : std::begin(toggles),\r\n                                  std::end(toggles));\r\n}\r\n\r\nstd::vector<const char*> WebGpuContext::GetDisabledDeviceToggles() const {\r\n  constexpr const char* toggles[] = {\r\n      \"lazy_clear_resource_on_first_use\",\r\n      \"timestamp_quantization\",\r\n  };\r\n  return std::vector<const char*>(std::begin(toggles), std::end(toggles));\r\n}\r\n\r\nstd::vector<wgpu::FeatureName> WebGpuContext::GetAvailableRequiredFeatures(const wgpu::Adapter& adapter) const {\r\n  std::vector<wgpu::FeatureName> required_features;\r\n  constexpr wgpu::FeatureName features[] {\r\n#if !defined(__wasm__)\r\n    wgpu::FeatureName::ChromiumExperimentalTimestampQueryInsidePasses,\r\n        wgpu::FeatureName::ChromiumExperimentalSubgroupMatrix,\r\n#endif\r\n        wgpu::FeatureName::TimestampQuery,\r\n        wgpu::FeatureName::ShaderF16,\r\n        wgpu::FeatureName::Subgroups,\r\n#if !defined(__wasm__)\r\n        wgpu::FeatureName::BufferMapExtendedUsages,\r\n#endif\r\n  };\r\n  for (auto feature : features) {\r\n    if (adapter.HasFeature(feature)) {\r\n      required_features.push_back(feature);\r\n    }\r\n  }\r\n  return required_features;\r\n}\r\n\r\nwgpu::Limits WebGpuContext::GetRequiredLimits(const wgpu::Adapter& adapter) const {\r\n  wgpu::Limits required_limits{};\r\n  wgpu::Limits adapter_limits;\r\n  ORT_ENFORCE(adapter.GetLimits(&adapter_limits));\r\n\r\n  required_limits.maxBindGroups = adapter_limits.maxBindGroups;\r\n  required_limits.maxComputeWorkgroupStorageSize = adapter_limits.maxComputeWorkgroupStorageSize;\r\n  required_limits.maxComputeWorkgroupsPerDimension = adapter_limits.maxComputeWorkgroupsPerDimension;\r\n  required_limits.maxStorageBuffersPerShaderStage = adapter_limits.maxStorageBuffersPerShaderStage;\r\n\r\n  if (small_storage_buffer_binding_size_for_testing_) {\r\n    // No matter how small it is set, the minimum storage buffer binding size in WebGPU is 128 MB.\r\n    required_limits.maxStorageBufferBindingSize = 134217728;\r\n  } else {\r\n    required_limits.maxStorageBufferBindingSize = adapter_limits.maxStorageBufferBindingSize;\r\n  }\r\n\r\n  required_limits.maxBufferSize = adapter_limits.maxBufferSize;\r\n  required_limits.maxComputeInvocationsPerWorkgroup = adapter_limits.maxComputeInvocationsPerWorkgroup;\r\n  required_limits.maxComputeWorkgroupSizeX = adapter_limits.maxComputeWorkgroupSizeX;\r\n  required_limits.maxComputeWorkgroupSizeY = adapter_limits.maxComputeWorkgroupSizeY;\r\n  required_limits.maxComputeWorkgroupSizeZ = adapter_limits.maxComputeWorkgroupSizeZ;\r\n\r\n  return required_limits;\r\n}\r\n\r\nvoid WebGpuContext::WriteTimestamp(uint32_t query_index) {\r\n  if (!is_profiling_ || query_type_ != TimestampQueryType::InsidePasses) {\r\n    return;\r\n  }\r\n\r\n  const auto& compute_pass_encoder = GetComputePassEncoder();\r\n  compute_pass_encoder.WriteTimestamp(query_set_, query_index);\r\n}\r\n\r\nvoid WebGpuContext::StartProfiling() {\r\n  if (query_type_ == TimestampQueryType::None) {\r\n    return;\r\n  }\r\n\r\n  is_profiling_ = true;\r\n\r\n  const uint32_t query_count = max_num_pending_dispatches_ * 2;\r\n\r\n  if (!query_set_) {\r\n    // Create query set\r\n    wgpu::QuerySetDescriptor querySetDescriptor;\r\n    querySetDescriptor.count = query_count;\r\n    querySetDescriptor.type = wgpu::QueryType::Timestamp;\r\n    query_set_ = device_.CreateQuerySet(&querySetDescriptor);\r\n  }\r\n\r\n  if (!query_resolve_buffer_) {\r\n    // Create resolve buffer\r\n    wgpu::BufferDescriptor bufferDescriptor;\r\n    bufferDescriptor.size = query_count * sizeof(uint64_t);\r\n    bufferDescriptor.usage = wgpu::BufferUsage::QueryResolve | wgpu::BufferUsage::CopySrc |\r\n                             wgpu::BufferUsage::CopyDst;\r\n    query_resolve_buffer_ = device_.CreateBuffer(&bufferDescriptor);\r\n  }\r\n}\r\n\r\nvoid WebGpuContext::CollectProfilingData(profiling::Events& events) {\r\n  if (!pending_queries_.empty()) {\r\n    for (const auto& pending_query : pending_queries_) {\r\n      const auto& pending_kernels = pending_query.kernels;\r\n      const auto& query_read_buffer = pending_query.query_buffer;\r\n\r\n      ORT_ENFORCE(Wait(query_read_buffer.MapAsync(wgpu::MapMode::Read,\r\n                                                  0,\r\n                                                  static_cast<size_t>(query_read_buffer.GetSize()),\r\n                                                  wgpu::CallbackMode::WaitAnyOnly,\r\n                                                  [](wgpu::MapAsyncStatus status, wgpu::StringView message) {\r\n                                                    ORT_ENFORCE(status == wgpu::MapAsyncStatus::Success, \"Failed to download data from buffer: \", std::string_view{message});\r\n                                                  })) == Status::OK());\r\n      auto mapped_data = static_cast<const uint64_t*>(query_read_buffer.GetConstMappedRange());\r\n\r\n      for (size_t i = 0; i < pending_kernels.size(); i++) {\r\n        const PendingKernelInfo& pending_kernel_info = pending_kernels[i];\r\n        const auto& inputs = pending_kernel_info.inputs;\r\n        const auto& outputs = pending_kernel_info.outputs;\r\n\r\n        SS(shapes, 128);\r\n        for (size_t s = 0; s < inputs.size(); s++) {\r\n          const auto& input = inputs[s];\r\n          shapes << \"inputs[\" << s << \"] = \" << input.override_shape.ToString() << \" \";\r\n        }\r\n        for (size_t s = 0; s < outputs.size(); s++) {\r\n          const auto& output = outputs[s];\r\n          shapes << \"outputs[\" << s << \"] = \" << output.override_shape.ToString() << \" \";\r\n        }\r\n\r\n        if (gpu_timestamp_offset_ == 0) {\r\n          gpu_timestamp_offset_ = mapped_data[i * 2];\r\n          // TODO: apply CPU-GPU time offset so that timestamps are aligned\r\n        }\r\n        uint64_t start_time = mapped_data[i * 2] - gpu_timestamp_offset_;\r\n        uint64_t end_time = mapped_data[i * 2 + 1] - gpu_timestamp_offset_;\r\n\r\n        const std::unordered_map<std::string, std::string>& event_args = {\r\n            {\"shapes\", SS_GET(shapes)},\r\n            {\"cache_key\", pending_kernel_info.cache_key},\r\n        };\r\n\r\n        profiling::EventRecord event(profiling::API_EVENT,\r\n                                     -1,\r\n                                     -1,\r\n                                     pending_kernel_info.name,\r\n                                     static_cast<int64_t>(std::round(start_time / 1000.0)),\r\n                                     static_cast<int64_t>(std::round((end_time - start_time) / 1000.0)),\r\n                                     event_args);\r\n        events.emplace_back(std::move(event));\r\n      }\r\n\r\n      query_read_buffer.Unmap();\r\n      query_read_buffer.Destroy();\r\n    }\r\n\r\n    pending_queries_.clear();\r\n  }\r\n\r\n  is_profiling_ = false;\r\n}\r\n\r\nvoid WebGpuContext::EndProfiling(TimePoint /* tp */, profiling::Events& events, profiling::Events& cached_events) {\r\n  // This function is called when no active inference is ongoing.\r\n  ORT_ENFORCE(!is_profiling_, \"Profiling is ongoing in an inference run.\");\r\n\r\n  if (query_type_ != TimestampQueryType::None) {\r\n    // No pending kernels or queries should be present at this point. They should have been collected in CollectProfilingData.\r\n    ORT_ENFORCE(pending_kernels_.empty() && pending_queries_.empty(), \"Pending kernels or queries are not empty.\");\r\n\r\n    events.insert(events.end(),\r\n                  std::make_move_iterator(cached_events.begin()),\r\n                  std::make_move_iterator(cached_events.end()));\r\n\r\n    cached_events.clear();\r\n  } else {\r\n    LOGS_DEFAULT(WARNING) << \"TimestampQuery is not supported in this device.\";\r\n  }\r\n}\r\n\r\nvoid WebGpuContext::PushErrorScope() { device_.PushErrorScope(wgpu::ErrorFilter::Validation); }\r\n\r\nStatus WebGpuContext::PopErrorScope() {\r\n  Status status{};\r\n  ORT_RETURN_IF_ERROR(Wait(device_.PopErrorScope(\r\n      wgpu::CallbackMode::WaitAnyOnly,\r\n      [](wgpu::PopErrorScopeStatus pop_status, wgpu::ErrorType error_type, char const* message, Status* status) {\r\n        ORT_ENFORCE(pop_status == wgpu::PopErrorScopeStatus::Success, \"Instance dropped.\");\r\n        if (error_type == wgpu::ErrorType::NoError) {\r\n          return;\r\n        }\r\n        *status = ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, \"WebGPU validation failed. \", message);\r\n      },\r\n      &status)));\r\n  return status;\r\n}\r\n\r\nvoid WebGpuContext::Flush(const webgpu::BufferManager& buffer_mgr) {\r\n  if (!current_command_encoder_) {\r\n    return;\r\n  }\r\n\r\n  EndComputePass();\r\n\r\n  if (is_profiling_ && num_pending_dispatches_ > 0) {\r\n    uint32_t query_count = num_pending_dispatches_ * 2;\r\n    current_command_encoder_.ResolveQuerySet(\r\n        query_set_,\r\n        0,\r\n        query_count,\r\n        query_resolve_buffer_,\r\n        0);\r\n\r\n    wgpu::BufferDescriptor bufferDescriptor;\r\n    bufferDescriptor.size = query_count * sizeof(uint64_t);\r\n    bufferDescriptor.usage = wgpu::BufferUsage::MapRead | wgpu::BufferUsage::CopyDst;\r\n    wgpu::Buffer query_read_buffer = device_.CreateBuffer(&bufferDescriptor);\r\n\r\n    current_command_encoder_.CopyBufferToBuffer(\r\n        query_resolve_buffer_,\r\n        0,\r\n        query_read_buffer,\r\n        0,\r\n        query_count * sizeof(uint64_t));\r\n\r\n    pending_queries_.emplace_back(std::move(pending_kernels_), query_read_buffer);\r\n    pending_kernels_.clear();\r\n  }\r\n  auto command_buffer = current_command_encoder_.Finish();\r\n  device_queue_.Submit(1, &command_buffer);\r\n  if (graph_capture_state_ != GraphCaptureState::Replaying) {\r\n    buffer_mgr.RefreshPendingBuffers(graph_capture_state_);\r\n  }\r\n  current_command_encoder_ = nullptr;\r\n  num_pending_dispatches_ = 0;\r\n}\r\n\r\nvoid WebGpuContext::OnRunEnd() {\r\n#if defined(ENABLE_PIX_FOR_WEBGPU_EP)\r\n  if (pix_frame_generator_) {\r\n    pix_frame_generator_->GeneratePIXFrame();\r\n  }\r\n#endif  // ENABLE_PIX_FOR_WEBGPU_EP\r\n}\r\n\r\nvoid WebGpuContext::LaunchComputePipeline(const wgpu::ComputePassEncoder& compute_pass_encoder,\r\n                                          const std::vector<WGPUBuffer>& bind_buffers,\r\n                                          const std::vector<uint32_t>& bind_buffers_segments,\r\n                                          const ProgramArtifact& program_artifact,\r\n                                          uint32_t x, uint32_t y, uint32_t z,\r\n                                          const Tensor* indirect_dispatch_tensor) {\r\n  uint32_t entry_index = 0;\r\n  std::vector<WGPUBindGroupEntry> bind_group_entries;\r\n\r\n  for (size_t buffer_idx = 0; buffer_idx < bind_buffers.size(); ++buffer_idx) {\r\n    WGPUBuffer buffer = bind_buffers[buffer_idx];\r\n    uint64_t buffer_size = wgpuBufferGetSize(buffer);\r\n    const uint64_t kMaxBufferSize = device_limits_.maxStorageBufferBindingSize;\r\n    const uint32_t total_segments = bind_buffers_segments[buffer_idx];\r\n    // `total_segments` we used is calculated by tensor size, not actual buffer size. Because for bucketed buffer,\r\n    // the actual buffer size may be larger than the tensor size, an extreme case is that tensor size = 127MB, buffer size = 256MB,\r\n    // maxStorageBufferBindingSize = 128MB, in this case we only need to bind 1 segment instead of 2 segments because\r\n    // there is no data for the second segment.\r\n    if (total_segments > 1) {\r\n      uint64_t offset = 0;\r\n      for (uint32_t segment = 0; segment < total_segments; ++segment) {\r\n        uint64_t segment_size = std::min(kMaxBufferSize, buffer_size - offset);\r\n        bind_group_entries.push_back({nullptr, entry_index++, buffer, offset, segment_size, nullptr, nullptr});\r\n        offset += segment_size;\r\n      }\r\n    } else {\r\n      bind_group_entries.push_back({nullptr, entry_index++, buffer, 0, std::min(kMaxBufferSize, buffer_size), nullptr, nullptr});\r\n    }\r\n  }\r\n\r\n  ORT_ENFORCE(entry_index < device_limits_.maxBindingsPerBindGroup, \"Number of bind group entries (\", entry_index,\r\n              \") exceeds device limit (\", device_limits_.maxBindingsPerBindGroup, \").\");\r\n\r\n  WGPUBindGroupLayout bind_group_layout = program_artifact.compute_pipeline.GetBindGroupLayout(0).MoveToCHandle();\r\n  WGPUBindGroupDescriptor bind_group_desc{};\r\n  bind_group_desc.layout = bind_group_layout;\r\n  bind_group_desc.entryCount = bind_group_entries.size();\r\n  bind_group_desc.entries = bind_group_entries.data();\r\n  bind_group_desc.label = {program_artifact.name.data(), program_artifact.name.length()};\r\n\r\n  auto bind_group = wgpuDeviceCreateBindGroup(Device().Get(), &bind_group_desc);\r\n  if (graph_capture_state_ == GraphCaptureState::Capturing) {\r\n    WGPUBuffer indirect_buffer = nullptr;\r\n    if (indirect_dispatch_tensor != nullptr) {\r\n      indirect_buffer = reinterpret_cast<WGPUBuffer>(const_cast<void*>(indirect_dispatch_tensor->DataRaw()));\r\n    }\r\n    external_captured_commands_->push_back({program_artifact.compute_pipeline,\r\n                                            bind_group,\r\n                                            bind_group_layout,\r\n                                            {x, y, z},\r\n                                            indirect_buffer});\r\n  } else {\r\n    compute_pass_encoder.SetPipeline(program_artifact.compute_pipeline);\r\n    wgpuComputePassEncoderSetBindGroup(compute_pass_encoder.Get(), 0, bind_group, 0, nullptr);\r\n\r\n    if (indirect_dispatch_tensor != nullptr) {\r\n      // Use indirect dispatch\r\n      WGPUBuffer indirect_buffer = reinterpret_cast<WGPUBuffer>(const_cast<void*>(indirect_dispatch_tensor->DataRaw()));\r\n      compute_pass_encoder.DispatchWorkgroupsIndirect(indirect_buffer, 0);\r\n    } else {\r\n      // Use direct dispatch\r\n      compute_pass_encoder.DispatchWorkgroups(x, y, z);\r\n    }\r\n\r\n    wgpuBindGroupRelease(bind_group);\r\n    wgpuBindGroupLayoutRelease(bind_group_layout);\r\n  }\r\n}\r\n\r\nvoid WebGpuContext::CaptureBegin(std::vector<webgpu::CapturedCommandInfo>* captured_commands, const webgpu::BufferManager& buffer_manager) {\r\n  LOGS_DEFAULT(VERBOSE) << \"CaptureBegin with external storage\";\r\n  // Flush any pending commands before we change the status\r\n  Flush(buffer_manager);\r\n\r\n  external_captured_commands_ = captured_commands;\r\n\r\n  // Make sure the external vector is empty before we start capturing\r\n  if (external_captured_commands_) {\r\n    external_captured_commands_->clear();\r\n  }\r\n\r\n  // TODO: support profiling with graph capture.\r\n  ORT_ENFORCE(!is_profiling_, \"profiling is not supported yet under graph capture mode\");\r\n\r\n  graph_capture_state_ = GraphCaptureState::Capturing;\r\n}\r\n\r\nvoid WebGpuContext::Replay(const std::vector<webgpu::CapturedCommandInfo>& captured_commands, const webgpu::BufferManager& buffer_manager) {\r\n  LOGS_DEFAULT(VERBOSE) << \"Replay with external storage\";\r\n  graph_capture_state_ = GraphCaptureState::Replaying;\r\n  // Replay all captured commands from the provided vector\r\n  const size_t command_count = captured_commands.size();\r\n  for (size_t i = 0; i < command_count; ++i) {\r\n    auto& command = captured_commands[i];\r\n    const auto& compute_pass_encoder = GetComputePassEncoder();\r\n    WriteTimestamp(num_pending_dispatches_ * 2);\r\n    compute_pass_encoder.SetPipeline(command.compute_pipeline);\r\n    wgpuComputePassEncoderSetBindGroup(compute_pass_encoder.Get(), 0, command.bind_group, 0, nullptr);\r\n\r\n    if (command.indirect_buffer != nullptr) {\r\n      // Use indirect dispatch\r\n      compute_pass_encoder.DispatchWorkgroupsIndirect(command.indirect_buffer, 0);\r\n    } else {\r\n      // Use direct dispatch\r\n      compute_pass_encoder.DispatchWorkgroups(command.dispatch_group[0], command.dispatch_group[1], command.dispatch_group[2]);\r\n    }\r\n\r\n    WriteTimestamp(num_pending_dispatches_ * 2 + 1);\r\n    ++num_pending_dispatches_;\r\n    if (num_pending_dispatches_ >= max_num_pending_dispatches_ ||\r\n        (is_profiling_ && query_type_ == TimestampQueryType::AtPasses)) {\r\n      EndComputePass();\r\n    }\r\n    if (num_pending_dispatches_ >= max_num_pending_dispatches_) {\r\n      Flush(buffer_manager);\r\n      num_pending_dispatches_ = 0;\r\n    }\r\n  }\r\n\r\n  // Flush any remaining commands\r\n  Flush(buffer_manager);\r\n\r\n  graph_capture_state_ = GraphCaptureState::Default;\r\n}\r\n\r\nvoid WebGpuContext::CaptureEnd() {\r\n  LOGS_DEFAULT(VERBOSE) << \"CaptureEnd\";\r\n\r\n  graph_capture_state_ = GraphCaptureState::Default;\r\n  external_captured_commands_ = nullptr;\r\n}\r\n\r\nvoid WebGpuContext::ReleaseGraphResources(std::vector<webgpu::CapturedCommandInfo>& captured_commands) {\r\n  LOGS_DEFAULT(VERBOSE) << \"ReleaseGraphResources: Releasing \" << captured_commands.size() << \" captured command resources\";\r\n\r\n  for (auto& command : captured_commands) {\r\n    if (command.bind_group != nullptr) {\r\n      wgpuBindGroupRelease(command.bind_group);\r\n      command.bind_group = nullptr;\r\n    }\r\n\r\n    if (command.bind_group_layout != nullptr) {\r\n      wgpuBindGroupLayoutRelease(command.bind_group_layout);\r\n      command.bind_group_layout = nullptr;\r\n    }\r\n  }\r\n}\r\n\r\nstd::unordered_map<int32_t, WebGpuContextFactory::WebGpuContextInfo> WebGpuContextFactory::contexts_;\r\nstd::mutex WebGpuContextFactory::mutex_;\r\nstd::once_flag WebGpuContextFactory::init_default_flag_;\r\nwgpu::Instance WebGpuContextFactory::default_instance_;\r\n\r\nWebGpuContext& WebGpuContextFactory::CreateContext(const WebGpuContextConfig& config) {\r\n  const int context_id = config.context_id;\r\n  WGPUInstance instance = config.instance;\r\n  WGPUDevice device = config.device;\r\n\r\n  std::call_once(init_default_flag_, [\r\n#if !defined(__wasm__)\r\n                                         dawn_proc_table = config.dawn_proc_table\r\n#endif\r\n  ]() {\r\n  // Step.1 - setup dawn proc table (only for non-WASM build)\r\n\r\n#if !defined(__wasm__)\r\n    const DawnProcTable* dawn_procs = reinterpret_cast<const DawnProcTable*>(dawn_proc_table);\r\n#if defined(BUILD_DAWN_SHARED_LIBRARY)\r\n    ORT_ENFORCE(dawn_procs == nullptr, \"setting DawnProcTable is not allowed when dynamically linked to webgpu_dawn.\");\r\n#else\r\n#if !defined(USE_EXTERNAL_DAWN)\r\n    if (dawn_procs == nullptr) {\r\n      dawn_procs = &dawn::native::GetProcs();\r\n    }\r\n#else\r\n    ORT_ENFORCE(dawn_procs != nullptr, \"DawnProcTable must be provided.\");\r\n#endif\r\n    dawnProcSetProcs(dawn_procs);\r\n#endif\r\n#endif\r\n\r\n    // Step.2 - Create wgpu::Instance\r\n    wgpu::InstanceFeatureName required_instance_features[] = {wgpu::InstanceFeatureName::TimedWaitAny};\r\n    wgpu::InstanceDescriptor instance_desc{};\r\n    instance_desc.requiredFeatures = required_instance_features;\r\n    instance_desc.requiredFeatureCount = sizeof(required_instance_features) / sizeof(required_instance_features[0]);\r\n    default_instance_ = wgpu::CreateInstance(&instance_desc);\r\n\r\n    ORT_ENFORCE(default_instance_ != nullptr, \"Failed to create wgpu::Instance.\");\r\n  });\r\n\r\n  if (context_id == 0) {\r\n    // context ID is preserved for the default context. User cannot use context ID 0 as a custom context.\r\n    ORT_ENFORCE(instance == nullptr && device == nullptr,\r\n                \"WebGPU EP default context (contextId=0) must not have custom WebGPU instance or device.\");\r\n\r\n    instance = default_instance_.Get();\r\n  } else {\r\n    // for context ID > 0, user must provide custom WebGPU instance and device.\r\n    ORT_ENFORCE(instance != nullptr && device != nullptr,\r\n                \"WebGPU EP custom context (contextId>0) must have custom WebGPU instance and device.\");\r\n  }\r\n\r\n  std::lock_guard<std::mutex> lock(mutex_);\r\n\r\n  auto it = contexts_.find(context_id);\r\n  if (it == contexts_.end()) {\r\n    GSL_SUPPRESS(r.11)\r\n    auto context = std::unique_ptr<WebGpuContext>(new WebGpuContext(instance, device, config.validation_mode, config.preserve_device, config.small_storage_buffer_binding_size_for_testing, config.power_preference));\r\n    it = contexts_.emplace(context_id, WebGpuContextFactory::WebGpuContextInfo{std::move(context), 0}).first;\r\n  } else if (context_id != 0) {\r\n    ORT_ENFORCE(it->second.context->instance_.Get() == instance &&\r\n                    it->second.context->device_.Get() == device,\r\n                \"WebGPU EP context ID \", context_id, \" is already created with different WebGPU instance or device.\");\r\n  }\r\n  it->second.ref_count++;\r\n  return *it->second.context;\r\n}\r\n\r\nWebGpuContext& WebGpuContextFactory::GetContext(int context_id) {\r\n  std::lock_guard<std::mutex> lock(mutex_);\r\n\r\n  auto it = contexts_.find(context_id);\r\n  ORT_ENFORCE(it != contexts_.end(), \"WebGPU EP context ID \", context_id, \" is not found.\");\r\n\r\n  return *it->second.context;\r\n}\r\n\r\nvoid WebGpuContextFactory::ReleaseContext(int context_id) {\r\n  std::lock_guard<std::mutex> lock(mutex_);\r\n\r\n  auto it = contexts_.find(context_id);\r\n  ORT_ENFORCE(it != contexts_.end(), \"WebGPU EP context ID \", context_id, \" is not found.\");\r\n\r\n  if (--it->second.ref_count == 0 && !it->second.context->preserve_device_) {\r\n    contexts_.erase(it);\r\n  }\r\n}\r\n\r\nvoid WebGpuContextFactory::Cleanup() {\r\n  std::lock_guard<std::mutex> lock(mutex_);\r\n  contexts_.clear();\r\n  default_instance_ = nullptr;\r\n}\r\n\r\nvoid CleanupWebGpuContexts() {\r\n  WebGpuContextFactory::Cleanup();\r\n}\r\n\r\nWGPUDevice GetDevice(int context_id) {\r\n  return WebGpuContextFactory::GetContext(context_id).Device().Get();\r\n}\r\n\r\n}  // namespace webgpu\r\n}  // namespace onnxruntime\r\n"}
{"path":"\\\\?\\C:\\Users\\Naomi\\Desktop\\ONNX\\onnxruntime\\orttraining\\orttraining\\core\\optimizer\\megatron_transformer.cc","line":null,"char":null,"code":"CLANGFORMAT","severity":"warning","name":"format","description":"See https://clang.llvm.org/docs/ClangFormat.html.\nRun `lintrunner -a` to apply this patch.","original":"// Copyright (c) Microsoft Corporation. All rights reserved.\r\n// Licensed under the MIT License.\r\n\r\n#include \"core/optimizer/initializer.h\"\r\n#include \"orttraining/core/framework/distributed_run_context.h\"\r\n#include \"orttraining/core/graph/optimizer_builder.h\"\r\n#include \"orttraining/core/optimizer/megatron_transformer.h\"\r\n#include \"core/graph/graph_utils.h\"\r\n#include \"core/optimizer/utils.h\"\r\n#include \"core/framework/random_seed.h\"\r\n#include <deque>\r\n\r\nusing namespace ONNX_NAMESPACE;\r\nusing namespace ::onnxruntime::common;\r\nnamespace onnxruntime {\r\n\r\nstruct OpInfo {\r\n  OpInfo(const char* op_type,\r\n         const std::initializer_list<OperatorSetVersion>& supported_versions,\r\n         const char* domain = kOnnxDomain,\r\n         const size_t output_count = 1) : op_type(op_type),\r\n                                          supported_versions(supported_versions),\r\n                                          domain(domain),\r\n                                          output_count(output_count) {};\r\n\r\n  std::string op_type;\r\n  std::initializer_list<OperatorSetVersion> supported_versions;\r\n  std::string domain;\r\n  size_t output_count;\r\n};\r\n\r\nconst std::initializer_list<ONNX_NAMESPACE::OperatorSetVersion> opset_v1_13 = {1, 13};\r\nconst std::initializer_list<ONNX_NAMESPACE::OperatorSetVersion> opset_v1_11_13 = {1, 11, 13};\r\nconst std::initializer_list<ONNX_NAMESPACE::OperatorSetVersion> opset_v2_11_13 = {2, 11, 13};\r\nconst std::initializer_list<ONNX_NAMESPACE::OperatorSetVersion> opset_v5_13 = {5, 13};\r\nconst std::initializer_list<ONNX_NAMESPACE::OperatorSetVersion> opset_v1_6_7_13_14 = {1, 6, 7, 13, 14};\r\nconst std::initializer_list<ONNX_NAMESPACE::OperatorSetVersion> opset_v7_13_14 = {7, 13, 14};\r\nconst std::initializer_list<ONNX_NAMESPACE::OperatorSetVersion> opset_v9 = {9};\r\nconst std::initializer_list<ONNX_NAMESPACE::OperatorSetVersion> opset_v9_13 = {9, 13};\r\nconst std::initializer_list<ONNX_NAMESPACE::OperatorSetVersion> opset_v12_13 = {12, 13};\r\nconst OpInfo add_info = OpInfo(\"Add\", opset_v7_13_14);\r\nconst OpInfo split_info = OpInfo(\"Split\", opset_v2_11_13, kOnnxDomain, 3);\r\nconst OpInfo reshape_info = OpInfo(\"Reshape\", opset_v5_13);\r\nconst OpInfo transpose_info = OpInfo(\"Transpose\", opset_v1_13);\r\nconst OpInfo matmul_info = OpInfo(\"MatMul\", opset_v9_13);\r\nconst OpInfo div_info = OpInfo(\"Div\", opset_v7_13_14);\r\nconst OpInfo mul_info = OpInfo(\"Mul\", opset_v1_6_7_13_14);\r\nconst OpInfo sub_info = OpInfo(\"Sub\", opset_v7_13_14);\r\nconst OpInfo softmax_info = OpInfo(\"Softmax\", opset_v1_11_13);\r\nconst OpInfo dropout_info = OpInfo(\"Dropout\", opset_v12_13);\r\nconst OpInfo where_info = OpInfo(\"Where\", opset_v9);\r\n\r\nstruct NodeInfo {\r\n  NodeInfo(const std::vector<OpInfo>& op_infos,\r\n           const bool required = true) : op_infos(op_infos),\r\n                                         required(required) {};\r\n\r\n  std::vector<OpInfo> op_infos;\r\n  bool required;\r\n};\r\n\r\n// Check if it's an expected node given the op infos and provider type.\r\nstatic bool IsExpectedOpAndProvider(const Node& node,\r\n                                    const OpInfo& op_info,\r\n                                    ProviderType provider_type) {\r\n  return graph_utils::IsSupportedOptypeVersionAndDomain(node, op_info.op_type, op_info.supported_versions, op_info.domain) &&\r\n         node.GetExecutionProviderType() == provider_type &&\r\n         node.GetOutputEdgesCount() == op_info.output_count;\r\n}\r\n\r\n// Try to match a linear sub-graph pattern given a list of graph node infos, input node excluded.\r\n// Each node info entry contains a vector of all possible Op infos, and a flag of required or not.\r\n// All visited nodes will be pushed back to a given node vector.\r\nstatic bool MatchLinearPattern(Graph& graph,\r\n                               Node* node,\r\n                               ProviderType provider_type,\r\n                               const std::vector<NodeInfo>& node_infos,\r\n                               InlinedVector<Node*>& sub_graph_node_ptrs) {\r\n  Node* curr_node_ptr = node;\r\n  if (curr_node_ptr->GetOutputEdgesCount() == 0) {\r\n    return node_infos.size() == 0;\r\n  }\r\n\r\n  for (const auto& node_info : node_infos) {\r\n    Node* next_node_ptr = graph.GetNode(curr_node_ptr->OutputNodesBegin()->Index());\r\n    bool has_matched_op = false;\r\n    for (const auto& op_info : node_info.op_infos) {\r\n      if (IsExpectedOpAndProvider(*next_node_ptr, op_info, provider_type)) {\r\n        has_matched_op = true;\r\n        break;\r\n      }\r\n    }\r\n\r\n    sub_graph_node_ptrs.push_back(has_matched_op ? next_node_ptr : nullptr);\r\n    if (has_matched_op) {\r\n      curr_node_ptr = next_node_ptr;\r\n    } else if (node_info.required) {\r\n      return false;\r\n    }\r\n  }\r\n\r\n  return true;\r\n}\r\n\r\n// std::hash only guarantee deterministic value in single execution of a program.\r\n// So use this simple hash to generate dropout seed by name.\r\nstatic uint32_t HashName(const std::string& name) {\r\n  uint32_t hash = 0;\r\n  for (char const& c : name) {\r\n    hash = hash * 101 + c;\r\n  }\r\n\r\n  return hash;\r\n}\r\n\r\ntemplate <class T>\r\nvoid MegatronTransformer::PartitionBufferByColumn(const T* input,\r\n                                                  const int64_t row_count,\r\n                                                  const int64_t column_count,\r\n                                                  const int64_t column_stride,\r\n                                                  const int stride,\r\n                                                  InlinedVector<T>& result) const {\r\n  const int64_t column_stride_partition = column_stride / horizontal_parallel_size_;\r\n\r\n  const int64_t stride_partition_column_offset = horizontal_parallel_rank_ * column_stride_partition;\r\n  for (auto row_index = 0; row_index < row_count; row_index++) {\r\n    const auto row_offset = row_index * column_count;\r\n    for (auto stride_index = 0; stride_index < stride; stride_index++) {\r\n      const auto column_offset = row_offset + stride_index * column_stride + stride_partition_column_offset;\r\n      std::copy(input + column_offset, input + column_offset + column_stride_partition, std::back_inserter(result));\r\n    }\r\n  }\r\n}\r\n\r\nbool MegatronTransformer::PartitionWeightByColumn(const Graph& graph, const NodeArg& input_arg,\r\n                                                  ONNX_NAMESPACE::TensorProto& initializer_partition,\r\n                                                  int stride) const {\r\n  const std::string original_name = input_arg.Name();\r\n  const ONNX_NAMESPACE::TensorProto* tensor_proto;\r\n  if (!graph.GetInitializedTensor(original_name, tensor_proto)) {\r\n    LOGS_DEFAULT(WARNING) << \"PartitionWeightByColumn: \" << original_name << \" is not an initializer\";\r\n    return false;\r\n  }\r\n  auto data_type = tensor_proto->data_type();\r\n  const ONNX_NAMESPACE::TensorShapeProto* shape = input_arg.Shape();\r\n  int rank = shape->dim_size();\r\n  int64_t row_count;\r\n  int64_t column_count;\r\n\r\n  if (rank == 2 && utils::HasDimValue(shape->dim(0)) && utils::HasDimValue(shape->dim(1))) {\r\n    row_count = shape->dim(0).dim_value();\r\n    column_count = shape->dim(1).dim_value();\r\n    weight_partition_info_[original_name].original_dim = TensorShapeVector{row_count, column_count};\r\n  } else if (rank == 1) {\r\n    row_count = 1;\r\n    column_count = shape->dim(0).dim_value();\r\n    weight_partition_info_[original_name].original_dim = TensorShapeVector{column_count};\r\n  } else {\r\n    LOGS_DEFAULT(WARNING) << \"Initializer tensor's rank is \" << rank << \" (expected to be 1 or 2).\";\r\n    return false;\r\n  }\r\n\r\n  if (column_count % (static_cast<int64_t>(horizontal_parallel_size_) * stride) != 0) {\r\n    LOGS_DEFAULT(WARNING) << \"last dim \" << column_count\r\n                          << \" is not divisible by horizontal_parallel_size_ times stride \"\r\n                          << (horizontal_parallel_size_ * stride) << \", not supported currently.\";\r\n    return false;\r\n  }\r\n\r\n  if (stride > 1) {\r\n    LOGS_DEFAULT(WARNING) << \"Checkpointing is not currently supported for graphs requiring partitioning of weight with stride > 1\";\r\n  }\r\n\r\n  auto initializer = Initializer{graph, *tensor_proto, graph.ModelPath()};\r\n  const float* a_weight = initializer.data<float>();\r\n\r\n  std::string new_initializer_name = original_name + \"_column_rank_\" + std::to_string(horizontal_parallel_rank_);\r\n\r\n  initializer_partition.set_name(new_initializer_name);\r\n  initializer_partition.set_data_type(data_type);\r\n\r\n  int64_t column_partition = column_count / horizontal_parallel_size_;\r\n  int64_t column_stride = column_count / stride;\r\n\r\n  TensorShapeVector new_shape;\r\n  if (rank == 2) {\r\n    initializer_partition.add_dims(row_count);\r\n    new_shape.push_back(row_count);\r\n  }\r\n\r\n  initializer_partition.add_dims(column_partition);\r\n  new_shape.push_back(column_partition);\r\n  const int64_t element_count = row_count * column_partition;\r\n\r\n  InlinedVector<float> result;\r\n  result.reserve(element_count);\r\n\r\n  PartitionBufferByColumn(a_weight, row_count, column_count, column_stride, stride, result);\r\n  initializer_partition.set_raw_data(result.data(), element_count * sizeof(float));\r\n\r\n  // Partition initial optimizer state if available\r\n  const auto optim_state_it = initial_optimizer_states_.find(original_name);\r\n  if (optim_state_it != initial_optimizer_states_.end()) {\r\n    auto& initial_states = optim_state_it->second;\r\n    // partition moments same way as the weight\r\n    auto alloc = cpu_execution_provider_.CreatePreferredAllocators()[0];\r\n    for (const auto& moments_prefix : training::MOMENTS_PREFIXES) {\r\n      const auto initial_state_it = initial_states.find(moments_prefix);\r\n      if (initial_state_it != initial_states.end()) {\r\n        auto* init_tensor = initial_state_it->second.GetMutable<Tensor>();\r\n\r\n        OrtValue partitioned;\r\n        auto element_type = init_tensor->DataType();\r\n        TensorShape partition_shape(new_shape);\r\n        std::unique_ptr<Tensor> p_tensor;\r\n\r\n        if (utils::IsPrimitiveDataType<float>(element_type)) {\r\n          float* data_buffer = init_tensor->MutableData<float>();\r\n\r\n          // allocate temporary memory to get the column partitioned state\r\n          InlinedVector<float> result_buffer;\r\n          result_buffer.reserve(element_count);\r\n          PartitionBufferByColumn(data_buffer, row_count, column_count, column_stride, stride, result_buffer);\r\n\r\n          // We need to maintain the initial optimizer states as an OrtValue,\r\n          // which is converted eventually to a TensorProto in the optimizer builder\r\n          // after Megatron and Zero partitioning. This approach saves CPU memory\r\n          // as creating a TensorProto involves a copy, and by delaying the copy until\r\n          // after the partitioning results in a smaller copy only for the optimizer\r\n          // states currently present on the rank.\r\n          // Allocate a new buffer to hold the partitioned optimizer state\r\n          // as column partitioning cannot re-use the original\r\n          // buffer as it is a non-contiguous read\r\n          p_tensor = std::make_unique<Tensor>(element_type,\r\n                                              partition_shape,\r\n                                              alloc);\r\n          float* out_buffer = p_tensor->MutableData<float>();\r\n          memcpy(out_buffer, result_buffer.data(), sizeof(float) * element_count);\r\n        } else if (utils::IsPrimitiveDataType<MLFloat16>(element_type)) {\r\n          MLFloat16* data_buffer = init_tensor->MutableData<MLFloat16>();\r\n\r\n          // allocate temporary memory to get the column partitioned state\r\n          InlinedVector<MLFloat16> result_buffer;\r\n          result_buffer.reserve(element_count);\r\n          PartitionBufferByColumn(data_buffer, row_count, column_count, column_stride, stride, result_buffer);\r\n\r\n          // allocate a new buffer as column partitioning cannot re-use the original\r\n          // buffer as it is a non-contiguous read on original buffer\r\n          p_tensor = std::make_unique<Tensor>(element_type,\r\n                                              partition_shape,\r\n                                              alloc);\r\n          MLFloat16* out_buffer = p_tensor->MutableData<MLFloat16>();\r\n          memcpy(out_buffer, result_buffer.data(), sizeof(MLFloat16) * element_count);\r\n        } else {\r\n          ORT_THROW(\"Unsupported type: \", element_type, \"for initial optimizer moments.\");\r\n        }\r\n        partitioned.Init(p_tensor.release(),\r\n                         DataTypeImpl::GetType<Tensor>(),\r\n                         DataTypeImpl::GetType<Tensor>()->GetDeleteFunc());\r\n        initial_states[moments_prefix] = std::move(partitioned);\r\n      } else {\r\n        LOGS_DEFAULT(WARNING) << \"Initial value for optimizer state: \" << moments_prefix\r\n                              << \" not found for weight: \" << original_name;\r\n      }\r\n    }\r\n  }\r\n\r\n  weight_partition_info_[original_name].megatron_row_partition = 0;\r\n  weight_partition_info_[original_name].partition_name = new_initializer_name;\r\n  weight_partition_info_[original_name].weight_partitioned = true;\r\n\r\n  return true;\r\n}\r\n\r\nbool MegatronTransformer::PartitionWeightByRow(const Graph& graph, const NodeArg& input_arg,\r\n                                               ONNX_NAMESPACE::TensorProto& initializer_partition) const {\r\n  const std::string original_name = input_arg.Name();\r\n  const ONNX_NAMESPACE::TensorProto* tensor_proto;\r\n  if (!graph.GetInitializedTensor(original_name, tensor_proto)) {\r\n    LOGS_DEFAULT(WARNING) << \"PartitionWeightByRow: \" << original_name << \" is not an initializer\";\r\n    return false;\r\n  }\r\n\r\n  auto data_type = tensor_proto->data_type();\r\n  const ONNX_NAMESPACE::TensorShapeProto* shape = input_arg.Shape();\r\n  int rank = shape->dim_size();\r\n  int64_t row_count;\r\n  int64_t column_count;\r\n\r\n  if (rank == 2 && utils::HasDimValue(shape->dim(0)) && utils::HasDimValue(shape->dim(1))) {\r\n    row_count = shape->dim(0).dim_value();\r\n    column_count = shape->dim(1).dim_value();\r\n    weight_partition_info_[original_name].original_dim = {row_count, column_count};\r\n  } else if (rank == 1) {\r\n    row_count = shape->dim(0).dim_value();\r\n    column_count = 1;\r\n    weight_partition_info_[original_name].original_dim = {row_count};\r\n  } else {\r\n    LOGS_DEFAULT(WARNING) << \"Initializer tensor's rank is more than \" << rank\r\n                          << \" (expected to be 1 or 2).\";\r\n    return false;\r\n  }\r\n\r\n  if (row_count % horizontal_parallel_size_ != 0) {\r\n    LOGS_DEFAULT(WARNING) << \"first dim \" << row_count << \" is not divisible by horizontal parallel size\"\r\n                          << horizontal_parallel_size_ << \", not supported currently.\";\r\n    return false;\r\n  }\r\n  auto initializer = Initializer{graph, *tensor_proto, graph.ModelPath()};\r\n  const float* a_weight = initializer.data<float>();\r\n\r\n  std::string new_initializer_name = original_name + \"_row_rank_\" + std::to_string(horizontal_parallel_rank_);\r\n\r\n  initializer_partition.set_name(new_initializer_name);\r\n  initializer_partition.set_data_type(data_type);\r\n\r\n  int64_t row_partition = row_count / horizontal_parallel_size_;\r\n\r\n  TensorShapeVector new_shape;\r\n  initializer_partition.add_dims(row_partition);\r\n  new_shape.push_back(row_partition);\r\n  if (rank == 2) {\r\n    initializer_partition.add_dims(column_count);\r\n    new_shape.push_back(column_count);\r\n  }\r\n  const int64_t element_count = row_partition * column_count;\r\n\r\n  InlinedVector<float> result;\r\n  result.reserve(element_count);\r\n\r\n  const int64_t row_index_offset = horizontal_parallel_rank_ * row_partition;\r\n  memcpy(result.data(), a_weight + row_index_offset * column_count, sizeof(float) * element_count);\r\n  initializer_partition.set_raw_data(result.data(), element_count * sizeof(float));\r\n\r\n  // Partition initial optimizer state if available\r\n  const auto optim_state_it = initial_optimizer_states_.find(original_name);\r\n  if (optim_state_it != initial_optimizer_states_.end()) {\r\n    auto& initial_states = optim_state_it->second;\r\n    for (const auto& moments_prefix : training::MOMENTS_PREFIXES) {\r\n      const auto initial_state_it = initial_states.find(moments_prefix);\r\n      if (initial_state_it != initial_states.end()) {\r\n        auto* init_tensor = initial_state_it->second.GetMutable<Tensor>();\r\n\r\n        OrtValue partitioned;\r\n        auto element_type = init_tensor->DataType();\r\n        TensorShape partition_shape(new_shape);\r\n        const OrtMemoryInfo& info = init_tensor->Location();\r\n        std::unique_ptr<Tensor> p_tensor;\r\n\r\n        if (utils::IsPrimitiveDataType<float>(element_type)) {\r\n          float* data_buffer = init_tensor->MutableData<float>();\r\n\r\n          p_tensor = std::make_unique<Tensor>(element_type,\r\n                                              partition_shape,\r\n                                              data_buffer + row_index_offset * column_count,\r\n                                              info);\r\n        } else if (utils::IsPrimitiveDataType<MLFloat16>(element_type)) {\r\n          MLFloat16* data_buffer = init_tensor->MutableData<MLFloat16>();\r\n\r\n          p_tensor = std::make_unique<Tensor>(element_type,\r\n                                              partition_shape,\r\n                                              data_buffer + row_index_offset * column_count,\r\n                                              info);\r\n\r\n        } else {\r\n          ORT_THROW(\"Unsupported type: \", element_type, \"for initial optimizer moments.\");\r\n        }\r\n        partitioned.Init(p_tensor.release(),\r\n                         DataTypeImpl::GetType<Tensor>(),\r\n                         DataTypeImpl::GetType<Tensor>()->GetDeleteFunc());\r\n        initial_states[moments_prefix] = std::move(partitioned);\r\n      } else {\r\n        LOGS_DEFAULT(WARNING) << \"Initial value for optimizer state: \" << moments_prefix\r\n                              << \" not found for weight: \" << original_name;\r\n      }\r\n    }\r\n  }\r\n\r\n  weight_partition_info_[original_name].megatron_row_partition = 1;\r\n  weight_partition_info_[original_name].partition_name = new_initializer_name;\r\n  weight_partition_info_[original_name].weight_partitioned = true;\r\n  return true;\r\n}\r\n\r\nStatus MegatronTransformer::TransformGPT2MLP(Graph& graph, bool& modified,\r\n                                             InlinedVector<Node*>& nodes_to_clear_shape,\r\n                                             int32_t& counter,\r\n                                             NodeIndex node_index) const {\r\n  auto skip_status = common::Status(common::ONNXRUNTIME, common::NOT_IMPLEMENTED, \"Skip BART Attention megatron transformation\");\r\n\r\n  auto& node = *graph.GetNode(node_index);\r\n\r\n  if (!graph_utils::IsSupportedOptypeVersionAndDomain(node, \"MatMul\", {9, 13}) ||\r\n      !graph_utils::IsSupportedProvider(node, GetCompatibleExecutionProviders()) ||\r\n      node.GetOutputEdgesCount() != 1) {\r\n    return skip_status;\r\n  }\r\n\r\n  if (node.GetInputEdgesCount() > 0) {\r\n    Node& matmul_input_node = const_cast<Node&>(*(node.InputNodesBegin()));\r\n    if (matmul_input_node.OpType().compare(\"MegatronF\") == 0) {\r\n      return skip_status;\r\n    }\r\n  }\r\n\r\n  Node& add_node = *graph.GetNode(node.OutputNodesBegin()->Index());\r\n  if (!graph_utils::IsSupportedOptypeVersionAndDomain(add_node, \"Add\", {7, 13}) ||\r\n      add_node.GetExecutionProviderType() != node.GetExecutionProviderType() ||\r\n      add_node.GetOutputEdgesCount() != 1) {\r\n    return skip_status;\r\n  }\r\n\r\n  Node& gelu_node = *graph.GetNode(add_node.OutputNodesBegin()->Index());\r\n  if (!(graph_utils::IsSupportedOptypeVersionAndDomain(gelu_node, \"Gelu\", {1}, kMSDomain) ||\r\n        graph_utils::IsSupportedOptypeVersionAndDomain(gelu_node, \"FastGelu\", {1}, kMSDomain)) ||\r\n      gelu_node.GetExecutionProviderType() != node.GetExecutionProviderType() ||\r\n      gelu_node.GetOutputEdgesCount() != 1) {\r\n    return skip_status;\r\n  }\r\n\r\n  Node& matmul2_node = *graph.GetNode(gelu_node.OutputNodesBegin()->Index());\r\n  if (!graph_utils::IsSupportedOptypeVersionAndDomain(matmul2_node, \"MatMul\", {9, 13}) ||\r\n      matmul2_node.GetExecutionProviderType() != node.GetExecutionProviderType() ||\r\n      matmul2_node.GetOutputEdgesCount() != 1) {\r\n    return skip_status;\r\n  }\r\n\r\n  Node& add2_node = *graph.GetNode(matmul2_node.OutputNodesBegin()->Index());\r\n  if (!graph_utils::IsSupportedOptypeVersionAndDomain(add2_node, \"Add\", {7, 13}) ||\r\n      add2_node.GetExecutionProviderType() != node.GetExecutionProviderType() ||\r\n      add2_node.GetOutputEdgesCount() != 1) {\r\n    return skip_status;\r\n  }\r\n\r\n  nodes_to_clear_shape.insert(nodes_to_clear_shape.end(), {&node, &add_node, &gelu_node,\r\n                                                           &matmul2_node});\r\n\r\n  auto a_weight_arg = node.MutableInputDefs()[1];\r\n  ONNX_NAMESPACE::TensorProto a_weight_initializer_partition;\r\n  if (!PartitionWeightByColumn(graph, *a_weight_arg, a_weight_initializer_partition)) {\r\n    return skip_status;\r\n  }\r\n\r\n  auto a_bias_arg = add_node.MutableInputDefs()[1];\r\n  ONNX_NAMESPACE::TensorProto a_bias_initializer_partition;\r\n  if (!PartitionWeightByColumn(graph, *a_bias_arg, a_bias_initializer_partition)) {\r\n    return skip_status;\r\n  }\r\n\r\n  auto b_weight_arg = matmul2_node.MutableInputDefs()[1];\r\n  ONNX_NAMESPACE::TensorProto b_weight_initializer_partition;\r\n  if (!PartitionWeightByRow(graph, *b_weight_arg, b_weight_initializer_partition)) {\r\n    return skip_status;\r\n  }\r\n\r\n  NodeArg& a_weight_partition_arg = graph_utils::AddInitializerWithOrtValue(graph, a_weight_initializer_partition);\r\n  graph_utils::ReplaceNodeInput(node, 1, a_weight_partition_arg);\r\n  updated_weight_names_.insert({a_weight_arg->Name(), a_weight_partition_arg.Name()});\r\n\r\n  NodeArg& a_bias_partition_arg = graph_utils::AddInitializerWithOrtValue(graph, a_bias_initializer_partition);\r\n  graph_utils::ReplaceNodeInput(add_node, 1, a_bias_partition_arg);\r\n  updated_weight_names_.insert({b_weight_arg->Name(), a_bias_partition_arg.Name()});\r\n\r\n  NodeArg& b_weight_partition_arg = graph_utils::AddInitializerWithOrtValue(graph, b_weight_initializer_partition);\r\n  graph_utils::ReplaceNodeInput(matmul2_node, 1, b_weight_partition_arg);\r\n  updated_weight_names_.insert({a_bias_arg->Name(), b_weight_partition_arg.Name()});\r\n\r\n  graph.RemoveInitializedTensor(a_weight_arg->Name());\r\n  graph.RemoveInitializedTensor(b_weight_arg->Name());\r\n  graph.RemoveInitializedTensor(a_bias_arg->Name());\r\n\r\n  const std::array mlp_f_input_defs{node.MutableInputDefs()[0]};\r\n  auto mlp_f_type_info = *node.MutableInputDefs()[0]->TypeAsProto();\r\n  auto& mlp_f_out_arg = graph.GetOrCreateNodeArg(graph.GenerateNodeArgName(\"MLP_MegatronF_Output\"), &mlp_f_type_info);\r\n  Node& mlp_f_node = graph.AddNode(graph.GenerateNodeName(\"MLP_MegatronF\"),\r\n                                   \"MegatronF\",\r\n                                   \"MLP MegatronF\",\r\n                                   mlp_f_input_defs,\r\n                                   {&mlp_f_out_arg}, {}, kMSDomain);\r\n  mlp_f_node.SetExecutionProviderType(node.GetExecutionProviderType());\r\n  const Node::EdgeEnd* edge = graph_utils::GetInputEdge(node, 0);\r\n  if (nullptr == edge) {  // handle input/initializer\r\n    graph_utils::ReplaceNodeInput(node, 0, *(mlp_f_node.MutableOutputDefs()[0]));\r\n  } else {\r\n    auto input_node = const_cast<Node*>(&edge->GetNode());\r\n    graph_utils::ReplaceDownstreamNodeInput(graph, *input_node, edge->GetDstArgIndex(), mlp_f_node, 0);\r\n  }\r\n\r\n  const std::array mlp_g_input_defs{matmul2_node.MutableOutputDefs()[0]};\r\n  auto mlp_g_type_info = *matmul2_node.MutableOutputDefs()[0]->TypeAsProto();\r\n  auto& mlp_g_out_arg = graph.GetOrCreateNodeArg(graph.GenerateNodeArgName(\"MLP_MegatronG_Output\"), &mlp_g_type_info);\r\n  Node& mlp_g_node = graph.AddNode(graph.GenerateNodeName(\"MLP_MegatronG\"),\r\n                                   \"MegatronG\",\r\n                                   \"MLP MegatronG\",\r\n                                   mlp_g_input_defs,\r\n                                   {&mlp_g_out_arg}, {}, kMSDomain);\r\n  mlp_g_node.AddAttribute(\"group_type\", static_cast<int64_t>(training::WorkerGroupType::HorizontalParallel));\r\n  mlp_g_node.SetExecutionProviderType(node.GetExecutionProviderType());\r\n  graph_utils::ReplaceDownstreamNodeInput(graph, matmul2_node, 0, mlp_g_node, 0);\r\n  modified = true;\r\n  counter++;\r\n\r\n  return Status::OK();\r\n}\r\n\r\n/*\r\nDenseWeight -- Transpose \\\r\n               MatMul -- BiasGelu -- Dropout -- MatMul -- Add -- Dropout\r\n*/\r\nStatus MegatronTransformer::TransformBARTMLP(Graph& graph, bool& modified,\r\n                                             InlinedVector<Node*>& nodes_to_clear_shape,\r\n                                             InlinedHashSet<Node*>& dropout_nodes_to_transform,\r\n                                             int32_t& counter,\r\n                                             NodeIndex node_index) const {\r\n  auto skip_status = common::Status(common::ONNXRUNTIME, common::NOT_IMPLEMENTED, \"Skip BART Attention megatron transformation\");\r\n\r\n  auto& node = *graph.GetNode(node_index);\r\n  if (!graph_utils::IsSupportedOptypeVersionAndDomain(node, \"MatMul\", {9, 13}) ||\r\n      !graph_utils::IsSupportedProvider(node, GetCompatibleExecutionProviders()) ||\r\n      node.GetOutputEdgesCount() != 1) {\r\n    return skip_status;\r\n  }\r\n  Node* second_op = const_cast<Node*>(graph.GetProducerNode(node.MutableInputDefs()[1]->Name()));\r\n  Node* first_op = const_cast<Node*>(graph.GetProducerNode(node.MutableInputDefs()[0]->Name()));\r\n  if (node.GetInputEdgesCount() > 0) {\r\n    if (second_op == nullptr) {\r\n      return skip_status;\r\n    }\r\n    if (first_op != nullptr && first_op->OpType().compare(\"MegatronF\") == 0) {\r\n      return skip_status;\r\n    }\r\n\r\n    if (second_op->OpType().compare(\"Transpose\") != 0) {\r\n      return skip_status;\r\n    }\r\n  } else {\r\n    return skip_status;\r\n  }\r\n  // check if transpose is only 2-dim\r\n  if (!optimizer_utils::IsAttributeWithExpectedValues(*second_op, \"perm\", {1LL, 0LL})) {\r\n    return skip_status;\r\n  }\r\n  ProviderType provider_type = node.GetExecutionProviderType();\r\n\r\n  Node* biasgelu_node_ptr = graph.GetNode(node.OutputNodesBegin()->Index());\r\n  Node& biasgelu_node = *biasgelu_node_ptr;\r\n  if (!graph_utils::IsSupportedOptypeVersionAndDomain(biasgelu_node, \"BiasGelu\", {1}, kMSDomain) ||\r\n      biasgelu_node.GetExecutionProviderType() != provider_type ||\r\n      biasgelu_node.GetOutputEdgesCount() != 1) {\r\n    return skip_status;\r\n  }\r\n\r\n  // Either Dropout->Matmul or just Matmul\r\n  Node* dropout_node = nullptr;\r\n  Node* next_node = graph.GetNode(biasgelu_node.OutputNodesBegin()->Index());\r\n  if (IsExpectedOpAndProvider(*next_node, dropout_info, provider_type)) {\r\n    dropout_node = next_node;\r\n    next_node = graph.GetNode(dropout_node->OutputNodesBegin()->Index());\r\n  }\r\n  if (!IsExpectedOpAndProvider(*next_node, matmul_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n  Node& matmul2_node = *next_node;\r\n\r\n  Node& add_node = *graph.GetNode(matmul2_node.OutputNodesBegin()->Index());\r\n  if (!IsExpectedOpAndProvider(add_node, add_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n  Node& dropout2_node = *graph.GetNode(add_node.OutputNodesBegin()->Index());\r\n  if (!IsExpectedOpAndProvider(dropout2_node, dropout_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n  Node* transpose_op_ptr = const_cast<Node*>(graph.GetProducerNode(matmul2_node.MutableInputDefs()[1]->Name()));\r\n  if (transpose_op_ptr == nullptr || !IsExpectedOpAndProvider(*transpose_op_ptr, transpose_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n\r\n  nodes_to_clear_shape.insert(nodes_to_clear_shape.end(), {&node, second_op, &biasgelu_node,\r\n                                                           &matmul2_node, transpose_op_ptr});\r\n  if (dropout_node != nullptr) {\r\n    nodes_to_clear_shape.insert(nodes_to_clear_shape.end(), {dropout_node});\r\n  }\r\n\r\n  auto dense_wi_weight_arg = second_op->MutableInputDefs()[0];\r\n  ONNX_NAMESPACE::TensorProto dense_wi_weight_initializer_partition;\r\n  if (!PartitionWeightByRow(graph, *dense_wi_weight_arg, dense_wi_weight_initializer_partition)) {\r\n    return skip_status;\r\n  }\r\n\r\n  // since the bias doesn't get transposed, partitioning by col\r\n  auto dense_wi_bias_arg = biasgelu_node.MutableInputDefs()[1];\r\n  ONNX_NAMESPACE::TensorProto dense_wi_bias_initializer_partition;\r\n  if (!PartitionWeightByColumn(graph, *dense_wi_bias_arg, dense_wi_bias_initializer_partition)) {\r\n    return skip_status;\r\n  }\r\n\r\n  auto dense_wo_weight_arg = transpose_op_ptr->MutableInputDefs()[0];\r\n  ONNX_NAMESPACE::TensorProto dense_wo_weight_initializer_partition;\r\n  if (!PartitionWeightByColumn(graph, *dense_wo_weight_arg, dense_wo_weight_initializer_partition)) {\r\n    return skip_status;\r\n  }\r\n\r\n  NodeArg& dense_wi_weight_partition_arg = graph_utils::AddInitializerWithOrtValue(graph, dense_wi_weight_initializer_partition);\r\n  graph_utils::ReplaceNodeInput(*second_op, 0, dense_wi_weight_partition_arg);\r\n  updated_weight_names_.insert({dense_wi_weight_arg->Name(), dense_wi_weight_partition_arg.Name()});\r\n\r\n  NodeArg& dense_wi_bias_partition_arg = graph_utils::AddInitializerWithOrtValue(graph, dense_wi_bias_initializer_partition);\r\n  graph_utils::ReplaceNodeInput(biasgelu_node, 1, dense_wi_bias_partition_arg);\r\n  updated_weight_names_.insert({dense_wi_bias_arg->Name(), dense_wi_bias_partition_arg.Name()});\r\n\r\n  NodeArg& dense_wo_weight_partition_arg = graph_utils::AddInitializerWithOrtValue(graph, dense_wo_weight_initializer_partition);\r\n  graph_utils::ReplaceNodeInput(*transpose_op_ptr, 0, dense_wo_weight_partition_arg);\r\n  updated_weight_names_.insert({dense_wo_weight_arg->Name(), dense_wo_weight_partition_arg.Name()});\r\n\r\n  graph.RemoveInitializedTensor(dense_wi_weight_arg->Name());\r\n  graph.RemoveInitializedTensor(dense_wi_bias_arg->Name());\r\n  graph.RemoveInitializedTensor(dense_wo_weight_arg->Name());\r\n\r\n  if (dropout_node) {\r\n    dropout_nodes_to_transform.insert(dropout_node);\r\n  }\r\n\r\n  const std::array mlp_f_input_defs{node.MutableInputDefs()[0]};\r\n  auto mlp_f_type_info = *node.MutableInputDefs()[0]->TypeAsProto();\r\n  auto& mlp_f_out_arg = graph.GetOrCreateNodeArg(graph.GenerateNodeArgName(\"BART_MLP_MegatronF_Output\"), &mlp_f_type_info);\r\n  Node& mlp_f_node = graph.AddNode(graph.GenerateNodeName(\"BART_MLP_MegatronF\"),\r\n                                   \"MegatronF\",\r\n                                   \"MLP MegatronF\",\r\n                                   mlp_f_input_defs,\r\n                                   {&mlp_f_out_arg}, {}, kMSDomain);\r\n  counter++;\r\n  mlp_f_node.SetExecutionProviderType(node.GetExecutionProviderType());\r\n  const Node::EdgeEnd* edge = graph_utils::GetInputEdge(node, 0);\r\n  if (nullptr == edge) {  // handle input/initializer\r\n    graph_utils::ReplaceNodeInput(node, 0, *(mlp_f_node.MutableOutputDefs()[0]));\r\n  } else {\r\n    auto input_node = const_cast<Node*>(&edge->GetNode());\r\n    graph_utils::ReplaceDownstreamNodeInput(graph, *input_node, edge->GetSrcArgIndex(), mlp_f_node, 0);\r\n  }\r\n\r\n  const std::array mlp_g_input_defs{matmul2_node.MutableOutputDefs()[0]};\r\n  auto mlp_g_type_info = *matmul2_node.MutableOutputDefs()[0]->TypeAsProto();\r\n  auto& mlp_g_out_arg = graph.GetOrCreateNodeArg(graph.GenerateNodeArgName(\"BART_MLP_MegatronG_Output\"), &mlp_g_type_info);\r\n  Node& mlp_g_node = graph.AddNode(graph.GenerateNodeName(\"BART_MLP_MegatronG\"),\r\n                                   \"MegatronG\",\r\n                                   \"MLP MegatronG\",\r\n                                   mlp_g_input_defs,\r\n                                   {&mlp_g_out_arg}, {}, kMSDomain);\r\n  mlp_g_node.AddAttribute(\"group_type\", static_cast<int64_t>(training::WorkerGroupType::HorizontalParallel));\r\n  mlp_g_node.SetExecutionProviderType(node.GetExecutionProviderType());\r\n  graph_utils::ReplaceDownstreamNodeInput(graph, matmul2_node, 0, mlp_g_node, 0);\r\n  modified = true;\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus MegatronTransformer::TransformGPT2Attention(Graph& graph, bool& modified,\r\n                                                   InlinedVector<Node*>& nodes_to_clear_shape,\r\n                                                   InlinedHashSet<Node*>& dropout_nodes_to_transform,\r\n                                                   int32_t& counter,\r\n                                                   NodeIndex node_index) const {\r\n  auto skip_status = common::Status(common::ONNXRUNTIME, common::NOT_IMPLEMENTED, \"Skip BART Attention megatron transformation\");\r\n\r\n  // Self attention sub-graph.\r\n  // MatMul->Add->Split->Reshape->Transpose->MatMul->Div->Mul->Sub->Softmax->Dropout->MatMul->Transpose->Reshape->MatMul->Add\r\n  //                  |->Reshape->Transpose->|                                        |\r\n  //                  |->Reshape->Transpose------------------------------------------>|\r\n\r\n  auto& node = *graph.GetNode(node_index);\r\n  if (!graph_utils::IsSupportedOptypeVersionAndDomain(node, \"MatMul\", opset_v9_13) ||\r\n      !graph_utils::IsSupportedProvider(node, GetCompatibleExecutionProviders()) ||\r\n      node.GetOutputEdgesCount() != 1) {\r\n    return skip_status;\r\n  }\r\n\r\n  if (node.GetInputEdgesCount() > 0 && node.InputNodesBegin()->OpType().compare(\"MegatronF\") == 0) {\r\n    return skip_status;\r\n  }\r\n\r\n  InlinedVector<Node*> sub_graph_node_ptrs;\r\n  sub_graph_node_ptrs.push_back(&node);\r\n  ProviderType provider_type = node.GetExecutionProviderType();\r\n\r\n  std::vector<NodeInfo> linear_pattern = {\r\n      NodeInfo({add_info}),  // -15\r\n      NodeInfo({split_info}),\r\n      NodeInfo({reshape_info}),\r\n      NodeInfo({transpose_info}),\r\n      NodeInfo({matmul_info}),  // -11\r\n      NodeInfo({div_info}),\r\n      NodeInfo({mul_info}),\r\n      NodeInfo({sub_info}),\r\n      NodeInfo({softmax_info}),\r\n      NodeInfo({dropout_info}, false),  // -6\r\n      NodeInfo({matmul_info}),\r\n      NodeInfo({transpose_info}),\r\n      NodeInfo({reshape_info}),\r\n      NodeInfo({matmul_info}),\r\n      NodeInfo({add_info})};  // -1\r\n  if (!MatchLinearPattern(graph, &node, provider_type, linear_pattern, sub_graph_node_ptrs)) {\r\n    return skip_status;\r\n  }\r\n\r\n  // Get all useful nodes here as more vector push back below will change the index.\r\n  Node& add_node = *sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 15];\r\n  Node& split_node = *sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 14];\r\n  Node& k_transpose_after_reshape_node = *sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 12];\r\n  Node* matmul_node_ptr = sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 11];\r\n  Node* dropout_node_ptr = sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 6];\r\n  Node* matmul_node_ptr1 = sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 5];\r\n  Node& transpose_node1 = *sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 4];\r\n  Node& matmul_node = *sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 2];\r\n\r\n  // Transpose node attribute checking.\r\n  if (!optimizer_utils::IsAttributeWithExpectedValues(k_transpose_after_reshape_node, \"perm\", {0LL, 2LL, 1LL, 3LL}) ||\r\n      !optimizer_utils::IsAttributeWithExpectedValues(transpose_node1, \"perm\", {0LL, 2LL, 1LL, 3LL})) {\r\n    return skip_status;\r\n  }\r\n\r\n  InlinedVector<Node*> transpose_node_ptrs;  // For the 2nd and 3rd transpose nodes after split node for sub-graph structure checking.\r\n  InlinedVector<Node*> reshape_node_ptrs;    // To keep the reshape node that need to change the shape constant.\r\n  reshape_node_ptrs.push_back(sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 13]);\r\n  reshape_node_ptrs.push_back(sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 3]);\r\n  auto split_output_iter = split_node.OutputNodesBegin();\r\n  ++split_output_iter;\r\n  for (; split_output_iter != split_node.OutputNodesEnd(); ++split_output_iter) {\r\n    Node* reshape_node_ptr = graph.GetNode((*split_output_iter).Index());\r\n    if (!IsExpectedOpAndProvider(*reshape_node_ptr, reshape_info, provider_type)) {\r\n      break;\r\n    }\r\n\r\n    Node* transpose_node_ptr = graph.GetNode(reshape_node_ptr->OutputNodesBegin()->Index());\r\n    if (!IsExpectedOpAndProvider(*transpose_node_ptr, transpose_info, provider_type)) {\r\n      break;\r\n    }\r\n\r\n    reshape_node_ptrs.push_back(reshape_node_ptr);\r\n    sub_graph_node_ptrs.push_back(reshape_node_ptr);\r\n    transpose_node_ptrs.push_back(transpose_node_ptr);\r\n    sub_graph_node_ptrs.push_back(transpose_node_ptr);\r\n  }\r\n\r\n  // Sub-graph structure and transpose attribute checking.\r\n  if (transpose_node_ptrs.size() != 2 ||\r\n      matmul_node_ptr != graph.GetNode(transpose_node_ptrs[0]->OutputNodesBegin()->Index()) ||\r\n      matmul_node_ptr1 != graph.GetNode(transpose_node_ptrs[1]->OutputNodesBegin()->Index()) ||\r\n      !optimizer_utils::IsAttributeWithExpectedValues(*transpose_node_ptrs[0], \"perm\", {0LL, 2LL, 3LL, 1LL}) ||\r\n      !optimizer_utils::IsAttributeWithExpectedValues(*transpose_node_ptrs[1], \"perm\", {0LL, 2LL, 1LL, 3LL})) {\r\n    return skip_status;\r\n  }\r\n\r\n  // Partition weights. If any of them fails, skip transforming this sub-graph.\r\n  auto qkv_weight_arg = node.MutableInputDefs()[1];\r\n  ONNX_NAMESPACE::TensorProto qkv_weight_initializer_partition;\r\n  if (!PartitionWeightByColumn(graph, *qkv_weight_arg, qkv_weight_initializer_partition, 3)) {\r\n    return skip_status;\r\n  }\r\n\r\n  auto qkv_bias_arg = add_node.MutableInputDefs()[1];\r\n  ONNX_NAMESPACE::TensorProto qkv_bias_initializer_partition;\r\n  if (!PartitionWeightByColumn(graph, *qkv_bias_arg, qkv_bias_initializer_partition, 3)) {\r\n    return skip_status;\r\n  }\r\n\r\n  auto dense_weight_arg = matmul_node.MutableInputDefs()[1];\r\n  ONNX_NAMESPACE::TensorProto dense_weight_initializer_partition;\r\n  if (!PartitionWeightByRow(graph, *dense_weight_arg, dense_weight_initializer_partition)) {\r\n    return skip_status;\r\n  }\r\n\r\n  // Check the constant value in the Reshape nodes.\r\n  bool is_reshape_valid = true;\r\n  for (Node* node_ptr : reshape_node_ptrs) {\r\n    auto shape_arg = node_ptr->MutableInputDefs()[1];\r\n    const ONNX_NAMESPACE::TensorProto* tensor;\r\n\r\n    if (!graph.GetInitializedTensor(shape_arg->Name(), tensor)) {\r\n      is_reshape_valid = false;\r\n      break;\r\n    }\r\n\r\n    auto data_type = tensor->data_type();\r\n    if (data_type != ONNX_NAMESPACE::TensorProto_DataType_INT64) {\r\n      is_reshape_valid = false;\r\n      break;\r\n    }\r\n\r\n    // The number of the values should be more than 2, and the 3rd value should be divisible by parallel size,\r\n    // i.e., the attention head number should be divisible by parallel size.\r\n    auto init_const = Initializer{graph, *tensor, graph.ModelPath()};\r\n    if (init_const.size() != 3 && init_const.size() != 4) {\r\n      is_reshape_valid = false;\r\n      break;\r\n    }\r\n\r\n    const int64_t* val = init_const.data<int64_t>();\r\n    if (val[2] % horizontal_parallel_size_ != 0) {\r\n      LOGS_DEFAULT(WARNING) << (init_const.size() == 3 ? \"Hidden size \" : \"Number of attention heads \") << val[2]\r\n                            << \" is not divisible by horizontal_parallel_size_ \"\r\n                            << horizontal_parallel_size_ << \", not supported currently.\";\r\n      is_reshape_valid = false;\r\n      break;\r\n    }\r\n  }\r\n\r\n  if (!is_reshape_valid) {\r\n    return skip_status;\r\n  }\r\n\r\n  // Ready to transform the sub-graph when reach here.\r\n  // It's possible that the node vector contains nullptr due to some optinal node infos during linear pattern matching.\r\n  std::copy_if(sub_graph_node_ptrs.begin(), sub_graph_node_ptrs.end(),\r\n               std::back_inserter(nodes_to_clear_shape),\r\n               [](Node* node_ptr) { return node_ptr != nullptr; });\r\n\r\n  // Replace by the partition weights.\r\n  NodeArg& qkv_weight_partition_arg = graph_utils::AddInitializerWithOrtValue(graph, qkv_weight_initializer_partition);\r\n  graph_utils::ReplaceNodeInput(node, 1, qkv_weight_partition_arg);\r\n  updated_weight_names_.insert({qkv_weight_arg->Name(), qkv_weight_partition_arg.Name()});\r\n\r\n  NodeArg& qkv_bias_partition_arg = graph_utils::AddInitializerWithOrtValue(graph, qkv_bias_initializer_partition);\r\n  graph_utils::ReplaceNodeInput(add_node, 1, qkv_bias_partition_arg);\r\n  updated_weight_names_.insert({qkv_bias_arg->Name(), qkv_bias_partition_arg.Name()});\r\n\r\n  NodeArg& dense_weight_partition_arg = graph_utils::AddInitializerWithOrtValue(graph, dense_weight_initializer_partition);\r\n  graph_utils::ReplaceNodeInput(matmul_node, 1, dense_weight_partition_arg);\r\n  updated_weight_names_.insert({dense_weight_arg->Name(), dense_weight_partition_arg.Name()});\r\n\r\n  graph.RemoveInitializedTensor(qkv_weight_arg->Name());\r\n  graph.RemoveInitializedTensor(qkv_bias_arg->Name());\r\n  graph.RemoveInitializedTensor(dense_weight_arg->Name());\r\n\r\n  // Change the constant for the reshape nodes.\r\n  for (Node* node_ptr : reshape_node_ptrs) {\r\n    auto shape_arg = node_ptr->MutableInputDefs()[1];\r\n    const ONNX_NAMESPACE::TensorProto* tensor;\r\n    graph.GetInitializedTensor(shape_arg->Name(), tensor);\r\n    auto data_type = tensor->data_type();\r\n    auto init_const = Initializer{graph, *tensor, graph.ModelPath()};\r\n    const int64_t* val = init_const.data<int64_t>();\r\n    int64_t size = init_const.size();\r\n    ONNX_NAMESPACE::TensorProto tensor_partition;\r\n    tensor_partition.set_name(graph.GenerateNodeArgName(\"partition_\" + shape_arg->Name()));\r\n    tensor_partition.set_data_type(data_type);\r\n    tensor_partition.add_dims(size);\r\n\r\n    InlinedVector<int64_t> val_partition;\r\n    val_partition.reserve(size);\r\n    val_partition.insert(val_partition.end(), val, val + size);\r\n    val_partition[2] /= horizontal_parallel_size_;\r\n    tensor_partition.set_raw_data(val_partition.data(), size * sizeof(int64_t));\r\n    NodeArg& node_arg_partition = graph_utils::AddInitializerWithOrtValue(graph, tensor_partition);\r\n    graph_utils::ReplaceNodeInput(*node_ptr, 1, node_arg_partition);\r\n    graph.RemoveInitializedTensor(shape_arg->Name());\r\n  }\r\n\r\n  if (dropout_node_ptr != nullptr) {\r\n    dropout_nodes_to_transform.insert(dropout_node_ptr);\r\n  }\r\n\r\n  // Add MegatronF before the 1st MatMul and MegatronG before the last Add.\r\n  const std::array sa_f_input_defs{node.MutableInputDefs()[0]};\r\n  auto sa_f_type_info = *node.MutableInputDefs()[0]->TypeAsProto();\r\n  auto& sa_f_out_arg = graph.GetOrCreateNodeArg(graph.GenerateNodeArgName(\"SeftAttention_MegatronF_Output\"), &sa_f_type_info);\r\n  Node& sa_f_node = graph.AddNode(graph.GenerateNodeName(node.Name() + \"SeftAttention_MegatronF\"),\r\n                                  \"MegatronF\",\r\n                                  \"SeftAttention MegatronF\",\r\n                                  sa_f_input_defs,\r\n                                  {&sa_f_out_arg}, {}, kMSDomain);\r\n  sa_f_node.SetExecutionProviderType(node.GetExecutionProviderType());\r\n  const Node::EdgeEnd* edge = graph_utils::GetInputEdge(node, 0);\r\n  if (nullptr == edge) {  // handle input/initializer\r\n    graph_utils::ReplaceNodeInput(node, 0, *(sa_f_node.MutableOutputDefs()[0]));\r\n  } else {\r\n    auto input_node = const_cast<Node*>(&edge->GetNode());\r\n    graph_utils::ReplaceDownstreamNodeInput(graph, *input_node, edge->GetDstArgIndex(), sa_f_node, 0);\r\n  }\r\n\r\n  const std::array sa_g_input_defs{matmul_node.MutableOutputDefs()[0]};\r\n  auto sa_g_type_info = *matmul_node.MutableOutputDefs()[0]->TypeAsProto();  // copy\r\n  auto& sa_g_out_arg = graph.GetOrCreateNodeArg(graph.GenerateNodeArgName(\"SeftAttention_MegatronG_Output\"), &sa_g_type_info);\r\n  Node& sa_g_node = graph.AddNode(graph.GenerateNodeName(node.Name() + \"SelfAttention_MegatronG\"),\r\n                                  \"MegatronG\",\r\n                                  \"Attention MegatronG\",\r\n                                  sa_g_input_defs,\r\n                                  {&sa_g_out_arg}, {}, kMSDomain);\r\n  sa_g_node.AddAttribute(\"group_type\", static_cast<int64_t>(training::WorkerGroupType::HorizontalParallel));\r\n  sa_g_node.SetExecutionProviderType(node.GetExecutionProviderType());\r\n  graph_utils::ReplaceDownstreamNodeInput(graph, matmul_node, 0, sa_g_node, 0);\r\n  modified = true;\r\n  counter++;\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus MegatronTransformer::TransformBARTAttention(Graph& graph, bool& modified,\r\n                                                   InlinedVector<Node*>& nodes_to_clear_shape,\r\n                                                   InlinedHashSet<Node*>& dropout_nodes_to_transform,\r\n                                                   int32_t& counter,\r\n                                                   NodeIndex node_index) const {\r\n  auto skip_status = common::Status(common::ONNXRUNTIME, common::NOT_IMPLEMENTED, \"Skip BART Attention megatron transformation\");\r\n\r\n  // Self/Enc-Dec Attention sub-graph.\r\n  //\r\n  // MatMul->Add->Mul->Reshape->Transpose->MatMul->Reshape->Where->Reshape->Softmax->Dropout->MatMul->Transpose->Reshape->MatMul->Add->Droupout\r\n  // MatMul->Add->Reshape->Transpose-------> |                                                  |\r\n  // MatMul->Add->Reshape->Transpose----------------------------------------------------------> |\r\n  auto& node = *graph.GetNode(node_index);\r\n\r\n  if (!graph_utils::IsSupportedOptypeVersionAndDomain(node, \"MatMul\", opset_v9_13) ||\r\n      !graph_utils::IsSupportedProvider(node, GetCompatibleExecutionProviders()) ||\r\n      node.GetOutputEdgesCount() != 1) {\r\n    return skip_status;\r\n  }\r\n\r\n  Node* q_matmul_input_node_ptr = const_cast<Node*>(graph.GetProducerNode(node.MutableInputDefs()[0]->Name()));\r\n  if (q_matmul_input_node_ptr != nullptr && q_matmul_input_node_ptr->OpType().compare(\"MegatronF\") == 0) {\r\n    return skip_status;\r\n  }\r\n  InlinedVector<Node*> sub_graph_node_ptrs;\r\n  sub_graph_node_ptrs.push_back(&node);\r\n  ProviderType provider_type = node.GetExecutionProviderType();\r\n\r\n  std::vector<NodeInfo> linear_pattern = {\r\n      NodeInfo({add_info}),\r\n      NodeInfo({mul_info}),\r\n      NodeInfo({reshape_info}),\r\n      NodeInfo({transpose_info}),\r\n      NodeInfo({matmul_info}),\r\n      NodeInfo({add_info}, false),  // -13\r\n      NodeInfo({reshape_info}),\r\n      NodeInfo({where_info}),\r\n      NodeInfo({reshape_info}),\r\n      NodeInfo({softmax_info}),\r\n      NodeInfo({dropout_info}, false),  // -8\r\n      NodeInfo({matmul_info}),\r\n      NodeInfo({add_info}, false),  // -6\r\n      NodeInfo({transpose_info}),\r\n      NodeInfo({reshape_info}),\r\n      NodeInfo({matmul_info}),  // -3\r\n      NodeInfo({add_info}),\r\n      NodeInfo({dropout_info}, false)};  // -1\r\n  if (!MatchLinearPattern(graph, &node, provider_type, linear_pattern, sub_graph_node_ptrs)) {\r\n    return skip_status;\r\n  }\r\n  // Get all useful nodes here as more vector push back below will change the index.\r\n  // Other than the optional nodes in the pattern, all other node pointers are valid\r\n  // if they match the linear pattern.\r\n  Node* q_biasadd_node_ptr = sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 18];\r\n  Node* q_transpose_after_reshape_node_ptr = sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 15];\r\n  Node* qk_matmul_node_ptr = sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 14];\r\n  Node* dropout_node_ptr = sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 8];\r\n  Node* qkv_matmul_node_ptr = sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 7];\r\n  Node* transpose_node1_ptr = sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 5];\r\n  Node& dense_matmul_node = *sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 3];\r\n\r\n  // Transpose node attribute checking.\r\n  if (!optimizer_utils::IsAttributeWithExpectedValues(*q_transpose_after_reshape_node_ptr, \"perm\", {1LL, 0LL, 2LL}) ||\r\n      !optimizer_utils::IsAttributeWithExpectedValues(*transpose_node1_ptr, \"perm\", {1LL, 0LL, 2LL})) {\r\n    return skip_status;\r\n  }\r\n  // map between reshape node and dim of reshape that must be modified\r\n  std::unordered_map<Node*, size_t> reshape_node_ptrs;\r\n  reshape_node_ptrs[sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 16]] = 1;\r\n  reshape_node_ptrs[sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 12]] = 1;\r\n  reshape_node_ptrs[sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 10]] = 0;\r\n  reshape_node_ptrs[sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 4]] = 2;\r\n  // till now node should be q matmul operation\r\n\r\n  InlinedVector<Node*> weight_transpose_node_ptrs;\r\n  InlinedVector<Node*> bias_add_node_ptrs;\r\n\r\n  Node* q_transpose_ptr = const_cast<Node*>(graph.GetProducerNode(node.MutableInputDefs()[1]->Name()));\r\n  if (q_transpose_ptr == nullptr || !IsExpectedOpAndProvider(*q_transpose_ptr, transpose_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n  weight_transpose_node_ptrs.push_back(q_transpose_ptr);\r\n  sub_graph_node_ptrs.push_back(q_transpose_ptr);\r\n  bias_add_node_ptrs.push_back(q_biasadd_node_ptr);\r\n\r\n  Node* k_transpose_ptr = const_cast<Node*>(graph.GetProducerNode(qk_matmul_node_ptr->MutableInputDefs()[1]->Name()));\r\n  if (k_transpose_ptr == nullptr || !IsExpectedOpAndProvider(*k_transpose_ptr, transpose_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n  sub_graph_node_ptrs.push_back(k_transpose_ptr);\r\n\r\n  Node* k_reshape_ptr = const_cast<Node*>(graph.GetProducerNode(k_transpose_ptr->MutableInputDefs()[0]->Name()));\r\n  if (k_reshape_ptr == nullptr || !IsExpectedOpAndProvider(*k_reshape_ptr, reshape_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n  reshape_node_ptrs[k_reshape_ptr] = 1;\r\n  sub_graph_node_ptrs.push_back(k_reshape_ptr);\r\n\r\n  Node* k_add_ptr = const_cast<Node*>(graph.GetProducerNode(k_reshape_ptr->MutableInputDefs()[0]->Name()));\r\n  if (k_add_ptr == nullptr || !IsExpectedOpAndProvider(*k_add_ptr, add_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n  sub_graph_node_ptrs.push_back(k_add_ptr);\r\n  bias_add_node_ptrs.push_back(k_add_ptr);\r\n\r\n  Node* k_matmul_ptr = const_cast<Node*>(graph.GetProducerNode(k_add_ptr->MutableInputDefs()[0]->Name()));\r\n  if (k_matmul_ptr == nullptr || !IsExpectedOpAndProvider(*k_matmul_ptr, matmul_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n  sub_graph_node_ptrs.push_back(k_matmul_ptr);\r\n\r\n  Node* k_weight_transpose_ptr = const_cast<Node*>(graph.GetProducerNode(k_matmul_ptr->MutableInputDefs()[1]->Name()));\r\n  if (k_weight_transpose_ptr == nullptr || !IsExpectedOpAndProvider(*k_weight_transpose_ptr, transpose_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n  sub_graph_node_ptrs.push_back(k_weight_transpose_ptr);\r\n  weight_transpose_node_ptrs.push_back(k_weight_transpose_ptr);\r\n\r\n  Node* v_transpose_ptr = const_cast<Node*>(graph.GetProducerNode(qkv_matmul_node_ptr->MutableInputDefs()[1]->Name()));\r\n  if (v_transpose_ptr == nullptr || !IsExpectedOpAndProvider(*v_transpose_ptr, transpose_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n  sub_graph_node_ptrs.push_back(v_transpose_ptr);\r\n\r\n  Node* v_reshape_ptr = const_cast<Node*>(graph.GetProducerNode(v_transpose_ptr->MutableInputDefs()[0]->Name()));\r\n  if (v_reshape_ptr == nullptr || !IsExpectedOpAndProvider(*v_reshape_ptr, reshape_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n  reshape_node_ptrs[v_reshape_ptr] = 1;\r\n  sub_graph_node_ptrs.push_back(v_reshape_ptr);\r\n\r\n  Node* v_add_ptr = const_cast<Node*>(graph.GetProducerNode(v_reshape_ptr->MutableInputDefs()[0]->Name()));\r\n  if (v_add_ptr == nullptr || !IsExpectedOpAndProvider(*v_add_ptr, add_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n  sub_graph_node_ptrs.push_back(v_add_ptr);\r\n  bias_add_node_ptrs.push_back(v_add_ptr);\r\n\r\n  Node* v_matmul_ptr = const_cast<Node*>(graph.GetProducerNode(v_add_ptr->MutableInputDefs()[0]->Name()));\r\n  if (k_matmul_ptr == nullptr || !IsExpectedOpAndProvider(*k_matmul_ptr, matmul_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n  sub_graph_node_ptrs.push_back(v_matmul_ptr);\r\n\r\n  Node* v_weight_transpose_ptr = const_cast<Node*>(graph.GetProducerNode(v_matmul_ptr->MutableInputDefs()[1]->Name()));\r\n  if (v_weight_transpose_ptr == nullptr || !IsExpectedOpAndProvider(*v_weight_transpose_ptr, transpose_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n  sub_graph_node_ptrs.push_back(v_weight_transpose_ptr);\r\n  weight_transpose_node_ptrs.push_back(v_weight_transpose_ptr);\r\n\r\n  // K and V matmul must have the same input\r\n  Node* q_matmul_ptr = &node;\r\n  if (k_matmul_ptr->MutableInputDefs()[0]->Name() != v_matmul_ptr->MutableInputDefs()[0]->Name()) {\r\n    return skip_status;\r\n  }\r\n\r\n  // Check the constant value in the Reshape nodes.\r\n  bool is_reshape_valid = true;\r\n  for (auto x : reshape_node_ptrs) {\r\n    Node* node_ptr = x.first;\r\n    auto idx = x.second;\r\n    auto shape_arg = node_ptr->MutableInputDefs()[1];\r\n    const ONNX_NAMESPACE::TensorProto* tensor;\r\n    if (!graph.GetInitializedTensor(shape_arg->Name(), tensor)) {\r\n      is_reshape_valid = false;\r\n      break;\r\n    }\r\n    auto data_type = tensor->data_type();\r\n    if (data_type != ONNX_NAMESPACE::TensorProto_DataType_INT64) {\r\n      is_reshape_valid = false;\r\n      break;\r\n    }\r\n    // The number of the values should be more than idx, and the idx'th value should be divisible by parallel size,\r\n    // i.e., the attention head number should be divisible by parallel size.\r\n    auto init_const = Initializer{graph, *tensor, graph.ModelPath()};\r\n    if (init_const.size() <= idx) {\r\n      is_reshape_valid = false;\r\n      break;\r\n    }\r\n    const int64_t* val = init_const.data<int64_t>();\r\n    if (val[idx] % horizontal_parallel_size_ != 0) {\r\n      LOGS_DEFAULT(WARNING) << \"dim[\" << idx << \"]: \" << val[idx]\r\n                            << \" is not divisible by horizontal_parallel_size_ \"\r\n                            << horizontal_parallel_size_ << \", not supported currently.\";\r\n      is_reshape_valid = false;\r\n      break;\r\n    }\r\n  }\r\n\r\n  if (!is_reshape_valid) {\r\n    return skip_status;\r\n  }\r\n\r\n  // Partition weights. If any of them fails, skip transforming the rest.\r\n  std::vector<ONNX_NAMESPACE::TensorProto> qkv_weight_initializer_partitions;\r\n  for (auto trans_ptr : weight_transpose_node_ptrs) {\r\n    auto qkv_weight_arg = trans_ptr->MutableInputDefs()[0];\r\n    ONNX_NAMESPACE::TensorProto qkv_weight_initializer_partition;\r\n    if (!PartitionWeightByRow(graph, *qkv_weight_arg, qkv_weight_initializer_partition)) {\r\n      break;\r\n    }\r\n    qkv_weight_initializer_partitions.push_back(qkv_weight_initializer_partition);\r\n  }\r\n\r\n  // Partition bias. If any of them fails, skip transforming the rest.\r\n  std::vector<ONNX_NAMESPACE::TensorProto> qkv_bias_initializer_partitions;\r\n  for (auto add_ptr : bias_add_node_ptrs) {\r\n    auto qkv_bias_arg = add_ptr->MutableInputDefs()[1];\r\n    ONNX_NAMESPACE::TensorProto qkv_bias_initializer_partition;\r\n    if (!PartitionWeightByColumn(graph, *qkv_bias_arg, qkv_bias_initializer_partition)) {\r\n      break;\r\n    }\r\n    qkv_bias_initializer_partitions.push_back(qkv_bias_initializer_partition);\r\n  }\r\n\r\n  // if all the weights or biases weren't transformed, skip transforming this subgraph\r\n  if (weight_transpose_node_ptrs.size() != qkv_weight_initializer_partitions.size()) {\r\n    return skip_status;\r\n  }\r\n  if (bias_add_node_ptrs.size() != qkv_bias_initializer_partitions.size()) {\r\n    return skip_status;\r\n  }\r\n\r\n  // transform the dense weight. If it fails, skip transforming this subgraph.\r\n  Node* last_transpose = const_cast<Node*>(graph.GetProducerNode(dense_matmul_node.MutableInputDefs()[1]->Name()));\r\n  auto dense_weight_arg = last_transpose->MutableInputDefs()[0];\r\n  ONNX_NAMESPACE::TensorProto dense_weight_initializer_partition;\r\n  if (!PartitionWeightByColumn(graph, *dense_weight_arg, dense_weight_initializer_partition)) {\r\n    return skip_status;\r\n  }\r\n\r\n  // Ready to transform the sub-graph when reach here.\r\n  // Replace node inputs\r\n  size_t i = 0;\r\n  for (auto trans_ptr : weight_transpose_node_ptrs) {\r\n    auto weight_name = trans_ptr->MutableInputDefs()[0]->Name();\r\n    NodeArg& qkv_weight_partition_arg = graph_utils::AddInitializerWithOrtValue(graph, qkv_weight_initializer_partitions[i]);\r\n    graph_utils::ReplaceNodeInput(*trans_ptr, 0, qkv_weight_partition_arg);\r\n    graph.RemoveInitializedTensor(weight_name);\r\n    updated_weight_names_.insert({weight_name, qkv_weight_partition_arg.Name()});\r\n    i++;\r\n  }\r\n  i = 0;\r\n  for (auto add_ptr : bias_add_node_ptrs) {\r\n    auto bias_name = add_ptr->MutableInputDefs()[1]->Name();\r\n    NodeArg& qkv_bias_partition_arg = graph_utils::AddInitializerWithOrtValue(graph, qkv_bias_initializer_partitions[i]);\r\n    graph_utils::ReplaceNodeInput(*add_ptr, 1, qkv_bias_partition_arg);\r\n    graph.RemoveInitializedTensor(bias_name);\r\n    updated_weight_names_.insert({bias_name, qkv_bias_partition_arg.Name()});\r\n    i++;\r\n  }\r\n\r\n  NodeArg& dense_weight_partition_arg = graph_utils::AddInitializerWithOrtValue(graph, dense_weight_initializer_partition);\r\n  graph_utils::ReplaceNodeInput(*last_transpose, 0, dense_weight_partition_arg);\r\n  graph.RemoveInitializedTensor(dense_weight_arg->Name());\r\n  updated_weight_names_.insert({dense_weight_arg->Name(), dense_weight_partition_arg.Name()});\r\n\r\n  // It's possible that the node vector contains nullptr due to some optinal node infos during linear pattern matching.\r\n  std::copy_if(sub_graph_node_ptrs.begin(), sub_graph_node_ptrs.end(),\r\n               std::back_inserter(nodes_to_clear_shape),\r\n               [](Node* node_ptr) { return node_ptr != nullptr; });\r\n\r\n  // Change the constant for the reshape nodes.\r\n  for (auto x : reshape_node_ptrs) {\r\n    Node* node_ptr = x.first;\r\n    int64_t idx = x.second;\r\n    auto shape_arg = node_ptr->MutableInputDefs()[1];\r\n    const ONNX_NAMESPACE::TensorProto* tensor;\r\n    ORT_RETURN_IF_NOT(graph.GetInitializedTensor(shape_arg->Name(), tensor),\r\n                      \"Expecting initializer present: \", shape_arg->Name());\r\n    auto data_type = tensor->data_type();\r\n    auto init_const = Initializer{graph, *tensor, graph.ModelPath()};\r\n    const int64_t* val = init_const.data<int64_t>();\r\n    int64_t size = init_const.size();\r\n    ONNX_NAMESPACE::TensorProto tensor_partition;\r\n    tensor_partition.set_name(graph.GenerateNodeArgName(\"partition_\" + shape_arg->Name()));\r\n    tensor_partition.set_data_type(data_type);\r\n    tensor_partition.add_dims(size);\r\n\r\n    InlinedVector<int64_t> val_partition;\r\n    val_partition.reserve(size);\r\n    val_partition.insert(val_partition.end(), val, val + size);\r\n    val_partition[idx] /= horizontal_parallel_size_;\r\n    tensor_partition.set_raw_data(val_partition.data(), size * sizeof(int64_t));\r\n    NodeArg& node_arg_partition = graph_utils::AddInitializerWithOrtValue(graph, tensor_partition);\r\n    graph_utils::ReplaceNodeInput(*node_ptr, 1, node_arg_partition);\r\n    graph.RemoveInitializedTensor(shape_arg->Name());\r\n  }\r\n\r\n  if (dropout_node_ptr != nullptr) {\r\n    dropout_nodes_to_transform.insert(dropout_node_ptr);\r\n  }\r\n\r\n  // Add MegatronF before the 1st MatMul and MegatronG before the last Add.\r\n\r\n  NodeArg* prev_input_node_ptr = k_matmul_ptr->MutableInputDefs()[0];\r\n  InlinedVector<Node*> new_consumer_nodes;\r\n  const auto& node_consumers = graph.GetConsumerNodes(prev_input_node_ptr->Name());\r\n  for (auto& n : node_consumers) {\r\n    if (n->Index() == k_matmul_ptr->Index() || n->Index() == v_matmul_ptr->Index() || n->Index() == q_matmul_ptr->Index()) {\r\n      continue;\r\n    }\r\n    new_consumer_nodes.emplace_back(const_cast<Node*>(n));\r\n  }\r\n\r\n  bool shared_same_input = k_matmul_ptr->MutableInputDefs()[0]->Name().compare(q_matmul_ptr->MutableInputDefs()[0]->Name()) == 0;\r\n\r\n  // then for q, and k&v will have different MegatronF node.\r\n  {\r\n    const std::array sa_f_input_defs{prev_input_node_ptr};\r\n    auto sa_f_type_info = *prev_input_node_ptr->TypeAsProto();\r\n    auto& sa_f_out_arg = graph.GetOrCreateNodeArg(graph.GenerateNodeArgName(k_matmul_ptr->Name() + \"BARTAttention_MegatronF_Output\"), &sa_f_type_info);\r\n    Node& sa_f_node = graph.AddNode(graph.GenerateNodeName(k_matmul_ptr->Name() + \"BARTAttention_MegatronF\"),\r\n                                    \"MegatronF\",\r\n                                    k_matmul_ptr->Name() + \" BARTAttention MegatronF\",\r\n                                    sa_f_input_defs,\r\n                                    {&sa_f_out_arg}, {}, kMSDomain);\r\n    sa_f_node.SetExecutionProviderType(k_matmul_ptr->GetExecutionProviderType());\r\n    graph_utils::ReplaceNodeInput(*k_matmul_ptr, 0, *(sa_f_node.MutableOutputDefs()[0]));\r\n    graph_utils::ReplaceNodeInput(*v_matmul_ptr, 0, *(sa_f_node.MutableOutputDefs()[0]));\r\n    if (shared_same_input) {\r\n      graph_utils::ReplaceNodeInput(*q_matmul_ptr, 0, *(sa_f_node.MutableOutputDefs()[0]));\r\n    }\r\n    new_consumer_nodes.push_back(&sa_f_node);\r\n  }\r\n  graph.UpdateConsumerNodes(prev_input_node_ptr->Name(), new_consumer_nodes);\r\n  counter++;\r\n  if (!shared_same_input) {\r\n    {\r\n      NodeArg* q_prev_input_node_ptr = q_matmul_ptr->MutableInputDefs()[0];\r\n      InlinedVector<Node*> q_new_consumer_nodes;\r\n      const auto& q_node_consumers = graph.GetConsumerNodes(q_prev_input_node_ptr->Name());\r\n      for (auto& n : q_node_consumers) {\r\n        if (n->Index() == k_matmul_ptr->Index() || n->Index() == v_matmul_ptr->Index() || n->Index() == q_matmul_ptr->Index()) {\r\n          continue;\r\n        }\r\n        q_new_consumer_nodes.emplace_back(const_cast<Node*>(n));\r\n      }\r\n\r\n      const std::array q_sa_f_input_defs{q_matmul_ptr->MutableInputDefs()[0]};\r\n      auto q_sa_f_type_info = *q_matmul_ptr->MutableInputDefs()[0]->TypeAsProto();\r\n      auto& q_sa_f_out_arg = graph.GetOrCreateNodeArg(graph.GenerateNodeArgName(q_matmul_ptr->Name() + \"BARTAttention_MegatronF_Output\"), &q_sa_f_type_info);\r\n      Node& q_sa_f_node = graph.AddNode(graph.GenerateNodeName(q_matmul_ptr->Name() + \"BARTAttention_MegatronF\"),\r\n                                        \"MegatronF\",\r\n                                        q_matmul_ptr->Name() + \" BARTAttention MegatronF\",\r\n                                        q_sa_f_input_defs,\r\n                                        {&q_sa_f_out_arg}, {}, kMSDomain);\r\n      q_sa_f_node.SetExecutionProviderType(q_matmul_ptr->GetExecutionProviderType());\r\n\r\n      graph_utils::ReplaceNodeInput(*q_matmul_ptr, 0, *(q_sa_f_node.MutableOutputDefs()[0]));\r\n      q_new_consumer_nodes.push_back(&q_sa_f_node);\r\n      graph.UpdateConsumerNodes(q_prev_input_node_ptr->Name(), q_new_consumer_nodes);\r\n      // todo: need update the consumer node for the input_node as well.\r\n    }\r\n  }\r\n\r\n  const std::array sa_g_input_defs{dense_matmul_node.MutableOutputDefs()[0]};\r\n  auto sa_g_type_info = *dense_matmul_node.MutableOutputDefs()[0]->TypeAsProto();  // copy\r\n  auto& sa_g_out_arg = graph.GetOrCreateNodeArg(graph.GenerateNodeArgName(\"BARTAttention_MegatronG_Output\"), &sa_g_type_info);\r\n  Node& sa_g_node = graph.AddNode(graph.GenerateNodeName(k_matmul_ptr->Name() + \"BARTAttention_MegatronG\"),\r\n                                  \"MegatronG\",\r\n                                  \"BARTAttention MegatronG\",\r\n                                  sa_g_input_defs,\r\n                                  {&sa_g_out_arg}, {}, kMSDomain);\r\n  sa_g_node.AddAttribute(\"group_type\", static_cast<int64_t>(training::WorkerGroupType::HorizontalParallel));\r\n  sa_g_node.SetExecutionProviderType(k_matmul_ptr->GetExecutionProviderType());\r\n  graph_utils::ReplaceDownstreamNodeInput(graph, dense_matmul_node, 0, sa_g_node, 0);\r\n\r\n  modified = true;\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus MegatronTransformer::DoTransform(Graph& graph, bool& modified, int graph_level,\r\n                                        const logging::Logger& logger,\r\n                                        InlinedVector<Node*>& nodes_to_clear_shape,\r\n                                        InlinedHashSet<Node*>& dropout_nodes_to_transform) const {\r\n  InlinedVector<int> counters(4);\r\n  GraphViewer graph_viewer(graph);\r\n  const auto& node_topology_list = graph_viewer.GetNodesInTopologicalOrder();\r\n  for (auto node_index : node_topology_list) {\r\n    auto& node = *graph.GetNode(node_index);\r\n    ORT_RETURN_IF_ERROR(Recurse(node, modified, graph_level, logger));\r\n    int i = 0;\r\n    auto ret = TransformGPT2Attention(graph, modified, nodes_to_clear_shape,\r\n                                      dropout_nodes_to_transform, counters[i++], node_index);\r\n    if (ret.Code() != common::NOT_IMPLEMENTED) {\r\n      ORT_ENFORCE(ret.IsOK());\r\n      continue;\r\n    }\r\n\r\n    ret = TransformGPT2MLP(graph, modified, nodes_to_clear_shape, counters[i++], node_index);\r\n    if (ret.Code() != common::NOT_IMPLEMENTED) {\r\n      ORT_ENFORCE(ret.IsOK());\r\n      continue;\r\n    }\r\n\r\n    ret = TransformBARTAttention(graph, modified, nodes_to_clear_shape,\r\n                                 dropout_nodes_to_transform, counters[i++], node_index);\r\n    if (ret.Code() != common::NOT_IMPLEMENTED) {\r\n      ORT_ENFORCE(ret.IsOK());\r\n      continue;\r\n    }\r\n\r\n    ret = TransformBARTMLP(graph, modified, nodes_to_clear_shape,\r\n                           dropout_nodes_to_transform, counters[i++], node_index);\r\n    if (ret.Code() != common::NOT_IMPLEMENTED) {\r\n      ORT_ENFORCE(ret.IsOK());\r\n      continue;\r\n    }\r\n  }\r\n\r\n  LOGS_DEFAULT(WARNING) << \"Megatron transformer result : Partitioned \"\r\n                        << counters[0] << \" GPT2 Attention Blocks, \"\r\n                        << counters[1] << \" GPT2 MLP Blocks, \"\r\n                        << counters[2] << \" BART Attention Blocks, \"\r\n                        << counters[3] << \" BART MLP Blocks.\";\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus MegatronTransformer::TransformDropout(Graph& graph, bool& modified, int graph_level, const logging::Logger& logger,\r\n                                             InlinedHashSet<Node*>& dropout_nodes_to_transform, int32_t& counter) const {\r\n  GraphViewer graph_viewer(graph);\r\n  const auto& node_topology_list = graph_viewer.GetNodesInTopologicalOrder();\r\n  for (auto node_index : node_topology_list) {\r\n    auto& node = *graph.GetNode(node_index);\r\n    ORT_RETURN_IF_ERROR(Recurse(node, modified, graph_level, logger));\r\n\r\n    if (!graph_utils::IsSupportedProvider(node, GetCompatibleExecutionProviders())) {\r\n      continue;\r\n    }\r\n\r\n    if (!graph_utils::IsSupportedOptypeVersionAndDomain(node, \"Dropout\", opset_v12_13)) {\r\n      continue;\r\n    }\r\n\r\n    // Only need to set the seed if it's a transformed self-attention dropout, or the seed attribute is not set.\r\n    if (dropout_nodes_to_transform.find(&node) != dropout_nodes_to_transform.end()) {\r\n      int64_t seed = static_cast<int64_t>(HashName(node.MutableOutputDefs()[0]->Name())) + utils::GetRandomSeed();\r\n      if (dropout_nodes_to_transform.find(&node) != dropout_nodes_to_transform.end()) {\r\n        seed += horizontal_parallel_rank_;\r\n      }\r\n\r\n      if (graph_utils::GetNodeAttribute(node, \"seed\") != nullptr) {\r\n        node.ClearAttribute(\"seed\");\r\n      }\r\n      node.AddAttribute(\"seed\", seed);\r\n      counter++;\r\n      modified = true;\r\n    }\r\n  }\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus MegatronTransformer::ApplyImpl(Graph& graph, bool& modified, int graph_level, const logging::Logger& logger) const {\r\n  if (horizontal_parallel_size_ <= 1) {\r\n    return Status::OK();\r\n  }\r\n\r\n  InlinedVector<Node*> nodes_to_clear_shape;\r\n  InlinedHashSet<Node*> dropout_nodes_to_transform;\r\n  int32_t dropout_changed = 0;\r\n\r\n  ORT_RETURN_IF_ERROR(DoTransform(graph, modified, graph_level, logger,\r\n                                  nodes_to_clear_shape, dropout_nodes_to_transform));\r\n  ORT_RETURN_IF_ERROR(TransformDropout(graph, modified, graph_level, logger,\r\n                                       dropout_nodes_to_transform, dropout_changed));\r\n\r\n  auto& graph_inputs = graph.GetInputs();\r\n  for (auto node : nodes_to_clear_shape) {\r\n    if (node != nullptr) {\r\n      auto& inputs = node->MutableInputDefs();\r\n      for (auto* input : inputs)\r\n        if (std::find(graph_inputs.begin(), graph_inputs.end(), input) == graph_inputs.end())\r\n          input->ClearShape();\r\n\r\n      for (auto* output : node->MutableOutputDefs())\r\n        if (std::find(graph_inputs.begin(), graph_inputs.end(), output) == graph_inputs.end())\r\n          output->ClearShape();\r\n    }\r\n  }\r\n\r\n  for (auto x : updated_weight_names_) {\r\n    auto old_initializer_name = x.first;\r\n    auto new_initializer_name = x.second;\r\n    if (weights_to_train_.find(old_initializer_name) != weights_to_train_.end()) {\r\n      weights_to_train_.erase(old_initializer_name);\r\n      weights_to_train_.insert(new_initializer_name);\r\n    }\r\n  }\r\n\r\n  if (modified) {\r\n    graph.SetGraphResolveNeeded();\r\n    auto ret = graph.Resolve();\r\n    LOGS(logger, WARNING) << \"Megatron transformer result: Reset seed for \" << dropout_changed\r\n                          << \" Dropout nodes. Error Message (if there is): \" << ret.ErrorMessage();\r\n    return ret;\r\n  }\r\n\r\n  LOGS(logger, WARNING) << \"Megatron transformer result : unmodified\\n\";\r\n  return Status::OK();\r\n}\r\n\r\n}  // namespace onnxruntime\r\n","replacement":"// Copyright (c) Microsoft Corporation. All rights reserved.\r\n// Licensed under the MIT License.\r\n\r\n#include \"core/optimizer/initializer.h\"\r\n#include \"orttraining/core/framework/distributed_run_context.h\"\r\n#include \"orttraining/core/graph/optimizer_builder.h\"\r\n#include \"orttraining/core/optimizer/megatron_transformer.h\"\r\n#include \"core/graph/graph_utils.h\"\r\n#include \"core/optimizer/utils.h\"\r\n#include \"core/framework/random_seed.h\"\r\n#include <deque>\r\n\r\nusing namespace ONNX_NAMESPACE;\r\nusing namespace ::onnxruntime::common;\r\nnamespace onnxruntime {\r\n\r\nstruct OpInfo {\r\n  OpInfo(const char* op_type,\r\n         const std::initializer_list<OperatorSetVersion>& supported_versions,\r\n         const char* domain = kOnnxDomain,\r\n         const size_t output_count = 1) : op_type(op_type),\r\n                                          supported_versions(supported_versions),\r\n                                          domain(domain),\r\n                                          output_count(output_count){};\r\n\r\n  std::string op_type;\r\n  std::initializer_list<OperatorSetVersion> supported_versions;\r\n  std::string domain;\r\n  size_t output_count;\r\n};\r\n\r\nconst std::initializer_list<ONNX_NAMESPACE::OperatorSetVersion> opset_v1_13 = {1, 13};\r\nconst std::initializer_list<ONNX_NAMESPACE::OperatorSetVersion> opset_v1_11_13 = {1, 11, 13};\r\nconst std::initializer_list<ONNX_NAMESPACE::OperatorSetVersion> opset_v2_11_13 = {2, 11, 13};\r\nconst std::initializer_list<ONNX_NAMESPACE::OperatorSetVersion> opset_v5_13 = {5, 13};\r\nconst std::initializer_list<ONNX_NAMESPACE::OperatorSetVersion> opset_v1_6_7_13_14 = {1, 6, 7, 13, 14};\r\nconst std::initializer_list<ONNX_NAMESPACE::OperatorSetVersion> opset_v7_13_14 = {7, 13, 14};\r\nconst std::initializer_list<ONNX_NAMESPACE::OperatorSetVersion> opset_v9 = {9};\r\nconst std::initializer_list<ONNX_NAMESPACE::OperatorSetVersion> opset_v9_13 = {9, 13};\r\nconst std::initializer_list<ONNX_NAMESPACE::OperatorSetVersion> opset_v12_13 = {12, 13};\r\nconst OpInfo add_info = OpInfo(\"Add\", opset_v7_13_14);\r\nconst OpInfo split_info = OpInfo(\"Split\", opset_v2_11_13, kOnnxDomain, 3);\r\nconst OpInfo reshape_info = OpInfo(\"Reshape\", opset_v5_13);\r\nconst OpInfo transpose_info = OpInfo(\"Transpose\", opset_v1_13);\r\nconst OpInfo matmul_info = OpInfo(\"MatMul\", opset_v9_13);\r\nconst OpInfo div_info = OpInfo(\"Div\", opset_v7_13_14);\r\nconst OpInfo mul_info = OpInfo(\"Mul\", opset_v1_6_7_13_14);\r\nconst OpInfo sub_info = OpInfo(\"Sub\", opset_v7_13_14);\r\nconst OpInfo softmax_info = OpInfo(\"Softmax\", opset_v1_11_13);\r\nconst OpInfo dropout_info = OpInfo(\"Dropout\", opset_v12_13);\r\nconst OpInfo where_info = OpInfo(\"Where\", opset_v9);\r\n\r\nstruct NodeInfo {\r\n  NodeInfo(const std::vector<OpInfo>& op_infos,\r\n           const bool required = true) : op_infos(op_infos),\r\n                                         required(required){};\r\n\r\n  std::vector<OpInfo> op_infos;\r\n  bool required;\r\n};\r\n\r\n// Check if it's an expected node given the op infos and provider type.\r\nstatic bool IsExpectedOpAndProvider(const Node& node,\r\n                                    const OpInfo& op_info,\r\n                                    ProviderType provider_type) {\r\n  return graph_utils::IsSupportedOptypeVersionAndDomain(node, op_info.op_type, op_info.supported_versions, op_info.domain) &&\r\n         node.GetExecutionProviderType() == provider_type &&\r\n         node.GetOutputEdgesCount() == op_info.output_count;\r\n}\r\n\r\n// Try to match a linear sub-graph pattern given a list of graph node infos, input node excluded.\r\n// Each node info entry contains a vector of all possible Op infos, and a flag of required or not.\r\n// All visited nodes will be pushed back to a given node vector.\r\nstatic bool MatchLinearPattern(Graph& graph,\r\n                               Node* node,\r\n                               ProviderType provider_type,\r\n                               const std::vector<NodeInfo>& node_infos,\r\n                               InlinedVector<Node*>& sub_graph_node_ptrs) {\r\n  Node* curr_node_ptr = node;\r\n  if (curr_node_ptr->GetOutputEdgesCount() == 0) {\r\n    return node_infos.size() == 0;\r\n  }\r\n\r\n  for (const auto& node_info : node_infos) {\r\n    Node* next_node_ptr = graph.GetNode(curr_node_ptr->OutputNodesBegin()->Index());\r\n    bool has_matched_op = false;\r\n    for (const auto& op_info : node_info.op_infos) {\r\n      if (IsExpectedOpAndProvider(*next_node_ptr, op_info, provider_type)) {\r\n        has_matched_op = true;\r\n        break;\r\n      }\r\n    }\r\n\r\n    sub_graph_node_ptrs.push_back(has_matched_op ? next_node_ptr : nullptr);\r\n    if (has_matched_op) {\r\n      curr_node_ptr = next_node_ptr;\r\n    } else if (node_info.required) {\r\n      return false;\r\n    }\r\n  }\r\n\r\n  return true;\r\n}\r\n\r\n// std::hash only guarantee deterministic value in single execution of a program.\r\n// So use this simple hash to generate dropout seed by name.\r\nstatic uint32_t HashName(const std::string& name) {\r\n  uint32_t hash = 0;\r\n  for (char const& c : name) {\r\n    hash = hash * 101 + c;\r\n  }\r\n\r\n  return hash;\r\n}\r\n\r\ntemplate <class T>\r\nvoid MegatronTransformer::PartitionBufferByColumn(const T* input,\r\n                                                  const int64_t row_count,\r\n                                                  const int64_t column_count,\r\n                                                  const int64_t column_stride,\r\n                                                  const int stride,\r\n                                                  InlinedVector<T>& result) const {\r\n  const int64_t column_stride_partition = column_stride / horizontal_parallel_size_;\r\n\r\n  const int64_t stride_partition_column_offset = horizontal_parallel_rank_ * column_stride_partition;\r\n  for (auto row_index = 0; row_index < row_count; row_index++) {\r\n    const auto row_offset = row_index * column_count;\r\n    for (auto stride_index = 0; stride_index < stride; stride_index++) {\r\n      const auto column_offset = row_offset + stride_index * column_stride + stride_partition_column_offset;\r\n      std::copy(input + column_offset, input + column_offset + column_stride_partition, std::back_inserter(result));\r\n    }\r\n  }\r\n}\r\n\r\nbool MegatronTransformer::PartitionWeightByColumn(const Graph& graph, const NodeArg& input_arg,\r\n                                                  ONNX_NAMESPACE::TensorProto& initializer_partition,\r\n                                                  int stride) const {\r\n  const std::string original_name = input_arg.Name();\r\n  const ONNX_NAMESPACE::TensorProto* tensor_proto;\r\n  if (!graph.GetInitializedTensor(original_name, tensor_proto)) {\r\n    LOGS_DEFAULT(WARNING) << \"PartitionWeightByColumn: \" << original_name << \" is not an initializer\";\r\n    return false;\r\n  }\r\n  auto data_type = tensor_proto->data_type();\r\n  const ONNX_NAMESPACE::TensorShapeProto* shape = input_arg.Shape();\r\n  int rank = shape->dim_size();\r\n  int64_t row_count;\r\n  int64_t column_count;\r\n\r\n  if (rank == 2 && utils::HasDimValue(shape->dim(0)) && utils::HasDimValue(shape->dim(1))) {\r\n    row_count = shape->dim(0).dim_value();\r\n    column_count = shape->dim(1).dim_value();\r\n    weight_partition_info_[original_name].original_dim = TensorShapeVector{row_count, column_count};\r\n  } else if (rank == 1) {\r\n    row_count = 1;\r\n    column_count = shape->dim(0).dim_value();\r\n    weight_partition_info_[original_name].original_dim = TensorShapeVector{column_count};\r\n  } else {\r\n    LOGS_DEFAULT(WARNING) << \"Initializer tensor's rank is \" << rank << \" (expected to be 1 or 2).\";\r\n    return false;\r\n  }\r\n\r\n  if (column_count % (static_cast<int64_t>(horizontal_parallel_size_) * stride) != 0) {\r\n    LOGS_DEFAULT(WARNING) << \"last dim \" << column_count\r\n                          << \" is not divisible by horizontal_parallel_size_ times stride \"\r\n                          << (horizontal_parallel_size_ * stride) << \", not supported currently.\";\r\n    return false;\r\n  }\r\n\r\n  if (stride > 1) {\r\n    LOGS_DEFAULT(WARNING) << \"Checkpointing is not currently supported for graphs requiring partitioning of weight with stride > 1\";\r\n  }\r\n\r\n  auto initializer = Initializer{graph, *tensor_proto, graph.ModelPath()};\r\n  const float* a_weight = initializer.data<float>();\r\n\r\n  std::string new_initializer_name = original_name + \"_column_rank_\" + std::to_string(horizontal_parallel_rank_);\r\n\r\n  initializer_partition.set_name(new_initializer_name);\r\n  initializer_partition.set_data_type(data_type);\r\n\r\n  int64_t column_partition = column_count / horizontal_parallel_size_;\r\n  int64_t column_stride = column_count / stride;\r\n\r\n  TensorShapeVector new_shape;\r\n  if (rank == 2) {\r\n    initializer_partition.add_dims(row_count);\r\n    new_shape.push_back(row_count);\r\n  }\r\n\r\n  initializer_partition.add_dims(column_partition);\r\n  new_shape.push_back(column_partition);\r\n  const int64_t element_count = row_count * column_partition;\r\n\r\n  InlinedVector<float> result;\r\n  result.reserve(element_count);\r\n\r\n  PartitionBufferByColumn(a_weight, row_count, column_count, column_stride, stride, result);\r\n  initializer_partition.set_raw_data(result.data(), element_count * sizeof(float));\r\n\r\n  // Partition initial optimizer state if available\r\n  const auto optim_state_it = initial_optimizer_states_.find(original_name);\r\n  if (optim_state_it != initial_optimizer_states_.end()) {\r\n    auto& initial_states = optim_state_it->second;\r\n    // partition moments same way as the weight\r\n    auto alloc = cpu_execution_provider_.CreatePreferredAllocators()[0];\r\n    for (const auto& moments_prefix : training::MOMENTS_PREFIXES) {\r\n      const auto initial_state_it = initial_states.find(moments_prefix);\r\n      if (initial_state_it != initial_states.end()) {\r\n        auto* init_tensor = initial_state_it->second.GetMutable<Tensor>();\r\n\r\n        OrtValue partitioned;\r\n        auto element_type = init_tensor->DataType();\r\n        TensorShape partition_shape(new_shape);\r\n        std::unique_ptr<Tensor> p_tensor;\r\n\r\n        if (utils::IsPrimitiveDataType<float>(element_type)) {\r\n          float* data_buffer = init_tensor->MutableData<float>();\r\n\r\n          // allocate temporary memory to get the column partitioned state\r\n          InlinedVector<float> result_buffer;\r\n          result_buffer.reserve(element_count);\r\n          PartitionBufferByColumn(data_buffer, row_count, column_count, column_stride, stride, result_buffer);\r\n\r\n          // We need to maintain the initial optimizer states as an OrtValue,\r\n          // which is converted eventually to a TensorProto in the optimizer builder\r\n          // after Megatron and Zero partitioning. This approach saves CPU memory\r\n          // as creating a TensorProto involves a copy, and by delaying the copy until\r\n          // after the partitioning results in a smaller copy only for the optimizer\r\n          // states currently present on the rank.\r\n          // Allocate a new buffer to hold the partitioned optimizer state\r\n          // as column partitioning cannot re-use the original\r\n          // buffer as it is a non-contiguous read\r\n          p_tensor = std::make_unique<Tensor>(element_type,\r\n                                              partition_shape,\r\n                                              alloc);\r\n          float* out_buffer = p_tensor->MutableData<float>();\r\n          memcpy(out_buffer, result_buffer.data(), sizeof(float) * element_count);\r\n        } else if (utils::IsPrimitiveDataType<MLFloat16>(element_type)) {\r\n          MLFloat16* data_buffer = init_tensor->MutableData<MLFloat16>();\r\n\r\n          // allocate temporary memory to get the column partitioned state\r\n          InlinedVector<MLFloat16> result_buffer;\r\n          result_buffer.reserve(element_count);\r\n          PartitionBufferByColumn(data_buffer, row_count, column_count, column_stride, stride, result_buffer);\r\n\r\n          // allocate a new buffer as column partitioning cannot re-use the original\r\n          // buffer as it is a non-contiguous read on original buffer\r\n          p_tensor = std::make_unique<Tensor>(element_type,\r\n                                              partition_shape,\r\n                                              alloc);\r\n          MLFloat16* out_buffer = p_tensor->MutableData<MLFloat16>();\r\n          memcpy(out_buffer, result_buffer.data(), sizeof(MLFloat16) * element_count);\r\n        } else {\r\n          ORT_THROW(\"Unsupported type: \", element_type, \"for initial optimizer moments.\");\r\n        }\r\n        partitioned.Init(p_tensor.release(),\r\n                         DataTypeImpl::GetType<Tensor>(),\r\n                         DataTypeImpl::GetType<Tensor>()->GetDeleteFunc());\r\n        initial_states[moments_prefix] = std::move(partitioned);\r\n      } else {\r\n        LOGS_DEFAULT(WARNING) << \"Initial value for optimizer state: \" << moments_prefix\r\n                              << \" not found for weight: \" << original_name;\r\n      }\r\n    }\r\n  }\r\n\r\n  weight_partition_info_[original_name].megatron_row_partition = 0;\r\n  weight_partition_info_[original_name].partition_name = new_initializer_name;\r\n  weight_partition_info_[original_name].weight_partitioned = true;\r\n\r\n  return true;\r\n}\r\n\r\nbool MegatronTransformer::PartitionWeightByRow(const Graph& graph, const NodeArg& input_arg,\r\n                                               ONNX_NAMESPACE::TensorProto& initializer_partition) const {\r\n  const std::string original_name = input_arg.Name();\r\n  const ONNX_NAMESPACE::TensorProto* tensor_proto;\r\n  if (!graph.GetInitializedTensor(original_name, tensor_proto)) {\r\n    LOGS_DEFAULT(WARNING) << \"PartitionWeightByRow: \" << original_name << \" is not an initializer\";\r\n    return false;\r\n  }\r\n\r\n  auto data_type = tensor_proto->data_type();\r\n  const ONNX_NAMESPACE::TensorShapeProto* shape = input_arg.Shape();\r\n  int rank = shape->dim_size();\r\n  int64_t row_count;\r\n  int64_t column_count;\r\n\r\n  if (rank == 2 && utils::HasDimValue(shape->dim(0)) && utils::HasDimValue(shape->dim(1))) {\r\n    row_count = shape->dim(0).dim_value();\r\n    column_count = shape->dim(1).dim_value();\r\n    weight_partition_info_[original_name].original_dim = {row_count, column_count};\r\n  } else if (rank == 1) {\r\n    row_count = shape->dim(0).dim_value();\r\n    column_count = 1;\r\n    weight_partition_info_[original_name].original_dim = {row_count};\r\n  } else {\r\n    LOGS_DEFAULT(WARNING) << \"Initializer tensor's rank is more than \" << rank\r\n                          << \" (expected to be 1 or 2).\";\r\n    return false;\r\n  }\r\n\r\n  if (row_count % horizontal_parallel_size_ != 0) {\r\n    LOGS_DEFAULT(WARNING) << \"first dim \" << row_count << \" is not divisible by horizontal parallel size\"\r\n                          << horizontal_parallel_size_ << \", not supported currently.\";\r\n    return false;\r\n  }\r\n  auto initializer = Initializer{graph, *tensor_proto, graph.ModelPath()};\r\n  const float* a_weight = initializer.data<float>();\r\n\r\n  std::string new_initializer_name = original_name + \"_row_rank_\" + std::to_string(horizontal_parallel_rank_);\r\n\r\n  initializer_partition.set_name(new_initializer_name);\r\n  initializer_partition.set_data_type(data_type);\r\n\r\n  int64_t row_partition = row_count / horizontal_parallel_size_;\r\n\r\n  TensorShapeVector new_shape;\r\n  initializer_partition.add_dims(row_partition);\r\n  new_shape.push_back(row_partition);\r\n  if (rank == 2) {\r\n    initializer_partition.add_dims(column_count);\r\n    new_shape.push_back(column_count);\r\n  }\r\n  const int64_t element_count = row_partition * column_count;\r\n\r\n  InlinedVector<float> result;\r\n  result.reserve(element_count);\r\n\r\n  const int64_t row_index_offset = horizontal_parallel_rank_ * row_partition;\r\n  memcpy(result.data(), a_weight + row_index_offset * column_count, sizeof(float) * element_count);\r\n  initializer_partition.set_raw_data(result.data(), element_count * sizeof(float));\r\n\r\n  // Partition initial optimizer state if available\r\n  const auto optim_state_it = initial_optimizer_states_.find(original_name);\r\n  if (optim_state_it != initial_optimizer_states_.end()) {\r\n    auto& initial_states = optim_state_it->second;\r\n    for (const auto& moments_prefix : training::MOMENTS_PREFIXES) {\r\n      const auto initial_state_it = initial_states.find(moments_prefix);\r\n      if (initial_state_it != initial_states.end()) {\r\n        auto* init_tensor = initial_state_it->second.GetMutable<Tensor>();\r\n\r\n        OrtValue partitioned;\r\n        auto element_type = init_tensor->DataType();\r\n        TensorShape partition_shape(new_shape);\r\n        const OrtMemoryInfo& info = init_tensor->Location();\r\n        std::unique_ptr<Tensor> p_tensor;\r\n\r\n        if (utils::IsPrimitiveDataType<float>(element_type)) {\r\n          float* data_buffer = init_tensor->MutableData<float>();\r\n\r\n          p_tensor = std::make_unique<Tensor>(element_type,\r\n                                              partition_shape,\r\n                                              data_buffer + row_index_offset * column_count,\r\n                                              info);\r\n        } else if (utils::IsPrimitiveDataType<MLFloat16>(element_type)) {\r\n          MLFloat16* data_buffer = init_tensor->MutableData<MLFloat16>();\r\n\r\n          p_tensor = std::make_unique<Tensor>(element_type,\r\n                                              partition_shape,\r\n                                              data_buffer + row_index_offset * column_count,\r\n                                              info);\r\n\r\n        } else {\r\n          ORT_THROW(\"Unsupported type: \", element_type, \"for initial optimizer moments.\");\r\n        }\r\n        partitioned.Init(p_tensor.release(),\r\n                         DataTypeImpl::GetType<Tensor>(),\r\n                         DataTypeImpl::GetType<Tensor>()->GetDeleteFunc());\r\n        initial_states[moments_prefix] = std::move(partitioned);\r\n      } else {\r\n        LOGS_DEFAULT(WARNING) << \"Initial value for optimizer state: \" << moments_prefix\r\n                              << \" not found for weight: \" << original_name;\r\n      }\r\n    }\r\n  }\r\n\r\n  weight_partition_info_[original_name].megatron_row_partition = 1;\r\n  weight_partition_info_[original_name].partition_name = new_initializer_name;\r\n  weight_partition_info_[original_name].weight_partitioned = true;\r\n  return true;\r\n}\r\n\r\nStatus MegatronTransformer::TransformGPT2MLP(Graph& graph, bool& modified,\r\n                                             InlinedVector<Node*>& nodes_to_clear_shape,\r\n                                             int32_t& counter,\r\n                                             NodeIndex node_index) const {\r\n  auto skip_status = common::Status(common::ONNXRUNTIME, common::NOT_IMPLEMENTED, \"Skip BART Attention megatron transformation\");\r\n\r\n  auto& node = *graph.GetNode(node_index);\r\n\r\n  if (!graph_utils::IsSupportedOptypeVersionAndDomain(node, \"MatMul\", {9, 13}) ||\r\n      !graph_utils::IsSupportedProvider(node, GetCompatibleExecutionProviders()) ||\r\n      node.GetOutputEdgesCount() != 1) {\r\n    return skip_status;\r\n  }\r\n\r\n  if (node.GetInputEdgesCount() > 0) {\r\n    Node& matmul_input_node = const_cast<Node&>(*(node.InputNodesBegin()));\r\n    if (matmul_input_node.OpType().compare(\"MegatronF\") == 0) {\r\n      return skip_status;\r\n    }\r\n  }\r\n\r\n  Node& add_node = *graph.GetNode(node.OutputNodesBegin()->Index());\r\n  if (!graph_utils::IsSupportedOptypeVersionAndDomain(add_node, \"Add\", {7, 13}) ||\r\n      add_node.GetExecutionProviderType() != node.GetExecutionProviderType() ||\r\n      add_node.GetOutputEdgesCount() != 1) {\r\n    return skip_status;\r\n  }\r\n\r\n  Node& gelu_node = *graph.GetNode(add_node.OutputNodesBegin()->Index());\r\n  if (!(graph_utils::IsSupportedOptypeVersionAndDomain(gelu_node, \"Gelu\", {1}, kMSDomain) ||\r\n        graph_utils::IsSupportedOptypeVersionAndDomain(gelu_node, \"FastGelu\", {1}, kMSDomain)) ||\r\n      gelu_node.GetExecutionProviderType() != node.GetExecutionProviderType() ||\r\n      gelu_node.GetOutputEdgesCount() != 1) {\r\n    return skip_status;\r\n  }\r\n\r\n  Node& matmul2_node = *graph.GetNode(gelu_node.OutputNodesBegin()->Index());\r\n  if (!graph_utils::IsSupportedOptypeVersionAndDomain(matmul2_node, \"MatMul\", {9, 13}) ||\r\n      matmul2_node.GetExecutionProviderType() != node.GetExecutionProviderType() ||\r\n      matmul2_node.GetOutputEdgesCount() != 1) {\r\n    return skip_status;\r\n  }\r\n\r\n  Node& add2_node = *graph.GetNode(matmul2_node.OutputNodesBegin()->Index());\r\n  if (!graph_utils::IsSupportedOptypeVersionAndDomain(add2_node, \"Add\", {7, 13}) ||\r\n      add2_node.GetExecutionProviderType() != node.GetExecutionProviderType() ||\r\n      add2_node.GetOutputEdgesCount() != 1) {\r\n    return skip_status;\r\n  }\r\n\r\n  nodes_to_clear_shape.insert(nodes_to_clear_shape.end(), {&node, &add_node, &gelu_node,\r\n                                                           &matmul2_node});\r\n\r\n  auto a_weight_arg = node.MutableInputDefs()[1];\r\n  ONNX_NAMESPACE::TensorProto a_weight_initializer_partition;\r\n  if (!PartitionWeightByColumn(graph, *a_weight_arg, a_weight_initializer_partition)) {\r\n    return skip_status;\r\n  }\r\n\r\n  auto a_bias_arg = add_node.MutableInputDefs()[1];\r\n  ONNX_NAMESPACE::TensorProto a_bias_initializer_partition;\r\n  if (!PartitionWeightByColumn(graph, *a_bias_arg, a_bias_initializer_partition)) {\r\n    return skip_status;\r\n  }\r\n\r\n  auto b_weight_arg = matmul2_node.MutableInputDefs()[1];\r\n  ONNX_NAMESPACE::TensorProto b_weight_initializer_partition;\r\n  if (!PartitionWeightByRow(graph, *b_weight_arg, b_weight_initializer_partition)) {\r\n    return skip_status;\r\n  }\r\n\r\n  NodeArg& a_weight_partition_arg = graph_utils::AddInitializerWithOrtValue(graph, a_weight_initializer_partition);\r\n  graph_utils::ReplaceNodeInput(node, 1, a_weight_partition_arg);\r\n  updated_weight_names_.insert({a_weight_arg->Name(), a_weight_partition_arg.Name()});\r\n\r\n  NodeArg& a_bias_partition_arg = graph_utils::AddInitializerWithOrtValue(graph, a_bias_initializer_partition);\r\n  graph_utils::ReplaceNodeInput(add_node, 1, a_bias_partition_arg);\r\n  updated_weight_names_.insert({b_weight_arg->Name(), a_bias_partition_arg.Name()});\r\n\r\n  NodeArg& b_weight_partition_arg = graph_utils::AddInitializerWithOrtValue(graph, b_weight_initializer_partition);\r\n  graph_utils::ReplaceNodeInput(matmul2_node, 1, b_weight_partition_arg);\r\n  updated_weight_names_.insert({a_bias_arg->Name(), b_weight_partition_arg.Name()});\r\n\r\n  graph.RemoveInitializedTensor(a_weight_arg->Name());\r\n  graph.RemoveInitializedTensor(b_weight_arg->Name());\r\n  graph.RemoveInitializedTensor(a_bias_arg->Name());\r\n\r\n  const std::array mlp_f_input_defs{node.MutableInputDefs()[0]};\r\n  auto mlp_f_type_info = *node.MutableInputDefs()[0]->TypeAsProto();\r\n  auto& mlp_f_out_arg = graph.GetOrCreateNodeArg(graph.GenerateNodeArgName(\"MLP_MegatronF_Output\"), &mlp_f_type_info);\r\n  Node& mlp_f_node = graph.AddNode(graph.GenerateNodeName(\"MLP_MegatronF\"),\r\n                                   \"MegatronF\",\r\n                                   \"MLP MegatronF\",\r\n                                   mlp_f_input_defs,\r\n                                   {&mlp_f_out_arg}, {}, kMSDomain);\r\n  mlp_f_node.SetExecutionProviderType(node.GetExecutionProviderType());\r\n  const Node::EdgeEnd* edge = graph_utils::GetInputEdge(node, 0);\r\n  if (nullptr == edge) {  // handle input/initializer\r\n    graph_utils::ReplaceNodeInput(node, 0, *(mlp_f_node.MutableOutputDefs()[0]));\r\n  } else {\r\n    auto input_node = const_cast<Node*>(&edge->GetNode());\r\n    graph_utils::ReplaceDownstreamNodeInput(graph, *input_node, edge->GetDstArgIndex(), mlp_f_node, 0);\r\n  }\r\n\r\n  const std::array mlp_g_input_defs{matmul2_node.MutableOutputDefs()[0]};\r\n  auto mlp_g_type_info = *matmul2_node.MutableOutputDefs()[0]->TypeAsProto();\r\n  auto& mlp_g_out_arg = graph.GetOrCreateNodeArg(graph.GenerateNodeArgName(\"MLP_MegatronG_Output\"), &mlp_g_type_info);\r\n  Node& mlp_g_node = graph.AddNode(graph.GenerateNodeName(\"MLP_MegatronG\"),\r\n                                   \"MegatronG\",\r\n                                   \"MLP MegatronG\",\r\n                                   mlp_g_input_defs,\r\n                                   {&mlp_g_out_arg}, {}, kMSDomain);\r\n  mlp_g_node.AddAttribute(\"group_type\", static_cast<int64_t>(training::WorkerGroupType::HorizontalParallel));\r\n  mlp_g_node.SetExecutionProviderType(node.GetExecutionProviderType());\r\n  graph_utils::ReplaceDownstreamNodeInput(graph, matmul2_node, 0, mlp_g_node, 0);\r\n  modified = true;\r\n  counter++;\r\n\r\n  return Status::OK();\r\n}\r\n\r\n/*\r\nDenseWeight -- Transpose \\\r\n               MatMul -- BiasGelu -- Dropout -- MatMul -- Add -- Dropout\r\n*/\r\nStatus MegatronTransformer::TransformBARTMLP(Graph& graph, bool& modified,\r\n                                             InlinedVector<Node*>& nodes_to_clear_shape,\r\n                                             InlinedHashSet<Node*>& dropout_nodes_to_transform,\r\n                                             int32_t& counter,\r\n                                             NodeIndex node_index) const {\r\n  auto skip_status = common::Status(common::ONNXRUNTIME, common::NOT_IMPLEMENTED, \"Skip BART Attention megatron transformation\");\r\n\r\n  auto& node = *graph.GetNode(node_index);\r\n  if (!graph_utils::IsSupportedOptypeVersionAndDomain(node, \"MatMul\", {9, 13}) ||\r\n      !graph_utils::IsSupportedProvider(node, GetCompatibleExecutionProviders()) ||\r\n      node.GetOutputEdgesCount() != 1) {\r\n    return skip_status;\r\n  }\r\n  Node* second_op = const_cast<Node*>(graph.GetProducerNode(node.MutableInputDefs()[1]->Name()));\r\n  Node* first_op = const_cast<Node*>(graph.GetProducerNode(node.MutableInputDefs()[0]->Name()));\r\n  if (node.GetInputEdgesCount() > 0) {\r\n    if (second_op == nullptr) {\r\n      return skip_status;\r\n    }\r\n    if (first_op != nullptr && first_op->OpType().compare(\"MegatronF\") == 0) {\r\n      return skip_status;\r\n    }\r\n\r\n    if (second_op->OpType().compare(\"Transpose\") != 0) {\r\n      return skip_status;\r\n    }\r\n  } else {\r\n    return skip_status;\r\n  }\r\n  // check if transpose is only 2-dim\r\n  if (!optimizer_utils::IsAttributeWithExpectedValues(*second_op, \"perm\", {1LL, 0LL})) {\r\n    return skip_status;\r\n  }\r\n  ProviderType provider_type = node.GetExecutionProviderType();\r\n\r\n  Node* biasgelu_node_ptr = graph.GetNode(node.OutputNodesBegin()->Index());\r\n  Node& biasgelu_node = *biasgelu_node_ptr;\r\n  if (!graph_utils::IsSupportedOptypeVersionAndDomain(biasgelu_node, \"BiasGelu\", {1}, kMSDomain) ||\r\n      biasgelu_node.GetExecutionProviderType() != provider_type ||\r\n      biasgelu_node.GetOutputEdgesCount() != 1) {\r\n    return skip_status;\r\n  }\r\n\r\n  // Either Dropout->Matmul or just Matmul\r\n  Node* dropout_node = nullptr;\r\n  Node* next_node = graph.GetNode(biasgelu_node.OutputNodesBegin()->Index());\r\n  if (IsExpectedOpAndProvider(*next_node, dropout_info, provider_type)) {\r\n    dropout_node = next_node;\r\n    next_node = graph.GetNode(dropout_node->OutputNodesBegin()->Index());\r\n  }\r\n  if (!IsExpectedOpAndProvider(*next_node, matmul_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n  Node& matmul2_node = *next_node;\r\n\r\n  Node& add_node = *graph.GetNode(matmul2_node.OutputNodesBegin()->Index());\r\n  if (!IsExpectedOpAndProvider(add_node, add_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n  Node& dropout2_node = *graph.GetNode(add_node.OutputNodesBegin()->Index());\r\n  if (!IsExpectedOpAndProvider(dropout2_node, dropout_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n  Node* transpose_op_ptr = const_cast<Node*>(graph.GetProducerNode(matmul2_node.MutableInputDefs()[1]->Name()));\r\n  if (transpose_op_ptr == nullptr || !IsExpectedOpAndProvider(*transpose_op_ptr, transpose_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n\r\n  nodes_to_clear_shape.insert(nodes_to_clear_shape.end(), {&node, second_op, &biasgelu_node,\r\n                                                           &matmul2_node, transpose_op_ptr});\r\n  if (dropout_node != nullptr) {\r\n    nodes_to_clear_shape.insert(nodes_to_clear_shape.end(), {dropout_node});\r\n  }\r\n\r\n  auto dense_wi_weight_arg = second_op->MutableInputDefs()[0];\r\n  ONNX_NAMESPACE::TensorProto dense_wi_weight_initializer_partition;\r\n  if (!PartitionWeightByRow(graph, *dense_wi_weight_arg, dense_wi_weight_initializer_partition)) {\r\n    return skip_status;\r\n  }\r\n\r\n  // since the bias doesn't get transposed, partitioning by col\r\n  auto dense_wi_bias_arg = biasgelu_node.MutableInputDefs()[1];\r\n  ONNX_NAMESPACE::TensorProto dense_wi_bias_initializer_partition;\r\n  if (!PartitionWeightByColumn(graph, *dense_wi_bias_arg, dense_wi_bias_initializer_partition)) {\r\n    return skip_status;\r\n  }\r\n\r\n  auto dense_wo_weight_arg = transpose_op_ptr->MutableInputDefs()[0];\r\n  ONNX_NAMESPACE::TensorProto dense_wo_weight_initializer_partition;\r\n  if (!PartitionWeightByColumn(graph, *dense_wo_weight_arg, dense_wo_weight_initializer_partition)) {\r\n    return skip_status;\r\n  }\r\n\r\n  NodeArg& dense_wi_weight_partition_arg = graph_utils::AddInitializerWithOrtValue(graph, dense_wi_weight_initializer_partition);\r\n  graph_utils::ReplaceNodeInput(*second_op, 0, dense_wi_weight_partition_arg);\r\n  updated_weight_names_.insert({dense_wi_weight_arg->Name(), dense_wi_weight_partition_arg.Name()});\r\n\r\n  NodeArg& dense_wi_bias_partition_arg = graph_utils::AddInitializerWithOrtValue(graph, dense_wi_bias_initializer_partition);\r\n  graph_utils::ReplaceNodeInput(biasgelu_node, 1, dense_wi_bias_partition_arg);\r\n  updated_weight_names_.insert({dense_wi_bias_arg->Name(), dense_wi_bias_partition_arg.Name()});\r\n\r\n  NodeArg& dense_wo_weight_partition_arg = graph_utils::AddInitializerWithOrtValue(graph, dense_wo_weight_initializer_partition);\r\n  graph_utils::ReplaceNodeInput(*transpose_op_ptr, 0, dense_wo_weight_partition_arg);\r\n  updated_weight_names_.insert({dense_wo_weight_arg->Name(), dense_wo_weight_partition_arg.Name()});\r\n\r\n  graph.RemoveInitializedTensor(dense_wi_weight_arg->Name());\r\n  graph.RemoveInitializedTensor(dense_wi_bias_arg->Name());\r\n  graph.RemoveInitializedTensor(dense_wo_weight_arg->Name());\r\n\r\n  if (dropout_node) {\r\n    dropout_nodes_to_transform.insert(dropout_node);\r\n  }\r\n\r\n  const std::array mlp_f_input_defs{node.MutableInputDefs()[0]};\r\n  auto mlp_f_type_info = *node.MutableInputDefs()[0]->TypeAsProto();\r\n  auto& mlp_f_out_arg = graph.GetOrCreateNodeArg(graph.GenerateNodeArgName(\"BART_MLP_MegatronF_Output\"), &mlp_f_type_info);\r\n  Node& mlp_f_node = graph.AddNode(graph.GenerateNodeName(\"BART_MLP_MegatronF\"),\r\n                                   \"MegatronF\",\r\n                                   \"MLP MegatronF\",\r\n                                   mlp_f_input_defs,\r\n                                   {&mlp_f_out_arg}, {}, kMSDomain);\r\n  counter++;\r\n  mlp_f_node.SetExecutionProviderType(node.GetExecutionProviderType());\r\n  const Node::EdgeEnd* edge = graph_utils::GetInputEdge(node, 0);\r\n  if (nullptr == edge) {  // handle input/initializer\r\n    graph_utils::ReplaceNodeInput(node, 0, *(mlp_f_node.MutableOutputDefs()[0]));\r\n  } else {\r\n    auto input_node = const_cast<Node*>(&edge->GetNode());\r\n    graph_utils::ReplaceDownstreamNodeInput(graph, *input_node, edge->GetSrcArgIndex(), mlp_f_node, 0);\r\n  }\r\n\r\n  const std::array mlp_g_input_defs{matmul2_node.MutableOutputDefs()[0]};\r\n  auto mlp_g_type_info = *matmul2_node.MutableOutputDefs()[0]->TypeAsProto();\r\n  auto& mlp_g_out_arg = graph.GetOrCreateNodeArg(graph.GenerateNodeArgName(\"BART_MLP_MegatronG_Output\"), &mlp_g_type_info);\r\n  Node& mlp_g_node = graph.AddNode(graph.GenerateNodeName(\"BART_MLP_MegatronG\"),\r\n                                   \"MegatronG\",\r\n                                   \"MLP MegatronG\",\r\n                                   mlp_g_input_defs,\r\n                                   {&mlp_g_out_arg}, {}, kMSDomain);\r\n  mlp_g_node.AddAttribute(\"group_type\", static_cast<int64_t>(training::WorkerGroupType::HorizontalParallel));\r\n  mlp_g_node.SetExecutionProviderType(node.GetExecutionProviderType());\r\n  graph_utils::ReplaceDownstreamNodeInput(graph, matmul2_node, 0, mlp_g_node, 0);\r\n  modified = true;\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus MegatronTransformer::TransformGPT2Attention(Graph& graph, bool& modified,\r\n                                                   InlinedVector<Node*>& nodes_to_clear_shape,\r\n                                                   InlinedHashSet<Node*>& dropout_nodes_to_transform,\r\n                                                   int32_t& counter,\r\n                                                   NodeIndex node_index) const {\r\n  auto skip_status = common::Status(common::ONNXRUNTIME, common::NOT_IMPLEMENTED, \"Skip BART Attention megatron transformation\");\r\n\r\n  // Self attention sub-graph.\r\n  // MatMul->Add->Split->Reshape->Transpose->MatMul->Div->Mul->Sub->Softmax->Dropout->MatMul->Transpose->Reshape->MatMul->Add\r\n  //                  |->Reshape->Transpose->|                                        |\r\n  //                  |->Reshape->Transpose------------------------------------------>|\r\n\r\n  auto& node = *graph.GetNode(node_index);\r\n  if (!graph_utils::IsSupportedOptypeVersionAndDomain(node, \"MatMul\", opset_v9_13) ||\r\n      !graph_utils::IsSupportedProvider(node, GetCompatibleExecutionProviders()) ||\r\n      node.GetOutputEdgesCount() != 1) {\r\n    return skip_status;\r\n  }\r\n\r\n  if (node.GetInputEdgesCount() > 0 && node.InputNodesBegin()->OpType().compare(\"MegatronF\") == 0) {\r\n    return skip_status;\r\n  }\r\n\r\n  InlinedVector<Node*> sub_graph_node_ptrs;\r\n  sub_graph_node_ptrs.push_back(&node);\r\n  ProviderType provider_type = node.GetExecutionProviderType();\r\n\r\n  std::vector<NodeInfo> linear_pattern = {\r\n      NodeInfo({add_info}),  // -15\r\n      NodeInfo({split_info}),\r\n      NodeInfo({reshape_info}),\r\n      NodeInfo({transpose_info}),\r\n      NodeInfo({matmul_info}),  // -11\r\n      NodeInfo({div_info}),\r\n      NodeInfo({mul_info}),\r\n      NodeInfo({sub_info}),\r\n      NodeInfo({softmax_info}),\r\n      NodeInfo({dropout_info}, false),  // -6\r\n      NodeInfo({matmul_info}),\r\n      NodeInfo({transpose_info}),\r\n      NodeInfo({reshape_info}),\r\n      NodeInfo({matmul_info}),\r\n      NodeInfo({add_info})};  // -1\r\n  if (!MatchLinearPattern(graph, &node, provider_type, linear_pattern, sub_graph_node_ptrs)) {\r\n    return skip_status;\r\n  }\r\n\r\n  // Get all useful nodes here as more vector push back below will change the index.\r\n  Node& add_node = *sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 15];\r\n  Node& split_node = *sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 14];\r\n  Node& k_transpose_after_reshape_node = *sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 12];\r\n  Node* matmul_node_ptr = sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 11];\r\n  Node* dropout_node_ptr = sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 6];\r\n  Node* matmul_node_ptr1 = sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 5];\r\n  Node& transpose_node1 = *sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 4];\r\n  Node& matmul_node = *sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 2];\r\n\r\n  // Transpose node attribute checking.\r\n  if (!optimizer_utils::IsAttributeWithExpectedValues(k_transpose_after_reshape_node, \"perm\", {0LL, 2LL, 1LL, 3LL}) ||\r\n      !optimizer_utils::IsAttributeWithExpectedValues(transpose_node1, \"perm\", {0LL, 2LL, 1LL, 3LL})) {\r\n    return skip_status;\r\n  }\r\n\r\n  InlinedVector<Node*> transpose_node_ptrs;  // For the 2nd and 3rd transpose nodes after split node for sub-graph structure checking.\r\n  InlinedVector<Node*> reshape_node_ptrs;    // To keep the reshape node that need to change the shape constant.\r\n  reshape_node_ptrs.push_back(sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 13]);\r\n  reshape_node_ptrs.push_back(sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 3]);\r\n  auto split_output_iter = split_node.OutputNodesBegin();\r\n  ++split_output_iter;\r\n  for (; split_output_iter != split_node.OutputNodesEnd(); ++split_output_iter) {\r\n    Node* reshape_node_ptr = graph.GetNode((*split_output_iter).Index());\r\n    if (!IsExpectedOpAndProvider(*reshape_node_ptr, reshape_info, provider_type)) {\r\n      break;\r\n    }\r\n\r\n    Node* transpose_node_ptr = graph.GetNode(reshape_node_ptr->OutputNodesBegin()->Index());\r\n    if (!IsExpectedOpAndProvider(*transpose_node_ptr, transpose_info, provider_type)) {\r\n      break;\r\n    }\r\n\r\n    reshape_node_ptrs.push_back(reshape_node_ptr);\r\n    sub_graph_node_ptrs.push_back(reshape_node_ptr);\r\n    transpose_node_ptrs.push_back(transpose_node_ptr);\r\n    sub_graph_node_ptrs.push_back(transpose_node_ptr);\r\n  }\r\n\r\n  // Sub-graph structure and transpose attribute checking.\r\n  if (transpose_node_ptrs.size() != 2 ||\r\n      matmul_node_ptr != graph.GetNode(transpose_node_ptrs[0]->OutputNodesBegin()->Index()) ||\r\n      matmul_node_ptr1 != graph.GetNode(transpose_node_ptrs[1]->OutputNodesBegin()->Index()) ||\r\n      !optimizer_utils::IsAttributeWithExpectedValues(*transpose_node_ptrs[0], \"perm\", {0LL, 2LL, 3LL, 1LL}) ||\r\n      !optimizer_utils::IsAttributeWithExpectedValues(*transpose_node_ptrs[1], \"perm\", {0LL, 2LL, 1LL, 3LL})) {\r\n    return skip_status;\r\n  }\r\n\r\n  // Partition weights. If any of them fails, skip transforming this sub-graph.\r\n  auto qkv_weight_arg = node.MutableInputDefs()[1];\r\n  ONNX_NAMESPACE::TensorProto qkv_weight_initializer_partition;\r\n  if (!PartitionWeightByColumn(graph, *qkv_weight_arg, qkv_weight_initializer_partition, 3)) {\r\n    return skip_status;\r\n  }\r\n\r\n  auto qkv_bias_arg = add_node.MutableInputDefs()[1];\r\n  ONNX_NAMESPACE::TensorProto qkv_bias_initializer_partition;\r\n  if (!PartitionWeightByColumn(graph, *qkv_bias_arg, qkv_bias_initializer_partition, 3)) {\r\n    return skip_status;\r\n  }\r\n\r\n  auto dense_weight_arg = matmul_node.MutableInputDefs()[1];\r\n  ONNX_NAMESPACE::TensorProto dense_weight_initializer_partition;\r\n  if (!PartitionWeightByRow(graph, *dense_weight_arg, dense_weight_initializer_partition)) {\r\n    return skip_status;\r\n  }\r\n\r\n  // Check the constant value in the Reshape nodes.\r\n  bool is_reshape_valid = true;\r\n  for (Node* node_ptr : reshape_node_ptrs) {\r\n    auto shape_arg = node_ptr->MutableInputDefs()[1];\r\n    const ONNX_NAMESPACE::TensorProto* tensor;\r\n\r\n    if (!graph.GetInitializedTensor(shape_arg->Name(), tensor)) {\r\n      is_reshape_valid = false;\r\n      break;\r\n    }\r\n\r\n    auto data_type = tensor->data_type();\r\n    if (data_type != ONNX_NAMESPACE::TensorProto_DataType_INT64) {\r\n      is_reshape_valid = false;\r\n      break;\r\n    }\r\n\r\n    // The number of the values should be more than 2, and the 3rd value should be divisible by parallel size,\r\n    // i.e., the attention head number should be divisible by parallel size.\r\n    auto init_const = Initializer{graph, *tensor, graph.ModelPath()};\r\n    if (init_const.size() != 3 && init_const.size() != 4) {\r\n      is_reshape_valid = false;\r\n      break;\r\n    }\r\n\r\n    const int64_t* val = init_const.data<int64_t>();\r\n    if (val[2] % horizontal_parallel_size_ != 0) {\r\n      LOGS_DEFAULT(WARNING) << (init_const.size() == 3 ? \"Hidden size \" : \"Number of attention heads \") << val[2]\r\n                            << \" is not divisible by horizontal_parallel_size_ \"\r\n                            << horizontal_parallel_size_ << \", not supported currently.\";\r\n      is_reshape_valid = false;\r\n      break;\r\n    }\r\n  }\r\n\r\n  if (!is_reshape_valid) {\r\n    return skip_status;\r\n  }\r\n\r\n  // Ready to transform the sub-graph when reach here.\r\n  // It's possible that the node vector contains nullptr due to some optinal node infos during linear pattern matching.\r\n  std::copy_if(sub_graph_node_ptrs.begin(), sub_graph_node_ptrs.end(),\r\n               std::back_inserter(nodes_to_clear_shape),\r\n               [](Node* node_ptr) { return node_ptr != nullptr; });\r\n\r\n  // Replace by the partition weights.\r\n  NodeArg& qkv_weight_partition_arg = graph_utils::AddInitializerWithOrtValue(graph, qkv_weight_initializer_partition);\r\n  graph_utils::ReplaceNodeInput(node, 1, qkv_weight_partition_arg);\r\n  updated_weight_names_.insert({qkv_weight_arg->Name(), qkv_weight_partition_arg.Name()});\r\n\r\n  NodeArg& qkv_bias_partition_arg = graph_utils::AddInitializerWithOrtValue(graph, qkv_bias_initializer_partition);\r\n  graph_utils::ReplaceNodeInput(add_node, 1, qkv_bias_partition_arg);\r\n  updated_weight_names_.insert({qkv_bias_arg->Name(), qkv_bias_partition_arg.Name()});\r\n\r\n  NodeArg& dense_weight_partition_arg = graph_utils::AddInitializerWithOrtValue(graph, dense_weight_initializer_partition);\r\n  graph_utils::ReplaceNodeInput(matmul_node, 1, dense_weight_partition_arg);\r\n  updated_weight_names_.insert({dense_weight_arg->Name(), dense_weight_partition_arg.Name()});\r\n\r\n  graph.RemoveInitializedTensor(qkv_weight_arg->Name());\r\n  graph.RemoveInitializedTensor(qkv_bias_arg->Name());\r\n  graph.RemoveInitializedTensor(dense_weight_arg->Name());\r\n\r\n  // Change the constant for the reshape nodes.\r\n  for (Node* node_ptr : reshape_node_ptrs) {\r\n    auto shape_arg = node_ptr->MutableInputDefs()[1];\r\n    const ONNX_NAMESPACE::TensorProto* tensor;\r\n    graph.GetInitializedTensor(shape_arg->Name(), tensor);\r\n    auto data_type = tensor->data_type();\r\n    auto init_const = Initializer{graph, *tensor, graph.ModelPath()};\r\n    const int64_t* val = init_const.data<int64_t>();\r\n    int64_t size = init_const.size();\r\n    ONNX_NAMESPACE::TensorProto tensor_partition;\r\n    tensor_partition.set_name(graph.GenerateNodeArgName(\"partition_\" + shape_arg->Name()));\r\n    tensor_partition.set_data_type(data_type);\r\n    tensor_partition.add_dims(size);\r\n\r\n    InlinedVector<int64_t> val_partition;\r\n    val_partition.reserve(size);\r\n    val_partition.insert(val_partition.end(), val, val + size);\r\n    val_partition[2] /= horizontal_parallel_size_;\r\n    tensor_partition.set_raw_data(val_partition.data(), size * sizeof(int64_t));\r\n    NodeArg& node_arg_partition = graph_utils::AddInitializerWithOrtValue(graph, tensor_partition);\r\n    graph_utils::ReplaceNodeInput(*node_ptr, 1, node_arg_partition);\r\n    graph.RemoveInitializedTensor(shape_arg->Name());\r\n  }\r\n\r\n  if (dropout_node_ptr != nullptr) {\r\n    dropout_nodes_to_transform.insert(dropout_node_ptr);\r\n  }\r\n\r\n  // Add MegatronF before the 1st MatMul and MegatronG before the last Add.\r\n  const std::array sa_f_input_defs{node.MutableInputDefs()[0]};\r\n  auto sa_f_type_info = *node.MutableInputDefs()[0]->TypeAsProto();\r\n  auto& sa_f_out_arg = graph.GetOrCreateNodeArg(graph.GenerateNodeArgName(\"SeftAttention_MegatronF_Output\"), &sa_f_type_info);\r\n  Node& sa_f_node = graph.AddNode(graph.GenerateNodeName(node.Name() + \"SeftAttention_MegatronF\"),\r\n                                  \"MegatronF\",\r\n                                  \"SeftAttention MegatronF\",\r\n                                  sa_f_input_defs,\r\n                                  {&sa_f_out_arg}, {}, kMSDomain);\r\n  sa_f_node.SetExecutionProviderType(node.GetExecutionProviderType());\r\n  const Node::EdgeEnd* edge = graph_utils::GetInputEdge(node, 0);\r\n  if (nullptr == edge) {  // handle input/initializer\r\n    graph_utils::ReplaceNodeInput(node, 0, *(sa_f_node.MutableOutputDefs()[0]));\r\n  } else {\r\n    auto input_node = const_cast<Node*>(&edge->GetNode());\r\n    graph_utils::ReplaceDownstreamNodeInput(graph, *input_node, edge->GetDstArgIndex(), sa_f_node, 0);\r\n  }\r\n\r\n  const std::array sa_g_input_defs{matmul_node.MutableOutputDefs()[0]};\r\n  auto sa_g_type_info = *matmul_node.MutableOutputDefs()[0]->TypeAsProto();  // copy\r\n  auto& sa_g_out_arg = graph.GetOrCreateNodeArg(graph.GenerateNodeArgName(\"SeftAttention_MegatronG_Output\"), &sa_g_type_info);\r\n  Node& sa_g_node = graph.AddNode(graph.GenerateNodeName(node.Name() + \"SelfAttention_MegatronG\"),\r\n                                  \"MegatronG\",\r\n                                  \"Attention MegatronG\",\r\n                                  sa_g_input_defs,\r\n                                  {&sa_g_out_arg}, {}, kMSDomain);\r\n  sa_g_node.AddAttribute(\"group_type\", static_cast<int64_t>(training::WorkerGroupType::HorizontalParallel));\r\n  sa_g_node.SetExecutionProviderType(node.GetExecutionProviderType());\r\n  graph_utils::ReplaceDownstreamNodeInput(graph, matmul_node, 0, sa_g_node, 0);\r\n  modified = true;\r\n  counter++;\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus MegatronTransformer::TransformBARTAttention(Graph& graph, bool& modified,\r\n                                                   InlinedVector<Node*>& nodes_to_clear_shape,\r\n                                                   InlinedHashSet<Node*>& dropout_nodes_to_transform,\r\n                                                   int32_t& counter,\r\n                                                   NodeIndex node_index) const {\r\n  auto skip_status = common::Status(common::ONNXRUNTIME, common::NOT_IMPLEMENTED, \"Skip BART Attention megatron transformation\");\r\n\r\n  // Self/Enc-Dec Attention sub-graph.\r\n  //\r\n  // MatMul->Add->Mul->Reshape->Transpose->MatMul->Reshape->Where->Reshape->Softmax->Dropout->MatMul->Transpose->Reshape->MatMul->Add->Droupout\r\n  // MatMul->Add->Reshape->Transpose-------> |                                                  |\r\n  // MatMul->Add->Reshape->Transpose----------------------------------------------------------> |\r\n  auto& node = *graph.GetNode(node_index);\r\n\r\n  if (!graph_utils::IsSupportedOptypeVersionAndDomain(node, \"MatMul\", opset_v9_13) ||\r\n      !graph_utils::IsSupportedProvider(node, GetCompatibleExecutionProviders()) ||\r\n      node.GetOutputEdgesCount() != 1) {\r\n    return skip_status;\r\n  }\r\n\r\n  Node* q_matmul_input_node_ptr = const_cast<Node*>(graph.GetProducerNode(node.MutableInputDefs()[0]->Name()));\r\n  if (q_matmul_input_node_ptr != nullptr && q_matmul_input_node_ptr->OpType().compare(\"MegatronF\") == 0) {\r\n    return skip_status;\r\n  }\r\n  InlinedVector<Node*> sub_graph_node_ptrs;\r\n  sub_graph_node_ptrs.push_back(&node);\r\n  ProviderType provider_type = node.GetExecutionProviderType();\r\n\r\n  std::vector<NodeInfo> linear_pattern = {\r\n      NodeInfo({add_info}),\r\n      NodeInfo({mul_info}),\r\n      NodeInfo({reshape_info}),\r\n      NodeInfo({transpose_info}),\r\n      NodeInfo({matmul_info}),\r\n      NodeInfo({add_info}, false),  // -13\r\n      NodeInfo({reshape_info}),\r\n      NodeInfo({where_info}),\r\n      NodeInfo({reshape_info}),\r\n      NodeInfo({softmax_info}),\r\n      NodeInfo({dropout_info}, false),  // -8\r\n      NodeInfo({matmul_info}),\r\n      NodeInfo({add_info}, false),  // -6\r\n      NodeInfo({transpose_info}),\r\n      NodeInfo({reshape_info}),\r\n      NodeInfo({matmul_info}),  // -3\r\n      NodeInfo({add_info}),\r\n      NodeInfo({dropout_info}, false)};  // -1\r\n  if (!MatchLinearPattern(graph, &node, provider_type, linear_pattern, sub_graph_node_ptrs)) {\r\n    return skip_status;\r\n  }\r\n  // Get all useful nodes here as more vector push back below will change the index.\r\n  // Other than the optional nodes in the pattern, all other node pointers are valid\r\n  // if they match the linear pattern.\r\n  Node* q_biasadd_node_ptr = sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 18];\r\n  Node* q_transpose_after_reshape_node_ptr = sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 15];\r\n  Node* qk_matmul_node_ptr = sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 14];\r\n  Node* dropout_node_ptr = sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 8];\r\n  Node* qkv_matmul_node_ptr = sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 7];\r\n  Node* transpose_node1_ptr = sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 5];\r\n  Node& dense_matmul_node = *sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 3];\r\n\r\n  // Transpose node attribute checking.\r\n  if (!optimizer_utils::IsAttributeWithExpectedValues(*q_transpose_after_reshape_node_ptr, \"perm\", {1LL, 0LL, 2LL}) ||\r\n      !optimizer_utils::IsAttributeWithExpectedValues(*transpose_node1_ptr, \"perm\", {1LL, 0LL, 2LL})) {\r\n    return skip_status;\r\n  }\r\n  // map between reshape node and dim of reshape that must be modified\r\n  std::unordered_map<Node*, size_t> reshape_node_ptrs;\r\n  reshape_node_ptrs[sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 16]] = 1;\r\n  reshape_node_ptrs[sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 12]] = 1;\r\n  reshape_node_ptrs[sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 10]] = 0;\r\n  reshape_node_ptrs[sub_graph_node_ptrs[sub_graph_node_ptrs.size() - 4]] = 2;\r\n  // till now node should be q matmul operation\r\n\r\n  InlinedVector<Node*> weight_transpose_node_ptrs;\r\n  InlinedVector<Node*> bias_add_node_ptrs;\r\n\r\n  Node* q_transpose_ptr = const_cast<Node*>(graph.GetProducerNode(node.MutableInputDefs()[1]->Name()));\r\n  if (q_transpose_ptr == nullptr || !IsExpectedOpAndProvider(*q_transpose_ptr, transpose_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n  weight_transpose_node_ptrs.push_back(q_transpose_ptr);\r\n  sub_graph_node_ptrs.push_back(q_transpose_ptr);\r\n  bias_add_node_ptrs.push_back(q_biasadd_node_ptr);\r\n\r\n  Node* k_transpose_ptr = const_cast<Node*>(graph.GetProducerNode(qk_matmul_node_ptr->MutableInputDefs()[1]->Name()));\r\n  if (k_transpose_ptr == nullptr || !IsExpectedOpAndProvider(*k_transpose_ptr, transpose_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n  sub_graph_node_ptrs.push_back(k_transpose_ptr);\r\n\r\n  Node* k_reshape_ptr = const_cast<Node*>(graph.GetProducerNode(k_transpose_ptr->MutableInputDefs()[0]->Name()));\r\n  if (k_reshape_ptr == nullptr || !IsExpectedOpAndProvider(*k_reshape_ptr, reshape_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n  reshape_node_ptrs[k_reshape_ptr] = 1;\r\n  sub_graph_node_ptrs.push_back(k_reshape_ptr);\r\n\r\n  Node* k_add_ptr = const_cast<Node*>(graph.GetProducerNode(k_reshape_ptr->MutableInputDefs()[0]->Name()));\r\n  if (k_add_ptr == nullptr || !IsExpectedOpAndProvider(*k_add_ptr, add_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n  sub_graph_node_ptrs.push_back(k_add_ptr);\r\n  bias_add_node_ptrs.push_back(k_add_ptr);\r\n\r\n  Node* k_matmul_ptr = const_cast<Node*>(graph.GetProducerNode(k_add_ptr->MutableInputDefs()[0]->Name()));\r\n  if (k_matmul_ptr == nullptr || !IsExpectedOpAndProvider(*k_matmul_ptr, matmul_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n  sub_graph_node_ptrs.push_back(k_matmul_ptr);\r\n\r\n  Node* k_weight_transpose_ptr = const_cast<Node*>(graph.GetProducerNode(k_matmul_ptr->MutableInputDefs()[1]->Name()));\r\n  if (k_weight_transpose_ptr == nullptr || !IsExpectedOpAndProvider(*k_weight_transpose_ptr, transpose_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n  sub_graph_node_ptrs.push_back(k_weight_transpose_ptr);\r\n  weight_transpose_node_ptrs.push_back(k_weight_transpose_ptr);\r\n\r\n  Node* v_transpose_ptr = const_cast<Node*>(graph.GetProducerNode(qkv_matmul_node_ptr->MutableInputDefs()[1]->Name()));\r\n  if (v_transpose_ptr == nullptr || !IsExpectedOpAndProvider(*v_transpose_ptr, transpose_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n  sub_graph_node_ptrs.push_back(v_transpose_ptr);\r\n\r\n  Node* v_reshape_ptr = const_cast<Node*>(graph.GetProducerNode(v_transpose_ptr->MutableInputDefs()[0]->Name()));\r\n  if (v_reshape_ptr == nullptr || !IsExpectedOpAndProvider(*v_reshape_ptr, reshape_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n  reshape_node_ptrs[v_reshape_ptr] = 1;\r\n  sub_graph_node_ptrs.push_back(v_reshape_ptr);\r\n\r\n  Node* v_add_ptr = const_cast<Node*>(graph.GetProducerNode(v_reshape_ptr->MutableInputDefs()[0]->Name()));\r\n  if (v_add_ptr == nullptr || !IsExpectedOpAndProvider(*v_add_ptr, add_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n  sub_graph_node_ptrs.push_back(v_add_ptr);\r\n  bias_add_node_ptrs.push_back(v_add_ptr);\r\n\r\n  Node* v_matmul_ptr = const_cast<Node*>(graph.GetProducerNode(v_add_ptr->MutableInputDefs()[0]->Name()));\r\n  if (k_matmul_ptr == nullptr || !IsExpectedOpAndProvider(*k_matmul_ptr, matmul_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n  sub_graph_node_ptrs.push_back(v_matmul_ptr);\r\n\r\n  Node* v_weight_transpose_ptr = const_cast<Node*>(graph.GetProducerNode(v_matmul_ptr->MutableInputDefs()[1]->Name()));\r\n  if (v_weight_transpose_ptr == nullptr || !IsExpectedOpAndProvider(*v_weight_transpose_ptr, transpose_info, provider_type)) {\r\n    return skip_status;\r\n  }\r\n  sub_graph_node_ptrs.push_back(v_weight_transpose_ptr);\r\n  weight_transpose_node_ptrs.push_back(v_weight_transpose_ptr);\r\n\r\n  // K and V matmul must have the same input\r\n  Node* q_matmul_ptr = &node;\r\n  if (k_matmul_ptr->MutableInputDefs()[0]->Name() != v_matmul_ptr->MutableInputDefs()[0]->Name()) {\r\n    return skip_status;\r\n  }\r\n\r\n  // Check the constant value in the Reshape nodes.\r\n  bool is_reshape_valid = true;\r\n  for (auto x : reshape_node_ptrs) {\r\n    Node* node_ptr = x.first;\r\n    auto idx = x.second;\r\n    auto shape_arg = node_ptr->MutableInputDefs()[1];\r\n    const ONNX_NAMESPACE::TensorProto* tensor;\r\n    if (!graph.GetInitializedTensor(shape_arg->Name(), tensor)) {\r\n      is_reshape_valid = false;\r\n      break;\r\n    }\r\n    auto data_type = tensor->data_type();\r\n    if (data_type != ONNX_NAMESPACE::TensorProto_DataType_INT64) {\r\n      is_reshape_valid = false;\r\n      break;\r\n    }\r\n    // The number of the values should be more than idx, and the idx'th value should be divisible by parallel size,\r\n    // i.e., the attention head number should be divisible by parallel size.\r\n    auto init_const = Initializer{graph, *tensor, graph.ModelPath()};\r\n    if (init_const.size() <= idx) {\r\n      is_reshape_valid = false;\r\n      break;\r\n    }\r\n    const int64_t* val = init_const.data<int64_t>();\r\n    if (val[idx] % horizontal_parallel_size_ != 0) {\r\n      LOGS_DEFAULT(WARNING) << \"dim[\" << idx << \"]: \" << val[idx]\r\n                            << \" is not divisible by horizontal_parallel_size_ \"\r\n                            << horizontal_parallel_size_ << \", not supported currently.\";\r\n      is_reshape_valid = false;\r\n      break;\r\n    }\r\n  }\r\n\r\n  if (!is_reshape_valid) {\r\n    return skip_status;\r\n  }\r\n\r\n  // Partition weights. If any of them fails, skip transforming the rest.\r\n  std::vector<ONNX_NAMESPACE::TensorProto> qkv_weight_initializer_partitions;\r\n  for (auto trans_ptr : weight_transpose_node_ptrs) {\r\n    auto qkv_weight_arg = trans_ptr->MutableInputDefs()[0];\r\n    ONNX_NAMESPACE::TensorProto qkv_weight_initializer_partition;\r\n    if (!PartitionWeightByRow(graph, *qkv_weight_arg, qkv_weight_initializer_partition)) {\r\n      break;\r\n    }\r\n    qkv_weight_initializer_partitions.push_back(qkv_weight_initializer_partition);\r\n  }\r\n\r\n  // Partition bias. If any of them fails, skip transforming the rest.\r\n  std::vector<ONNX_NAMESPACE::TensorProto> qkv_bias_initializer_partitions;\r\n  for (auto add_ptr : bias_add_node_ptrs) {\r\n    auto qkv_bias_arg = add_ptr->MutableInputDefs()[1];\r\n    ONNX_NAMESPACE::TensorProto qkv_bias_initializer_partition;\r\n    if (!PartitionWeightByColumn(graph, *qkv_bias_arg, qkv_bias_initializer_partition)) {\r\n      break;\r\n    }\r\n    qkv_bias_initializer_partitions.push_back(qkv_bias_initializer_partition);\r\n  }\r\n\r\n  // if all the weights or biases weren't transformed, skip transforming this subgraph\r\n  if (weight_transpose_node_ptrs.size() != qkv_weight_initializer_partitions.size()) {\r\n    return skip_status;\r\n  }\r\n  if (bias_add_node_ptrs.size() != qkv_bias_initializer_partitions.size()) {\r\n    return skip_status;\r\n  }\r\n\r\n  // transform the dense weight. If it fails, skip transforming this subgraph.\r\n  Node* last_transpose = const_cast<Node*>(graph.GetProducerNode(dense_matmul_node.MutableInputDefs()[1]->Name()));\r\n  auto dense_weight_arg = last_transpose->MutableInputDefs()[0];\r\n  ONNX_NAMESPACE::TensorProto dense_weight_initializer_partition;\r\n  if (!PartitionWeightByColumn(graph, *dense_weight_arg, dense_weight_initializer_partition)) {\r\n    return skip_status;\r\n  }\r\n\r\n  // Ready to transform the sub-graph when reach here.\r\n  // Replace node inputs\r\n  size_t i = 0;\r\n  for (auto trans_ptr : weight_transpose_node_ptrs) {\r\n    auto weight_name = trans_ptr->MutableInputDefs()[0]->Name();\r\n    NodeArg& qkv_weight_partition_arg = graph_utils::AddInitializerWithOrtValue(graph, qkv_weight_initializer_partitions[i]);\r\n    graph_utils::ReplaceNodeInput(*trans_ptr, 0, qkv_weight_partition_arg);\r\n    graph.RemoveInitializedTensor(weight_name);\r\n    updated_weight_names_.insert({weight_name, qkv_weight_partition_arg.Name()});\r\n    i++;\r\n  }\r\n  i = 0;\r\n  for (auto add_ptr : bias_add_node_ptrs) {\r\n    auto bias_name = add_ptr->MutableInputDefs()[1]->Name();\r\n    NodeArg& qkv_bias_partition_arg = graph_utils::AddInitializerWithOrtValue(graph, qkv_bias_initializer_partitions[i]);\r\n    graph_utils::ReplaceNodeInput(*add_ptr, 1, qkv_bias_partition_arg);\r\n    graph.RemoveInitializedTensor(bias_name);\r\n    updated_weight_names_.insert({bias_name, qkv_bias_partition_arg.Name()});\r\n    i++;\r\n  }\r\n\r\n  NodeArg& dense_weight_partition_arg = graph_utils::AddInitializerWithOrtValue(graph, dense_weight_initializer_partition);\r\n  graph_utils::ReplaceNodeInput(*last_transpose, 0, dense_weight_partition_arg);\r\n  graph.RemoveInitializedTensor(dense_weight_arg->Name());\r\n  updated_weight_names_.insert({dense_weight_arg->Name(), dense_weight_partition_arg.Name()});\r\n\r\n  // It's possible that the node vector contains nullptr due to some optinal node infos during linear pattern matching.\r\n  std::copy_if(sub_graph_node_ptrs.begin(), sub_graph_node_ptrs.end(),\r\n               std::back_inserter(nodes_to_clear_shape),\r\n               [](Node* node_ptr) { return node_ptr != nullptr; });\r\n\r\n  // Change the constant for the reshape nodes.\r\n  for (auto x : reshape_node_ptrs) {\r\n    Node* node_ptr = x.first;\r\n    int64_t idx = x.second;\r\n    auto shape_arg = node_ptr->MutableInputDefs()[1];\r\n    const ONNX_NAMESPACE::TensorProto* tensor;\r\n    ORT_RETURN_IF_NOT(graph.GetInitializedTensor(shape_arg->Name(), tensor),\r\n                      \"Expecting initializer present: \", shape_arg->Name());\r\n    auto data_type = tensor->data_type();\r\n    auto init_const = Initializer{graph, *tensor, graph.ModelPath()};\r\n    const int64_t* val = init_const.data<int64_t>();\r\n    int64_t size = init_const.size();\r\n    ONNX_NAMESPACE::TensorProto tensor_partition;\r\n    tensor_partition.set_name(graph.GenerateNodeArgName(\"partition_\" + shape_arg->Name()));\r\n    tensor_partition.set_data_type(data_type);\r\n    tensor_partition.add_dims(size);\r\n\r\n    InlinedVector<int64_t> val_partition;\r\n    val_partition.reserve(size);\r\n    val_partition.insert(val_partition.end(), val, val + size);\r\n    val_partition[idx] /= horizontal_parallel_size_;\r\n    tensor_partition.set_raw_data(val_partition.data(), size * sizeof(int64_t));\r\n    NodeArg& node_arg_partition = graph_utils::AddInitializerWithOrtValue(graph, tensor_partition);\r\n    graph_utils::ReplaceNodeInput(*node_ptr, 1, node_arg_partition);\r\n    graph.RemoveInitializedTensor(shape_arg->Name());\r\n  }\r\n\r\n  if (dropout_node_ptr != nullptr) {\r\n    dropout_nodes_to_transform.insert(dropout_node_ptr);\r\n  }\r\n\r\n  // Add MegatronF before the 1st MatMul and MegatronG before the last Add.\r\n\r\n  NodeArg* prev_input_node_ptr = k_matmul_ptr->MutableInputDefs()[0];\r\n  InlinedVector<Node*> new_consumer_nodes;\r\n  const auto& node_consumers = graph.GetConsumerNodes(prev_input_node_ptr->Name());\r\n  for (auto& n : node_consumers) {\r\n    if (n->Index() == k_matmul_ptr->Index() || n->Index() == v_matmul_ptr->Index() || n->Index() == q_matmul_ptr->Index()) {\r\n      continue;\r\n    }\r\n    new_consumer_nodes.emplace_back(const_cast<Node*>(n));\r\n  }\r\n\r\n  bool shared_same_input = k_matmul_ptr->MutableInputDefs()[0]->Name().compare(q_matmul_ptr->MutableInputDefs()[0]->Name()) == 0;\r\n\r\n  // then for q, and k&v will have different MegatronF node.\r\n  {\r\n    const std::array sa_f_input_defs{prev_input_node_ptr};\r\n    auto sa_f_type_info = *prev_input_node_ptr->TypeAsProto();\r\n    auto& sa_f_out_arg = graph.GetOrCreateNodeArg(graph.GenerateNodeArgName(k_matmul_ptr->Name() + \"BARTAttention_MegatronF_Output\"), &sa_f_type_info);\r\n    Node& sa_f_node = graph.AddNode(graph.GenerateNodeName(k_matmul_ptr->Name() + \"BARTAttention_MegatronF\"),\r\n                                    \"MegatronF\",\r\n                                    k_matmul_ptr->Name() + \" BARTAttention MegatronF\",\r\n                                    sa_f_input_defs,\r\n                                    {&sa_f_out_arg}, {}, kMSDomain);\r\n    sa_f_node.SetExecutionProviderType(k_matmul_ptr->GetExecutionProviderType());\r\n    graph_utils::ReplaceNodeInput(*k_matmul_ptr, 0, *(sa_f_node.MutableOutputDefs()[0]));\r\n    graph_utils::ReplaceNodeInput(*v_matmul_ptr, 0, *(sa_f_node.MutableOutputDefs()[0]));\r\n    if (shared_same_input) {\r\n      graph_utils::ReplaceNodeInput(*q_matmul_ptr, 0, *(sa_f_node.MutableOutputDefs()[0]));\r\n    }\r\n    new_consumer_nodes.push_back(&sa_f_node);\r\n  }\r\n  graph.UpdateConsumerNodes(prev_input_node_ptr->Name(), new_consumer_nodes);\r\n  counter++;\r\n  if (!shared_same_input) {\r\n    {\r\n      NodeArg* q_prev_input_node_ptr = q_matmul_ptr->MutableInputDefs()[0];\r\n      InlinedVector<Node*> q_new_consumer_nodes;\r\n      const auto& q_node_consumers = graph.GetConsumerNodes(q_prev_input_node_ptr->Name());\r\n      for (auto& n : q_node_consumers) {\r\n        if (n->Index() == k_matmul_ptr->Index() || n->Index() == v_matmul_ptr->Index() || n->Index() == q_matmul_ptr->Index()) {\r\n          continue;\r\n        }\r\n        q_new_consumer_nodes.emplace_back(const_cast<Node*>(n));\r\n      }\r\n\r\n      const std::array q_sa_f_input_defs{q_matmul_ptr->MutableInputDefs()[0]};\r\n      auto q_sa_f_type_info = *q_matmul_ptr->MutableInputDefs()[0]->TypeAsProto();\r\n      auto& q_sa_f_out_arg = graph.GetOrCreateNodeArg(graph.GenerateNodeArgName(q_matmul_ptr->Name() + \"BARTAttention_MegatronF_Output\"), &q_sa_f_type_info);\r\n      Node& q_sa_f_node = graph.AddNode(graph.GenerateNodeName(q_matmul_ptr->Name() + \"BARTAttention_MegatronF\"),\r\n                                        \"MegatronF\",\r\n                                        q_matmul_ptr->Name() + \" BARTAttention MegatronF\",\r\n                                        q_sa_f_input_defs,\r\n                                        {&q_sa_f_out_arg}, {}, kMSDomain);\r\n      q_sa_f_node.SetExecutionProviderType(q_matmul_ptr->GetExecutionProviderType());\r\n\r\n      graph_utils::ReplaceNodeInput(*q_matmul_ptr, 0, *(q_sa_f_node.MutableOutputDefs()[0]));\r\n      q_new_consumer_nodes.push_back(&q_sa_f_node);\r\n      graph.UpdateConsumerNodes(q_prev_input_node_ptr->Name(), q_new_consumer_nodes);\r\n      // todo: need update the consumer node for the input_node as well.\r\n    }\r\n  }\r\n\r\n  const std::array sa_g_input_defs{dense_matmul_node.MutableOutputDefs()[0]};\r\n  auto sa_g_type_info = *dense_matmul_node.MutableOutputDefs()[0]->TypeAsProto();  // copy\r\n  auto& sa_g_out_arg = graph.GetOrCreateNodeArg(graph.GenerateNodeArgName(\"BARTAttention_MegatronG_Output\"), &sa_g_type_info);\r\n  Node& sa_g_node = graph.AddNode(graph.GenerateNodeName(k_matmul_ptr->Name() + \"BARTAttention_MegatronG\"),\r\n                                  \"MegatronG\",\r\n                                  \"BARTAttention MegatronG\",\r\n                                  sa_g_input_defs,\r\n                                  {&sa_g_out_arg}, {}, kMSDomain);\r\n  sa_g_node.AddAttribute(\"group_type\", static_cast<int64_t>(training::WorkerGroupType::HorizontalParallel));\r\n  sa_g_node.SetExecutionProviderType(k_matmul_ptr->GetExecutionProviderType());\r\n  graph_utils::ReplaceDownstreamNodeInput(graph, dense_matmul_node, 0, sa_g_node, 0);\r\n\r\n  modified = true;\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus MegatronTransformer::DoTransform(Graph& graph, bool& modified, int graph_level,\r\n                                        const logging::Logger& logger,\r\n                                        InlinedVector<Node*>& nodes_to_clear_shape,\r\n                                        InlinedHashSet<Node*>& dropout_nodes_to_transform) const {\r\n  InlinedVector<int> counters(4);\r\n  GraphViewer graph_viewer(graph);\r\n  const auto& node_topology_list = graph_viewer.GetNodesInTopologicalOrder();\r\n  for (auto node_index : node_topology_list) {\r\n    auto& node = *graph.GetNode(node_index);\r\n    ORT_RETURN_IF_ERROR(Recurse(node, modified, graph_level, logger));\r\n    int i = 0;\r\n    auto ret = TransformGPT2Attention(graph, modified, nodes_to_clear_shape,\r\n                                      dropout_nodes_to_transform, counters[i++], node_index);\r\n    if (ret.Code() != common::NOT_IMPLEMENTED) {\r\n      ORT_ENFORCE(ret.IsOK());\r\n      continue;\r\n    }\r\n\r\n    ret = TransformGPT2MLP(graph, modified, nodes_to_clear_shape, counters[i++], node_index);\r\n    if (ret.Code() != common::NOT_IMPLEMENTED) {\r\n      ORT_ENFORCE(ret.IsOK());\r\n      continue;\r\n    }\r\n\r\n    ret = TransformBARTAttention(graph, modified, nodes_to_clear_shape,\r\n                                 dropout_nodes_to_transform, counters[i++], node_index);\r\n    if (ret.Code() != common::NOT_IMPLEMENTED) {\r\n      ORT_ENFORCE(ret.IsOK());\r\n      continue;\r\n    }\r\n\r\n    ret = TransformBARTMLP(graph, modified, nodes_to_clear_shape,\r\n                           dropout_nodes_to_transform, counters[i++], node_index);\r\n    if (ret.Code() != common::NOT_IMPLEMENTED) {\r\n      ORT_ENFORCE(ret.IsOK());\r\n      continue;\r\n    }\r\n  }\r\n\r\n  LOGS_DEFAULT(WARNING) << \"Megatron transformer result : Partitioned \"\r\n                        << counters[0] << \" GPT2 Attention Blocks, \"\r\n                        << counters[1] << \" GPT2 MLP Blocks, \"\r\n                        << counters[2] << \" BART Attention Blocks, \"\r\n                        << counters[3] << \" BART MLP Blocks.\";\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus MegatronTransformer::TransformDropout(Graph& graph, bool& modified, int graph_level, const logging::Logger& logger,\r\n                                             InlinedHashSet<Node*>& dropout_nodes_to_transform, int32_t& counter) const {\r\n  GraphViewer graph_viewer(graph);\r\n  const auto& node_topology_list = graph_viewer.GetNodesInTopologicalOrder();\r\n  for (auto node_index : node_topology_list) {\r\n    auto& node = *graph.GetNode(node_index);\r\n    ORT_RETURN_IF_ERROR(Recurse(node, modified, graph_level, logger));\r\n\r\n    if (!graph_utils::IsSupportedProvider(node, GetCompatibleExecutionProviders())) {\r\n      continue;\r\n    }\r\n\r\n    if (!graph_utils::IsSupportedOptypeVersionAndDomain(node, \"Dropout\", opset_v12_13)) {\r\n      continue;\r\n    }\r\n\r\n    // Only need to set the seed if it's a transformed self-attention dropout, or the seed attribute is not set.\r\n    if (dropout_nodes_to_transform.find(&node) != dropout_nodes_to_transform.end()) {\r\n      int64_t seed = static_cast<int64_t>(HashName(node.MutableOutputDefs()[0]->Name())) + utils::GetRandomSeed();\r\n      if (dropout_nodes_to_transform.find(&node) != dropout_nodes_to_transform.end()) {\r\n        seed += horizontal_parallel_rank_;\r\n      }\r\n\r\n      if (graph_utils::GetNodeAttribute(node, \"seed\") != nullptr) {\r\n        node.ClearAttribute(\"seed\");\r\n      }\r\n      node.AddAttribute(\"seed\", seed);\r\n      counter++;\r\n      modified = true;\r\n    }\r\n  }\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus MegatronTransformer::ApplyImpl(Graph& graph, bool& modified, int graph_level, const logging::Logger& logger) const {\r\n  if (horizontal_parallel_size_ <= 1) {\r\n    return Status::OK();\r\n  }\r\n\r\n  InlinedVector<Node*> nodes_to_clear_shape;\r\n  InlinedHashSet<Node*> dropout_nodes_to_transform;\r\n  int32_t dropout_changed = 0;\r\n\r\n  ORT_RETURN_IF_ERROR(DoTransform(graph, modified, graph_level, logger,\r\n                                  nodes_to_clear_shape, dropout_nodes_to_transform));\r\n  ORT_RETURN_IF_ERROR(TransformDropout(graph, modified, graph_level, logger,\r\n                                       dropout_nodes_to_transform, dropout_changed));\r\n\r\n  auto& graph_inputs = graph.GetInputs();\r\n  for (auto node : nodes_to_clear_shape) {\r\n    if (node != nullptr) {\r\n      auto& inputs = node->MutableInputDefs();\r\n      for (auto* input : inputs)\r\n        if (std::find(graph_inputs.begin(), graph_inputs.end(), input) == graph_inputs.end())\r\n          input->ClearShape();\r\n\r\n      for (auto* output : node->MutableOutputDefs())\r\n        if (std::find(graph_inputs.begin(), graph_inputs.end(), output) == graph_inputs.end())\r\n          output->ClearShape();\r\n    }\r\n  }\r\n\r\n  for (auto x : updated_weight_names_) {\r\n    auto old_initializer_name = x.first;\r\n    auto new_initializer_name = x.second;\r\n    if (weights_to_train_.find(old_initializer_name) != weights_to_train_.end()) {\r\n      weights_to_train_.erase(old_initializer_name);\r\n      weights_to_train_.insert(new_initializer_name);\r\n    }\r\n  }\r\n\r\n  if (modified) {\r\n    graph.SetGraphResolveNeeded();\r\n    auto ret = graph.Resolve();\r\n    LOGS(logger, WARNING) << \"Megatron transformer result: Reset seed for \" << dropout_changed\r\n                          << \" Dropout nodes. Error Message (if there is): \" << ret.ErrorMessage();\r\n    return ret;\r\n  }\r\n\r\n  LOGS(logger, WARNING) << \"Megatron transformer result : unmodified\\n\";\r\n  return Status::OK();\r\n}\r\n\r\n}  // namespace onnxruntime\r\n"}
{"path":"\\\\?\\C:\\Users\\Naomi\\Desktop\\ONNX\\onnxruntime\\onnxruntime\\test\\python\\quantization\\test_op_matmul_bnb4.py","line":8,"char":1,"code":"RUFF","severity":"warning","name":"I001","description":"Import block is un-sorted or un-formatted.\nSee https://docs.astral.sh/ruff/rules/unsorted-imports"}
{"path":"\\\\?\\C:\\Users\\Naomi\\Desktop\\ONNX\\onnxruntime\\onnxruntime\\test\\python\\quantization\\test_op_matmul_bnb4.py","line":null,"char":null,"code":"RUFF","severity":"warning","name":"format","description":"Run `lintrunner -a` to apply this patch.","original":"#!/usr/bin/env python\r\n# -------------------------------------------------------------------------\r\n# Copyright (c) Microsoft Corporation. All rights reserved.\r\n# Licensed under the MIT License. See License.txt in the project root for\r\n# license information.\r\n# --------------------------------------------------------------------------\r\n\r\nimport tempfile\r\nimport unittest\r\nfrom pathlib import Path\r\n\r\nimport numpy as np\r\nimport onnx\r\nfrom onnx import TensorProto, helper\r\nfrom op_test_utils import TestDataFeeds, check_model_correctness, check_op_type_count\r\n\r\nfrom onnxruntime.quantization import quant_utils\r\n\r\nquant_maps = {\r\n    0: [\r\n        0.00000000,\r\n        5.208333333e-03,\r\n        0.66666667,\r\n        1.00000000,\r\n        0.33333333,\r\n        0.50000000,\r\n        0.16666667,\r\n        0.25000000,\r\n        -0.00000000,\r\n        -5.208333333e-03,\r\n        -0.66666667,\r\n        -1.00000000,\r\n        -0.33333333,\r\n        -0.50000000,\r\n        -0.16666667,\r\n        -0.25000000,\r\n    ],\r\n    1: [\r\n        -1.0,\r\n        -0.6961928009986877,\r\n        -0.5250730514526367,\r\n        -0.39491748809814453,\r\n        -0.28444138169288635,\r\n        -0.18477343022823334,\r\n        -0.09105003625154495,\r\n        0.0,\r\n        0.07958029955625534,\r\n        0.16093020141124725,\r\n        0.24611230194568634,\r\n        0.33791524171829224,\r\n        0.44070982933044434,\r\n        0.5626170039176941,\r\n        0.7229568362236023,\r\n        1.0,\r\n    ],\r\n}\r\n\r\n\r\nclass TestOpMatMulBnb4(unittest.TestCase):\r\n    @classmethod\r\n    def setUpClass(cls):\r\n        cls._tmp_model_dir = tempfile.TemporaryDirectory(prefix=\"test_matmulbnb4.\")\r\n\r\n    @classmethod\r\n    def tearDownClass(cls):\r\n        cls._tmp_model_dir.cleanup()\r\n\r\n    def fill_bnb4_data(self, shape: tuple[int, int], quant_type: int) -> np.ndarray:\r\n        rows, cols = shape\r\n        line = np.zeros(shape)\r\n        line = line.reshape(-1)\r\n        quant_map = np.array(quant_maps[quant_type], dtype=np.float32)\r\n\r\n        v = 0\r\n        for i in range(line.shape[0]):\r\n            line[i] = quant_map[v]\r\n            v += 1\r\n            if v >= 16:\r\n                v = 0\r\n\r\n        # bnb quantization quantizes weight.T after flattening\r\n        line = line.reshape(cols, rows).transpose()\r\n        return line.reshape(shape)\r\n\r\n    def input_feeds(self, n: int, name2shape: dict[str, int | tuple[int, ...]]) -> TestDataFeeds:\r\n        input_data_list = []\r\n        for _i in range(n):\r\n            inputs = {}\r\n            for name, shape in name2shape.items():\r\n                inputs.update({name: np.random.randint(-1, 2, shape).astype(np.float32)})\r\n            input_data_list.extend([inputs])\r\n        dr = TestDataFeeds(input_data_list)\r\n        return dr\r\n\r\n    def construct_model_matmul(self, output_model_path: str, quant_type: int) -> None:\r\n        #      (input)\r\n        #         |\r\n        #       MatMul\r\n        #         |\r\n        #      (output)\r\n        input_name = \"input\"\r\n        output_name = \"output\"\r\n        initializers = []\r\n\r\n        def make_matmul(input_name, weight_shape: int | tuple[int, ...], weight_name: str, output_name: str):\r\n            weight_data = self.fill_bnb4_data(weight_shape, quant_type).astype(np.float32)\r\n            initializers.append(onnx.numpy_helper.from_array(weight_data, name=weight_name))\r\n            return onnx.helper.make_node(\r\n                \"MatMul\",\r\n                [input_name, weight_name],\r\n                [output_name],\r\n            )\r\n\r\n        # for this to work (in_features * out_features) % block_size == 0\r\n        in_features = 52\r\n        out_features = 288\r\n        # make MatMul node\r\n        matmul_node = make_matmul(\r\n            input_name,\r\n            [in_features, out_features],\r\n            \"linear1.weight\",\r\n            output_name,\r\n        )\r\n\r\n        # make graph\r\n        input_tensor = helper.make_tensor_value_info(input_name, TensorProto.FLOAT, [-1, in_features])\r\n        output_tensor = helper.make_tensor_value_info(output_name, TensorProto.FLOAT, [-1, out_features])\r\n        graph_name = \"matmul_bnb4_test\"\r\n        graph = helper.make_graph(\r\n            [matmul_node],\r\n            graph_name,\r\n            [input_tensor],\r\n            [output_tensor],\r\n            initializer=initializers,\r\n        )\r\n        model = helper.make_model(graph, opset_imports=[helper.make_opsetid(\"\", 13)])\r\n        model.ir_version = 7  # use stable onnx ir version\r\n\r\n        onnx.save(model, output_model_path)\r\n\r\n    def quant_test(self, quant_type: int, block_size: int):\r\n        model_fp32_path = str(Path(self._tmp_model_dir.name).joinpath(f\"matmul_fp32_{quant_type}.onnx\").absolute())\r\n        self.construct_model_matmul(model_fp32_path, quant_type)\r\n        data_reader = self.input_feeds(1, {\"input\": [100, 52]})\r\n\r\n        model_bnb4_path = str(\r\n            Path(self._tmp_model_dir.name).joinpath(f\"MatMulBnb4_{quant_type}_{block_size}.onnx\").absolute()\r\n        )\r\n\r\n        # Quantize fp32 model to bnb4 model\r\n        from onnxruntime.quantization import matmul_bnb4_quantizer  # noqa: PLC0415\r\n\r\n        model = quant_utils.load_model_with_shape_infer(Path(model_fp32_path))\r\n        quant = matmul_bnb4_quantizer.MatMulBnb4Quantizer(model, quant_type, block_size)\r\n        quant.process()\r\n        quant.model.save_model_to_file(model_bnb4_path, False)\r\n\r\n        quant_nodes = {\"MatMulBnb4\": 1}\r\n        check_op_type_count(self, model_bnb4_path, **quant_nodes)\r\n\r\n        data_reader.rewind()\r\n\r\n        try:\r\n            check_model_correctness(self, model_fp32_path, model_bnb4_path, data_reader.get_next())\r\n        except Exception as exception:\r\n            raise exception\r\n\r\n    def test_quantize_matmul_bnb4_fp4(self):\r\n        np.random.seed(13)\r\n        self.quant_test(0, 64)\r\n\r\n    def test_quantize_matmul_bnb4_nf4(self):\r\n        np.random.seed(13)\r\n        self.quant_test(1, 64)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    unittest.main()\r\n","replacement":"#!/usr/bin/env python\r\n# -------------------------------------------------------------------------\r\n# Copyright (c) Microsoft Corporation. All rights reserved.\r\n# Licensed under the MIT License. See License.txt in the project root for\r\n# license information.\r\n# --------------------------------------------------------------------------\r\n\r\nimport tempfile\r\nimport unittest\r\nfrom pathlib import Path\r\n\r\nimport numpy as np\r\nimport onnx\r\nfrom onnx import TensorProto, helper\r\nfrom onnxruntime.quantization import quant_utils\r\nfrom op_test_utils import TestDataFeeds, check_model_correctness, check_op_type_count\r\n\r\nquant_maps = {\r\n    0: [\r\n        0.00000000,\r\n        5.208333333e-03,\r\n        0.66666667,\r\n        1.00000000,\r\n        0.33333333,\r\n        0.50000000,\r\n        0.16666667,\r\n        0.25000000,\r\n        -0.00000000,\r\n        -5.208333333e-03,\r\n        -0.66666667,\r\n        -1.00000000,\r\n        -0.33333333,\r\n        -0.50000000,\r\n        -0.16666667,\r\n        -0.25000000,\r\n    ],\r\n    1: [\r\n        -1.0,\r\n        -0.6961928009986877,\r\n        -0.5250730514526367,\r\n        -0.39491748809814453,\r\n        -0.28444138169288635,\r\n        -0.18477343022823334,\r\n        -0.09105003625154495,\r\n        0.0,\r\n        0.07958029955625534,\r\n        0.16093020141124725,\r\n        0.24611230194568634,\r\n        0.33791524171829224,\r\n        0.44070982933044434,\r\n        0.5626170039176941,\r\n        0.7229568362236023,\r\n        1.0,\r\n    ],\r\n}\r\n\r\n\r\nclass TestOpMatMulBnb4(unittest.TestCase):\r\n    @classmethod\r\n    def setUpClass(cls):\r\n        cls._tmp_model_dir = tempfile.TemporaryDirectory(prefix=\"test_matmulbnb4.\")\r\n\r\n    @classmethod\r\n    def tearDownClass(cls):\r\n        cls._tmp_model_dir.cleanup()\r\n\r\n    def fill_bnb4_data(self, shape: tuple[int, int], quant_type: int) -> np.ndarray:\r\n        rows, cols = shape\r\n        line = np.zeros(shape)\r\n        line = line.reshape(-1)\r\n        quant_map = np.array(quant_maps[quant_type], dtype=np.float32)\r\n\r\n        v = 0\r\n        for i in range(line.shape[0]):\r\n            line[i] = quant_map[v]\r\n            v += 1\r\n            if v >= 16:\r\n                v = 0\r\n\r\n        # bnb quantization quantizes weight.T after flattening\r\n        line = line.reshape(cols, rows).transpose()\r\n        return line.reshape(shape)\r\n\r\n    def input_feeds(self, n: int, name2shape: dict[str, int | tuple[int, ...]]) -> TestDataFeeds:\r\n        input_data_list = []\r\n        for _i in range(n):\r\n            inputs = {}\r\n            for name, shape in name2shape.items():\r\n                inputs.update({name: np.random.randint(-1, 2, shape).astype(np.float32)})\r\n            input_data_list.extend([inputs])\r\n        dr = TestDataFeeds(input_data_list)\r\n        return dr\r\n\r\n    def construct_model_matmul(self, output_model_path: str, quant_type: int) -> None:\r\n        #      (input)\r\n        #         |\r\n        #       MatMul\r\n        #         |\r\n        #      (output)\r\n        input_name = \"input\"\r\n        output_name = \"output\"\r\n        initializers = []\r\n\r\n        def make_matmul(input_name, weight_shape: int | tuple[int, ...], weight_name: str, output_name: str):\r\n            weight_data = self.fill_bnb4_data(weight_shape, quant_type).astype(np.float32)\r\n            initializers.append(onnx.numpy_helper.from_array(weight_data, name=weight_name))\r\n            return onnx.helper.make_node(\r\n                \"MatMul\",\r\n                [input_name, weight_name],\r\n                [output_name],\r\n            )\r\n\r\n        # for this to work (in_features * out_features) % block_size == 0\r\n        in_features = 52\r\n        out_features = 288\r\n        # make MatMul node\r\n        matmul_node = make_matmul(\r\n            input_name,\r\n            [in_features, out_features],\r\n            \"linear1.weight\",\r\n            output_name,\r\n        )\r\n\r\n        # make graph\r\n        input_tensor = helper.make_tensor_value_info(input_name, TensorProto.FLOAT, [-1, in_features])\r\n        output_tensor = helper.make_tensor_value_info(output_name, TensorProto.FLOAT, [-1, out_features])\r\n        graph_name = \"matmul_bnb4_test\"\r\n        graph = helper.make_graph(\r\n            [matmul_node],\r\n            graph_name,\r\n            [input_tensor],\r\n            [output_tensor],\r\n            initializer=initializers,\r\n        )\r\n        model = helper.make_model(graph, opset_imports=[helper.make_opsetid(\"\", 13)])\r\n        model.ir_version = 7  # use stable onnx ir version\r\n\r\n        onnx.save(model, output_model_path)\r\n\r\n    def quant_test(self, quant_type: int, block_size: int):\r\n        model_fp32_path = str(Path(self._tmp_model_dir.name).joinpath(f\"matmul_fp32_{quant_type}.onnx\").absolute())\r\n        self.construct_model_matmul(model_fp32_path, quant_type)\r\n        data_reader = self.input_feeds(1, {\"input\": [100, 52]})\r\n\r\n        model_bnb4_path = str(\r\n            Path(self._tmp_model_dir.name).joinpath(f\"MatMulBnb4_{quant_type}_{block_size}.onnx\").absolute()\r\n        )\r\n\r\n        # Quantize fp32 model to bnb4 model\r\n        from onnxruntime.quantization import matmul_bnb4_quantizer  # noqa: PLC0415\r\n\r\n        model = quant_utils.load_model_with_shape_infer(Path(model_fp32_path))\r\n        quant = matmul_bnb4_quantizer.MatMulBnb4Quantizer(model, quant_type, block_size)\r\n        quant.process()\r\n        quant.model.save_model_to_file(model_bnb4_path, False)\r\n\r\n        quant_nodes = {\"MatMulBnb4\": 1}\r\n        check_op_type_count(self, model_bnb4_path, **quant_nodes)\r\n\r\n        data_reader.rewind()\r\n\r\n        try:\r\n            check_model_correctness(self, model_fp32_path, model_bnb4_path, data_reader.get_next())\r\n        except Exception as exception:\r\n            raise exception\r\n\r\n    def test_quantize_matmul_bnb4_fp4(self):\r\n        np.random.seed(13)\r\n        self.quant_test(0, 64)\r\n\r\n    def test_quantize_matmul_bnb4_nf4(self):\r\n        np.random.seed(13)\r\n        self.quant_test(1, 64)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    unittest.main()\r\n"}
{"path":"\\\\?\\C:\\Users\\Naomi\\Desktop\\ONNX\\onnxruntime\\onnxruntime\\core\\providers\\qnn\\builder\\qnn_backend_manager.cc","line":null,"char":null,"code":"CLANGFORMAT","severity":"warning","name":"format","description":"See https://clang.llvm.org/docs/ClangFormat.html.\nRun `lintrunner -a` to apply this patch.","original":"//\r\n// Copyright (c) Microsoft Corporation. All rights reserved.\r\n// Licensed under the MIT License.\r\n\r\n#include \"qnn_backend_manager.h\"\r\n#include \"qnn_model.h\"\r\n#include <filesystem>\r\n#include <fstream>\r\n#include <string>\r\n#include \"QnnOpDef.h\"\r\n#include \"CPU/QnnCpuCommon.h\"\r\n#include \"GPU/QnnGpuCommon.h\"\r\n#include \"DSP/QnnDspCommon.h\"\r\n#include \"HTP/QnnHtpCommon.h\"\r\n#include \"HTP/QnnHtpContext.h\"\r\n#include \"HTP/QnnHtpPerfInfrastructure.h\"\r\n#include \"HTP/QnnHtpSystemContext.h\"\r\n#include \"IR/QnnIrCommon.h\"\r\n#include \"IR/QnnIrGraph.h\"\r\n#include \"Saver/QnnSaver.h\"\r\n#include \"Saver/QnnSaverCommon.h\"\r\n#include <gsl/gsl>\r\n\r\n#include \"core/providers/qnn/ort_api.h\"\r\n#include \"core/providers/qnn/qnn_allocator.h\"\r\n#include \"core/providers/qnn/qnn_telemetry.h\"\r\n#include \"core/providers/qnn/shared_context.h\"\r\n#include \"core/providers/qnn/builder/onnx_ctx_model_helper.h\"\r\n#include \"core/providers/qnn/builder/qnn_configs_helper.h\"\r\n#include \"core/providers/qnn/builder/qnn_utils.h\"\r\n\r\n// Flag to determine if Backend should do node validation for each opNode added\r\n#define DO_GRAPH_NODE_VALIDATIONS 1\r\n\r\nnamespace onnxruntime {\r\nnamespace qnn {\r\n\r\ntypedef Qnn_ErrorHandle_t (*QnnInterfaceGetProvidersFn_t)(const QnnInterface_t*** providerList,\r\n                                                          uint32_t* numProviders);\r\ntypedef Qnn_ErrorHandle_t (*QnnSystemInterfaceGetProvidersFn_t)(const QnnSystemInterface_t*** providerList,\r\n                                                                uint32_t* numProviders);\r\n\r\nstatic Qnn_Version_t GetQnnInterfaceApiVersion(const QnnInterface_t* qnn_interface) {\r\n  return qnn_interface->apiVersion.coreApiVersion;\r\n}\r\n\r\nstatic Qnn_Version_t GetQnnInterfaceApiVersion(const QnnSystemInterface_t* qnn_interface) {\r\n  return qnn_interface->systemApiVersion;\r\n}\r\n\r\nstatic const char* DlError() {\r\n#ifdef _WIN32\r\n  return \"\";\r\n#else\r\n  return ::dlerror();\r\n#endif\r\n}\r\n\r\n// Workaround for a missing comma in QNN_IR_GRAPH_CUSTOM_CONFIG_INIT.\r\nstatic QnnIrGraph_CustomConfig_t EmptyIrGraphConfig() {\r\n  return {\r\n      QNN_IR_GRAPH_CONFIG_OPTION_SERIALIZATION, {QNN_IR_GRAPH_SERIALIZATION_TYPE_FLAT_BUFFER, \"\"}};\r\n}\r\n\r\nclass QnnIrConfig : public QnnSerializerConfig {\r\n public:\r\n  QnnIrConfig(std::string backend_path, std::string dlc_dir)\r\n      : QnnSerializerConfig(std::move(backend_path)), dlc_dir_(std::move(dlc_dir)), configs_builder_(MakeConfigsBuilder()) {\r\n  }\r\n\r\n  const QnnGraph_Config_t** Configure() override {\r\n    auto configs_builder = MakeConfigsBuilder();\r\n\r\n    std::filesystem::path dlc_path = (dlc_dir_ / (GetGraphName() + \".dlc\"));\r\n    std::string dlc_path_str = dlc_path.string();\r\n    gsl::not_null<QnnIrGraph_CustomConfig_t*> dlc_path_config = configs_builder.PushCustomConfig();\r\n    dlc_path_config->option = QNN_IR_GRAPH_CONFIG_OPTION_SERIALIZATION;\r\n    dlc_path_config->serializationOption.serializationType = QNN_IR_GRAPH_SERIALIZATION_TYPE_FLAT_BUFFER;\r\n    dlc_path_config->serializationOption.outputPath = dlc_path_str.c_str();\r\n\r\n    gsl::not_null<QnnGraph_Config_t*> dlc_path_custom_config = configs_builder.PushConfig();\r\n    dlc_path_custom_config->option = QNN_GRAPH_CONFIG_OPTION_CUSTOM;\r\n    dlc_path_custom_config->customConfig = dlc_path_config;\r\n\r\n    std::filesystem::create_directories(dlc_path);\r\n\r\n    // Keep the pointer to dlc_path_str's null-terminated string alive.\r\n    std::swap(dlc_path_str, dlc_path_str_);\r\n\r\n    std::swap(configs_builder, configs_builder_);\r\n    return configs_builder_.GetQnnConfigs();\r\n  }\r\n\r\n  bool SupportsArbitraryGraphConfigs() const override {\r\n    return false;\r\n  }\r\n\r\n private:\r\n  static QnnConfigsBuilder<QnnGraph_Config_t, QnnIrGraph_CustomConfig_t> MakeConfigsBuilder() {\r\n    return QnnConfigsBuilder<QnnGraph_Config_t, QnnIrGraph_CustomConfig_t>(QNN_GRAPH_CONFIG_INIT, EmptyIrGraphConfig());\r\n  }\r\n\r\n  std::filesystem::path dlc_dir_;\r\n  std::string dlc_path_str_;\r\n  QnnConfigsBuilder<QnnGraph_Config_t, QnnIrGraph_CustomConfig_t> configs_builder_;\r\n};\r\n\r\nclass QnnSaverConfig : public QnnSerializerConfig {\r\n public:\r\n  QnnSaverConfig(std::string backend_path) : QnnSerializerConfig(std::move(backend_path)) {}\r\n\r\n  const QnnGraph_Config_t** Configure() override {\r\n    return nullptr;\r\n  }\r\n\r\n  bool SupportsArbitraryGraphConfigs() const override {\r\n    return true;\r\n  }\r\n};\r\n\r\nQnnSerializerConfig::~QnnSerializerConfig() = default;\r\n\r\nQnnSerializerConfig::QnnSerializerConfig(std::string backend_path)\r\n    : backend_path_(std::move(backend_path)) {}\r\n\r\nstd::unique_ptr<QnnSerializerConfig> QnnSerializerConfig::CreateIr(std::string backend_path, std::string dlc_dir) {\r\n  return std::make_unique<QnnIrConfig>(std::move(backend_path), std::move(dlc_dir));\r\n}\r\n\r\nstd::unique_ptr<QnnSerializerConfig> QnnSerializerConfig::CreateSaver(std::string backend_path) {\r\n  return std::make_unique<QnnSaverConfig>(std::move(backend_path));\r\n}\r\n\r\nconst std::string& QnnSerializerConfig::GetBackendPath() const {\r\n  return backend_path_;\r\n}\r\n\r\nconst std::string& QnnSerializerConfig::GetGraphName() const {\r\n  return graph_name_;\r\n}\r\n\r\nvoid QnnSerializerConfig::SetGraphName(std::string graph_name) {\r\n  graph_name_ = std::move(graph_name);\r\n}\r\n\r\nStatus ReadBinaryFromFile(const std::string& file_path, uint8_t* buffer, size_t buffer_size) {\r\n  ORT_RETURN_IF(nullptr == buffer, \"Binary buffer is nullptr\");\r\n  std::ifstream in(file_path, std::ifstream::binary);\r\n  ORT_RETURN_IF(!in, \"Failed to open input file: \", file_path.c_str());\r\n  ORT_RETURN_IF(!in.read(reinterpret_cast<char*>(buffer), buffer_size), \"Failed to read the contents of: \", file_path.c_str());\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::ParseLoraConfig(std::string lora_config_path) {\r\n  LOGS_DEFAULT(INFO) << \"Acquiring the QnnInterface \" << lora_config_path;\r\n\r\n  // QNN Lora Config file format should be a single line, with the graph name first,\r\n  // followed by the qnn lora context binary path, separated by a semicolon (;)\r\n  // Example: <graph_name>;<binary_path>\r\n  LOGS_DEFAULT(INFO) << \"Loading Lora Config \" << lora_config_path;\r\n  std::ifstream file(lora_config_path);\r\n  std::string line;\r\n\r\n  if (file.is_open()) {\r\n    if (std::getline(file, line)) {\r\n      std::istringstream ss(line);\r\n      std::string graph_name;\r\n      std::string lora_adapter_bin_path;\r\n\r\n      if (std::getline(ss, graph_name, ';') && std::getline(ss, lora_adapter_bin_path)) {\r\n        size_t buffer_size = std::filesystem::file_size(lora_adapter_bin_path.c_str());\r\n\r\n        ORT_RETURN_IF(0 == buffer_size, \"Received path to an empty file. Nothing to deserialize.\");\r\n        std::unique_ptr<uint8_t[]> buffer = std::make_unique<uint8_t[]>(buffer_size);\r\n        void* voidBufferPtr = static_cast<void*>(buffer.get());\r\n        QnnContext_Buffer_t contextBuffer{QNN_CONTEXT_BUFFER_VERSION_1,\r\n                                          {QNN_CONTEXTMEMTYPE_RAW, {{voidBufferPtr, buffer_size}}}};\r\n\r\n        auto status = ReadBinaryFromFile(lora_adapter_bin_path,\r\n                                         reinterpret_cast<uint8_t*>(buffer.get()),\r\n                                         buffer_size);\r\n\r\n        ORT_RETURN_IF(status != Status::OK(), \"Failed to read binary data.\");\r\n        Qnn_GraphHandle_t graph;\r\n        bool graph_retrieve_success = false;\r\n        for (size_t cIdx = 0; cIdx < contexts_.size(); cIdx++) {\r\n          auto graph_retrieve_rt = qnn_interface_.graphRetrieve(contexts_[cIdx], graph_name.c_str(), &graph);\r\n          if (QNN_SUCCESS != graph_retrieve_rt) {\r\n            continue;\r\n          }\r\n\r\n          graph_retrieve_success = true;\r\n\r\n          auto context_apply_binary_section_rt = qnn_interface_.contextApplyBinarySection(\r\n              contexts_[cIdx], graph, QNN_CONTEXT_SECTION_UPDATABLE, &contextBuffer, profile_backend_handle_, nullptr);\r\n          ORT_RETURN_IF(QNN_SUCCESS != context_apply_binary_section_rt, \"Failed to apply binary section.\");\r\n          break;\r\n        }\r\n        ORT_RETURN_IF_NOT(graph_retrieve_success, \"Failed to retrieve graph: \", graph_name, \" and apply binary section.\");\r\n      }\r\n    }\r\n    file.close();\r\n  } else {\r\n    LOGS_DEFAULT(ERROR) << \"Unable to load Lora Config \" << lora_config_path;\r\n  }\r\n\r\n  return Status::OK();\r\n}\r\n\r\ntemplate <typename F, class T>\r\nStatus QnnBackendManager::GetQnnInterfaceProvider(const char* lib_path,\r\n                                                  const char* interface_provider_name,\r\n                                                  void** backend_lib_handle,\r\n                                                  Qnn_Version_t req_version,\r\n                                                  T** interface_provider) {\r\n  std::string error_msg;\r\n  *backend_lib_handle = LoadLib(lib_path,\r\n                                static_cast<int>(DlOpenFlag::DL_NOW) | static_cast<int>(DlOpenFlag::DL_GLOBAL),\r\n                                error_msg);\r\n  ORT_RETURN_IF(nullptr == *backend_lib_handle, \"Unable to load backend, error: \", error_msg, \" \", DlError());\r\n\r\n  // Get QNN Interface providers\r\n  F GetInterfaceProviders{nullptr};\r\n  GetInterfaceProviders = ResolveSymbol<F>(*backend_lib_handle, interface_provider_name, *logger_);\r\n  ORT_RETURN_IF(nullptr == GetInterfaceProviders, \"Failed to get QNN providers!\");\r\n\r\n  T** interface_providers{nullptr};\r\n  uint32_t num_providers{0};\r\n\r\n  auto result = GetInterfaceProviders((const T***)&interface_providers, &num_providers);\r\n  ORT_RETURN_IF((QNN_SUCCESS != result || nullptr == *interface_providers || 0 == num_providers),\r\n                \"Failed to get QNN providers.\");\r\n\r\n  bool found_valid_interface{false};\r\n  for (size_t pIdx = 0; pIdx < num_providers; pIdx++) {\r\n    Qnn_Version_t interface_version = GetQnnInterfaceApiVersion(interface_providers[pIdx]);\r\n\r\n    LOGS_DEFAULT(VERBOSE) << lib_path << \" interface version: \" << interface_version.major << \".\"\r\n                          << interface_version.minor << \".\" << interface_version.patch;\r\n\r\n    // Check the interface's API version against the required version.\r\n    // Major versions must match. The interface's minor version must be greater OR equal with a suitable patch version.\r\n    if (interface_version.major == req_version.major) {\r\n      bool minor_and_patch_version_ok = (interface_version.minor > req_version.minor) ||\r\n                                        (interface_version.minor == req_version.minor &&\r\n                                         interface_version.patch >= req_version.patch);\r\n      if (minor_and_patch_version_ok) {\r\n        found_valid_interface = true;\r\n        *interface_provider = interface_providers[pIdx];\r\n        break;\r\n      }\r\n    }\r\n  }\r\n\r\n  ORT_RETURN_IF_NOT(found_valid_interface, \"Unable to find a valid interface for \", lib_path);\r\n\r\n  return Status::OK();\r\n}\r\n\r\nvoid QnnBackendManager::SetQnnBackendType(uint32_t backend_id) {\r\n  switch (backend_id) {\r\n    case QNN_BACKEND_ID_CPU:\r\n      qnn_backend_type_ = QnnBackendType::CPU;\r\n      break;\r\n    case QNN_BACKEND_ID_GPU:\r\n      qnn_backend_type_ = QnnBackendType::GPU;\r\n      break;\r\n    case QNN_BACKEND_ID_DSP:\r\n      qnn_backend_type_ = QnnBackendType::DSP;\r\n      break;\r\n    case QNN_BACKEND_ID_HTP:\r\n      qnn_backend_type_ = QnnBackendType::HTP;\r\n      break;\r\n    case QNN_BACKEND_ID_IR:\r\n    case QNN_BACKEND_ID_SAVER:\r\n      qnn_backend_type_ = QnnBackendType::SERIALIZER;\r\n      break;\r\n    default:\r\n      qnn_backend_type_ = QnnBackendType::CPU;\r\n      break;\r\n  }\r\n}\r\n\r\nStatus QnnBackendManager::LoadBackend() {\r\n  QnnInterface_t* backend_interface_provider{nullptr};\r\n  auto rt = GetQnnInterfaceProvider<QnnInterfaceGetProvidersFn_t,\r\n                                    QnnInterface_t>(backend_path_.c_str(),\r\n                                                    \"QnnInterface_getProviders\",\r\n                                                    &backend_lib_handle_,\r\n                                                    {QNN_API_VERSION_MAJOR,\r\n                                                     QNN_API_VERSION_MINOR,\r\n                                                     QNN_API_VERSION_PATCH},\r\n                                                    &backend_interface_provider);\r\n  ORT_RETURN_IF_ERROR(rt);\r\n  qnn_interface_ = backend_interface_provider->QNN_INTERFACE_VER_NAME;\r\n  auto backend_id = backend_interface_provider->backendId;\r\n  SetQnnBackendType(backend_id);\r\n\r\n  Qnn_Version_t backend_interface_version = GetQnnInterfaceApiVersion(backend_interface_provider);\r\n  LOGS_DEFAULT(INFO) << \"Found valid interface, version: \" << backend_interface_version.major\r\n                     << \".\" << backend_interface_version.minor << \".\" << backend_interface_version.patch\r\n                     << \" backend provider name: \" << backend_interface_provider->providerName\r\n                     << \" backend id: \" << backend_id;\r\n\r\n  return Status::OK();\r\n}\r\n\r\nQnnSerializerConfig* QnnBackendManager::GetQnnSerializerConfig() {\r\n  return qnn_serializer_config_.get();\r\n}\r\n\r\n// Loads the intended backend (e.g., HTP, CPU, etc) to get its type, and then\r\n// sets QnnSaver or QnnIr as the active backend. QNN op builders will still see the intended backend\r\n// (e.g., HTP) as the backend type to ensure they emit the expected QNN API calls. Note, however, that\r\n// calls to QnnBackend_validateOpConfig will be to the saver backend, not the \"intended\" one.\r\n//\r\n// QnnSaver and QnnIr are \"debugging\" backends that serializes all QNN API calls (and weights) into\r\n// local files: Saver dumps to C++ sources and Ir to .dlc archives.\r\n// This information can be used to debug issues by replaying QNN API calls with another backend.\r\nStatus QnnBackendManager::LoadQnnSerializerBackend() {\r\n  void* backend_lib_handle = nullptr;\r\n\r\n  // Helper that unloads the intended backend library handle when the `unload_backend_lib` variable\r\n  // goes out of scope. Similar to `defer` in other languages.\r\n  auto unload_backend_lib = gsl::finally([&] {\r\n    if (backend_lib_handle != nullptr) {\r\n      auto result = UnloadLib(backend_lib_handle);\r\n      if (Status::OK() != result) {\r\n        ORT_THROW(\"Failed to unload backend library.\");\r\n      }\r\n    }\r\n  });\r\n\r\n  // Load the intended backend (e.g., HTP, CPU) to ensure it is valid and to get its type.\r\n  QnnInterface_t* backend_interface_provider{nullptr};\r\n  auto rt = GetQnnInterfaceProvider<QnnInterfaceGetProvidersFn_t,\r\n                                    QnnInterface_t>(backend_path_.c_str(),\r\n                                                    \"QnnInterface_getProviders\",\r\n                                                    &backend_lib_handle,\r\n                                                    {QNN_API_VERSION_MAJOR,\r\n                                                     QNN_API_VERSION_MINOR,\r\n                                                     QNN_API_VERSION_PATCH},\r\n                                                    &backend_interface_provider);\r\n  ORT_RETURN_IF_ERROR(rt);\r\n\r\n  // Set the \"intended\" backend type so that QNN builders still make the expected QNN API calls.\r\n  auto backend_id = backend_interface_provider->backendId;\r\n  SetQnnBackendType(backend_id);\r\n\r\n  // Load the serializer backend and set it as the activate backend.\r\n  QnnInterface_t* serializer_interface_provider{nullptr};\r\n  auto saver_rt = GetQnnInterfaceProvider<QnnInterfaceGetProvidersFn_t,\r\n                                          QnnInterface_t>(qnn_serializer_config_->GetBackendPath().c_str(),\r\n                                                          \"QnnInterface_getProviders\",\r\n                                                          &backend_lib_handle_,  // NOTE: QnnSaver/Ir library handle is set\r\n                                                          {QNN_API_VERSION_MAJOR,\r\n                                                           QNN_API_VERSION_MINOR,\r\n                                                           QNN_API_VERSION_PATCH},\r\n                                                          &serializer_interface_provider);\r\n  ORT_RETURN_IF_ERROR(saver_rt);\r\n  qnn_interface_ = serializer_interface_provider->QNN_INTERFACE_VER_NAME;  // NOTE: QnnSaver/Ir will provide the interfaces\r\n\r\n  Qnn_Version_t backend_interface_version = GetQnnInterfaceApiVersion(backend_interface_provider);\r\n  Qnn_Version_t serializer_interface_version = GetQnnInterfaceApiVersion(serializer_interface_provider);\r\n\r\n  LOGS_DEFAULT(INFO) << \"Using QnnSaver/Ir version: \" << serializer_interface_version.major << \".\"\r\n                     << serializer_interface_version.minor << \".\" << serializer_interface_version.patch\r\n                     << \" provider name : \" << serializer_interface_provider->providerName;\r\n\r\n  LOGS_DEFAULT(INFO) << \"Intended backend provider name: \" << backend_interface_provider->providerName\r\n                     << \" backend id: \" << backend_id\r\n                     << \" interface version: \" << backend_interface_version.major\r\n                     << \".\" << backend_interface_version.minor << \".\" << backend_interface_version.patch;\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::LoadQnnSystemLib() {\r\n  if (!system_lib_loaded_) {\r\n#ifdef _WIN32\r\n    std::string system_lib_file = \"QnnSystem.dll\";\r\n#else\r\n    std::string system_lib_file = \"libQnnSystem.so\";\r\n#endif  // #ifdef _WIN32\r\n    LOGS_DEFAULT(INFO) << \"Loading QnnSystem lib\";\r\n    std::filesystem::path lib_file_path(backend_path_.c_str());\r\n    std::string sys_file_path(lib_file_path.remove_filename().string() + system_lib_file);\r\n    QnnSystemInterface_t* system_interface_provider{nullptr};\r\n    auto rt = GetQnnInterfaceProvider<QnnSystemInterfaceGetProvidersFn_t,\r\n                                      QnnSystemInterface_t>(sys_file_path.c_str(),\r\n                                                            \"QnnSystemInterface_getProviders\",\r\n                                                            &system_lib_handle_,\r\n                                                            {QNN_SYSTEM_API_VERSION_MAJOR,\r\n                                                             QNN_SYSTEM_API_VERSION_MINOR,\r\n                                                             QNN_SYSTEM_API_VERSION_PATCH},\r\n                                                            &system_interface_provider);\r\n    ORT_RETURN_IF_ERROR(rt);\r\n    Qnn_Version_t system_interface_version = GetQnnInterfaceApiVersion(system_interface_provider);\r\n    qnn_sys_interface_ = system_interface_provider->QNN_SYSTEM_INTERFACE_VER_NAME;\r\n\r\n    LOGS_DEFAULT(INFO) << \"Found valid system interface, version: \" << system_interface_version.major\r\n                       << \".\" << system_interface_version.minor\r\n                       << \" backend provider name: \" << system_interface_provider->providerName;\r\n\r\n    system_lib_loaded_ = true;\r\n  }\r\n  return Status::OK();\r\n}\r\n\r\nvoid QnnLogging(const char* format,\r\n                QnnLog_Level_t level,\r\n                uint64_t timestamp,\r\n                va_list argument_parameter) {\r\n  ORT_UNUSED_PARAMETER(level);\r\n  ORT_UNUSED_PARAMETER(timestamp);\r\n\r\n  if (!::onnxruntime::logging::LoggingManager::HasDefaultLogger()) {\r\n    // QNN may call this logging callback at any point, which means that we need to explicitly check\r\n    // that the default logger has been initialized before trying to use it (otherwise get segfault).\r\n    return;\r\n  }\r\n\r\n  const auto& logger = ::onnxruntime::logging::LoggingManager::DefaultLogger();\r\n  const auto severity = ::onnxruntime::logging::Severity::kVERBOSE;\r\n  const auto data_type = ::onnxruntime::logging::DataType::SYSTEM;\r\n\r\n  if (logger.OutputIsEnabled(severity, data_type)) {\r\n    auto log_capture = Factory<logging::Capture>::Create(logger,\r\n                                                         severity,\r\n                                                         logging::Category::onnxruntime,\r\n                                                         data_type,\r\n                                                         ORT_WHERE);\r\n    log_capture->ProcessPrintf(format, argument_parameter);\r\n  }\r\n}\r\n\r\nStatus QnnBackendManager::InitializeQnnLog(const logging::Logger& logger) {\r\n  logger_ = &logger;\r\n\r\n  // Set Qnn log level align with Ort log level\r\n  auto ort_log_level = logger_->GetSeverity();\r\n  QnnLog_Level_t qnn_log_level = MapOrtSeverityToQNNLogLevel(ort_log_level);\r\n  LOGS(*logger_, VERBOSE) << \"Set Qnn log level: \" << qnn_log_level;\r\n\r\n  // NOTE: Even if logCreate() fails and QNN does not return a valid log_handle_, QNN may still\r\n  // call the QnnLogging() callback. So, we have to make sure that QnnLogging() can handle calls\r\n  // in which ORT logging is not available.\r\n  Qnn_ErrorHandle_t result = qnn_interface_.logCreate(QnnLogging, qnn_log_level, &log_handle_);\r\n\r\n  if (result != QNN_SUCCESS) {\r\n    switch (result) {\r\n      case QNN_COMMON_ERROR_NOT_SUPPORTED:\r\n        LOGS(*logger_, ERROR) << \"Logging not supported in the QNN backend.\";\r\n        break;\r\n      case QNN_LOG_ERROR_INVALID_ARGUMENT:\r\n        LOGS(*logger_, ERROR) << \"Invalid argument provided to QnnLog_create.\";\r\n        break;\r\n      case QNN_LOG_ERROR_MEM_ALLOC:\r\n        LOGS(*logger_, ERROR) << \"Memory allocation error during QNN logging initialization.\";\r\n        break;\r\n      case QNN_LOG_ERROR_INITIALIZATION:\r\n        LOGS(*logger_, ERROR) << \"Initialization of logging failed in the QNN backend.\";\r\n        break;\r\n      default:\r\n        LOGS(*logger_, WARNING) << \"Unknown error occurred while initializing logging in the QNN backend.\";\r\n        break;\r\n    }\r\n  }\r\n\r\n  ORT_RETURN_IF(QNN_BACKEND_NO_ERROR != result, \"Failed to initialize logging in the QNN backend. Error: \", QnnErrorHandleToString(result));\r\n  return Status::OK();\r\n}\r\n\r\nQnnLog_Level_t QnnBackendManager::MapOrtSeverityToQNNLogLevel(logging::Severity ort_log_level) {\r\n  // Map ORT log severity to Qnn log level\r\n  switch (ort_log_level) {\r\n    case logging::Severity::kVERBOSE: {\r\n      switch ((GetQnnBackendType())) {\r\n        case QnnBackendType::GPU:\r\n          // Currently GPU needs this log level to work.\r\n          // This switch will be removed once this is resolved.\r\n          return QNN_LOG_LEVEL_DEBUG;\r\n        default:\r\n          return QNN_LOG_LEVEL_VERBOSE;\r\n      }\r\n    }\r\n    case logging::Severity::kINFO:\r\n      return QNN_LOG_LEVEL_INFO;\r\n    case logging::Severity::kWARNING:\r\n      return QNN_LOG_LEVEL_WARN;\r\n    case logging::Severity::kERROR:\r\n    case logging::Severity::kFATAL:\r\n    default:\r\n      return QNN_LOG_LEVEL_ERROR;\r\n  }\r\n}\r\n\r\nStatus QnnBackendManager::ResetQnnLogLevel(std::optional<logging::Severity> ort_log_level) {\r\n  std::lock_guard<std::recursive_mutex> lock(logger_recursive_mutex_);\r\n  if (!backend_setup_completed_ || logger_ == nullptr) {\r\n    return Status::OK();\r\n  }\r\n  ORT_RETURN_IF(nullptr == log_handle_, \"Unable to update QNN Log Level. Invalid QNN log handle.\");\r\n\r\n  logging::Severity actual_log_level = ort_log_level.has_value() ? *ort_log_level : logger_->GetSeverity();\r\n  QnnLog_Level_t qnn_log_level = MapOrtSeverityToQNNLogLevel(actual_log_level);\r\n\r\n  LOGS(*logger_, INFO) << \"Updating Qnn log level to: \" << qnn_log_level;\r\n\r\n  // Use the QnnLog_setLogLevel API to set the new log level\r\n  Qnn_ErrorHandle_t result = qnn_interface_.logSetLogLevel(log_handle_, qnn_log_level);\r\n  if (QNN_SUCCESS != result) {\r\n    if (result == QNN_LOG_ERROR_INVALID_ARGUMENT) {\r\n      LOGS(*logger_, ERROR) << \"Invalid log level argument provided to QnnLog_setLogLevel.\";\r\n    } else if (result == QNN_LOG_ERROR_INVALID_HANDLE) {\r\n      LOGS(*logger_, ERROR) << \"Invalid log handle provided to QnnLog_setLogLevel.\";\r\n    }\r\n  }\r\n  ORT_RETURN_IF(QNN_BACKEND_NO_ERROR != result,\r\n                \"Failed to set log level in Qnn backend. Error: \", QnnErrorHandleToString(result));\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::InitializeBackend() {\r\n  if (true == backend_initialized_) {\r\n    LOGS_DEFAULT(INFO) << \"Backend initialized already.\";\r\n    return Status::OK();\r\n  }\r\n\r\n  Qnn_ErrorHandle_t result = qnn_interface_.backendCreate(log_handle_, (const QnnBackend_Config_t**)backend_config_, &backend_handle_);\r\n  ORT_RETURN_IF(QNN_BACKEND_NO_ERROR != result, \"Failed to initialize backend. Error: \", QnnErrorHandleToString(result));\r\n\r\n  backend_initialized_ = true;\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::ShutdownBackend() {\r\n  if (false == backend_initialized_) {\r\n    return Status::OK();\r\n  }\r\n\r\n  if (nullptr != qnn_interface_.backendFree) {\r\n    ORT_RETURN_IF(QNN_BACKEND_NO_ERROR != qnn_interface_.backendFree(backend_handle_),\r\n                  \"Failed to shutdown backend!\");\r\n  }\r\n\r\n  backend_initialized_ = false;\r\n\r\n  return Status::OK();\r\n}\r\n\r\nbool QnnBackendManager::IsDevicePropertySupported() {\r\n  if (nullptr != qnn_interface_.propertyHasCapability) {\r\n    auto rt = qnn_interface_.propertyHasCapability(QNN_PROPERTY_GROUP_DEVICE);\r\n    if (QNN_PROPERTY_NOT_SUPPORTED == rt || QNN_PROPERTY_ERROR_UNKNOWN_KEY == rt) {\r\n      LOGS_DEFAULT(INFO) << \"Device property not supported or unknown to backend.\";\r\n      return false;\r\n    }\r\n  }\r\n\r\n  return true;\r\n}\r\n\r\nStatus QnnBackendManager::CreateDevice() {\r\n  if (true == device_created_) {\r\n    LOGS_DEFAULT(INFO) << \"Device initialized already.\";\r\n    return Status::OK();\r\n  }\r\n\r\n  // Create device if its property supported\r\n  if (!IsDevicePropertySupported()) {\r\n    LOGS_DEFAULT(INFO) << \"Skip to create device.\";\r\n    return Status::OK();\r\n  }\r\n\r\n  qnn::QnnConfigsBuilder<QnnDevice_Config_t, QnnHtpDevice_CustomConfig_t> device_configs_builder(QNN_DEVICE_CONFIG_INIT,\r\n                                                                                                 {});\r\n  if (qnn_backend_type_ == QnnBackendType::HTP) {\r\n    // Set SoC Model. The *enum* Qnn_SocModel_t is deprecated and will not be updated in the future. Therefore,\r\n    // must use the latest SDK documentation to get the SoC model of the latest HW.\r\n    if (soc_model_ != QNN_SOC_MODEL_UNKNOWN) {\r\n      gsl::not_null<QnnHtpDevice_CustomConfig_t*> custom_config = device_configs_builder.PushCustomConfig();\r\n      custom_config->option = QNN_HTP_DEVICE_CONFIG_OPTION_SOC;\r\n      custom_config->socModel = soc_model_;\r\n\r\n      gsl::not_null<QnnDevice_Config_t*> device_config = device_configs_builder.PushConfig();\r\n      device_config->option = QNN_DEVICE_CONFIG_OPTION_CUSTOM;\r\n      device_config->customConfig = custom_config;\r\n    }\r\n\r\n    // Set the minimum HTP architecture. The driver will use ops that are compatible with this minimum architecture.\r\n    if (htp_arch_ != QNN_HTP_DEVICE_ARCH_NONE) {\r\n      gsl::not_null<QnnHtpDevice_CustomConfig_t*> custom_config = device_configs_builder.PushCustomConfig();\r\n      custom_config->option = QNN_HTP_DEVICE_CONFIG_OPTION_ARCH;\r\n      custom_config->arch.arch = htp_arch_;\r\n      custom_config->arch.deviceId = device_id_;\r\n\r\n      gsl::not_null<QnnDevice_Config_t*> device_config = device_configs_builder.PushConfig();\r\n      device_config->option = QNN_DEVICE_CONFIG_OPTION_CUSTOM;\r\n      device_config->customConfig = custom_config;\r\n    }\r\n  }\r\n\r\n  LOGS_DEFAULT(INFO) << \"Create device.\";\r\n  if (nullptr != qnn_interface_.deviceCreate) {\r\n    Qnn_ErrorHandle_t result = qnn_interface_.deviceCreate(log_handle_, device_configs_builder.GetQnnConfigs(), &device_handle_);\r\n    if (QNN_SUCCESS != result) {\r\n      return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, \"Failed to create device. Error: \", QnnErrorHandleToString(result));\r\n    }\r\n  }\r\n  device_created_ = true;\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::ReleaseDevice() {\r\n  if (false == device_created_) {\r\n    return Status::OK();\r\n  }\r\n\r\n  if (nullptr != qnn_interface_.deviceFree) {\r\n    Qnn_ErrorHandle_t result = qnn_interface_.deviceFree(device_handle_);\r\n    if (QNN_SUCCESS != result) {\r\n      return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, \"Failed to release device. Error: \", QnnErrorHandleToString(result));\r\n    }\r\n  }\r\n\r\n  device_created_ = false;\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::InitializeProfiling() {\r\n  profiling_level_merge_ = profiling_level_;\r\n  // use profiling level from ETW if ETW is enabled\r\n  if (profiling_level_etw_ != ProfilingLevel::INVALID) {\r\n    profiling_level_merge_ = profiling_level_etw_;\r\n  }\r\n\r\n  if (ProfilingLevel::OFF == profiling_level_merge_ || ProfilingLevel::INVALID == profiling_level_merge_) {\r\n    LOGS_DEFAULT(INFO) << \"Profiling turned off.\";\r\n    return Status::OK();\r\n  }\r\n\r\n  bool enable_optrace = false;\r\n  QnnProfile_Level_t qnn_profile_level = QNN_PROFILE_LEVEL_BASIC;\r\n  if (ProfilingLevel::BASIC == profiling_level_merge_) {\r\n    qnn_profile_level = QNN_PROFILE_LEVEL_BASIC;\r\n    LOGS_DEFAULT(VERBOSE) << \"Profiling level set to basic.\";\r\n  } else if (ProfilingLevel::DETAILED == profiling_level_merge_) {\r\n    qnn_profile_level = QNN_PROFILE_LEVEL_DETAILED;\r\n    LOGS_DEFAULT(VERBOSE) << \"Profiling level set to detailed.\";\r\n  } else if (ProfilingLevel::OPTRACE == profiling_level_merge_) {\r\n    qnn_profile_level = QNN_PROFILE_LEVEL_DETAILED;\r\n    enable_optrace = true;\r\n    LOGS_DEFAULT(VERBOSE) << \"Profiling level set to optrace.\";\r\n  }\r\n\r\n  Qnn_ErrorHandle_t result = qnn_interface_.profileCreate(backend_handle_, qnn_profile_level, &profile_backend_handle_);\r\n  ORT_RETURN_IF(QNN_PROFILE_NO_ERROR != result, \"Failed to create QNN profile! Error: \", QnnErrorHandleToString(result));\r\n\r\n#ifdef QNN_SYSTEM_PROFILE_API_ENABLED\r\n  profiling_enabled_ = true;\r\n  ORT_RETURN_IF_ERROR(LoadQnnSystemLib());\r\n\r\n  if (enable_optrace) {\r\n    QnnProfile_Config_t optrace_config = QNN_PROFILE_CONFIG_INIT;\r\n    optrace_config.option = QNN_PROFILE_CONFIG_OPTION_ENABLE_OPTRACE;\r\n    optrace_config.enableOptrace = enable_optrace;\r\n\r\n    const QnnProfile_Config_t* profile_configs[] = {&optrace_config, nullptr};\r\n    result = qnn_interface_.profileSetConfig(profile_backend_handle_, profile_configs);\r\n\r\n    ORT_RETURN_IF(QNN_PROFILE_NO_ERROR != result, \"Failed to enable op trace! Error: \", QnnErrorHandleToString(result));\r\n  }\r\n#else\r\n  if (enable_optrace) {\r\n    LOGS_DEFAULT(WARNING) << \"Profiling level set to optrace, but QNN SDK Version is older than 2.29.0. \"\r\n                          << \"Profiling level will be set to detailed instead.\";\r\n  }\r\n#endif\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::ReleaseProfilehandle() {\r\n  // Free Profiling object if it was created\r\n  if (nullptr != profile_backend_handle_) {\r\n    ORT_RETURN_IF(QNN_PROFILE_NO_ERROR != qnn_interface_.profileFree(profile_backend_handle_),\r\n                  \"Could not free backend profile handle!\");\r\n  }\r\n  profile_backend_handle_ = nullptr;\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::SetProfilingLevelETW(ProfilingLevel profiling_level_etw_param) {\r\n  if (profiling_level_etw_ != profiling_level_etw_param) {\r\n    profiling_level_etw_ = profiling_level_etw_param;\r\n\r\n    auto result = ReleaseProfilehandle();\r\n    if (Status::OK() != result) {\r\n      ORT_THROW(\"Failed to ReleaseProfilehandle for previous QNN profiling\");\r\n    }\r\n\r\n    result = InitializeProfiling();\r\n    if (Status::OK() != result) {\r\n      ORT_THROW(\"Failed to Re-InitializeProfiling for QNN ETW profiling\");\r\n    }\r\n  }\r\n  return Status::OK();\r\n}\r\n\r\nStatus SetQnnContextConfig(ContextPriority context_priority, QnnContext_Config_t& qnn_context_config) {\r\n  qnn_context_config.option = QNN_CONTEXT_CONFIG_OPTION_PRIORITY;\r\n  switch (context_priority) {\r\n    case ContextPriority::LOW: {\r\n      qnn_context_config.priority = QNN_PRIORITY_LOW;\r\n      break;\r\n    }\r\n    case ContextPriority::NORMAL: {\r\n      qnn_context_config.priority = QNN_PRIORITY_NORMAL;\r\n      break;\r\n    }\r\n    case ContextPriority::NORMAL_HIGH: {\r\n      qnn_context_config.priority = QNN_PRIORITY_NORMAL_HIGH;\r\n      break;\r\n    }\r\n    case ContextPriority::HIGH: {\r\n      qnn_context_config.priority = QNN_PRIORITY_HIGH;\r\n      break;\r\n    }\r\n    case ContextPriority::UNDEFINED: {\r\n      return ORT_MAKE_STATUS(ONNXRUNTIME, INVALID_ARGUMENT, \"Invalid Qnn context priority.\");\r\n    }\r\n    default:\r\n      qnn_context_config.priority = QNN_PRIORITY_NORMAL;\r\n  }  // switch\r\n\r\n  return Status::OK();\r\n}\r\n\r\n// callback required to add context handles to class list\r\n// when using contextCreateFromBinaryListAsync()\r\nvoid ContextCreateAsyncCallback(Qnn_ContextHandle_t context,\r\n                                Qnn_GraphHandle_t graph,\r\n                                const char* graphName,\r\n                                QnnContext_createFromBinaryAsyncNotifyType_t notifyType,\r\n                                void* notifyParam,\r\n                                Qnn_ErrorHandle_t status) {\r\n  auto qnn_backend_manager = SharedContext::GetInstance().GetSharedQnnBackendManager();\r\n\r\n  if (context) {\r\n    qnn_backend_manager->ProcessContextFromBinListAsync(context, notifyParam);\r\n  }\r\n\r\n  if (nullptr == graphName || graph || notifyType || status) {\r\n    // Avoid compilation unused var warning error\r\n  }\r\n}\r\n\r\nvoid QnnBackendManager::ProcessContextFromBinListAsync(Qnn_ContextHandle_t context, void* notifyParam) {\r\n  std::lock_guard<std::mutex> guard(ep_context_handle_map_mutex_);\r\n  if (!notifyParam) {\r\n    LOGS(*logger_, WARNING) << \"No known node names associated with context handle: \" << context;\r\n    return;\r\n  }\r\n\r\n  std::vector<std::string>* ep_node_names = reinterpret_cast<std::vector<std::string>*>(notifyParam);\r\n  for (const auto& node_name : *ep_node_names) {\r\n    if (!(ep_context_handle_map_.emplace(node_name, context).second)) {\r\n      LOGS(*logger_, VERBOSE) << \"Unable to map \" << context << \" to \" << node_name;\r\n    }\r\n  }\r\n\r\n  auto s = AddQnnContextHandle(context);\r\n  if (s != Status::OK()) {\r\n    LOGS(*logger_, WARNING) << \"Unable to add context \" << context;\r\n  }\r\n}\r\n\r\nStatus QnnBackendManager::CreateContextVtcmBackupBufferSharingEnabled(std::unordered_map<std::string, std::unique_ptr<std::vector<std::string>>>& context_bin_map) {\r\n#if QNN_API_VERSION_MAJOR == 2 && (QNN_API_VERSION_MINOR >= 26)\r\n  QnnContext_Config_t context_config_resource_sharing = QNN_CONTEXT_CONFIG_INIT;\r\n  QnnHtpContext_CustomConfig_t resource_sharing_custom_config;\r\n  resource_sharing_custom_config.option = QNN_HTP_CONTEXT_CONFIG_OPTION_SHARE_RESOURCES;\r\n  resource_sharing_custom_config.shareResources = true;\r\n  context_config_resource_sharing.option = QNN_CONTEXT_CONFIG_OPTION_CUSTOM;\r\n  context_config_resource_sharing.customConfig = &resource_sharing_custom_config;\r\n\r\n  QnnHtpContext_CustomConfig_t context_config_resource_sharing_opt_type;\r\n  context_config_resource_sharing_opt_type.option = QNN_HTP_CONTEXT_CONFIG_OPTION_SHARE_RESOURCES_OPTIMIZATION_TYPE;\r\n  context_config_resource_sharing_opt_type.shareResOptType = SEQUENTIAL_WITHOUT_VA_OPTIMIZATION;\r\n  QnnContext_Config_t resource_sharing_opt_type_config;\r\n  resource_sharing_opt_type_config.option = QNN_CONTEXT_CONFIG_OPTION_CUSTOM;\r\n  resource_sharing_opt_type_config.customConfig = &context_config_resource_sharing_opt_type;\r\n\r\n  QnnContext_Config_t context_config_weight_sharing = QNN_CONTEXT_CONFIG_INIT;\r\n  QnnHtpContext_CustomConfig_t custom_config;\r\n  custom_config.option = QNN_HTP_CONTEXT_CONFIG_OPTION_WEIGHT_SHARING_ENABLED;\r\n  custom_config.weightSharingEnabled = true;\r\n  context_config_weight_sharing.option = QNN_CONTEXT_CONFIG_OPTION_CUSTOM;\r\n  context_config_weight_sharing.customConfig = &custom_config;\r\n#else\r\n  LOGS(*logger_, WARNING) << \"Called CreateContextVtcmBackupBufferSharingEnabled() but QNN API version is older than 2.26!\";\r\n#endif\r\n  QnnContext_Config_t context_priority_config = QNN_CONTEXT_CONFIG_INIT;\r\n  ORT_RETURN_IF_ERROR(SetQnnContextConfig(context_priority_, context_priority_config));\r\n\r\n  const QnnContext_Config_t* configs[] = {&context_priority_config,\r\n#if QNN_API_VERSION_MAJOR == 2 && (QNN_API_VERSION_MINOR >= 26)\r\n                                          &context_config_resource_sharing,\r\n                                          &resource_sharing_opt_type_config,\r\n                                          &context_config_weight_sharing,\r\n#endif\r\n                                          nullptr};\r\n\r\n  std::vector<QnnContext_Params_t> context_params_list;\r\n  std::vector<QnnContext_ParamsV1_t> context_paramsv1_list;\r\n  std::vector<const QnnContext_Params_t*> context_params_ptr_list;\r\n  std::vector<std::unique_ptr<char[]>> buffer_list;\r\n\r\n  context_params_list.reserve(context_bin_map.size());\r\n  context_params_ptr_list.reserve(context_bin_map.size() + 1);\r\n\r\n  for (auto& it : context_bin_map) {\r\n    auto context_bin_filepath = it.first;\r\n\r\n    std::ifstream cache_file(context_bin_filepath.c_str(), std::ifstream::binary);\r\n    ORT_RETURN_IF(!cache_file || !cache_file.good(), \"Failed to retrieve context binary from: \", context_bin_filepath);\r\n\r\n    cache_file.seekg(0, cache_file.end);\r\n    size_t buffer_size = static_cast<size_t>(cache_file.tellg());\r\n    ORT_RETURN_IF(0 == buffer_size, \"Empty cache file encountered.\");\r\n\r\n    cache_file.seekg(0, cache_file.beg);\r\n    std::unique_ptr<char[]> buffer = std::make_unique<char[]>(buffer_size);\r\n    ORT_RETURN_IF(nullptr == buffer, \"Failed to allocate memory for cache file.\");\r\n    const auto& read_result = cache_file.read(buffer.get(), buffer_size);\r\n    ORT_RETURN_IF(!read_result, \"Failed to read contents from cached context file.\");\r\n\r\n    cache_file.close();\r\n    QnnContext_ParamsV1_t context_params_v1 = {nullptr,\r\n                                               buffer.get(),\r\n                                               buffer_size,\r\n                                               nullptr,\r\n                                               ContextCreateAsyncCallback,\r\n                                               it.second.get()};\r\n\r\n    QnnContext_Params_t context_params = {QnnContext_ParamsVersion_t::QNN_CONTEXT_PARAMS_VERSION_1,\r\n                                          {context_params_v1}};\r\n\r\n    buffer_list.push_back(std::move(buffer));\r\n    context_params_list.push_back(std::move(context_params));\r\n    context_paramsv1_list.push_back(std::move(context_params_v1));\r\n    context_params_ptr_list.push_back(&context_params_list.back());\r\n  }\r\n  context_params_ptr_list.push_back(nullptr);\r\n  auto result = qnn_interface_.contextCreateFromBinaryListAsync(backend_handle_,\r\n                                                                device_handle_,\r\n                                                                context_params_ptr_list.data(),\r\n                                                                configs,\r\n                                                                nullptr);\r\n\r\n  context_params_ptr_list.clear();\r\n  context_paramsv1_list.clear();\r\n  context_params_list.clear();\r\n  buffer_list.clear();\r\n\r\n  ORT_RETURN_IF(QNN_CONTEXT_NO_ERROR != result, \"Failed to create context. Error: \", QnnErrorHandleToString(result), \", Code:\", result);\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::SetContextPriority(ContextPriority context_priority) {\r\n  QnnContext_Config_t context_priority_config = QNN_CONTEXT_CONFIG_INIT;\r\n  ORT_RETURN_IF_ERROR(SetQnnContextConfig(context_priority, context_priority_config));\r\n\r\n  QnnContext_Config_t* configs[] = {&context_priority_config, nullptr};\r\n  for (const auto& context_handle : contexts_) {\r\n    auto result = qnn_interface_.contextSetConfig(context_handle, (const QnnContext_Config_t**)configs);\r\n    ORT_RETURN_IF(QNN_CONTEXT_NO_ERROR != result, \"Failed to set context priority for context handle: \", context_handle);\r\n  }\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::ResetContextPriority() {\r\n  return SetContextPriority(context_priority_);\r\n}\r\n\r\nStatus QnnBackendManager::CreateContext(bool enable_htp_weight_sharing) {\r\n  if (true == context_created_) {\r\n    LOGS_DEFAULT(INFO) << \"Context created already.\";\r\n    return Status::OK();\r\n  }\r\n\r\n  QnnContext_Config_t context_config_weight_sharing = QNN_CONTEXT_CONFIG_INIT;\r\n  QnnHtpContext_CustomConfig_t custom_config;\r\n  custom_config.option = QNN_HTP_CONTEXT_CONFIG_OPTION_WEIGHT_SHARING_ENABLED;\r\n  custom_config.weightSharingEnabled = enable_htp_weight_sharing;\r\n  context_config_weight_sharing.option = QNN_CONTEXT_CONFIG_OPTION_CUSTOM;\r\n  context_config_weight_sharing.customConfig = &custom_config;\r\n\r\n  QnnContext_Config_t context_priority_config = QNN_CONTEXT_CONFIG_INIT;\r\n  ORT_RETURN_IF_ERROR(SetQnnContextConfig(context_priority_, context_priority_config));\r\n\r\n  const QnnContext_Config_t* npu_context_configs[] = {&context_priority_config,\r\n                                                      &context_config_weight_sharing,\r\n                                                      nullptr};\r\n\r\n  const QnnContext_Config_t* empty_context_configs[] = {nullptr};\r\n\r\n  const QnnContext_Config_t** configs = nullptr;\r\n  switch (GetQnnBackendType()) {\r\n    case QnnBackendType::HTP:\r\n    case QnnBackendType::DSP:\r\n      configs = npu_context_configs;\r\n      break;\r\n    case QnnBackendType::GPU:\r\n    case QnnBackendType::SERIALIZER:\r\n      configs = nullptr;\r\n      break;\r\n    default:\r\n      configs = empty_context_configs;\r\n      break;\r\n  }\r\n\r\n  // Not all serialization backends allow for hardware configs to be applied.\r\n  if (qnn_serializer_config_ && !qnn_serializer_config_->SupportsArbitraryGraphConfigs()) {\r\n    configs = nullptr;\r\n  }\r\n\r\n  Qnn_ContextHandle_t context = nullptr;\r\n  Qnn_ErrorHandle_t result = 0;\r\n\r\n  result = qnn_interface_.contextCreate(backend_handle_,\r\n                                        device_handle_,\r\n                                        configs,\r\n                                        &context);\r\n\r\n  ORT_RETURN_IF(QNN_CONTEXT_NO_ERROR != result, \"Failed to create context. Error: \", QnnErrorHandleToString(result), \", Code:\", result);\r\n\r\n  ORT_RETURN_IF_ERROR(AddQnnContextHandle(context));\r\n\r\n  context_created_ = true;\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::ReleaseContext() {\r\n  if (false == context_created_) {\r\n    return Status::OK();\r\n  }\r\n\r\n  // release QNN context handles\r\n  contexts_.clear();\r\n  context_map_.clear();\r\n\r\n  context_created_ = false;\r\n  return Status::OK();\r\n}\r\n\r\nstd::unique_ptr<unsigned char[]> QnnBackendManager::GetContextBinaryBuffer(uint64_t& written_buffer_size) {\r\n  if (nullptr == qnn_interface_.contextGetBinarySize ||\r\n      nullptr == qnn_interface_.contextGetBinary) {\r\n    LOGS(*logger_, ERROR) << \"Failed to get valid function pointer.\";\r\n    return nullptr;\r\n  }\r\n  ORT_ENFORCE(contexts_.size() > 0, \"No valid QNN context!\");\r\n  uint64_t required_buffer_size(0);\r\n  // Generate all graphs in one single context\r\n  Qnn_ErrorHandle_t rt = qnn_interface_.contextGetBinarySize(contexts_[0], &required_buffer_size);\r\n  if (QNN_CONTEXT_NO_ERROR != rt) {\r\n    LOGS(*logger_, ERROR) << \"Failed to get QNN context binary size. Error: \" << QnnErrorHandleToString(rt);\r\n    return nullptr;\r\n  }\r\n\r\n  std::unique_ptr<unsigned char[]> context_buffer = std::make_unique<unsigned char[]>(required_buffer_size);\r\n  if (nullptr == context_buffer) {\r\n    LOGS(*logger_, ERROR) << \"Failed to allocate buffer for context cache.\";\r\n    return nullptr;\r\n  }\r\n\r\n  rt = qnn_interface_.contextGetBinary(contexts_[0],\r\n                                       reinterpret_cast<void*>(context_buffer.get()),\r\n                                       required_buffer_size,\r\n                                       &written_buffer_size);\r\n  if (QNN_CONTEXT_NO_ERROR != rt) {\r\n    LOGS(*logger_, ERROR) << \"Failed to get context binary. Error: \" << QnnErrorHandleToString(rt);\r\n    return nullptr;\r\n  }\r\n\r\n  if (required_buffer_size < written_buffer_size) {\r\n    LOGS(*logger_, ERROR) << \"Context written buffer size: \" << written_buffer_size\r\n                          << \" exceeds allocated buffer size: \" << required_buffer_size;\r\n    return nullptr;\r\n  }\r\n\r\n  LOGS(*logger_, VERBOSE) << \"Get context binary buffer succeed.\";\r\n  return context_buffer;\r\n}\r\n\r\nStatus QnnBackendManager::GetMaxSpillFillBufferSize(unsigned char* buffer,\r\n                                                    uint64_t buffer_length,\r\n                                                    uint64_t& max_spill_fill_buffer_size) {\r\n  max_spill_fill_buffer_size = 0;\r\n  // spill fill starts from 2.28\r\n#if QNN_API_VERSION_MAJOR == 2 && (QNN_API_VERSION_MINOR >= 21)\r\n  bool result = nullptr == qnn_sys_interface_.systemContextCreate ||\r\n                nullptr == qnn_sys_interface_.systemContextGetBinaryInfo ||\r\n                nullptr == qnn_sys_interface_.systemContextFree;\r\n  ORT_RETURN_IF(result, \"Failed to get valid function pointer.\");\r\n\r\n  QnnSystemContext_Handle_t sys_ctx_handle = nullptr;\r\n  auto rt = qnn_sys_interface_.systemContextCreate(&sys_ctx_handle);\r\n  ORT_RETURN_IF(QNN_SUCCESS != rt, \"Failed to create system handle.\");\r\n\r\n  const QnnSystemContext_BinaryInfo_t* binary_info = nullptr;\r\n  Qnn_ContextBinarySize_t binary_info_size{0};\r\n  rt = qnn_sys_interface_.systemContextGetBinaryInfo(sys_ctx_handle,\r\n                                                     static_cast<void*>(buffer),\r\n                                                     buffer_length,\r\n                                                     &binary_info,\r\n                                                     &binary_info_size);\r\n  ORT_RETURN_IF(QNN_SUCCESS != rt, \"Failed to get context binary info.\");\r\n\r\n  // binary_info life cycle is here\r\n  // Binary info to graph info\r\n  // retrieve Qnn graph info from binary info\r\n  ORT_RETURN_IF(nullptr == binary_info, \"Qnn cached binary info is nullptr.\");\r\n  uint32_t graph_count = 0;\r\n  QnnSystemContext_GraphInfo_t* graphs_info = nullptr;\r\n  if (binary_info->version == QNN_SYSTEM_CONTEXT_BINARY_INFO_VERSION_3) {\r\n    graph_count = binary_info->contextBinaryInfoV3.numGraphs;\r\n    graphs_info = binary_info->contextBinaryInfoV3.graphs;\r\n  } else if (binary_info->version == QNN_SYSTEM_CONTEXT_BINARY_INFO_VERSION_2) {\r\n    graph_count = binary_info->contextBinaryInfoV2.numGraphs;\r\n    graphs_info = binary_info->contextBinaryInfoV2.graphs;\r\n  } else if (binary_info->version == QNN_SYSTEM_CONTEXT_BINARY_INFO_VERSION_1) {\r\n    graph_count = binary_info->contextBinaryInfoV1.numGraphs;\r\n    graphs_info = binary_info->contextBinaryInfoV1.graphs;\r\n  } else {\r\n    return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, \"Unsupported context binary info version.\");\r\n  }\r\n\r\n  for (uint32_t i = 0; i < graph_count; ++i) {\r\n    if (graphs_info[i].version == QNN_SYSTEM_CONTEXT_GRAPH_INFO_VERSION_3) {\r\n      auto htp_graph_info = reinterpret_cast<QnnHtpSystemContext_GraphBlobInfo_t*>(graphs_info[i].graphInfoV3.graphBlobInfo);\r\n      if (htp_graph_info->version == QNN_SYSTEM_CONTEXT_HTP_GRAPH_INFO_BLOB_VERSION_V1) {\r\n        auto spill_fill_buffer_size = htp_graph_info->contextBinaryGraphBlobInfoV1.spillFillBufferSize;\r\n        max_spill_fill_buffer_size = spill_fill_buffer_size > max_spill_fill_buffer_size ? spill_fill_buffer_size : max_spill_fill_buffer_size;\r\n      } else {\r\n        LOGS(*logger_, VERBOSE) << \"Unknown context binary graph info blob version.\";\r\n      }\r\n    } else if (graphs_info[i].version == QNN_SYSTEM_CONTEXT_GRAPH_INFO_VERSION_2 ||\r\n               graphs_info[i].version == QNN_SYSTEM_CONTEXT_GRAPH_INFO_VERSION_1) {\r\n      LOGS(*logger_, VERBOSE) << \"Skip retrieve spill file buffer size, it is not supported with graph info v1 & v2.\";\r\n    } else {\r\n      LOGS(*logger_, VERBOSE) << \"Unknown context binary graph info version.\";\r\n    }\r\n  }\r\n#else\r\n  ORT_UNUSED_PARAMETER(buffer);\r\n  ORT_UNUSED_PARAMETER(buffer_length);\r\n#endif\r\n\r\n  LOGS(*logger_, VERBOSE) << \"Get max spill fill buffer size completed.\";\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::LoadCachedQnnContextFromBuffer(char* buffer, uint64_t buffer_length,\r\n                                                         std::string node_name,\r\n                                                         QnnModelLookupTable& qnn_models,\r\n                                                         int64_t max_spill_fill_size) {\r\n  bool result = nullptr == qnn_sys_interface_.systemContextCreate ||\r\n                nullptr == qnn_sys_interface_.systemContextGetBinaryInfo ||\r\n                nullptr == qnn_sys_interface_.systemContextFree;\r\n  ORT_RETURN_IF(result, \"Failed to get valid function pointer.\");\r\n\r\n  QnnSystemContext_Handle_t sys_ctx_handle = nullptr;\r\n  auto rt = qnn_sys_interface_.systemContextCreate(&sys_ctx_handle);\r\n  ORT_RETURN_IF(QNN_SUCCESS != rt, \"Failed to create system handle.\");\r\n\r\n  const QnnSystemContext_BinaryInfo_t* binary_info = nullptr;\r\n  Qnn_ContextBinarySize_t binary_info_size{0};\r\n  rt = qnn_sys_interface_.systemContextGetBinaryInfo(sys_ctx_handle,\r\n                                                     static_cast<void*>(buffer),\r\n                                                     buffer_length,\r\n                                                     &binary_info,\r\n                                                     &binary_info_size);\r\n  ORT_RETURN_IF(QNN_SUCCESS != rt, \"Failed to get context binary info.\");\r\n\r\n  // binary_info life cycle is here\r\n  // Binary info to graph info\r\n  // retrieve Qnn graph info from binary info\r\n  ORT_RETURN_IF(nullptr == binary_info, \"Qnn cached binary info is nullptr.\");\r\n  uint32_t graph_count = 0;\r\n  QnnSystemContext_GraphInfo_t* graphs_info = nullptr;\r\n  if (binary_info->version == QNN_SYSTEM_CONTEXT_BINARY_INFO_VERSION_1) {\r\n    graph_count = binary_info->contextBinaryInfoV1.numGraphs;\r\n    graphs_info = binary_info->contextBinaryInfoV1.graphs;\r\n  }\r\n#if QNN_API_VERSION_MAJOR == 2 && (QNN_API_VERSION_MINOR >= 15)  // starts from 2.22\r\n  else if (binary_info->version == QNN_SYSTEM_CONTEXT_BINARY_INFO_VERSION_2) {\r\n    graph_count = binary_info->contextBinaryInfoV2.numGraphs;\r\n    graphs_info = binary_info->contextBinaryInfoV2.graphs;\r\n  }\r\n#endif\r\n#if QNN_API_VERSION_MAJOR == 2 && (QNN_API_VERSION_MINOR >= 21)  // starts from 2.28\r\n  else if (binary_info->version == QNN_SYSTEM_CONTEXT_BINARY_INFO_VERSION_3) {\r\n    graph_count = binary_info->contextBinaryInfoV3.numGraphs;\r\n    graphs_info = binary_info->contextBinaryInfoV3.graphs;\r\n  }\r\n#endif\r\n  else {\r\n    return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, \"Unsupported context binary info version.\");\r\n  }\r\n\r\n  ORT_RETURN_IF(graph_count < 1 || graphs_info == nullptr, \"Failed to get graph info from Qnn cached context.\");\r\n  LOGS(*logger_, VERBOSE) << \"Graph count from QNN context: \" << graph_count;\r\n\r\n  Qnn_ContextHandle_t context = nullptr;\r\n#if QNN_API_VERSION_MAJOR == 2 && (QNN_API_VERSION_MINOR >= 26)\r\n  if (vtcm_backup_buffer_sharing_enabled_) {\r\n    if (ep_context_handle_map_.find(node_name) != ep_context_handle_map_.end()) {\r\n      context = ep_context_handle_map_.at(node_name);\r\n    }\r\n    ORT_RETURN_IF(nullptr == context, \"Failed to retrieve context for \", node_name);\r\n\r\n  } else {\r\n#endif\r\n    QnnContext_Config_t qnn_context_config = QNN_CONTEXT_CONFIG_INIT;\r\n    ORT_RETURN_IF_ERROR(SetQnnContextConfig(context_priority_, qnn_context_config));\r\n\r\n    // Register spill fill buffer for multi context\r\n    QnnContext_Config_t spill_fill_config = QNN_CONTEXT_CONFIG_INIT;\r\n\r\n    // The spill fill buffer is available since 2.28, API version starts from 2.21\r\n#if QNN_API_VERSION_MAJOR == 2 && (QNN_API_VERSION_MINOR >= 21)\r\n    QnnHtpContext_CustomConfig_t custom_config;\r\n    custom_config.option = QNN_HTP_CONTEXT_CONFIG_OPTION_REGISTER_MULTI_CONTEXTS;\r\n    QnnHtpContext_GroupRegistration_t group_info;\r\n    size_t current_contexts_size = GetQnnContextSize();\r\n    // set to 0x0 (new group) if this is the first context, otherwise point to the first context handle\r\n    // note that we already move the context with max spill fill size to the beginning of the list\r\n    group_info.firstGroupHandle = (max_spill_fill_size > 0 && current_contexts_size > 0) ? GetQnnContext(0) : 0x0;\r\n    group_info.maxSpillFillBuffer = max_spill_fill_size;  // Max spill-fill buffer across contexts. Must be >0\r\n    custom_config.groupRegistration = group_info;\r\n    spill_fill_config.option = QNN_CONTEXT_CONFIG_OPTION_CUSTOM;\r\n    spill_fill_config.customConfig = &custom_config;\r\n\r\n#endif\r\n\r\n    QnnContext_Config_t* spill_fill_config_pointer = max_spill_fill_size > 0 ? &spill_fill_config : nullptr;\r\n    LOGS(*logger_, VERBOSE) << \"Max spill fill buffer size:\" << max_spill_fill_size;\r\n\r\n    const QnnContext_Config_t* context_configs[] = {&qnn_context_config, spill_fill_config_pointer, nullptr};\r\n\r\n    ORT_RETURN_IF(nullptr == qnn_interface_.contextCreateFromBinary,\r\n                  \"Invalid function pointer for contextCreateFromBinary.\");\r\n\r\n    qnn::profile::ProfilingInfo profiling_info;\r\n#ifdef QNN_SYSTEM_PROFILE_API_ENABLED\r\n    if (ProfilingEnabled()) {\r\n      profiling_info.start_time = qnn::utils::GetTimeStampInUs();\r\n    }\r\n#endif\r\n\r\n    rt = qnn_interface_.contextCreateFromBinary(backend_handle_,\r\n                                                device_handle_,\r\n                                                context_configs,\r\n                                                static_cast<void*>(buffer),\r\n                                                buffer_length,\r\n                                                &context,\r\n                                                profile_backend_handle_);\r\n\r\n#ifdef QNN_SYSTEM_PROFILE_API_ENABLED\r\n    if (ProfilingEnabled()) {\r\n      profiling_info.stop_time = qnn::utils::GetTimeStampInUs();\r\n      profiling_info.method_type = ProfilingMethodType::CREATE_FROM_BINARY;\r\n      profiling_info.graph_name = node_name;\r\n    }\r\n#endif\r\n\r\n    ORT_RETURN_IF(QNN_SUCCESS != rt, \"Failed to create context from binary. Error code: \", rt);\r\n    ORT_RETURN_IF_ERROR(AddQnnContextHandle(context));\r\n\r\n    ORT_RETURN_IF_ERROR(ExtractBackendProfilingInfo(profiling_info));\r\n\r\n#if QNN_API_VERSION_MAJOR == 2 && (QNN_API_VERSION_MINOR >= 26)\r\n  }\r\n#endif\r\n\r\n  if (1 == graph_count) {\r\n    // in case the EPContext node is generated from script\r\n    // the graph name from the context binary may not match the EPContext node name\r\n    auto qnn_model = std::make_unique<qnn::QnnModel>(this);\r\n    ORT_RETURN_IF_ERROR(qnn_model->DeserializeGraphInfoFromBinaryInfo(graphs_info[0], context));\r\n    qnn_models.emplace(node_name, std::move(qnn_model));\r\n  } else {\r\n    for (uint32_t i = 0; i < graph_count; ++i) {\r\n      auto qnn_model = std::make_unique<qnn::QnnModel>(this);\r\n      ORT_RETURN_IF_ERROR(qnn_model->DeserializeGraphInfoFromBinaryInfo(graphs_info[i], context));\r\n      qnn_models.emplace(graphs_info[i].graphInfoV1.graphName, std::move(qnn_model));\r\n    }\r\n  }\r\n\r\n  qnn_sys_interface_.systemContextFree(sys_ctx_handle);\r\n  sys_ctx_handle = nullptr;\r\n  context_created_ = true;\r\n\r\n  LOGS(*logger_, VERBOSE) << \"Load from cached QNN Context completed.\";\r\n  return Status::OK();\r\n}\r\n\r\n// need to load system lib if load from Qnn context binary\r\n// or generate Qnn context binary is enabled -- to get the max spill fill buffer size\r\nStatus QnnBackendManager::SetupBackend(const logging::Logger& logger,\r\n                                       bool load_from_cached_context,\r\n                                       bool need_load_system_lib,\r\n                                       bool share_ep_contexts,\r\n                                       bool enable_vtcm_backup_buffer_sharing,\r\n                                       std::unordered_map<std::string, std::unique_ptr<std::vector<std::string>>>& context_bin_map) {\r\n  std::lock_guard<std::recursive_mutex> lock(logger_recursive_mutex_);\r\n  if (backend_setup_completed_) {\r\n    LOGS(logger, VERBOSE) << \"Backend setup already!\";\r\n\r\n#if QNN_API_VERSION_MAJOR == 2 && (QNN_API_VERSION_MINOR >= 26)\r\n    if (vtcm_backup_buffer_sharing_enabled_) {\r\n      // If a context bin filepath has not been processed yet,\r\n      // then a new context must be created for the set of context bins\r\n      auto first_mapping_it = ep_context_handle_map_.find(context_bin_map.begin()->first);\r\n      if (first_mapping_it == ep_context_handle_map_.end()) {\r\n        LOGS(logger, VERBOSE) << \"Creating context for new set of context binaries\";\r\n        return CreateContextVtcmBackupBufferSharingEnabled(context_bin_map);\r\n      }\r\n\r\n      LOGS(logger, VERBOSE) << \"Mapping contexts to new EP main context nodes\";\r\n\r\n      for (auto& it : context_bin_map) {\r\n        auto context_bin_filepath = it.first;\r\n        auto ep_node_names = *(it.second);\r\n\r\n        auto context = ep_context_handle_map_.at(context_bin_filepath);\r\n        for (auto node_name : ep_node_names) {\r\n          ep_context_handle_map_.emplace(node_name, context);\r\n        }\r\n      }\r\n    }\r\n#endif\r\n    return Status::OK();\r\n  }\r\n\r\n  vtcm_backup_buffer_sharing_enabled_ = enable_vtcm_backup_buffer_sharing;\r\n\r\n  Status status = Status::OK();\r\n  if (!qnn_serializer_config_) {\r\n    status = LoadBackend();\r\n  } else {\r\n    status = LoadQnnSerializerBackend();\r\n  }\r\n  if (status.IsOK()) {\r\n    LOGS(logger, VERBOSE) << \"LoadBackend succeed.\";\r\n  }\r\n\r\n  if (status.IsOK() && (load_from_cached_context || need_load_system_lib)) {\r\n    status = LoadQnnSystemLib();\r\n  }\r\n\r\n  if (status.IsOK()) {\r\n    sdk_build_version_ = GetBackendBuildId();\r\n    LOGS(logger, VERBOSE) << \"Backend build version: \"\r\n                          << sdk_build_version_;\r\n  }\r\n\r\n  if (status.IsOK()) {\r\n    status = InitializeQnnLog(logger);\r\n  }\r\n  if (status.IsOK()) {\r\n    LOGS(logger, VERBOSE) << \"SetLogger succeed.\";\r\n  }\r\n\r\n  if (status.IsOK()) {\r\n    status = InitializeBackend();\r\n  }\r\n  if (status.IsOK()) {\r\n    LOGS(logger, VERBOSE) << \"InitializeBackend succeed.\";\r\n  }\r\n\r\n  if (status.IsOK()) {\r\n    status = CreateDevice();\r\n  }\r\n  if (status.IsOK()) {\r\n    LOGS(logger, VERBOSE) << \"CreateDevice succeed.\";\r\n  }\r\n\r\n  if (status.IsOK()) {\r\n    status = InitializeProfiling();\r\n  }\r\n  if (status.IsOK()) {\r\n    LOGS(logger, VERBOSE) << \"InitializeProfiling succeed.\";\r\n  }\r\n\r\n  if (status.IsOK()) {\r\n    ORT_RETURN_IF_ERROR(LoadOpPackage());\r\n    LOGS(logger, VERBOSE) << \"LoadOpPackage succeed.\";\r\n  }\r\n\r\n  bool enable_htp_weight_sharing = false;\r\n  if (share_ep_contexts && !load_from_cached_context) {\r\n#if defined(__aarch64__) || defined(_M_ARM64)\r\n    LOGS(logger, WARNING) << \"Weight sharing only available with offline generation on x64 platform, not work on real device.\";\r\n#else\r\n    enable_htp_weight_sharing = true;\r\n#endif\r\n  }\r\n\r\n  if (status.IsOK() && (vtcm_backup_buffer_sharing_enabled_ || !load_from_cached_context)) {\r\n    status = vtcm_backup_buffer_sharing_enabled_ ? CreateContextVtcmBackupBufferSharingEnabled(context_bin_map)\r\n                                                 : CreateContext(enable_htp_weight_sharing);\r\n\r\n    if (status.IsOK()) {\r\n      LOGS(logger, VERBOSE) << \"CreateContext succeed.\";\r\n    }\r\n  }\r\n\r\n  if (status.IsOK()) {\r\n    LOGS(logger, VERBOSE) << \"QNN SetupBackend succeed\";\r\n    backend_setup_completed_ = true;\r\n  } else {\r\n    LOGS_DEFAULT(WARNING) << \"Failed to setup so cleaning up\";\r\n    ReleaseResources();\r\n  }\r\n\r\n  return status;\r\n}\r\n\r\nStatus QnnBackendManager::CreateHtpPowerCfgId(uint32_t device_id, uint32_t core_id, uint32_t& htp_power_config_id) {\r\n  // This function is called in QNN EP's OnRunStart() even if QNN backend setup failed and the model is assigned\r\n  // to a different EP. Therefore, we have to check that backend setup actually completed before trying to\r\n  // create an HTP power config ID. Otherwise, this causes a segfault because the QNN backend lib is unloaded.\r\n  ORT_RETURN_IF_NOT(backend_setup_completed_, \"Cannot create HTP power config ID if backend setup is not complete.\");\r\n  QnnDevice_Infrastructure_t qnn_device_infra = nullptr;\r\n  auto status = qnn_interface_.deviceGetInfrastructure(&qnn_device_infra);\r\n  ORT_RETURN_IF(QNN_SUCCESS != status, \"backendGetPerfInfrastructure failed.\");\r\n\r\n  auto* htp_infra = static_cast<QnnHtpDevice_Infrastructure_t*>(qnn_device_infra);\r\n  ORT_RETURN_IF(QNN_HTP_DEVICE_INFRASTRUCTURE_TYPE_PERF != htp_infra->infraType,\r\n                \"HTP infra type = \", htp_infra->infraType, \", which is not perf infra type.\");\r\n  QnnHtpDevice_PerfInfrastructure_t& htp_perf_infra = htp_infra->perfInfra;\r\n  // Get power client id\r\n  status = htp_perf_infra.createPowerConfigId(device_id, core_id, &htp_power_config_id);\r\n  ORT_RETURN_IF(QNN_SUCCESS != status, \"createPowerConfigId failed.\");\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::SetHtpPowerConfig(uint32_t htp_power_config_client_id,\r\n                                            HtpPerformanceMode htp_performance_mode) {\r\n  // This function is called in QNN EP's OnRunStart() even if QNN backend setup failed and the model is assigned\r\n  // to a different EP. Therefore, we have to check that backend setup actually completed before trying to\r\n  // set an HTP power config ID. Otherwise, this causes a segfault because the QNN backend lib is unloaded.\r\n  ORT_RETURN_IF_NOT(backend_setup_completed_, \"Cannot set HTP power config ID if backend setup is not complete.\");\r\n  QnnDevice_Infrastructure_t qnn_device_infra = nullptr;\r\n  auto status = qnn_interface_.deviceGetInfrastructure(&qnn_device_infra);\r\n  ORT_RETURN_IF(QNN_SUCCESS != status, \"backendGetPerfInfrastructure failed.\");\r\n\r\n  auto* htp_infra = static_cast<QnnHtpDevice_Infrastructure_t*>(qnn_device_infra);\r\n  ORT_RETURN_IF(QNN_HTP_DEVICE_INFRASTRUCTURE_TYPE_PERF != htp_infra->infraType,\r\n                \"HTP infra type = \", htp_infra->infraType, \", which is not perf infra type.\");\r\n  QnnHtpDevice_PerfInfrastructure_t& htp_perf_infra = htp_infra->perfInfra;\r\n\r\n  constexpr const int kNumConfigs = 1;\r\n  std::vector<QnnHtpPerfInfrastructure_PowerConfig_t> power_configs(\r\n      kNumConfigs);\r\n  QnnHtpPerfInfrastructure_PowerConfig_t& dcvs_config = power_configs[0];\r\n  dcvs_config.option = QNN_HTP_PERF_INFRASTRUCTURE_POWER_CONFIGOPTION_DCVS_V3;\r\n  QnnHtpPerfInfrastructure_DcvsV3_t& dcvs_v3 = dcvs_config.dcvsV3Config;\r\n  dcvs_v3.contextId = htp_power_config_client_id;\r\n  dcvs_v3.setSleepDisable = 0;\r\n  dcvs_v3.sleepDisable = 0;\r\n  dcvs_v3.setDcvsEnable = 1;\r\n  dcvs_v3.powerMode = QNN_HTP_PERF_INFRASTRUCTURE_POWERMODE_PERFORMANCE_MODE;\r\n  // choose performance mode\r\n  switch (htp_performance_mode) {\r\n    case HtpPerformanceMode::kHtpBurst:\r\n      dcvs_v3.setSleepLatency = 1;  // true\r\n      dcvs_v3.sleepLatency = kSleepMinLatency;\r\n      dcvs_v3.dcvsEnable = kDcvsDisable;\r\n      dcvs_v3.setBusParams = 1;\r\n      dcvs_v3.busVoltageCornerMin = DCVS_VOLTAGE_VCORNER_MAX_VOLTAGE_CORNER;\r\n      dcvs_v3.busVoltageCornerTarget = DCVS_VOLTAGE_VCORNER_MAX_VOLTAGE_CORNER;\r\n      dcvs_v3.busVoltageCornerMax = DCVS_VOLTAGE_VCORNER_MAX_VOLTAGE_CORNER;\r\n      dcvs_v3.setCoreParams = 1;\r\n      dcvs_v3.coreVoltageCornerMin = DCVS_VOLTAGE_VCORNER_MAX_VOLTAGE_CORNER;\r\n      dcvs_v3.coreVoltageCornerTarget = DCVS_VOLTAGE_VCORNER_MAX_VOLTAGE_CORNER;\r\n      dcvs_v3.coreVoltageCornerMax = DCVS_VOLTAGE_VCORNER_MAX_VOLTAGE_CORNER;\r\n      break;\r\n    case HtpPerformanceMode::kHtpSustainedHighPerformance:\r\n    case HtpPerformanceMode::kHtpHighPerformance:\r\n      dcvs_v3.setSleepLatency = 1;  // true\r\n      dcvs_v3.sleepLatency = kSleepLowLatency;\r\n      dcvs_v3.dcvsEnable = kDcvsDisable;\r\n      dcvs_v3.setBusParams = 1;\r\n      dcvs_v3.busVoltageCornerMin = DCVS_VOLTAGE_VCORNER_TURBO;\r\n      dcvs_v3.busVoltageCornerTarget = DCVS_VOLTAGE_VCORNER_TURBO;\r\n      dcvs_v3.busVoltageCornerMax = DCVS_VOLTAGE_VCORNER_TURBO;\r\n      dcvs_v3.setCoreParams = 1;\r\n      dcvs_v3.coreVoltageCornerMin = DCVS_VOLTAGE_VCORNER_TURBO;\r\n      dcvs_v3.coreVoltageCornerTarget = DCVS_VOLTAGE_VCORNER_TURBO;\r\n      dcvs_v3.coreVoltageCornerMax = DCVS_VOLTAGE_VCORNER_TURBO;\r\n      break;\r\n    case HtpPerformanceMode::kHtpBalanced:\r\n      dcvs_v3.setSleepLatency = 1;  // true\r\n      dcvs_v3.sleepLatency = kSleepMediumLatency;\r\n      dcvs_v3.dcvsEnable = kDcvsEnable;\r\n      dcvs_v3.setBusParams = 1;\r\n      dcvs_v3.busVoltageCornerMin = DCVS_VOLTAGE_VCORNER_NOM_PLUS;\r\n      dcvs_v3.busVoltageCornerTarget = DCVS_VOLTAGE_VCORNER_NOM_PLUS;\r\n      dcvs_v3.busVoltageCornerMax = DCVS_VOLTAGE_VCORNER_NOM_PLUS;\r\n      dcvs_v3.setCoreParams = 1;\r\n      dcvs_v3.coreVoltageCornerMin = DCVS_VOLTAGE_VCORNER_NOM_PLUS;\r\n      dcvs_v3.coreVoltageCornerTarget = DCVS_VOLTAGE_VCORNER_NOM_PLUS;\r\n      dcvs_v3.coreVoltageCornerMax = DCVS_VOLTAGE_VCORNER_NOM_PLUS;\r\n      break;\r\n    case HtpPerformanceMode::kHtpLowBalanced:\r\n      dcvs_v3.setSleepLatency = 1;  // true\r\n      dcvs_v3.sleepLatency = kSleepMediumLatency;\r\n      dcvs_v3.dcvsEnable = kDcvsEnable;\r\n      dcvs_v3.setBusParams = 1;\r\n      dcvs_v3.busVoltageCornerMin = DCVS_VOLTAGE_VCORNER_NOM;\r\n      dcvs_v3.busVoltageCornerTarget = DCVS_VOLTAGE_VCORNER_NOM;\r\n      dcvs_v3.busVoltageCornerMax = DCVS_VOLTAGE_VCORNER_NOM;\r\n      dcvs_v3.setCoreParams = 1;\r\n      dcvs_v3.coreVoltageCornerMin = DCVS_VOLTAGE_VCORNER_NOM;\r\n      dcvs_v3.coreVoltageCornerTarget = DCVS_VOLTAGE_VCORNER_NOM;\r\n      dcvs_v3.coreVoltageCornerMax = DCVS_VOLTAGE_VCORNER_NOM;\r\n      break;\r\n    case HtpPerformanceMode::kHtpHighPowerSaver:\r\n      dcvs_v3.setSleepLatency = 1;  // true\r\n      dcvs_v3.sleepLatency = kSleepMediumLatency;\r\n      dcvs_v3.dcvsEnable = kDcvsEnable;\r\n      dcvs_v3.setBusParams = 1;\r\n      dcvs_v3.busVoltageCornerMin = DCVS_VOLTAGE_VCORNER_SVS_PLUS;\r\n      dcvs_v3.busVoltageCornerTarget = DCVS_VOLTAGE_VCORNER_SVS_PLUS;\r\n      dcvs_v3.busVoltageCornerMax = DCVS_VOLTAGE_VCORNER_SVS_PLUS;\r\n      dcvs_v3.setCoreParams = 1;\r\n      dcvs_v3.coreVoltageCornerMin = DCVS_VOLTAGE_VCORNER_SVS_PLUS;\r\n      dcvs_v3.coreVoltageCornerTarget = DCVS_VOLTAGE_VCORNER_SVS_PLUS;\r\n      dcvs_v3.coreVoltageCornerMax = DCVS_VOLTAGE_VCORNER_SVS_PLUS;\r\n      break;\r\n    case HtpPerformanceMode::kHtpPowerSaver:\r\n      dcvs_v3.setSleepLatency = 1;  // true\r\n      dcvs_v3.sleepLatency = kSleepMediumLatency;\r\n      dcvs_v3.dcvsEnable = kDcvsEnable;\r\n      dcvs_v3.setBusParams = 1;\r\n      dcvs_v3.busVoltageCornerMin = DCVS_VOLTAGE_VCORNER_SVS;\r\n      dcvs_v3.busVoltageCornerTarget = DCVS_VOLTAGE_VCORNER_SVS;\r\n      dcvs_v3.busVoltageCornerMax = DCVS_VOLTAGE_VCORNER_SVS;\r\n      dcvs_v3.setCoreParams = 1;\r\n      dcvs_v3.coreVoltageCornerMin = DCVS_VOLTAGE_VCORNER_SVS;\r\n      dcvs_v3.coreVoltageCornerTarget = DCVS_VOLTAGE_VCORNER_SVS;\r\n      dcvs_v3.coreVoltageCornerMax = DCVS_VOLTAGE_VCORNER_SVS;\r\n      break;\r\n    case HtpPerformanceMode::kHtpLowPowerSaver:\r\n      dcvs_v3.setSleepLatency = 1;  // true\r\n      dcvs_v3.sleepLatency = kSleepMediumLatency;\r\n      dcvs_v3.dcvsEnable = kDcvsEnable;\r\n      dcvs_v3.setBusParams = 1;\r\n      dcvs_v3.busVoltageCornerMin = DCVS_VOLTAGE_VCORNER_SVS2;\r\n      dcvs_v3.busVoltageCornerTarget = DCVS_VOLTAGE_VCORNER_SVS2;\r\n      dcvs_v3.busVoltageCornerMax = DCVS_VOLTAGE_VCORNER_SVS2;\r\n      dcvs_v3.setCoreParams = 1;\r\n      dcvs_v3.coreVoltageCornerMin = DCVS_VOLTAGE_VCORNER_SVS2;\r\n      dcvs_v3.coreVoltageCornerTarget = DCVS_VOLTAGE_VCORNER_SVS2;\r\n      dcvs_v3.coreVoltageCornerMax = DCVS_VOLTAGE_VCORNER_SVS2;\r\n      break;\r\n    case HtpPerformanceMode::kHtpExtremePowerSaver:\r\n      dcvs_v3.powerMode = QNN_HTP_PERF_INFRASTRUCTURE_POWERMODE_POWER_SAVER_MODE;\r\n      dcvs_v3.setSleepLatency = 1;  // true\r\n      dcvs_v3.sleepLatency = kSleepMediumLatency;\r\n      dcvs_v3.dcvsEnable = kDcvsEnable;\r\n      dcvs_v3.setBusParams = 1;\r\n      dcvs_v3.busVoltageCornerMin = DCVS_VOLTAGE_CORNER_DISABLE;\r\n      dcvs_v3.busVoltageCornerTarget = DCVS_VOLTAGE_CORNER_DISABLE;\r\n      dcvs_v3.busVoltageCornerMax = DCVS_VOLTAGE_CORNER_DISABLE;\r\n      dcvs_v3.setCoreParams = 1;\r\n      dcvs_v3.coreVoltageCornerMin = DCVS_VOLTAGE_CORNER_DISABLE;\r\n      dcvs_v3.coreVoltageCornerTarget = DCVS_VOLTAGE_CORNER_DISABLE;\r\n      dcvs_v3.coreVoltageCornerMax = DCVS_VOLTAGE_CORNER_DISABLE;\r\n      break;\r\n    default:\r\n      ORT_THROW(\"Invalid performance profile %d\", static_cast<int>(htp_performance_mode));\r\n      break;\r\n  }\r\n  std::vector<const QnnHtpPerfInfrastructure_PowerConfig_t*> perf_power_configs_ptr = ObtainNullTermPtrVector(power_configs);\r\n  status = htp_perf_infra.setPowerConfig(htp_power_config_client_id, perf_power_configs_ptr.data());\r\n  ORT_RETURN_IF(QNN_SUCCESS != status, \"setPowerConfig failed for HTP performance mode.\");\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::SetRpcPowerConfigs(uint32_t htp_power_config_client_id,\r\n                                             uint32_t rpc_control_latency,\r\n                                             uint32_t rpc_polling_time) {\r\n  // This function is called in QNN EP's OnRunStart() even if QNN backend setup failed and the model is assigned\r\n  // to a different EP. Therefore, we have to check that backend setup actually completed before trying to\r\n  // set RPC control latency. Otherwise, this causes a segfault because the QNN backend library is unloaded.\r\n  ORT_RETURN_IF_NOT(backend_setup_completed_, \"Cannot set HTP RPC control latency if backend setup is not complete.\");\r\n\r\n  constexpr int kNumRpcPollingPowerConfigs = 2;\r\n  std::vector<QnnHtpPerfInfrastructure_PowerConfig_t> rpc_power_configs;\r\n  rpc_power_configs.reserve(kNumRpcPollingPowerConfigs);\r\n\r\n  // Set rpc control latency here\r\n  if (rpc_control_latency != 0) {\r\n    auto& rpc_control_latency_cfg = rpc_power_configs.emplace_back();\r\n    rpc_control_latency_cfg.option = QNN_HTP_PERF_INFRASTRUCTURE_POWER_CONFIGOPTION_RPC_CONTROL_LATENCY;\r\n    rpc_control_latency_cfg.rpcControlLatencyConfig = rpc_control_latency;\r\n  }\r\n\r\n  // Note: v68 does not support rpc polling mode\r\n  if (rpc_polling_time != 0) {\r\n    auto& rpc_polling_time_cfg = rpc_power_configs.emplace_back();\r\n    rpc_polling_time_cfg.option = QNN_HTP_PERF_INFRASTRUCTURE_POWER_CONFIGOPTION_RPC_POLLING_TIME;\r\n    rpc_polling_time_cfg.rpcPollingTimeConfig = rpc_polling_time;\r\n  }\r\n\r\n  if (rpc_power_configs.size() > 0) {\r\n    QnnDevice_Infrastructure_t qnn_device_infra = nullptr;\r\n    auto status = qnn_interface_.deviceGetInfrastructure(&qnn_device_infra);\r\n    ORT_RETURN_IF(QNN_SUCCESS != status, \"backendGetPerfInfrastructure failed.\");\r\n\r\n    auto* htp_infra = static_cast<QnnHtpDevice_Infrastructure_t*>(qnn_device_infra);\r\n    ORT_RETURN_IF(QNN_HTP_DEVICE_INFRASTRUCTURE_TYPE_PERF != htp_infra->infraType,\r\n                  \"HTP infra type = \", htp_infra->infraType, \", which is not perf infra type.\");\r\n    QnnHtpDevice_PerfInfrastructure_t& htp_perf_infra = htp_infra->perfInfra;\r\n\r\n    std::vector<const QnnHtpPerfInfrastructure_PowerConfig_t*> perf_power_configs_ptr =\r\n        ObtainNullTermPtrVector(rpc_power_configs);\r\n    status = htp_perf_infra.setPowerConfig(htp_power_config_client_id, perf_power_configs_ptr.data());\r\n    ORT_RETURN_IF(QNN_SUCCESS != status, \"setPowerConfig failed for RPC control latency.\");\r\n  }\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::DestroyHTPPowerConfigID(uint32_t htp_power_config_id) {\r\n  QnnDevice_Infrastructure_t qnn_device_infra = nullptr;\r\n  auto status = qnn_interface_.deviceGetInfrastructure(&qnn_device_infra);\r\n  ORT_RETURN_IF(QNN_SUCCESS != status, \"backendGetPerfInfrastructure failed.\");\r\n\r\n  auto* htp_infra = static_cast<QnnHtpDevice_Infrastructure_t*>(qnn_device_infra);\r\n  ORT_RETURN_IF(QNN_HTP_DEVICE_INFRASTRUCTURE_TYPE_PERF != htp_infra->infraType,\r\n                \"HTP infra type = \", htp_infra->infraType, \", which is not perf infra type.\");\r\n  QnnHtpDevice_PerfInfrastructure_t& htp_perf_infra = htp_infra->perfInfra;\r\n\r\n  Qnn_ErrorHandle_t destroy_ret = htp_perf_infra.destroyPowerConfigId(htp_power_config_id);\r\n  ORT_RETURN_IF(QNN_SUCCESS != destroy_ret, \"destroyPowerConfigId failed.\");\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::TerminateQnnLog() {\r\n  std::lock_guard<std::recursive_mutex> lock(logger_recursive_mutex_);\r\n  if (logger_ == nullptr) {\r\n    return Status::OK();\r\n  }\r\n\r\n  if (nullptr != qnn_interface_.logFree && nullptr != log_handle_) {\r\n    auto ret_val = qnn_interface_.logFree(log_handle_);\r\n\r\n    // Reset QNN log handle to nullptr so other threads that are waiting on logger_recursive_mutex_ know it was freed.\r\n    log_handle_ = nullptr;\r\n    ORT_RETURN_IF(QNN_SUCCESS != ret_val,\r\n                  \"Unable to terminate logging in the backend.\");\r\n  }\r\n\r\n  return Status::OK();\r\n}\r\n\r\nvoid QnnBackendManager::ReleaseResources() {\r\n  auto result = ReleaseContext();\r\n  if (Status::OK() != result) {\r\n    LOGS_DEFAULT(ERROR) << \"Failed to ReleaseContext: \" << result.ErrorMessage();\r\n  }\r\n\r\n  result = ReleaseProfilehandle();\r\n  if (Status::OK() != result) {\r\n    LOGS_DEFAULT(ERROR) << \"Failed to ReleaseProfilehandle: \" << result.ErrorMessage();\r\n  }\r\n\r\n  result = ReleaseDevice();\r\n  if (Status::OK() != result) {\r\n    LOGS_DEFAULT(ERROR) << \"Failed to ReleaseDevice: \" << result.ErrorMessage();\r\n  }\r\n\r\n  result = ShutdownBackend();\r\n  if (Status::OK() != result) {\r\n    LOGS_DEFAULT(ERROR) << \"Failed to ShutdownBackend: \" << result.ErrorMessage();\r\n  }\r\n\r\n  result = TerminateQnnLog();\r\n  if (Status::OK() != result) {\r\n    LOGS_DEFAULT(ERROR) << \"Failed to TerminateQnnLog: \" << result.ErrorMessage();\r\n  }\r\n\r\n  if (backend_lib_handle_) {\r\n    result = UnloadLib(backend_lib_handle_);\r\n    if (Status::OK() != result) {\r\n      LOGS_DEFAULT(ERROR) << \"Failed to unload backend library: \" << result.ErrorMessage();\r\n    }\r\n  }\r\n\r\n  backend_setup_completed_ = false;\r\n\r\n  return;\r\n}\r\n\r\nStatus QnnBackendManager::ExtractBackendProfilingInfo(qnn::profile::ProfilingInfo& profiling_info) {\r\n  if (ProfilingLevel::OFF == profiling_level_merge_ || ProfilingLevel::INVALID == profiling_level_merge_) {\r\n    return Status::OK();\r\n  }\r\n\r\n  bool tracelogging_provider_ep_enabled = false;\r\n#ifdef _WIN32\r\n  auto& provider = QnnTelemetry::Instance();\r\n  if (provider.IsEnabled()) {\r\n    auto level = provider.Level();\r\n    auto keyword = provider.Keyword();\r\n    if ((keyword & static_cast<uint64_t>(onnxruntime::logging::ORTTraceLoggingKeyword::Profiling)) != 0 && level >= 5) {\r\n      tracelogging_provider_ep_enabled = true;\r\n    }\r\n  }\r\n#endif  // defined(_WIN32)\r\n\r\n  // ETW disabled previously, but enabled now\r\n  if (ProfilingLevel::INVALID == profiling_level_etw_ && tracelogging_provider_ep_enabled) {\r\n    LOGS(*logger_, ERROR) << \"ETW disabled previously, but enabled now. Can't do the switch! Won't output any profiling.\";\r\n    return Status::OK();\r\n  }\r\n\r\n  // ETW enabled previously, but disabled now\r\n  if (ProfilingLevel::INVALID != profiling_level_etw_ && !tracelogging_provider_ep_enabled) {\r\n    LOGS(*logger_, ERROR) << \"ETW enabled previously, but disabled now. Can't do the switch! Won't output any profiling.\";\r\n    return Status::OK();\r\n  }\r\n\r\n  ORT_RETURN_IF(!tracelogging_provider_ep_enabled && profiling_file_path_.empty(),\r\n                \"Need to specify a CSV file via provider option profiling_file_path if ETW not enabled.\");\r\n\r\n  ORT_RETURN_IF(nullptr == profile_backend_handle_, \"Backend profile handle not valid.\");\r\n\r\n  LOGS(*logger_, VERBOSE) << \"Extracting profiling events for graph \" << profiling_info.graph_name;\r\n  const QnnProfile_EventId_t* profile_events{nullptr};\r\n  uint32_t num_events{0};\r\n  Qnn_ErrorHandle_t result = qnn_interface_.profileGetEvents(profile_backend_handle_, &profile_events, &num_events);\r\n  if (qnn_serializer_config_) {  // Using QNN Saver or IR backend\r\n    // QNN SDK 2.28.2 returns QNN_SAVER_ERROR_DUMMY_RETVALUE, but previous QNN versions return QNN_PROFILE_NO_ERROR.\r\n    // We accept both values.\r\n    ORT_RETURN_IF(QNN_PROFILE_NO_ERROR != result && QNN_SAVER_ERROR_DUMMY_RETVALUE != result,\r\n                  \"Failed to get profile events. Error: \", QnnErrorHandleToString(result));\r\n  } else {\r\n    ORT_RETURN_IF(QNN_PROFILE_NO_ERROR != result, \"Failed to get profile events. Error: \", QnnErrorHandleToString(result));\r\n  }\r\n\r\n  if (num_events > 0) {\r\n    LOGS(*logger_, VERBOSE) << \"profile_events: \" << profile_events << \" num_events: \" << num_events;\r\n\r\n    bool backendSupportsExtendedEventData = false;\r\n    Qnn_ErrorHandle_t resultPropertyHasCapability =\r\n        qnn_interface_.propertyHasCapability(QNN_PROPERTY_PROFILE_SUPPORTS_EXTENDED_EVENT);\r\n    uint16_t errorCodePropertyHasCapability = static_cast<uint16_t>(resultPropertyHasCapability & 0xFFFF);\r\n    if (errorCodePropertyHasCapability == QNN_PROPERTY_SUPPORTED) {\r\n      LOGS(*logger_, VERBOSE) << \"The QNN backend supports extended event data.\";\r\n      backendSupportsExtendedEventData = true;\r\n    } else {\r\n      LOGS(*logger_, VERBOSE) << \"The QNN backend does not support extended event data.\";\r\n    }\r\n\r\n    profiling_info.csv_output_filepath = profiling_file_path_;\r\n#ifdef QNN_SYSTEM_PROFILE_API_ENABLED\r\n    profiling_info.num_events = num_events;\r\n#endif\r\n\r\n    profile::Serializer profile_writer(profiling_info,\r\n                                       qnn_sys_interface_,\r\n                                       tracelogging_provider_ep_enabled);\r\n    if (!profiling_file_path_.empty()) {\r\n      ORT_RETURN_IF_ERROR(profile_writer.InitCsvFile());\r\n    }\r\n\r\n    for (size_t event_idx = 0; event_idx < num_events; event_idx++) {\r\n      ORT_RETURN_IF_ERROR(\r\n          ExtractProfilingEvent(*(profile_events + event_idx), \"ROOT\", profile_writer,\r\n                                backendSupportsExtendedEventData));\r\n      ORT_RETURN_IF_ERROR(\r\n          ExtractProfilingSubEvents(*(profile_events + event_idx), profile_writer,\r\n                                    backendSupportsExtendedEventData));\r\n    }\r\n#ifdef QNN_SYSTEM_PROFILE_API_ENABLED\r\n    ORT_RETURN_IF_ERROR(profile_writer.SerializeEventsToQnnLog());\r\n#endif\r\n\r\n    if (!profiling_file_path_.empty()) {\r\n      LOGS(*logger_, VERBOSE) << \"Wrote QNN profiling events (\" << num_events << \") to file (\"\r\n                              << profiling_file_path_ << \")\";\r\n    }\r\n\r\n    if (tracelogging_provider_ep_enabled) {\r\n      LOGS(*logger_, VERBOSE) << \"Wrote QNN profiling events (\" << num_events << \") to ETW\";\r\n    }\r\n  }\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::ExtractProfilingSubEvents(\r\n    QnnProfile_EventId_t profile_event_id,\r\n    profile::Serializer& profile_writer,\r\n    bool useExtendedEventData) {\r\n  const QnnProfile_EventId_t* profile_sub_events{nullptr};\r\n  uint32_t num_sub_events{0};\r\n  Qnn_ErrorHandle_t result = qnn_interface_.profileGetSubEvents(profile_event_id, &profile_sub_events, &num_sub_events);\r\n  ORT_RETURN_IF(QNN_PROFILE_NO_ERROR != result, \"Failed to get profile sub events. Error: \", QnnErrorHandleToString(result));\r\n\r\n  if (num_sub_events > 0) {\r\n    LOGS(*logger_, VERBOSE) << \"profile_sub_events: \" << profile_sub_events << \" num_sub_events: \" << num_sub_events;\r\n\r\n#ifdef QNN_SYSTEM_PROFILE_API_ENABLED\r\n    QnnSystemProfile_ProfileEventV1_t* parent_system_event = nullptr;\r\n    parent_system_event = profile_writer.GetParentSystemEvent(profile_event_id);\r\n    if (parent_system_event == nullptr) {\r\n      parent_system_event = profile_writer.GetSystemEventPointer(profile_event_id);\r\n      profile_writer.AddSubEventList(num_sub_events, parent_system_event);\r\n    }\r\n#endif\r\n\r\n    for (size_t sub_event_idx = 0; sub_event_idx < num_sub_events; sub_event_idx++) {\r\n      QnnProfile_EventId_t subevent_id = *(profile_sub_events + sub_event_idx);\r\n\r\n#ifdef QNN_SYSTEM_PROFILE_API_ENABLED\r\n\r\n      ORT_RETURN_IF_ERROR(profile_writer.SetParentSystemEvent(subevent_id, parent_system_event));\r\n\r\n#endif\r\n\r\n      ORT_RETURN_IF_ERROR(\r\n          ExtractProfilingEvent(subevent_id, \"SUB-EVENT\", profile_writer, useExtendedEventData));\r\n      ORT_RETURN_IF_ERROR(\r\n          ExtractProfilingSubEvents(subevent_id, profile_writer, useExtendedEventData));\r\n    }\r\n\r\n    LOGS(*logger_, VERBOSE) << \"Wrote QNN profiling sub events (\" << num_sub_events << \")\";\r\n  }\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::ExtractProfilingEvent(\r\n    QnnProfile_EventId_t profile_event_id,\r\n    const std::string& event_level,\r\n    profile::Serializer& profile_writer,\r\n    bool useExtendedEventData) {\r\n  if (useExtendedEventData) {\r\n    return ExtractProfilingEventExtended(profile_event_id, event_level, profile_writer);\r\n  } else {\r\n    return ExtractProfilingEventBasic(profile_event_id, event_level, profile_writer);\r\n  }\r\n}\r\n\r\nStatus QnnBackendManager::ExtractProfilingEventBasic(\r\n    QnnProfile_EventId_t profile_event_id,\r\n    const std::string& event_level,\r\n    profile::Serializer& profile_writer) {\r\n  QnnProfile_EventData_t event_data;\r\n  Qnn_ErrorHandle_t result = qnn_interface_.profileGetEventData(profile_event_id, &event_data);\r\n  QnnProfile_Error_t errorCode = static_cast<QnnProfile_Error_t>(result & 0xFFFF);\r\n  ORT_RETURN_IF(QNN_PROFILE_NO_ERROR != result, \"Failed to get profile event data: \" + std::string(QnnProfileErrorToString(errorCode)));\r\n\r\n  ORT_RETURN_IF_ERROR(profile_writer.ProcessEvent(profile_event_id, event_level, event_data));\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::ExtractProfilingEventExtended(\r\n    QnnProfile_EventId_t profile_event_id,\r\n    const std::string& event_level,\r\n    profile::Serializer& profile_writer) {\r\n  QnnProfile_ExtendedEventData_t event_data_extended;\r\n  auto resultGetExtendedEventData = qnn_interface_.profileGetExtendedEventData(profile_event_id, &event_data_extended);\r\n  QnnProfile_Error_t errorCode = static_cast<QnnProfile_Error_t>(resultGetExtendedEventData & 0xFFFF);\r\n  ORT_RETURN_IF(QNN_PROFILE_NO_ERROR != errorCode, \"Failed to get profile event data: \" + std::string(QnnProfileErrorToString(errorCode)));\r\n\r\n  ORT_RETURN_IF_ERROR(profile_writer.ProcessExtendedEvent(profile_event_id, event_level, event_data_extended));\r\n\r\n  return Status::OK();\r\n}\r\n\r\nconst char* QnnBackendManager::QnnProfileErrorToString(QnnProfile_Error_t error) {\r\n  switch (error) {\r\n    case QNN_PROFILE_NO_ERROR:\r\n      return \"QNN_PROFILE_NO_ERROR\";\r\n    case QNN_PROFILE_ERROR_UNSUPPORTED:\r\n      return \"QNN_PROFILE_ERROR_UNSUPPORTED\";\r\n    case QNN_PROFILE_ERROR_INVALID_ARGUMENT:\r\n      return \"QNN_PROFILE_ERROR_INVALID_ARGUMENT\";\r\n    case QNN_PROFILE_ERROR_MEM_ALLOC:\r\n      return \"QNN_PROFILE_ERROR_MEM_ALLOC\";\r\n    case QNN_PROFILE_ERROR_INVALID_HANDLE:\r\n      return \"QNN_PROFILE_ERROR_INVALID_HANDLE\";\r\n    case QNN_PROFILE_ERROR_HANDLE_IN_USE:\r\n      return \"QNN_PROFILE_ERROR_HANDLE_IN_USE\";\r\n    case QNN_PROFILE_ERROR_INCOMPATIBLE_EVENT:\r\n      return \"QNN_PROFILE_ERROR_INCOMPATIBLE_EVENT\";\r\n    default:\r\n      return \"UNKNOWN_ERROR\";\r\n  }\r\n}\r\n\r\nstd::string QnnBackendManager::QnnErrorHandleToString(Qnn_ErrorHandle_t error) {\r\n  return utils::GetQnnErrorMessage(qnn_interface_, error);\r\n}\r\n\r\nQnnBackendManager::~QnnBackendManager() {\r\n  ReleaseResources();\r\n}\r\n\r\nvoid* QnnBackendManager::LoadLib(const char* file_name, int flags, std::string& error_msg) {\r\n#ifdef _WIN32\r\n  DWORD as_is, to_be;\r\n  bool loaded_before = false;\r\n\r\n  if (!file_name || ::strlen(file_name) == 0) {\r\n    error_msg = \"filename is null or empty\";\r\n    return nullptr;\r\n  }\r\n\r\n  // POSIX asks one of symbol resolving approaches:\r\n  // NOW or LAZY must be specified\r\n  if (!(flags & static_cast<int>(DlOpenFlag::DL_NOW))) {\r\n    error_msg = \"flags must include DL_NOW\";\r\n    return nullptr;\r\n  }\r\n\r\n  HANDLE cur_proc = GetCurrentProcess();\r\n\r\n  if (EnumProcessModules(cur_proc, nullptr, 0, &as_is) == 0) {\r\n    error_msg = \"enumerate modules failed before loading module\";\r\n    return nullptr;\r\n  }\r\n\r\n  HMODULE mod;\r\n  auto file_path = std::filesystem::path(file_name);\r\n  if (!file_path.is_absolute()) {\r\n    // construct an absolute path from ORT runtime path + file_name and check whether it exists.\r\n    const Env& env = GetDefaultEnv();\r\n    auto pathstring = env.GetRuntimePath() + ToPathString(file_name);\r\n    auto absolute_path = pathstring.c_str();\r\n    if (std::filesystem::exists(std::filesystem::path(absolute_path))) {\r\n      // load library from absolute path and search for dependencies there.\r\n      mod = LoadLibraryExW(absolute_path, nullptr, LOAD_WITH_ALTERED_SEARCH_PATH);\r\n    } else {\r\n      // use default dll search order for file_name.\r\n      mod = LoadLibraryExA(file_name, nullptr, 0);\r\n    }\r\n  } else {\r\n    // file_name represents an absolute path.\r\n    // load library from absolute path and search for dependencies there.\r\n    mod = LoadLibraryExA(file_name, nullptr, LOAD_WITH_ALTERED_SEARCH_PATH);\r\n  }\r\n  if (!mod) {\r\n    error_msg = \"load library failed\";\r\n    return nullptr;\r\n  }\r\n\r\n  if (EnumProcessModules(cur_proc, nullptr, 0, &to_be) == 0) {\r\n    error_msg = \"enumerate modules failed after loading module\";\r\n    FreeLibrary(mod);\r\n    return nullptr;\r\n  }\r\n\r\n  if (as_is == to_be) {\r\n    loaded_before = true;\r\n  }\r\n\r\n  // (not loaded_before) and DL_LOCAL means this lib was not loaded yet\r\n  // add it into the local set\r\n  //\r\n  // If loaded_before and DL_LOCAL, means this lib was already loaded\r\n  // 2 cases here for how it was loaded before:\r\n  // a. with DL_LOCAL, just ignore since it was already in local set\r\n  // b. with DL_GLOBAL, POSIX asks it in global, ignore it, too\r\n  if ((!loaded_before) && (flags & static_cast<int>(DlOpenFlag::DL_LOCAL))) {\r\n    mod_handles_.insert(mod);\r\n  }\r\n\r\n  // once callers ask for global, needs to be in global thereafter\r\n  // so the lib should be removed from local set\r\n  if (flags & static_cast<int>(DlOpenFlag::DL_GLOBAL)) {\r\n    mod_handles_.erase(mod);\r\n  }\r\n\r\n  return static_cast<void*>(mod);\r\n#else\r\n  ORT_UNUSED_PARAMETER(error_msg);\r\n  int real_flags = 0;\r\n\r\n  if (flags & static_cast<int>(DlOpenFlag::DL_NOW)) {\r\n    real_flags |= RTLD_NOW;\r\n  }\r\n\r\n  if (flags & static_cast<int>(DlOpenFlag::DL_LOCAL)) {\r\n    real_flags |= RTLD_LOCAL;\r\n  }\r\n\r\n  if (flags & static_cast<int>(DlOpenFlag::DL_GLOBAL)) {\r\n    real_flags |= RTLD_GLOBAL;\r\n  }\r\n\r\n  return ::dlopen(file_name, real_flags);\r\n#endif\r\n}\r\n\r\nStatus QnnBackendManager::UnloadLib(void* handle) {\r\n  if (!handle) {\r\n    return Status::OK();\r\n  }\r\n\r\n#ifdef _WIN32\r\n  HMODULE mod = static_cast<HMODULE>(handle);\r\n\r\n  if (FreeLibrary(mod) == 0) {\r\n    return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, \"Failed to free library.\");\r\n  }\r\n  mod_handles_.erase(mod);\r\n#else\r\n  auto rt = ::dlclose(handle);\r\n  if (rt != 0) {\r\n    return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, \"Failed to free library.\");\r\n  }\r\n#endif  // defined(_WIN32)\r\n\r\n  return Status::OK();\r\n}\r\n\r\nvoid* QnnBackendManager::LibFunction(void* handle, const char* symbol, std::string& error_msg) {\r\n#ifdef _WIN32\r\n  FARPROC sym_addr = nullptr;\r\n  DWORD size, size_needed;\r\n  HMODULE mod = 0;\r\n\r\n  if ((!handle) || (!symbol)) {\r\n    return nullptr;\r\n  }\r\n\r\n  HANDLE cur_proc = GetCurrentProcess();\r\n\r\n  if (EnumProcessModules(cur_proc, nullptr, 0, &size) == 0) {\r\n    error_msg = \"enumerate modules failed before memory allocation\";\r\n    return nullptr;\r\n  }\r\n\r\n  HMODULE* mod_list = static_cast<HMODULE*>(malloc(size));\r\n  if (!mod_list) {\r\n    error_msg = \"malloc failed\";\r\n    return nullptr;\r\n  }\r\n\r\n  if (EnumProcessModules(cur_proc, mod_list, size, &size_needed) == 0) {\r\n    error_msg = \"enumerate modules failed after memory allocation\";\r\n    free(mod_list);\r\n    return nullptr;\r\n  }\r\n\r\n  // DL_DEFAULT needs to bypass those modules with DL_LOCAL flag\r\n  if (handle == DL_DEFAULT) {\r\n    for (size_t i = 0; i < (size / sizeof(HMODULE)); i++) {\r\n      auto iter = mod_handles_.find(mod_list[i]);\r\n      if (iter != mod_handles_.end()) {\r\n        continue;\r\n      }\r\n      // once find the first non-local module with symbol\r\n      // return its address here to avoid unnecessary looping\r\n      sym_addr = GetProcAddress(mod_list[i], symbol);\r\n      if (sym_addr) {\r\n        free(mod_list);\r\n        return *(void**)(&sym_addr);\r\n      }\r\n    }\r\n  } else {\r\n    mod = static_cast<HMODULE>(handle);\r\n  }\r\n\r\n  free(mod_list);\r\n  sym_addr = GetProcAddress(mod, symbol);\r\n  if (!sym_addr) {\r\n    error_msg = \"can't resolve symbol\";\r\n    return NULL;\r\n  }\r\n\r\n  return *(void**)(&sym_addr);\r\n#else\r\n  ORT_UNUSED_PARAMETER(error_msg);\r\n  if (handle == DL_DEFAULT) {\r\n    return ::dlsym(RTLD_DEFAULT, symbol);\r\n  }\r\n\r\n  return ::dlsym(handle, symbol);\r\n#endif\r\n}\r\n\r\nStatus QnnBackendManager::AddQnnContextHandle(Qnn_ContextHandle_t raw_context_handle) {\r\n  ORT_RETURN_IF(logger_ == nullptr, \"logger_ should be set.\");\r\n\r\n  auto free_context_handle = [this, &logger = *logger_](Qnn_ContextHandle_t raw_context_handle) {\r\n    const auto free_result = qnn_interface_.contextFree(raw_context_handle, nullptr);\r\n    if (free_result != QNN_CONTEXT_NO_ERROR) {\r\n      LOGS(logger, ERROR) << \"qnn_interface.contextFree() failed: \"\r\n                          << utils::GetVerboseQnnErrorMessage(qnn_interface_, free_result);\r\n    }\r\n  };\r\n\r\n  // take ownership of `raw_context_handle`\r\n  auto context_handle = UniqueQnnContextHandle(raw_context_handle, free_context_handle);\r\n  auto mem_handle_manager = std::make_unique<QnnContextMemHandleManager>(GetQnnInterface(), raw_context_handle,\r\n                                                                         *logger_);\r\n\r\n  auto context_handle_record = std::make_shared<QnnContextHandleRecord>();\r\n  context_handle_record->context_handle = std::move(context_handle);\r\n  context_handle_record->mem_handles = std::move(mem_handle_manager);\r\n\r\n  const bool inserted = context_map_.try_emplace(raw_context_handle, std::move(context_handle_record)).second;\r\n  ORT_RETURN_IF_NOT(inserted, \"QNN context was already added: \", raw_context_handle);\r\n\r\n  contexts_.push_back(raw_context_handle);\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::GetOrRegisterContextMemHandle(Qnn_ContextHandle_t context_handle,\r\n                                                        void* shared_memory_address,\r\n                                                        const Qnn_Tensor_t& qnn_tensor,\r\n                                                        Qnn_MemHandle_t& mem_handle) {\r\n  // Multi-threading situations to consider:\r\n  // 1) Shared memory allocation is being freed in another thread while we are processing `shared_memory_address`.\r\n  //    This implies incorrect usage as the memory is being freed while it is still in use. Let's assume this won't\r\n  //    happen.\r\n  // 2) The shared memory allocation clean up function is being run from another thread while the\r\n  //    QnnContextHandleRecord or QnnBackendManager objects are being destroyed.\r\n  //    Usage of weak_ptrs from the clean up function should ensure that those objects are only accessed while they are\r\n  //    in scope.\r\n\r\n  const auto context_handle_record_it = context_map_.find(context_handle);\r\n  ORT_RETURN_IF_NOT(context_handle_record_it != context_map_.end(), \"QNN context not found: \", context_handle);\r\n\r\n  auto& context_handle_record = context_handle_record_it->second;\r\n  auto& context_mem_handle_manager = context_handle_record->mem_handles;\r\n\r\n  bool did_register{};\r\n  ORT_RETURN_IF_ERROR(context_mem_handle_manager->GetOrRegister(shared_memory_address, qnn_tensor,\r\n                                                                mem_handle, did_register));\r\n\r\n  if (did_register) {\r\n    HtpSharedMemoryAllocator::AllocationCleanUpFn unregister_mem_handle =\r\n        [&logger = *logger_,\r\n         shared_memory_address,\r\n         weak_backend_manager = weak_from_this(),\r\n         weak_context_handle_record = std::weak_ptr{context_handle_record}](\r\n            void* /* allocation_base_address */) {\r\n          // Lock QnnBackendManager shared_ptr to ensure that QNN interface is still valid.\r\n          auto backend_manager = weak_backend_manager.lock();\r\n          if (!backend_manager) {\r\n            return;\r\n          }\r\n\r\n          // Lock QnnContextHandleRecord shared_ptr to ensure that QNN context handle is still valid.\r\n          auto context_handle_record = weak_context_handle_record.lock();\r\n          if (!context_handle_record) {\r\n            return;\r\n          }\r\n\r\n          auto& context_mem_handle_manager = context_handle_record->mem_handles;\r\n\r\n          auto unregister_status = context_mem_handle_manager->Unregister(shared_memory_address);\r\n          if (!unregister_status.IsOK()) {\r\n            LOGS(logger, ERROR) << \"Failed to unregister shared memory mem handle for address: \"\r\n                                << shared_memory_address << \", error: \" << unregister_status.ErrorMessage();\r\n          }\r\n        };\r\n\r\n    ORT_RETURN_IF_ERROR(HtpSharedMemoryAllocator::AddAllocationCleanUp(shared_memory_address,\r\n                                                                       std::move(unregister_mem_handle)));\r\n  }\r\n\r\n  return Status::OK();\r\n}\r\n\r\n}  // namespace qnn\r\n}  // namespace onnxruntime\r\n","replacement":"//\r\n// Copyright (c) Microsoft Corporation. All rights reserved.\r\n// Licensed under the MIT License.\r\n\r\n#include \"qnn_backend_manager.h\"\r\n#include \"qnn_model.h\"\r\n#include <filesystem>\r\n#include <fstream>\r\n#include <string>\r\n#include \"QnnOpDef.h\"\r\n#include \"CPU/QnnCpuCommon.h\"\r\n#include \"GPU/QnnGpuCommon.h\"\r\n#include \"DSP/QnnDspCommon.h\"\r\n#include \"HTP/QnnHtpCommon.h\"\r\n#include \"HTP/QnnHtpContext.h\"\r\n#include \"HTP/QnnHtpPerfInfrastructure.h\"\r\n#include \"HTP/QnnHtpSystemContext.h\"\r\n#include \"IR/QnnIrCommon.h\"\r\n#include \"IR/QnnIrGraph.h\"\r\n#include \"Saver/QnnSaver.h\"\r\n#include \"Saver/QnnSaverCommon.h\"\r\n#include <gsl/gsl>\r\n\r\n#include \"core/providers/qnn/ort_api.h\"\r\n#include \"core/providers/qnn/qnn_allocator.h\"\r\n#include \"core/providers/qnn/qnn_telemetry.h\"\r\n#include \"core/providers/qnn/shared_context.h\"\r\n#include \"core/providers/qnn/builder/onnx_ctx_model_helper.h\"\r\n#include \"core/providers/qnn/builder/qnn_configs_helper.h\"\r\n#include \"core/providers/qnn/builder/qnn_utils.h\"\r\n\r\n// Flag to determine if Backend should do node validation for each opNode added\r\n#define DO_GRAPH_NODE_VALIDATIONS 1\r\n\r\nnamespace onnxruntime {\r\nnamespace qnn {\r\n\r\ntypedef Qnn_ErrorHandle_t (*QnnInterfaceGetProvidersFn_t)(const QnnInterface_t*** providerList,\r\n                                                          uint32_t* numProviders);\r\ntypedef Qnn_ErrorHandle_t (*QnnSystemInterfaceGetProvidersFn_t)(const QnnSystemInterface_t*** providerList,\r\n                                                                uint32_t* numProviders);\r\n\r\nstatic Qnn_Version_t GetQnnInterfaceApiVersion(const QnnInterface_t* qnn_interface) {\r\n  return qnn_interface->apiVersion.coreApiVersion;\r\n}\r\n\r\nstatic Qnn_Version_t GetQnnInterfaceApiVersion(const QnnSystemInterface_t* qnn_interface) {\r\n  return qnn_interface->systemApiVersion;\r\n}\r\n\r\nstatic const char* DlError() {\r\n#ifdef _WIN32\r\n  return \"\";\r\n#else\r\n  return ::dlerror();\r\n#endif\r\n}\r\n\r\n// Workaround for a missing comma in QNN_IR_GRAPH_CUSTOM_CONFIG_INIT.\r\nstatic QnnIrGraph_CustomConfig_t EmptyIrGraphConfig() {\r\n  return {\r\n      QNN_IR_GRAPH_CONFIG_OPTION_SERIALIZATION, {QNN_IR_GRAPH_SERIALIZATION_TYPE_FLAT_BUFFER, \"\"}};\r\n}\r\n\r\nclass QnnIrConfig : public QnnSerializerConfig {\r\n public:\r\n  QnnIrConfig(std::string backend_path, std::string dlc_dir)\r\n      : QnnSerializerConfig(std::move(backend_path)), dlc_dir_(std::move(dlc_dir)), configs_builder_(MakeConfigsBuilder()) {\r\n  }\r\n\r\n  const QnnGraph_Config_t** Configure() override {\r\n    auto configs_builder = MakeConfigsBuilder();\r\n\r\n    std::filesystem::path dlc_path = (dlc_dir_ / (GetGraphName() + \".dlc\"));\r\n    std::string dlc_path_str = dlc_path.string();\r\n    gsl::not_null<QnnIrGraph_CustomConfig_t*> dlc_path_config = configs_builder.PushCustomConfig();\r\n    dlc_path_config->option = QNN_IR_GRAPH_CONFIG_OPTION_SERIALIZATION;\r\n    dlc_path_config->serializationOption.serializationType = QNN_IR_GRAPH_SERIALIZATION_TYPE_FLAT_BUFFER;\r\n    dlc_path_config->serializationOption.outputPath = dlc_path_str.c_str();\r\n\r\n    gsl::not_null<QnnGraph_Config_t*> dlc_path_custom_config = configs_builder.PushConfig();\r\n    dlc_path_custom_config->option = QNN_GRAPH_CONFIG_OPTION_CUSTOM;\r\n    dlc_path_custom_config->customConfig = dlc_path_config;\r\n\r\n    std::filesystem::create_directories(dlc_path);\r\n\r\n    // Keep the pointer to dlc_path_str's null-terminated string alive.\r\n    std::swap(dlc_path_str, dlc_path_str_);\r\n\r\n    std::swap(configs_builder, configs_builder_);\r\n    return configs_builder_.GetQnnConfigs();\r\n  }\r\n\r\n  bool SupportsArbitraryGraphConfigs() const override {\r\n    return false;\r\n  }\r\n\r\n private:\r\n  static QnnConfigsBuilder<QnnGraph_Config_t, QnnIrGraph_CustomConfig_t> MakeConfigsBuilder() {\r\n    return QnnConfigsBuilder<QnnGraph_Config_t, QnnIrGraph_CustomConfig_t>(QNN_GRAPH_CONFIG_INIT, EmptyIrGraphConfig());\r\n  }\r\n\r\n  std::filesystem::path dlc_dir_;\r\n  std::string dlc_path_str_;\r\n  QnnConfigsBuilder<QnnGraph_Config_t, QnnIrGraph_CustomConfig_t> configs_builder_;\r\n};\r\n\r\nclass QnnSaverConfig : public QnnSerializerConfig {\r\n public:\r\n  QnnSaverConfig(std::string backend_path) : QnnSerializerConfig(std::move(backend_path)) {}\r\n\r\n  const QnnGraph_Config_t** Configure() override {\r\n    return nullptr;\r\n  }\r\n\r\n  bool SupportsArbitraryGraphConfigs() const override {\r\n    return true;\r\n  }\r\n};\r\n\r\nQnnSerializerConfig::~QnnSerializerConfig() = default;\r\n\r\nQnnSerializerConfig::QnnSerializerConfig(std::string backend_path)\r\n    : backend_path_(std::move(backend_path)) {}\r\n\r\nstd::unique_ptr<QnnSerializerConfig> QnnSerializerConfig::CreateIr(std::string backend_path, std::string dlc_dir) {\r\n  return std::make_unique<QnnIrConfig>(std::move(backend_path), std::move(dlc_dir));\r\n}\r\n\r\nstd::unique_ptr<QnnSerializerConfig> QnnSerializerConfig::CreateSaver(std::string backend_path) {\r\n  return std::make_unique<QnnSaverConfig>(std::move(backend_path));\r\n}\r\n\r\nconst std::string& QnnSerializerConfig::GetBackendPath() const {\r\n  return backend_path_;\r\n}\r\n\r\nconst std::string& QnnSerializerConfig::GetGraphName() const {\r\n  return graph_name_;\r\n}\r\n\r\nvoid QnnSerializerConfig::SetGraphName(std::string graph_name) {\r\n  graph_name_ = std::move(graph_name);\r\n}\r\n\r\nStatus ReadBinaryFromFile(const std::string& file_path, uint8_t* buffer, size_t buffer_size) {\r\n  ORT_RETURN_IF(nullptr == buffer, \"Binary buffer is nullptr\");\r\n  std::ifstream in(file_path, std::ifstream::binary);\r\n  ORT_RETURN_IF(!in, \"Failed to open input file: \", file_path.c_str());\r\n  ORT_RETURN_IF(!in.read(reinterpret_cast<char*>(buffer), buffer_size), \"Failed to read the contents of: \", file_path.c_str());\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::ParseLoraConfig(std::string lora_config_path) {\r\n  LOGS_DEFAULT(INFO) << \"Acquiring the QnnInterface \" << lora_config_path;\r\n\r\n  // QNN Lora Config file format should be a single line, with the graph name first,\r\n  // followed by the qnn lora context binary path, separated by a semicolon (;)\r\n  // Example: <graph_name>;<binary_path>\r\n  LOGS_DEFAULT(INFO) << \"Loading Lora Config \" << lora_config_path;\r\n  std::ifstream file(lora_config_path);\r\n  std::string line;\r\n\r\n  if (file.is_open()) {\r\n    if (std::getline(file, line)) {\r\n      std::istringstream ss(line);\r\n      std::string graph_name;\r\n      std::string lora_adapter_bin_path;\r\n\r\n      if (std::getline(ss, graph_name, ';') && std::getline(ss, lora_adapter_bin_path)) {\r\n        size_t buffer_size = std::filesystem::file_size(lora_adapter_bin_path.c_str());\r\n\r\n        ORT_RETURN_IF(0 == buffer_size, \"Received path to an empty file. Nothing to deserialize.\");\r\n        std::unique_ptr<uint8_t[]> buffer = std::make_unique<uint8_t[]>(buffer_size);\r\n        void* voidBufferPtr = static_cast<void*>(buffer.get());\r\n        QnnContext_Buffer_t contextBuffer{QNN_CONTEXT_BUFFER_VERSION_1,\r\n                                          {QNN_CONTEXTMEMTYPE_RAW, {{voidBufferPtr, buffer_size}}}};\r\n\r\n        auto status = ReadBinaryFromFile(lora_adapter_bin_path,\r\n                                         reinterpret_cast<uint8_t*>(buffer.get()),\r\n                                         buffer_size);\r\n\r\n        ORT_RETURN_IF(status != Status::OK(), \"Failed to read binary data.\");\r\n        Qnn_GraphHandle_t graph;\r\n        bool graph_retrieve_success = false;\r\n        for (size_t cIdx = 0; cIdx < contexts_.size(); cIdx++) {\r\n          auto graph_retrieve_rt = qnn_interface_.graphRetrieve(contexts_[cIdx], graph_name.c_str(), &graph);\r\n          if (QNN_SUCCESS != graph_retrieve_rt) {\r\n            continue;\r\n          }\r\n\r\n          graph_retrieve_success = true;\r\n\r\n          auto context_apply_binary_section_rt = qnn_interface_.contextApplyBinarySection(\r\n              contexts_[cIdx], graph, QNN_CONTEXT_SECTION_UPDATABLE, &contextBuffer, profile_backend_handle_, nullptr);\r\n          ORT_RETURN_IF(QNN_SUCCESS != context_apply_binary_section_rt, \"Failed to apply binary section.\");\r\n          break;\r\n        }\r\n        ORT_RETURN_IF_NOT(graph_retrieve_success, \"Failed to retrieve graph: \", graph_name, \" and apply binary section.\");\r\n      }\r\n    }\r\n    file.close();\r\n  } else {\r\n    LOGS_DEFAULT(ERROR) << \"Unable to load Lora Config \" << lora_config_path;\r\n  }\r\n\r\n  return Status::OK();\r\n}\r\n\r\ntemplate <typename F, class T>\r\nStatus QnnBackendManager::GetQnnInterfaceProvider(const char* lib_path,\r\n                                                  const char* interface_provider_name,\r\n                                                  void** backend_lib_handle,\r\n                                                  Qnn_Version_t req_version,\r\n                                                  T** interface_provider) {\r\n  std::string error_msg;\r\n  *backend_lib_handle = LoadLib(lib_path,\r\n                                static_cast<int>(DlOpenFlag::DL_NOW) | static_cast<int>(DlOpenFlag::DL_GLOBAL),\r\n                                error_msg);\r\n  ORT_RETURN_IF(nullptr == *backend_lib_handle, \"Unable to load backend, error: \", error_msg, \" \", DlError());\r\n\r\n  // Get QNN Interface providers\r\n  F GetInterfaceProviders{nullptr};\r\n  GetInterfaceProviders = ResolveSymbol<F>(*backend_lib_handle, interface_provider_name, *logger_);\r\n  ORT_RETURN_IF(nullptr == GetInterfaceProviders, \"Failed to get QNN providers!\");\r\n\r\n  T** interface_providers{nullptr};\r\n  uint32_t num_providers{0};\r\n\r\n  auto result = GetInterfaceProviders((const T***)&interface_providers, &num_providers);\r\n  ORT_RETURN_IF((QNN_SUCCESS != result || nullptr == *interface_providers || 0 == num_providers),\r\n                \"Failed to get QNN providers.\");\r\n\r\n  bool found_valid_interface{false};\r\n  for (size_t pIdx = 0; pIdx < num_providers; pIdx++) {\r\n    Qnn_Version_t interface_version = GetQnnInterfaceApiVersion(interface_providers[pIdx]);\r\n\r\n    LOGS_DEFAULT(VERBOSE) << lib_path << \" interface version: \" << interface_version.major << \".\"\r\n                          << interface_version.minor << \".\" << interface_version.patch;\r\n\r\n    // Check the interface's API version against the required version.\r\n    // Major versions must match. The interface's minor version must be greater OR equal with a suitable patch version.\r\n    if (interface_version.major == req_version.major) {\r\n      bool minor_and_patch_version_ok = (interface_version.minor > req_version.minor) ||\r\n                                        (interface_version.minor == req_version.minor &&\r\n                                         interface_version.patch >= req_version.patch);\r\n      if (minor_and_patch_version_ok) {\r\n        found_valid_interface = true;\r\n        *interface_provider = interface_providers[pIdx];\r\n        break;\r\n      }\r\n    }\r\n  }\r\n\r\n  ORT_RETURN_IF_NOT(found_valid_interface, \"Unable to find a valid interface for \", lib_path);\r\n\r\n  return Status::OK();\r\n}\r\n\r\nvoid QnnBackendManager::SetQnnBackendType(uint32_t backend_id) {\r\n  switch (backend_id) {\r\n    case QNN_BACKEND_ID_CPU:\r\n      qnn_backend_type_ = QnnBackendType::CPU;\r\n      break;\r\n    case QNN_BACKEND_ID_GPU:\r\n      qnn_backend_type_ = QnnBackendType::GPU;\r\n      break;\r\n    case QNN_BACKEND_ID_DSP:\r\n      qnn_backend_type_ = QnnBackendType::DSP;\r\n      break;\r\n    case QNN_BACKEND_ID_HTP:\r\n      qnn_backend_type_ = QnnBackendType::HTP;\r\n      break;\r\n    case QNN_BACKEND_ID_IR:\r\n    case QNN_BACKEND_ID_SAVER:\r\n      qnn_backend_type_ = QnnBackendType::SERIALIZER;\r\n      break;\r\n    default:\r\n      qnn_backend_type_ = QnnBackendType::CPU;\r\n      break;\r\n  }\r\n}\r\n\r\nStatus QnnBackendManager::LoadBackend() {\r\n  QnnInterface_t* backend_interface_provider{nullptr};\r\n  auto rt = GetQnnInterfaceProvider<QnnInterfaceGetProvidersFn_t,\r\n                                    QnnInterface_t>(backend_path_.c_str(),\r\n                                                    \"QnnInterface_getProviders\",\r\n                                                    &backend_lib_handle_,\r\n                                                    {QNN_API_VERSION_MAJOR,\r\n                                                     QNN_API_VERSION_MINOR,\r\n                                                     QNN_API_VERSION_PATCH},\r\n                                                    &backend_interface_provider);\r\n  ORT_RETURN_IF_ERROR(rt);\r\n  qnn_interface_ = backend_interface_provider->QNN_INTERFACE_VER_NAME;\r\n  auto backend_id = backend_interface_provider->backendId;\r\n  SetQnnBackendType(backend_id);\r\n\r\n  Qnn_Version_t backend_interface_version = GetQnnInterfaceApiVersion(backend_interface_provider);\r\n  LOGS_DEFAULT(INFO) << \"Found valid interface, version: \" << backend_interface_version.major\r\n                     << \".\" << backend_interface_version.minor << \".\" << backend_interface_version.patch\r\n                     << \" backend provider name: \" << backend_interface_provider->providerName\r\n                     << \" backend id: \" << backend_id;\r\n\r\n  return Status::OK();\r\n}\r\n\r\nQnnSerializerConfig* QnnBackendManager::GetQnnSerializerConfig() {\r\n  return qnn_serializer_config_.get();\r\n}\r\n\r\n// Loads the intended backend (e.g., HTP, CPU, etc) to get its type, and then\r\n// sets QnnSaver or QnnIr as the active backend. QNN op builders will still see the intended backend\r\n// (e.g., HTP) as the backend type to ensure they emit the expected QNN API calls. Note, however, that\r\n// calls to QnnBackend_validateOpConfig will be to the saver backend, not the \"intended\" one.\r\n//\r\n// QnnSaver and QnnIr are \"debugging\" backends that serializes all QNN API calls (and weights) into\r\n// local files: Saver dumps to C++ sources and Ir to .dlc archives.\r\n// This information can be used to debug issues by replaying QNN API calls with another backend.\r\nStatus QnnBackendManager::LoadQnnSerializerBackend() {\r\n  void* backend_lib_handle = nullptr;\r\n\r\n  // Helper that unloads the intended backend library handle when the `unload_backend_lib` variable\r\n  // goes out of scope. Similar to `defer` in other languages.\r\n  auto unload_backend_lib = gsl::finally([&] {\r\n    if (backend_lib_handle != nullptr) {\r\n      auto result = UnloadLib(backend_lib_handle);\r\n      if (Status::OK() != result) {\r\n        ORT_THROW(\"Failed to unload backend library.\");\r\n      }\r\n    }\r\n  });\r\n\r\n  // Load the intended backend (e.g., HTP, CPU) to ensure it is valid and to get its type.\r\n  QnnInterface_t* backend_interface_provider{nullptr};\r\n  auto rt = GetQnnInterfaceProvider<QnnInterfaceGetProvidersFn_t,\r\n                                    QnnInterface_t>(backend_path_.c_str(),\r\n                                                    \"QnnInterface_getProviders\",\r\n                                                    &backend_lib_handle,\r\n                                                    {QNN_API_VERSION_MAJOR,\r\n                                                     QNN_API_VERSION_MINOR,\r\n                                                     QNN_API_VERSION_PATCH},\r\n                                                    &backend_interface_provider);\r\n  ORT_RETURN_IF_ERROR(rt);\r\n\r\n  // Set the \"intended\" backend type so that QNN builders still make the expected QNN API calls.\r\n  auto backend_id = backend_interface_provider->backendId;\r\n  SetQnnBackendType(backend_id);\r\n\r\n  // Load the serializer backend and set it as the activate backend.\r\n  QnnInterface_t* serializer_interface_provider{nullptr};\r\n  auto saver_rt = GetQnnInterfaceProvider<QnnInterfaceGetProvidersFn_t,\r\n                                          QnnInterface_t>(qnn_serializer_config_->GetBackendPath().c_str(),\r\n                                                          \"QnnInterface_getProviders\",\r\n                                                          &backend_lib_handle_,  // NOTE: QnnSaver/Ir library handle is set\r\n                                                          {QNN_API_VERSION_MAJOR,\r\n                                                           QNN_API_VERSION_MINOR,\r\n                                                           QNN_API_VERSION_PATCH},\r\n                                                          &serializer_interface_provider);\r\n  ORT_RETURN_IF_ERROR(saver_rt);\r\n  qnn_interface_ = serializer_interface_provider->QNN_INTERFACE_VER_NAME;  // NOTE: QnnSaver/Ir will provide the interfaces\r\n\r\n  Qnn_Version_t backend_interface_version = GetQnnInterfaceApiVersion(backend_interface_provider);\r\n  Qnn_Version_t serializer_interface_version = GetQnnInterfaceApiVersion(serializer_interface_provider);\r\n\r\n  LOGS_DEFAULT(INFO) << \"Using QnnSaver/Ir version: \" << serializer_interface_version.major << \".\"\r\n                     << serializer_interface_version.minor << \".\" << serializer_interface_version.patch\r\n                     << \" provider name : \" << serializer_interface_provider->providerName;\r\n\r\n  LOGS_DEFAULT(INFO) << \"Intended backend provider name: \" << backend_interface_provider->providerName\r\n                     << \" backend id: \" << backend_id\r\n                     << \" interface version: \" << backend_interface_version.major\r\n                     << \".\" << backend_interface_version.minor << \".\" << backend_interface_version.patch;\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::LoadQnnSystemLib() {\r\n  if (!system_lib_loaded_) {\r\n#ifdef _WIN32\r\n    std::string system_lib_file = \"QnnSystem.dll\";\r\n#else\r\n    std::string system_lib_file = \"libQnnSystem.so\";\r\n#endif  // #ifdef _WIN32\r\n    LOGS_DEFAULT(INFO) << \"Loading QnnSystem lib\";\r\n    std::filesystem::path lib_file_path(backend_path_.c_str());\r\n    std::string sys_file_path(lib_file_path.remove_filename().string() + system_lib_file);\r\n    QnnSystemInterface_t* system_interface_provider{nullptr};\r\n    auto rt = GetQnnInterfaceProvider<QnnSystemInterfaceGetProvidersFn_t,\r\n                                      QnnSystemInterface_t>(sys_file_path.c_str(),\r\n                                                            \"QnnSystemInterface_getProviders\",\r\n                                                            &system_lib_handle_,\r\n                                                            {QNN_SYSTEM_API_VERSION_MAJOR,\r\n                                                             QNN_SYSTEM_API_VERSION_MINOR,\r\n                                                             QNN_SYSTEM_API_VERSION_PATCH},\r\n                                                            &system_interface_provider);\r\n    ORT_RETURN_IF_ERROR(rt);\r\n    Qnn_Version_t system_interface_version = GetQnnInterfaceApiVersion(system_interface_provider);\r\n    qnn_sys_interface_ = system_interface_provider->QNN_SYSTEM_INTERFACE_VER_NAME;\r\n\r\n    LOGS_DEFAULT(INFO) << \"Found valid system interface, version: \" << system_interface_version.major\r\n                       << \".\" << system_interface_version.minor\r\n                       << \" backend provider name: \" << system_interface_provider->providerName;\r\n\r\n    system_lib_loaded_ = true;\r\n  }\r\n  return Status::OK();\r\n}\r\n\r\nvoid QnnLogging(const char* format,\r\n                QnnLog_Level_t level,\r\n                uint64_t timestamp,\r\n                va_list argument_parameter) {\r\n  ORT_UNUSED_PARAMETER(level);\r\n  ORT_UNUSED_PARAMETER(timestamp);\r\n\r\n  if (!::onnxruntime::logging::LoggingManager::HasDefaultLogger()) {\r\n    // QNN may call this logging callback at any point, which means that we need to explicitly check\r\n    // that the default logger has been initialized before trying to use it (otherwise get segfault).\r\n    return;\r\n  }\r\n\r\n  const auto& logger = ::onnxruntime::logging::LoggingManager::DefaultLogger();\r\n  const auto severity = ::onnxruntime::logging::Severity::kVERBOSE;\r\n  const auto data_type = ::onnxruntime::logging::DataType::SYSTEM;\r\n\r\n  if (logger.OutputIsEnabled(severity, data_type)) {\r\n    auto log_capture = Factory<logging::Capture>::Create(logger,\r\n                                                         severity,\r\n                                                         logging::Category::onnxruntime,\r\n                                                         data_type,\r\n                                                         ORT_WHERE);\r\n    log_capture->ProcessPrintf(format, argument_parameter);\r\n  }\r\n}\r\n\r\nStatus QnnBackendManager::InitializeQnnLog(const logging::Logger& logger) {\r\n  logger_ = &logger;\r\n\r\n  // Set Qnn log level align with Ort log level\r\n  auto ort_log_level = logger_->GetSeverity();\r\n  QnnLog_Level_t qnn_log_level = MapOrtSeverityToQNNLogLevel(ort_log_level);\r\n  LOGS(*logger_, VERBOSE) << \"Set Qnn log level: \" << qnn_log_level;\r\n\r\n  // NOTE: Even if logCreate() fails and QNN does not return a valid log_handle_, QNN may still\r\n  // call the QnnLogging() callback. So, we have to make sure that QnnLogging() can handle calls\r\n  // in which ORT logging is not available.\r\n  Qnn_ErrorHandle_t result = qnn_interface_.logCreate(QnnLogging, qnn_log_level, &log_handle_);\r\n\r\n  if (result != QNN_SUCCESS) {\r\n    switch (result) {\r\n      case QNN_COMMON_ERROR_NOT_SUPPORTED:\r\n        LOGS(*logger_, ERROR) << \"Logging not supported in the QNN backend.\";\r\n        break;\r\n      case QNN_LOG_ERROR_INVALID_ARGUMENT:\r\n        LOGS(*logger_, ERROR) << \"Invalid argument provided to QnnLog_create.\";\r\n        break;\r\n      case QNN_LOG_ERROR_MEM_ALLOC:\r\n        LOGS(*logger_, ERROR) << \"Memory allocation error during QNN logging initialization.\";\r\n        break;\r\n      case QNN_LOG_ERROR_INITIALIZATION:\r\n        LOGS(*logger_, ERROR) << \"Initialization of logging failed in the QNN backend.\";\r\n        break;\r\n      default:\r\n        LOGS(*logger_, WARNING) << \"Unknown error occurred while initializing logging in the QNN backend.\";\r\n        break;\r\n    }\r\n  }\r\n\r\n  ORT_RETURN_IF(QNN_BACKEND_NO_ERROR != result, \"Failed to initialize logging in the QNN backend. Error: \", QnnErrorHandleToString(result));\r\n  return Status::OK();\r\n}\r\n\r\nQnnLog_Level_t QnnBackendManager::MapOrtSeverityToQNNLogLevel(logging::Severity ort_log_level) {\r\n  // Map ORT log severity to Qnn log level\r\n  switch (ort_log_level) {\r\n    case logging::Severity::kVERBOSE: {\r\n      switch ((GetQnnBackendType())) {\r\n        case QnnBackendType::GPU:\r\n          // Currently GPU needs this log level to work.\r\n          // This switch will be removed once this is resolved.\r\n          return QNN_LOG_LEVEL_DEBUG;\r\n        default:\r\n          return QNN_LOG_LEVEL_VERBOSE;\r\n      }\r\n    }\r\n    case logging::Severity::kINFO:\r\n      return QNN_LOG_LEVEL_INFO;\r\n    case logging::Severity::kWARNING:\r\n      return QNN_LOG_LEVEL_WARN;\r\n    case logging::Severity::kERROR:\r\n    case logging::Severity::kFATAL:\r\n    default:\r\n      return QNN_LOG_LEVEL_ERROR;\r\n  }\r\n}\r\n\r\nStatus QnnBackendManager::ResetQnnLogLevel(std::optional<logging::Severity> ort_log_level) {\r\n  std::lock_guard<std::recursive_mutex> lock(logger_recursive_mutex_);\r\n  if (!backend_setup_completed_ || logger_ == nullptr) {\r\n    return Status::OK();\r\n  }\r\n  ORT_RETURN_IF(nullptr == log_handle_, \"Unable to update QNN Log Level. Invalid QNN log handle.\");\r\n\r\n  logging::Severity actual_log_level = ort_log_level.has_value() ? *ort_log_level : logger_->GetSeverity();\r\n  QnnLog_Level_t qnn_log_level = MapOrtSeverityToQNNLogLevel(actual_log_level);\r\n\r\n  LOGS(*logger_, INFO) << \"Updating Qnn log level to: \" << qnn_log_level;\r\n\r\n  // Use the QnnLog_setLogLevel API to set the new log level\r\n  Qnn_ErrorHandle_t result = qnn_interface_.logSetLogLevel(log_handle_, qnn_log_level);\r\n  if (QNN_SUCCESS != result) {\r\n    if (result == QNN_LOG_ERROR_INVALID_ARGUMENT) {\r\n      LOGS(*logger_, ERROR) << \"Invalid log level argument provided to QnnLog_setLogLevel.\";\r\n    } else if (result == QNN_LOG_ERROR_INVALID_HANDLE) {\r\n      LOGS(*logger_, ERROR) << \"Invalid log handle provided to QnnLog_setLogLevel.\";\r\n    }\r\n  }\r\n  ORT_RETURN_IF(QNN_BACKEND_NO_ERROR != result,\r\n                \"Failed to set log level in Qnn backend. Error: \", QnnErrorHandleToString(result));\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::InitializeBackend() {\r\n  if (true == backend_initialized_) {\r\n    LOGS_DEFAULT(INFO) << \"Backend initialized already.\";\r\n    return Status::OK();\r\n  }\r\n\r\n  Qnn_ErrorHandle_t result = qnn_interface_.backendCreate(log_handle_, (const QnnBackend_Config_t**)backend_config_, &backend_handle_);\r\n  ORT_RETURN_IF(QNN_BACKEND_NO_ERROR != result, \"Failed to initialize backend. Error: \", QnnErrorHandleToString(result));\r\n\r\n  backend_initialized_ = true;\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::ShutdownBackend() {\r\n  if (false == backend_initialized_) {\r\n    return Status::OK();\r\n  }\r\n\r\n  if (nullptr != qnn_interface_.backendFree) {\r\n    ORT_RETURN_IF(QNN_BACKEND_NO_ERROR != qnn_interface_.backendFree(backend_handle_),\r\n                  \"Failed to shutdown backend!\");\r\n  }\r\n\r\n  backend_initialized_ = false;\r\n\r\n  return Status::OK();\r\n}\r\n\r\nbool QnnBackendManager::IsDevicePropertySupported() {\r\n  if (nullptr != qnn_interface_.propertyHasCapability) {\r\n    auto rt = qnn_interface_.propertyHasCapability(QNN_PROPERTY_GROUP_DEVICE);\r\n    if (QNN_PROPERTY_NOT_SUPPORTED == rt || QNN_PROPERTY_ERROR_UNKNOWN_KEY == rt) {\r\n      LOGS_DEFAULT(INFO) << \"Device property not supported or unknown to backend.\";\r\n      return false;\r\n    }\r\n  }\r\n\r\n  return true;\r\n}\r\n\r\nStatus QnnBackendManager::CreateDevice() {\r\n  if (true == device_created_) {\r\n    LOGS_DEFAULT(INFO) << \"Device initialized already.\";\r\n    return Status::OK();\r\n  }\r\n\r\n  // Create device if its property supported\r\n  if (!IsDevicePropertySupported()) {\r\n    LOGS_DEFAULT(INFO) << \"Skip to create device.\";\r\n    return Status::OK();\r\n  }\r\n\r\n  qnn::QnnConfigsBuilder<QnnDevice_Config_t, QnnHtpDevice_CustomConfig_t> device_configs_builder(QNN_DEVICE_CONFIG_INIT,\r\n                                                                                                 {});\r\n  if (qnn_backend_type_ == QnnBackendType::HTP) {\r\n    // Set SoC Model. The *enum* Qnn_SocModel_t is deprecated and will not be updated in the future. Therefore,\r\n    // must use the latest SDK documentation to get the SoC model of the latest HW.\r\n    if (soc_model_ != QNN_SOC_MODEL_UNKNOWN) {\r\n      gsl::not_null<QnnHtpDevice_CustomConfig_t*> custom_config = device_configs_builder.PushCustomConfig();\r\n      custom_config->option = QNN_HTP_DEVICE_CONFIG_OPTION_SOC;\r\n      custom_config->socModel = soc_model_;\r\n\r\n      gsl::not_null<QnnDevice_Config_t*> device_config = device_configs_builder.PushConfig();\r\n      device_config->option = QNN_DEVICE_CONFIG_OPTION_CUSTOM;\r\n      device_config->customConfig = custom_config;\r\n    }\r\n\r\n    // Set the minimum HTP architecture. The driver will use ops that are compatible with this minimum architecture.\r\n    if (htp_arch_ != QNN_HTP_DEVICE_ARCH_NONE) {\r\n      gsl::not_null<QnnHtpDevice_CustomConfig_t*> custom_config = device_configs_builder.PushCustomConfig();\r\n      custom_config->option = QNN_HTP_DEVICE_CONFIG_OPTION_ARCH;\r\n      custom_config->arch.arch = htp_arch_;\r\n      custom_config->arch.deviceId = device_id_;\r\n\r\n      gsl::not_null<QnnDevice_Config_t*> device_config = device_configs_builder.PushConfig();\r\n      device_config->option = QNN_DEVICE_CONFIG_OPTION_CUSTOM;\r\n      device_config->customConfig = custom_config;\r\n    }\r\n  }\r\n\r\n  LOGS_DEFAULT(INFO) << \"Create device.\";\r\n  if (nullptr != qnn_interface_.deviceCreate) {\r\n    Qnn_ErrorHandle_t result = qnn_interface_.deviceCreate(log_handle_, device_configs_builder.GetQnnConfigs(), &device_handle_);\r\n    if (QNN_SUCCESS != result) {\r\n      return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, \"Failed to create device. Error: \", QnnErrorHandleToString(result));\r\n    }\r\n  }\r\n  device_created_ = true;\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::ReleaseDevice() {\r\n  if (false == device_created_) {\r\n    return Status::OK();\r\n  }\r\n\r\n  if (nullptr != qnn_interface_.deviceFree) {\r\n    Qnn_ErrorHandle_t result = qnn_interface_.deviceFree(device_handle_);\r\n    if (QNN_SUCCESS != result) {\r\n      return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, \"Failed to release device. Error: \", QnnErrorHandleToString(result));\r\n    }\r\n  }\r\n\r\n  device_created_ = false;\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::InitializeProfiling() {\r\n  profiling_level_merge_ = profiling_level_;\r\n  // use profiling level from ETW if ETW is enabled\r\n  if (profiling_level_etw_ != ProfilingLevel::INVALID) {\r\n    profiling_level_merge_ = profiling_level_etw_;\r\n  }\r\n\r\n  if (ProfilingLevel::OFF == profiling_level_merge_ || ProfilingLevel::INVALID == profiling_level_merge_) {\r\n    LOGS_DEFAULT(INFO) << \"Profiling turned off.\";\r\n    return Status::OK();\r\n  }\r\n\r\n  bool enable_optrace = false;\r\n  QnnProfile_Level_t qnn_profile_level = QNN_PROFILE_LEVEL_BASIC;\r\n  if (ProfilingLevel::BASIC == profiling_level_merge_) {\r\n    qnn_profile_level = QNN_PROFILE_LEVEL_BASIC;\r\n    LOGS_DEFAULT(VERBOSE) << \"Profiling level set to basic.\";\r\n  } else if (ProfilingLevel::DETAILED == profiling_level_merge_) {\r\n    qnn_profile_level = QNN_PROFILE_LEVEL_DETAILED;\r\n    LOGS_DEFAULT(VERBOSE) << \"Profiling level set to detailed.\";\r\n  } else if (ProfilingLevel::OPTRACE == profiling_level_merge_) {\r\n    qnn_profile_level = QNN_PROFILE_LEVEL_DETAILED;\r\n    enable_optrace = true;\r\n    LOGS_DEFAULT(VERBOSE) << \"Profiling level set to optrace.\";\r\n  }\r\n\r\n  Qnn_ErrorHandle_t result = qnn_interface_.profileCreate(backend_handle_, qnn_profile_level, &profile_backend_handle_);\r\n  ORT_RETURN_IF(QNN_PROFILE_NO_ERROR != result, \"Failed to create QNN profile! Error: \", QnnErrorHandleToString(result));\r\n\r\n#ifdef QNN_SYSTEM_PROFILE_API_ENABLED\r\n  profiling_enabled_ = true;\r\n  ORT_RETURN_IF_ERROR(LoadQnnSystemLib());\r\n\r\n  if (enable_optrace) {\r\n    QnnProfile_Config_t optrace_config = QNN_PROFILE_CONFIG_INIT;\r\n    optrace_config.option = QNN_PROFILE_CONFIG_OPTION_ENABLE_OPTRACE;\r\n    optrace_config.enableOptrace = enable_optrace;\r\n\r\n    const QnnProfile_Config_t* profile_configs[] = {&optrace_config, nullptr};\r\n    result = qnn_interface_.profileSetConfig(profile_backend_handle_, profile_configs);\r\n\r\n    ORT_RETURN_IF(QNN_PROFILE_NO_ERROR != result, \"Failed to enable op trace! Error: \", QnnErrorHandleToString(result));\r\n  }\r\n#else\r\n  if (enable_optrace) {\r\n    LOGS_DEFAULT(WARNING) << \"Profiling level set to optrace, but QNN SDK Version is older than 2.29.0. \"\r\n                          << \"Profiling level will be set to detailed instead.\";\r\n  }\r\n#endif\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::ReleaseProfilehandle() {\r\n  // Free Profiling object if it was created\r\n  if (nullptr != profile_backend_handle_) {\r\n    ORT_RETURN_IF(QNN_PROFILE_NO_ERROR != qnn_interface_.profileFree(profile_backend_handle_),\r\n                  \"Could not free backend profile handle!\");\r\n  }\r\n  profile_backend_handle_ = nullptr;\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::SetProfilingLevelETW(ProfilingLevel profiling_level_etw_param) {\r\n  if (profiling_level_etw_ != profiling_level_etw_param) {\r\n    profiling_level_etw_ = profiling_level_etw_param;\r\n\r\n    auto result = ReleaseProfilehandle();\r\n    if (Status::OK() != result) {\r\n      ORT_THROW(\"Failed to ReleaseProfilehandle for previous QNN profiling\");\r\n    }\r\n\r\n    result = InitializeProfiling();\r\n    if (Status::OK() != result) {\r\n      ORT_THROW(\"Failed to Re-InitializeProfiling for QNN ETW profiling\");\r\n    }\r\n  }\r\n  return Status::OK();\r\n}\r\n\r\nStatus SetQnnContextConfig(ContextPriority context_priority, QnnContext_Config_t& qnn_context_config) {\r\n  qnn_context_config.option = QNN_CONTEXT_CONFIG_OPTION_PRIORITY;\r\n  switch (context_priority) {\r\n    case ContextPriority::LOW: {\r\n      qnn_context_config.priority = QNN_PRIORITY_LOW;\r\n      break;\r\n    }\r\n    case ContextPriority::NORMAL: {\r\n      qnn_context_config.priority = QNN_PRIORITY_NORMAL;\r\n      break;\r\n    }\r\n    case ContextPriority::NORMAL_HIGH: {\r\n      qnn_context_config.priority = QNN_PRIORITY_NORMAL_HIGH;\r\n      break;\r\n    }\r\n    case ContextPriority::HIGH: {\r\n      qnn_context_config.priority = QNN_PRIORITY_HIGH;\r\n      break;\r\n    }\r\n    case ContextPriority::UNDEFINED: {\r\n      return ORT_MAKE_STATUS(ONNXRUNTIME, INVALID_ARGUMENT, \"Invalid Qnn context priority.\");\r\n    }\r\n    default:\r\n      qnn_context_config.priority = QNN_PRIORITY_NORMAL;\r\n  }  // switch\r\n\r\n  return Status::OK();\r\n}\r\n\r\n// callback required to add context handles to class list\r\n// when using contextCreateFromBinaryListAsync()\r\nvoid ContextCreateAsyncCallback(Qnn_ContextHandle_t context,\r\n                                Qnn_GraphHandle_t graph,\r\n                                const char* graphName,\r\n                                QnnContext_createFromBinaryAsyncNotifyType_t notifyType,\r\n                                void* notifyParam,\r\n                                Qnn_ErrorHandle_t status) {\r\n  auto qnn_backend_manager = SharedContext::GetInstance().GetSharedQnnBackendManager();\r\n\r\n  if (context) {\r\n    qnn_backend_manager->ProcessContextFromBinListAsync(context, notifyParam);\r\n  }\r\n\r\n  if (nullptr == graphName || graph || notifyType || status) {\r\n    // Avoid compilation unused var warning error\r\n  }\r\n}\r\n\r\nvoid QnnBackendManager::ProcessContextFromBinListAsync(Qnn_ContextHandle_t context, void* notifyParam) {\r\n  std::lock_guard<std::mutex> guard(ep_context_handle_map_mutex_);\r\n  if (!notifyParam) {\r\n    LOGS(*logger_, WARNING) << \"No known node names associated with context handle: \" << context;\r\n    return;\r\n  }\r\n\r\n  std::vector<std::string>* ep_node_names = reinterpret_cast<std::vector<std::string>*>(notifyParam);\r\n  for (const auto& node_name : *ep_node_names) {\r\n    if (!(ep_context_handle_map_.emplace(node_name, context).second)) {\r\n      LOGS(*logger_, VERBOSE) << \"Unable to map \" << context << \" to \" << node_name;\r\n    }\r\n  }\r\n\r\n  auto s = AddQnnContextHandle(context);\r\n  if (s != Status::OK()) {\r\n    LOGS(*logger_, WARNING) << \"Unable to add context \" << context;\r\n  }\r\n}\r\n\r\nStatus QnnBackendManager::CreateContextVtcmBackupBufferSharingEnabled(std::unordered_map<std::string, std::unique_ptr<std::vector<std::string>>>& context_bin_map) {\r\n#if QNN_API_VERSION_MAJOR == 2 && (QNN_API_VERSION_MINOR >= 26)\r\n  QnnContext_Config_t context_config_resource_sharing = QNN_CONTEXT_CONFIG_INIT;\r\n  QnnHtpContext_CustomConfig_t resource_sharing_custom_config;\r\n  resource_sharing_custom_config.option = QNN_HTP_CONTEXT_CONFIG_OPTION_SHARE_RESOURCES;\r\n  resource_sharing_custom_config.shareResources = true;\r\n  context_config_resource_sharing.option = QNN_CONTEXT_CONFIG_OPTION_CUSTOM;\r\n  context_config_resource_sharing.customConfig = &resource_sharing_custom_config;\r\n\r\n  QnnHtpContext_CustomConfig_t context_config_resource_sharing_opt_type;\r\n  context_config_resource_sharing_opt_type.option = QNN_HTP_CONTEXT_CONFIG_OPTION_SHARE_RESOURCES_OPTIMIZATION_TYPE;\r\n  context_config_resource_sharing_opt_type.shareResOptType = SEQUENTIAL_WITHOUT_VA_OPTIMIZATION;\r\n  QnnContext_Config_t resource_sharing_opt_type_config;\r\n  resource_sharing_opt_type_config.option = QNN_CONTEXT_CONFIG_OPTION_CUSTOM;\r\n  resource_sharing_opt_type_config.customConfig = &context_config_resource_sharing_opt_type;\r\n\r\n  QnnContext_Config_t context_config_weight_sharing = QNN_CONTEXT_CONFIG_INIT;\r\n  QnnHtpContext_CustomConfig_t custom_config;\r\n  custom_config.option = QNN_HTP_CONTEXT_CONFIG_OPTION_WEIGHT_SHARING_ENABLED;\r\n  custom_config.weightSharingEnabled = true;\r\n  context_config_weight_sharing.option = QNN_CONTEXT_CONFIG_OPTION_CUSTOM;\r\n  context_config_weight_sharing.customConfig = &custom_config;\r\n#else\r\n  LOGS(*logger_, WARNING) << \"Called CreateContextVtcmBackupBufferSharingEnabled() but QNN API version is older than 2.26!\";\r\n#endif\r\n  QnnContext_Config_t context_priority_config = QNN_CONTEXT_CONFIG_INIT;\r\n  ORT_RETURN_IF_ERROR(SetQnnContextConfig(context_priority_, context_priority_config));\r\n\r\n  const QnnContext_Config_t* configs[] = { &context_priority_config,\r\n#if QNN_API_VERSION_MAJOR == 2 && (QNN_API_VERSION_MINOR >= 26)\r\n                                           &context_config_resource_sharing,\r\n                                           &resource_sharing_opt_type_config,\r\n                                           &context_config_weight_sharing,\r\n#endif\r\n                                           nullptr };\r\n\r\n  std::vector<QnnContext_Params_t> context_params_list;\r\n  std::vector<QnnContext_ParamsV1_t> context_paramsv1_list;\r\n  std::vector<const QnnContext_Params_t*> context_params_ptr_list;\r\n  std::vector<std::unique_ptr<char[]>> buffer_list;\r\n\r\n  context_params_list.reserve(context_bin_map.size());\r\n  context_params_ptr_list.reserve(context_bin_map.size() + 1);\r\n\r\n  for (auto& it : context_bin_map) {\r\n    auto context_bin_filepath = it.first;\r\n\r\n    std::ifstream cache_file(context_bin_filepath.c_str(), std::ifstream::binary);\r\n    ORT_RETURN_IF(!cache_file || !cache_file.good(), \"Failed to retrieve context binary from: \", context_bin_filepath);\r\n\r\n    cache_file.seekg(0, cache_file.end);\r\n    size_t buffer_size = static_cast<size_t>(cache_file.tellg());\r\n    ORT_RETURN_IF(0 == buffer_size, \"Empty cache file encountered.\");\r\n\r\n    cache_file.seekg(0, cache_file.beg);\r\n    std::unique_ptr<char[]> buffer = std::make_unique<char[]>(buffer_size);\r\n    ORT_RETURN_IF(nullptr == buffer, \"Failed to allocate memory for cache file.\");\r\n    const auto& read_result = cache_file.read(buffer.get(), buffer_size);\r\n    ORT_RETURN_IF(!read_result, \"Failed to read contents from cached context file.\");\r\n\r\n    cache_file.close();\r\n    QnnContext_ParamsV1_t context_params_v1 = {nullptr,\r\n                                               buffer.get(),\r\n                                               buffer_size,\r\n                                               nullptr,\r\n                                               ContextCreateAsyncCallback,\r\n                                               it.second.get()};\r\n\r\n    QnnContext_Params_t context_params = {QnnContext_ParamsVersion_t::QNN_CONTEXT_PARAMS_VERSION_1,\r\n                                          {context_params_v1}};\r\n\r\n    buffer_list.push_back(std::move(buffer));\r\n    context_params_list.push_back(std::move(context_params));\r\n    context_paramsv1_list.push_back(std::move(context_params_v1));\r\n    context_params_ptr_list.push_back(&context_params_list.back());\r\n  }\r\n  context_params_ptr_list.push_back(nullptr);\r\n  auto result = qnn_interface_.contextCreateFromBinaryListAsync(backend_handle_,\r\n                                                                device_handle_,\r\n                                                                context_params_ptr_list.data(),\r\n                                                                configs,\r\n                                                                nullptr);\r\n\r\n  context_params_ptr_list.clear();\r\n  context_paramsv1_list.clear();\r\n  context_params_list.clear();\r\n  buffer_list.clear();\r\n\r\n  ORT_RETURN_IF(QNN_CONTEXT_NO_ERROR != result, \"Failed to create context. Error: \", QnnErrorHandleToString(result), \", Code:\", result);\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::SetContextPriority(ContextPriority context_priority) {\r\n  QnnContext_Config_t context_priority_config = QNN_CONTEXT_CONFIG_INIT;\r\n  ORT_RETURN_IF_ERROR(SetQnnContextConfig(context_priority, context_priority_config));\r\n\r\n  QnnContext_Config_t* configs[] = {&context_priority_config, nullptr};\r\n  for (const auto& context_handle : contexts_) {\r\n    auto result = qnn_interface_.contextSetConfig(context_handle, (const QnnContext_Config_t**)configs);\r\n    ORT_RETURN_IF(QNN_CONTEXT_NO_ERROR != result, \"Failed to set context priority for context handle: \", context_handle);\r\n  }\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::ResetContextPriority() {\r\n  return SetContextPriority(context_priority_);\r\n}\r\n\r\nStatus QnnBackendManager::CreateContext(bool enable_htp_weight_sharing) {\r\n  if (true == context_created_) {\r\n    LOGS_DEFAULT(INFO) << \"Context created already.\";\r\n    return Status::OK();\r\n  }\r\n\r\n  QnnContext_Config_t context_config_weight_sharing = QNN_CONTEXT_CONFIG_INIT;\r\n  QnnHtpContext_CustomConfig_t custom_config;\r\n  custom_config.option = QNN_HTP_CONTEXT_CONFIG_OPTION_WEIGHT_SHARING_ENABLED;\r\n  custom_config.weightSharingEnabled = enable_htp_weight_sharing;\r\n  context_config_weight_sharing.option = QNN_CONTEXT_CONFIG_OPTION_CUSTOM;\r\n  context_config_weight_sharing.customConfig = &custom_config;\r\n\r\n  QnnContext_Config_t context_priority_config = QNN_CONTEXT_CONFIG_INIT;\r\n  ORT_RETURN_IF_ERROR(SetQnnContextConfig(context_priority_, context_priority_config));\r\n\r\n  const QnnContext_Config_t* npu_context_configs[] = {&context_priority_config,\r\n                                                      &context_config_weight_sharing,\r\n                                                      nullptr};\r\n\r\n  const QnnContext_Config_t* empty_context_configs[] = {nullptr};\r\n\r\n  const QnnContext_Config_t** configs = nullptr;\r\n  switch (GetQnnBackendType()) {\r\n    case QnnBackendType::HTP:\r\n    case QnnBackendType::DSP:\r\n      configs = npu_context_configs;\r\n      break;\r\n    case QnnBackendType::GPU:\r\n    case QnnBackendType::SERIALIZER:\r\n      configs = nullptr;\r\n      break;\r\n    default:\r\n      configs = empty_context_configs;\r\n      break;\r\n  }\r\n\r\n  // Not all serialization backends allow for hardware configs to be applied.\r\n  if (qnn_serializer_config_ && !qnn_serializer_config_->SupportsArbitraryGraphConfigs()) {\r\n    configs = nullptr;\r\n  }\r\n\r\n  Qnn_ContextHandle_t context = nullptr;\r\n  Qnn_ErrorHandle_t result = 0;\r\n\r\n  result = qnn_interface_.contextCreate(backend_handle_,\r\n                                        device_handle_,\r\n                                        configs,\r\n                                        &context);\r\n\r\n  ORT_RETURN_IF(QNN_CONTEXT_NO_ERROR != result, \"Failed to create context. Error: \", QnnErrorHandleToString(result), \", Code:\", result);\r\n\r\n  ORT_RETURN_IF_ERROR(AddQnnContextHandle(context));\r\n\r\n  context_created_ = true;\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::ReleaseContext() {\r\n  if (false == context_created_) {\r\n    return Status::OK();\r\n  }\r\n\r\n  // release QNN context handles\r\n  contexts_.clear();\r\n  context_map_.clear();\r\n\r\n  context_created_ = false;\r\n  return Status::OK();\r\n}\r\n\r\nstd::unique_ptr<unsigned char[]> QnnBackendManager::GetContextBinaryBuffer(uint64_t& written_buffer_size) {\r\n  if (nullptr == qnn_interface_.contextGetBinarySize ||\r\n      nullptr == qnn_interface_.contextGetBinary) {\r\n    LOGS(*logger_, ERROR) << \"Failed to get valid function pointer.\";\r\n    return nullptr;\r\n  }\r\n  ORT_ENFORCE(contexts_.size() > 0, \"No valid QNN context!\");\r\n  uint64_t required_buffer_size(0);\r\n  // Generate all graphs in one single context\r\n  Qnn_ErrorHandle_t rt = qnn_interface_.contextGetBinarySize(contexts_[0], &required_buffer_size);\r\n  if (QNN_CONTEXT_NO_ERROR != rt) {\r\n    LOGS(*logger_, ERROR) << \"Failed to get QNN context binary size. Error: \" << QnnErrorHandleToString(rt);\r\n    return nullptr;\r\n  }\r\n\r\n  std::unique_ptr<unsigned char[]> context_buffer = std::make_unique<unsigned char[]>(required_buffer_size);\r\n  if (nullptr == context_buffer) {\r\n    LOGS(*logger_, ERROR) << \"Failed to allocate buffer for context cache.\";\r\n    return nullptr;\r\n  }\r\n\r\n  rt = qnn_interface_.contextGetBinary(contexts_[0],\r\n                                       reinterpret_cast<void*>(context_buffer.get()),\r\n                                       required_buffer_size,\r\n                                       &written_buffer_size);\r\n  if (QNN_CONTEXT_NO_ERROR != rt) {\r\n    LOGS(*logger_, ERROR) << \"Failed to get context binary. Error: \" << QnnErrorHandleToString(rt);\r\n    return nullptr;\r\n  }\r\n\r\n  if (required_buffer_size < written_buffer_size) {\r\n    LOGS(*logger_, ERROR) << \"Context written buffer size: \" << written_buffer_size\r\n                          << \" exceeds allocated buffer size: \" << required_buffer_size;\r\n    return nullptr;\r\n  }\r\n\r\n  LOGS(*logger_, VERBOSE) << \"Get context binary buffer succeed.\";\r\n  return context_buffer;\r\n}\r\n\r\nStatus QnnBackendManager::GetMaxSpillFillBufferSize(unsigned char* buffer,\r\n                                                    uint64_t buffer_length,\r\n                                                    uint64_t& max_spill_fill_buffer_size) {\r\n  max_spill_fill_buffer_size = 0;\r\n  // spill fill starts from 2.28\r\n#if QNN_API_VERSION_MAJOR == 2 && (QNN_API_VERSION_MINOR >= 21)\r\n  bool result = nullptr == qnn_sys_interface_.systemContextCreate ||\r\n                nullptr == qnn_sys_interface_.systemContextGetBinaryInfo ||\r\n                nullptr == qnn_sys_interface_.systemContextFree;\r\n  ORT_RETURN_IF(result, \"Failed to get valid function pointer.\");\r\n\r\n  QnnSystemContext_Handle_t sys_ctx_handle = nullptr;\r\n  auto rt = qnn_sys_interface_.systemContextCreate(&sys_ctx_handle);\r\n  ORT_RETURN_IF(QNN_SUCCESS != rt, \"Failed to create system handle.\");\r\n\r\n  const QnnSystemContext_BinaryInfo_t* binary_info = nullptr;\r\n  Qnn_ContextBinarySize_t binary_info_size{0};\r\n  rt = qnn_sys_interface_.systemContextGetBinaryInfo(sys_ctx_handle,\r\n                                                     static_cast<void*>(buffer),\r\n                                                     buffer_length,\r\n                                                     &binary_info,\r\n                                                     &binary_info_size);\r\n  ORT_RETURN_IF(QNN_SUCCESS != rt, \"Failed to get context binary info.\");\r\n\r\n  // binary_info life cycle is here\r\n  // Binary info to graph info\r\n  // retrieve Qnn graph info from binary info\r\n  ORT_RETURN_IF(nullptr == binary_info, \"Qnn cached binary info is nullptr.\");\r\n  uint32_t graph_count = 0;\r\n  QnnSystemContext_GraphInfo_t* graphs_info = nullptr;\r\n  if (binary_info->version == QNN_SYSTEM_CONTEXT_BINARY_INFO_VERSION_3) {\r\n    graph_count = binary_info->contextBinaryInfoV3.numGraphs;\r\n    graphs_info = binary_info->contextBinaryInfoV3.graphs;\r\n  } else if (binary_info->version == QNN_SYSTEM_CONTEXT_BINARY_INFO_VERSION_2) {\r\n    graph_count = binary_info->contextBinaryInfoV2.numGraphs;\r\n    graphs_info = binary_info->contextBinaryInfoV2.graphs;\r\n  } else if (binary_info->version == QNN_SYSTEM_CONTEXT_BINARY_INFO_VERSION_1) {\r\n    graph_count = binary_info->contextBinaryInfoV1.numGraphs;\r\n    graphs_info = binary_info->contextBinaryInfoV1.graphs;\r\n  } else {\r\n    return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, \"Unsupported context binary info version.\");\r\n  }\r\n\r\n  for (uint32_t i = 0; i < graph_count; ++i) {\r\n    if (graphs_info[i].version == QNN_SYSTEM_CONTEXT_GRAPH_INFO_VERSION_3) {\r\n      auto htp_graph_info = reinterpret_cast<QnnHtpSystemContext_GraphBlobInfo_t*>(graphs_info[i].graphInfoV3.graphBlobInfo);\r\n      if (htp_graph_info->version == QNN_SYSTEM_CONTEXT_HTP_GRAPH_INFO_BLOB_VERSION_V1) {\r\n        auto spill_fill_buffer_size = htp_graph_info->contextBinaryGraphBlobInfoV1.spillFillBufferSize;\r\n        max_spill_fill_buffer_size = spill_fill_buffer_size > max_spill_fill_buffer_size ? spill_fill_buffer_size : max_spill_fill_buffer_size;\r\n      } else {\r\n        LOGS(*logger_, VERBOSE) << \"Unknown context binary graph info blob version.\";\r\n      }\r\n    } else if (graphs_info[i].version == QNN_SYSTEM_CONTEXT_GRAPH_INFO_VERSION_2 ||\r\n               graphs_info[i].version == QNN_SYSTEM_CONTEXT_GRAPH_INFO_VERSION_1) {\r\n      LOGS(*logger_, VERBOSE) << \"Skip retrieve spill file buffer size, it is not supported with graph info v1 & v2.\";\r\n    } else {\r\n      LOGS(*logger_, VERBOSE) << \"Unknown context binary graph info version.\";\r\n    }\r\n  }\r\n#else\r\n  ORT_UNUSED_PARAMETER(buffer);\r\n  ORT_UNUSED_PARAMETER(buffer_length);\r\n#endif\r\n\r\n  LOGS(*logger_, VERBOSE) << \"Get max spill fill buffer size completed.\";\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::LoadCachedQnnContextFromBuffer(char* buffer, uint64_t buffer_length,\r\n                                                         std::string node_name,\r\n                                                         QnnModelLookupTable& qnn_models,\r\n                                                         int64_t max_spill_fill_size) {\r\n  bool result = nullptr == qnn_sys_interface_.systemContextCreate ||\r\n                nullptr == qnn_sys_interface_.systemContextGetBinaryInfo ||\r\n                nullptr == qnn_sys_interface_.systemContextFree;\r\n  ORT_RETURN_IF(result, \"Failed to get valid function pointer.\");\r\n\r\n  QnnSystemContext_Handle_t sys_ctx_handle = nullptr;\r\n  auto rt = qnn_sys_interface_.systemContextCreate(&sys_ctx_handle);\r\n  ORT_RETURN_IF(QNN_SUCCESS != rt, \"Failed to create system handle.\");\r\n\r\n  const QnnSystemContext_BinaryInfo_t* binary_info = nullptr;\r\n  Qnn_ContextBinarySize_t binary_info_size{0};\r\n  rt = qnn_sys_interface_.systemContextGetBinaryInfo(sys_ctx_handle,\r\n                                                     static_cast<void*>(buffer),\r\n                                                     buffer_length,\r\n                                                     &binary_info,\r\n                                                     &binary_info_size);\r\n  ORT_RETURN_IF(QNN_SUCCESS != rt, \"Failed to get context binary info.\");\r\n\r\n  // binary_info life cycle is here\r\n  // Binary info to graph info\r\n  // retrieve Qnn graph info from binary info\r\n  ORT_RETURN_IF(nullptr == binary_info, \"Qnn cached binary info is nullptr.\");\r\n  uint32_t graph_count = 0;\r\n  QnnSystemContext_GraphInfo_t* graphs_info = nullptr;\r\n  if (binary_info->version == QNN_SYSTEM_CONTEXT_BINARY_INFO_VERSION_1) {\r\n    graph_count = binary_info->contextBinaryInfoV1.numGraphs;\r\n    graphs_info = binary_info->contextBinaryInfoV1.graphs;\r\n  }\r\n#if QNN_API_VERSION_MAJOR == 2 && (QNN_API_VERSION_MINOR >= 15)  // starts from 2.22\r\n  else if (binary_info->version == QNN_SYSTEM_CONTEXT_BINARY_INFO_VERSION_2) {\r\n    graph_count = binary_info->contextBinaryInfoV2.numGraphs;\r\n    graphs_info = binary_info->contextBinaryInfoV2.graphs;\r\n  }\r\n#endif\r\n#if QNN_API_VERSION_MAJOR == 2 && (QNN_API_VERSION_MINOR >= 21)  // starts from 2.28\r\n  else if (binary_info->version == QNN_SYSTEM_CONTEXT_BINARY_INFO_VERSION_3) {\r\n    graph_count = binary_info->contextBinaryInfoV3.numGraphs;\r\n    graphs_info = binary_info->contextBinaryInfoV3.graphs;\r\n  }\r\n#endif\r\n  else {\r\n    return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, \"Unsupported context binary info version.\");\r\n  }\r\n\r\n  ORT_RETURN_IF(graph_count < 1 || graphs_info == nullptr, \"Failed to get graph info from Qnn cached context.\");\r\n  LOGS(*logger_, VERBOSE) << \"Graph count from QNN context: \" << graph_count;\r\n\r\n  Qnn_ContextHandle_t context = nullptr;\r\n#if QNN_API_VERSION_MAJOR == 2 && (QNN_API_VERSION_MINOR >= 26)\r\n  if (vtcm_backup_buffer_sharing_enabled_) {\r\n    if (ep_context_handle_map_.find(node_name) != ep_context_handle_map_.end()) {\r\n      context = ep_context_handle_map_.at(node_name);\r\n    }\r\n    ORT_RETURN_IF(nullptr == context, \"Failed to retrieve context for \", node_name);\r\n\r\n  } else {\r\n#endif\r\n    QnnContext_Config_t qnn_context_config = QNN_CONTEXT_CONFIG_INIT;\r\n    ORT_RETURN_IF_ERROR(SetQnnContextConfig(context_priority_, qnn_context_config));\r\n\r\n    // Register spill fill buffer for multi context\r\n    QnnContext_Config_t spill_fill_config = QNN_CONTEXT_CONFIG_INIT;\r\n\r\n    // The spill fill buffer is available since 2.28, API version starts from 2.21\r\n#if QNN_API_VERSION_MAJOR == 2 && (QNN_API_VERSION_MINOR >= 21)\r\n    QnnHtpContext_CustomConfig_t custom_config;\r\n    custom_config.option = QNN_HTP_CONTEXT_CONFIG_OPTION_REGISTER_MULTI_CONTEXTS;\r\n    QnnHtpContext_GroupRegistration_t group_info;\r\n    size_t current_contexts_size = GetQnnContextSize();\r\n    // set to 0x0 (new group) if this is the first context, otherwise point to the first context handle\r\n    // note that we already move the context with max spill fill size to the beginning of the list\r\n    group_info.firstGroupHandle = (max_spill_fill_size > 0 && current_contexts_size > 0) ? GetQnnContext(0) : 0x0;\r\n    group_info.maxSpillFillBuffer = max_spill_fill_size;  // Max spill-fill buffer across contexts. Must be >0\r\n    custom_config.groupRegistration = group_info;\r\n    spill_fill_config.option = QNN_CONTEXT_CONFIG_OPTION_CUSTOM;\r\n    spill_fill_config.customConfig = &custom_config;\r\n\r\n#endif\r\n\r\n    QnnContext_Config_t* spill_fill_config_pointer = max_spill_fill_size > 0 ? &spill_fill_config : nullptr;\r\n    LOGS(*logger_, VERBOSE) << \"Max spill fill buffer size:\" << max_spill_fill_size;\r\n\r\n    const QnnContext_Config_t* context_configs[] = {&qnn_context_config, spill_fill_config_pointer, nullptr};\r\n\r\n    ORT_RETURN_IF(nullptr == qnn_interface_.contextCreateFromBinary,\r\n                  \"Invalid function pointer for contextCreateFromBinary.\");\r\n\r\n    qnn::profile::ProfilingInfo profiling_info;\r\n#ifdef QNN_SYSTEM_PROFILE_API_ENABLED\r\n    if (ProfilingEnabled()) {\r\n      profiling_info.start_time = qnn::utils::GetTimeStampInUs();\r\n    }\r\n#endif\r\n\r\n    rt = qnn_interface_.contextCreateFromBinary(backend_handle_,\r\n                                                device_handle_,\r\n                                                context_configs,\r\n                                                static_cast<void*>(buffer),\r\n                                                buffer_length,\r\n                                                &context,\r\n                                                profile_backend_handle_);\r\n\r\n#ifdef QNN_SYSTEM_PROFILE_API_ENABLED\r\n    if (ProfilingEnabled()) {\r\n      profiling_info.stop_time = qnn::utils::GetTimeStampInUs();\r\n      profiling_info.method_type = ProfilingMethodType::CREATE_FROM_BINARY;\r\n      profiling_info.graph_name = node_name;\r\n    }\r\n#endif\r\n\r\n    ORT_RETURN_IF(QNN_SUCCESS != rt, \"Failed to create context from binary. Error code: \", rt);\r\n    ORT_RETURN_IF_ERROR(AddQnnContextHandle(context));\r\n\r\n    ORT_RETURN_IF_ERROR(ExtractBackendProfilingInfo(profiling_info));\r\n\r\n#if QNN_API_VERSION_MAJOR == 2 && (QNN_API_VERSION_MINOR >= 26)\r\n  }\r\n#endif\r\n\r\n  if (1 == graph_count) {\r\n    // in case the EPContext node is generated from script\r\n    // the graph name from the context binary may not match the EPContext node name\r\n    auto qnn_model = std::make_unique<qnn::QnnModel>(this);\r\n    ORT_RETURN_IF_ERROR(qnn_model->DeserializeGraphInfoFromBinaryInfo(graphs_info[0], context));\r\n    qnn_models.emplace(node_name, std::move(qnn_model));\r\n  } else {\r\n    for (uint32_t i = 0; i < graph_count; ++i) {\r\n      auto qnn_model = std::make_unique<qnn::QnnModel>(this);\r\n      ORT_RETURN_IF_ERROR(qnn_model->DeserializeGraphInfoFromBinaryInfo(graphs_info[i], context));\r\n      qnn_models.emplace(graphs_info[i].graphInfoV1.graphName, std::move(qnn_model));\r\n    }\r\n  }\r\n\r\n  qnn_sys_interface_.systemContextFree(sys_ctx_handle);\r\n  sys_ctx_handle = nullptr;\r\n  context_created_ = true;\r\n\r\n  LOGS(*logger_, VERBOSE) << \"Load from cached QNN Context completed.\";\r\n  return Status::OK();\r\n}\r\n\r\n// need to load system lib if load from Qnn context binary\r\n// or generate Qnn context binary is enabled -- to get the max spill fill buffer size\r\nStatus QnnBackendManager::SetupBackend(const logging::Logger& logger,\r\n                                       bool load_from_cached_context,\r\n                                       bool need_load_system_lib,\r\n                                       bool share_ep_contexts,\r\n                                       bool enable_vtcm_backup_buffer_sharing,\r\n                                       std::unordered_map<std::string, std::unique_ptr<std::vector<std::string>>>& context_bin_map) {\r\n  std::lock_guard<std::recursive_mutex> lock(logger_recursive_mutex_);\r\n  if (backend_setup_completed_) {\r\n    LOGS(logger, VERBOSE) << \"Backend setup already!\";\r\n\r\n#if QNN_API_VERSION_MAJOR == 2 && (QNN_API_VERSION_MINOR >= 26)\r\n    if (vtcm_backup_buffer_sharing_enabled_) {\r\n      // If a context bin filepath has not been processed yet,\r\n      // then a new context must be created for the set of context bins\r\n      auto first_mapping_it = ep_context_handle_map_.find(context_bin_map.begin()->first);\r\n      if (first_mapping_it == ep_context_handle_map_.end()) {\r\n        LOGS(logger, VERBOSE) << \"Creating context for new set of context binaries\";\r\n        return CreateContextVtcmBackupBufferSharingEnabled(context_bin_map);\r\n      }\r\n\r\n      LOGS(logger, VERBOSE) << \"Mapping contexts to new EP main context nodes\";\r\n\r\n      for (auto& it : context_bin_map) {\r\n        auto context_bin_filepath = it.first;\r\n        auto ep_node_names = *(it.second);\r\n\r\n        auto context = ep_context_handle_map_.at(context_bin_filepath);\r\n        for (auto node_name : ep_node_names) {\r\n          ep_context_handle_map_.emplace(node_name, context);\r\n        }\r\n      }\r\n    }\r\n#endif\r\n    return Status::OK();\r\n  }\r\n\r\n  vtcm_backup_buffer_sharing_enabled_ = enable_vtcm_backup_buffer_sharing;\r\n\r\n  Status status = Status::OK();\r\n  if (!qnn_serializer_config_) {\r\n    status = LoadBackend();\r\n  } else {\r\n    status = LoadQnnSerializerBackend();\r\n  }\r\n  if (status.IsOK()) {\r\n    LOGS(logger, VERBOSE) << \"LoadBackend succeed.\";\r\n  }\r\n\r\n  if (status.IsOK() && (load_from_cached_context || need_load_system_lib)) {\r\n    status = LoadQnnSystemLib();\r\n  }\r\n\r\n  if (status.IsOK()) {\r\n    sdk_build_version_ = GetBackendBuildId();\r\n    LOGS(logger, VERBOSE) << \"Backend build version: \"\r\n                          << sdk_build_version_;\r\n  }\r\n\r\n  if (status.IsOK()) {\r\n    status = InitializeQnnLog(logger);\r\n  }\r\n  if (status.IsOK()) {\r\n    LOGS(logger, VERBOSE) << \"SetLogger succeed.\";\r\n  }\r\n\r\n  if (status.IsOK()) {\r\n    status = InitializeBackend();\r\n  }\r\n  if (status.IsOK()) {\r\n    LOGS(logger, VERBOSE) << \"InitializeBackend succeed.\";\r\n  }\r\n\r\n  if (status.IsOK()) {\r\n    status = CreateDevice();\r\n  }\r\n  if (status.IsOK()) {\r\n    LOGS(logger, VERBOSE) << \"CreateDevice succeed.\";\r\n  }\r\n\r\n  if (status.IsOK()) {\r\n    status = InitializeProfiling();\r\n  }\r\n  if (status.IsOK()) {\r\n    LOGS(logger, VERBOSE) << \"InitializeProfiling succeed.\";\r\n  }\r\n\r\n  if (status.IsOK()) {\r\n    ORT_RETURN_IF_ERROR(LoadOpPackage());\r\n    LOGS(logger, VERBOSE) << \"LoadOpPackage succeed.\";\r\n  }\r\n\r\n  bool enable_htp_weight_sharing = false;\r\n  if (share_ep_contexts && !load_from_cached_context) {\r\n#if defined(__aarch64__) || defined(_M_ARM64)\r\n    LOGS(logger, WARNING) << \"Weight sharing only available with offline generation on x64 platform, not work on real device.\";\r\n#else\r\n    enable_htp_weight_sharing = true;\r\n#endif\r\n  }\r\n\r\n  if (status.IsOK() && (vtcm_backup_buffer_sharing_enabled_ || !load_from_cached_context)) {\r\n    status = vtcm_backup_buffer_sharing_enabled_ ? CreateContextVtcmBackupBufferSharingEnabled(context_bin_map)\r\n                                                 : CreateContext(enable_htp_weight_sharing);\r\n\r\n    if (status.IsOK()) {\r\n      LOGS(logger, VERBOSE) << \"CreateContext succeed.\";\r\n    }\r\n  }\r\n\r\n  if (status.IsOK()) {\r\n    LOGS(logger, VERBOSE) << \"QNN SetupBackend succeed\";\r\n    backend_setup_completed_ = true;\r\n  } else {\r\n    LOGS_DEFAULT(WARNING) << \"Failed to setup so cleaning up\";\r\n    ReleaseResources();\r\n  }\r\n\r\n  return status;\r\n}\r\n\r\nStatus QnnBackendManager::CreateHtpPowerCfgId(uint32_t device_id, uint32_t core_id, uint32_t& htp_power_config_id) {\r\n  // This function is called in QNN EP's OnRunStart() even if QNN backend setup failed and the model is assigned\r\n  // to a different EP. Therefore, we have to check that backend setup actually completed before trying to\r\n  // create an HTP power config ID. Otherwise, this causes a segfault because the QNN backend lib is unloaded.\r\n  ORT_RETURN_IF_NOT(backend_setup_completed_, \"Cannot create HTP power config ID if backend setup is not complete.\");\r\n  QnnDevice_Infrastructure_t qnn_device_infra = nullptr;\r\n  auto status = qnn_interface_.deviceGetInfrastructure(&qnn_device_infra);\r\n  ORT_RETURN_IF(QNN_SUCCESS != status, \"backendGetPerfInfrastructure failed.\");\r\n\r\n  auto* htp_infra = static_cast<QnnHtpDevice_Infrastructure_t*>(qnn_device_infra);\r\n  ORT_RETURN_IF(QNN_HTP_DEVICE_INFRASTRUCTURE_TYPE_PERF != htp_infra->infraType,\r\n                \"HTP infra type = \", htp_infra->infraType, \", which is not perf infra type.\");\r\n  QnnHtpDevice_PerfInfrastructure_t& htp_perf_infra = htp_infra->perfInfra;\r\n  // Get power client id\r\n  status = htp_perf_infra.createPowerConfigId(device_id, core_id, &htp_power_config_id);\r\n  ORT_RETURN_IF(QNN_SUCCESS != status, \"createPowerConfigId failed.\");\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::SetHtpPowerConfig(uint32_t htp_power_config_client_id,\r\n                                            HtpPerformanceMode htp_performance_mode) {\r\n  // This function is called in QNN EP's OnRunStart() even if QNN backend setup failed and the model is assigned\r\n  // to a different EP. Therefore, we have to check that backend setup actually completed before trying to\r\n  // set an HTP power config ID. Otherwise, this causes a segfault because the QNN backend lib is unloaded.\r\n  ORT_RETURN_IF_NOT(backend_setup_completed_, \"Cannot set HTP power config ID if backend setup is not complete.\");\r\n  QnnDevice_Infrastructure_t qnn_device_infra = nullptr;\r\n  auto status = qnn_interface_.deviceGetInfrastructure(&qnn_device_infra);\r\n  ORT_RETURN_IF(QNN_SUCCESS != status, \"backendGetPerfInfrastructure failed.\");\r\n\r\n  auto* htp_infra = static_cast<QnnHtpDevice_Infrastructure_t*>(qnn_device_infra);\r\n  ORT_RETURN_IF(QNN_HTP_DEVICE_INFRASTRUCTURE_TYPE_PERF != htp_infra->infraType,\r\n                \"HTP infra type = \", htp_infra->infraType, \", which is not perf infra type.\");\r\n  QnnHtpDevice_PerfInfrastructure_t& htp_perf_infra = htp_infra->perfInfra;\r\n\r\n  constexpr const int kNumConfigs = 1;\r\n  std::vector<QnnHtpPerfInfrastructure_PowerConfig_t> power_configs(\r\n      kNumConfigs);\r\n  QnnHtpPerfInfrastructure_PowerConfig_t& dcvs_config = power_configs[0];\r\n  dcvs_config.option = QNN_HTP_PERF_INFRASTRUCTURE_POWER_CONFIGOPTION_DCVS_V3;\r\n  QnnHtpPerfInfrastructure_DcvsV3_t& dcvs_v3 = dcvs_config.dcvsV3Config;\r\n  dcvs_v3.contextId = htp_power_config_client_id;\r\n  dcvs_v3.setSleepDisable = 0;\r\n  dcvs_v3.sleepDisable = 0;\r\n  dcvs_v3.setDcvsEnable = 1;\r\n  dcvs_v3.powerMode = QNN_HTP_PERF_INFRASTRUCTURE_POWERMODE_PERFORMANCE_MODE;\r\n  // choose performance mode\r\n  switch (htp_performance_mode) {\r\n    case HtpPerformanceMode::kHtpBurst:\r\n      dcvs_v3.setSleepLatency = 1;  // true\r\n      dcvs_v3.sleepLatency = kSleepMinLatency;\r\n      dcvs_v3.dcvsEnable = kDcvsDisable;\r\n      dcvs_v3.setBusParams = 1;\r\n      dcvs_v3.busVoltageCornerMin = DCVS_VOLTAGE_VCORNER_MAX_VOLTAGE_CORNER;\r\n      dcvs_v3.busVoltageCornerTarget = DCVS_VOLTAGE_VCORNER_MAX_VOLTAGE_CORNER;\r\n      dcvs_v3.busVoltageCornerMax = DCVS_VOLTAGE_VCORNER_MAX_VOLTAGE_CORNER;\r\n      dcvs_v3.setCoreParams = 1;\r\n      dcvs_v3.coreVoltageCornerMin = DCVS_VOLTAGE_VCORNER_MAX_VOLTAGE_CORNER;\r\n      dcvs_v3.coreVoltageCornerTarget = DCVS_VOLTAGE_VCORNER_MAX_VOLTAGE_CORNER;\r\n      dcvs_v3.coreVoltageCornerMax = DCVS_VOLTAGE_VCORNER_MAX_VOLTAGE_CORNER;\r\n      break;\r\n    case HtpPerformanceMode::kHtpSustainedHighPerformance:\r\n    case HtpPerformanceMode::kHtpHighPerformance:\r\n      dcvs_v3.setSleepLatency = 1;  // true\r\n      dcvs_v3.sleepLatency = kSleepLowLatency;\r\n      dcvs_v3.dcvsEnable = kDcvsDisable;\r\n      dcvs_v3.setBusParams = 1;\r\n      dcvs_v3.busVoltageCornerMin = DCVS_VOLTAGE_VCORNER_TURBO;\r\n      dcvs_v3.busVoltageCornerTarget = DCVS_VOLTAGE_VCORNER_TURBO;\r\n      dcvs_v3.busVoltageCornerMax = DCVS_VOLTAGE_VCORNER_TURBO;\r\n      dcvs_v3.setCoreParams = 1;\r\n      dcvs_v3.coreVoltageCornerMin = DCVS_VOLTAGE_VCORNER_TURBO;\r\n      dcvs_v3.coreVoltageCornerTarget = DCVS_VOLTAGE_VCORNER_TURBO;\r\n      dcvs_v3.coreVoltageCornerMax = DCVS_VOLTAGE_VCORNER_TURBO;\r\n      break;\r\n    case HtpPerformanceMode::kHtpBalanced:\r\n      dcvs_v3.setSleepLatency = 1;  // true\r\n      dcvs_v3.sleepLatency = kSleepMediumLatency;\r\n      dcvs_v3.dcvsEnable = kDcvsEnable;\r\n      dcvs_v3.setBusParams = 1;\r\n      dcvs_v3.busVoltageCornerMin = DCVS_VOLTAGE_VCORNER_NOM_PLUS;\r\n      dcvs_v3.busVoltageCornerTarget = DCVS_VOLTAGE_VCORNER_NOM_PLUS;\r\n      dcvs_v3.busVoltageCornerMax = DCVS_VOLTAGE_VCORNER_NOM_PLUS;\r\n      dcvs_v3.setCoreParams = 1;\r\n      dcvs_v3.coreVoltageCornerMin = DCVS_VOLTAGE_VCORNER_NOM_PLUS;\r\n      dcvs_v3.coreVoltageCornerTarget = DCVS_VOLTAGE_VCORNER_NOM_PLUS;\r\n      dcvs_v3.coreVoltageCornerMax = DCVS_VOLTAGE_VCORNER_NOM_PLUS;\r\n      break;\r\n    case HtpPerformanceMode::kHtpLowBalanced:\r\n      dcvs_v3.setSleepLatency = 1;  // true\r\n      dcvs_v3.sleepLatency = kSleepMediumLatency;\r\n      dcvs_v3.dcvsEnable = kDcvsEnable;\r\n      dcvs_v3.setBusParams = 1;\r\n      dcvs_v3.busVoltageCornerMin = DCVS_VOLTAGE_VCORNER_NOM;\r\n      dcvs_v3.busVoltageCornerTarget = DCVS_VOLTAGE_VCORNER_NOM;\r\n      dcvs_v3.busVoltageCornerMax = DCVS_VOLTAGE_VCORNER_NOM;\r\n      dcvs_v3.setCoreParams = 1;\r\n      dcvs_v3.coreVoltageCornerMin = DCVS_VOLTAGE_VCORNER_NOM;\r\n      dcvs_v3.coreVoltageCornerTarget = DCVS_VOLTAGE_VCORNER_NOM;\r\n      dcvs_v3.coreVoltageCornerMax = DCVS_VOLTAGE_VCORNER_NOM;\r\n      break;\r\n    case HtpPerformanceMode::kHtpHighPowerSaver:\r\n      dcvs_v3.setSleepLatency = 1;  // true\r\n      dcvs_v3.sleepLatency = kSleepMediumLatency;\r\n      dcvs_v3.dcvsEnable = kDcvsEnable;\r\n      dcvs_v3.setBusParams = 1;\r\n      dcvs_v3.busVoltageCornerMin = DCVS_VOLTAGE_VCORNER_SVS_PLUS;\r\n      dcvs_v3.busVoltageCornerTarget = DCVS_VOLTAGE_VCORNER_SVS_PLUS;\r\n      dcvs_v3.busVoltageCornerMax = DCVS_VOLTAGE_VCORNER_SVS_PLUS;\r\n      dcvs_v3.setCoreParams = 1;\r\n      dcvs_v3.coreVoltageCornerMin = DCVS_VOLTAGE_VCORNER_SVS_PLUS;\r\n      dcvs_v3.coreVoltageCornerTarget = DCVS_VOLTAGE_VCORNER_SVS_PLUS;\r\n      dcvs_v3.coreVoltageCornerMax = DCVS_VOLTAGE_VCORNER_SVS_PLUS;\r\n      break;\r\n    case HtpPerformanceMode::kHtpPowerSaver:\r\n      dcvs_v3.setSleepLatency = 1;  // true\r\n      dcvs_v3.sleepLatency = kSleepMediumLatency;\r\n      dcvs_v3.dcvsEnable = kDcvsEnable;\r\n      dcvs_v3.setBusParams = 1;\r\n      dcvs_v3.busVoltageCornerMin = DCVS_VOLTAGE_VCORNER_SVS;\r\n      dcvs_v3.busVoltageCornerTarget = DCVS_VOLTAGE_VCORNER_SVS;\r\n      dcvs_v3.busVoltageCornerMax = DCVS_VOLTAGE_VCORNER_SVS;\r\n      dcvs_v3.setCoreParams = 1;\r\n      dcvs_v3.coreVoltageCornerMin = DCVS_VOLTAGE_VCORNER_SVS;\r\n      dcvs_v3.coreVoltageCornerTarget = DCVS_VOLTAGE_VCORNER_SVS;\r\n      dcvs_v3.coreVoltageCornerMax = DCVS_VOLTAGE_VCORNER_SVS;\r\n      break;\r\n    case HtpPerformanceMode::kHtpLowPowerSaver:\r\n      dcvs_v3.setSleepLatency = 1;  // true\r\n      dcvs_v3.sleepLatency = kSleepMediumLatency;\r\n      dcvs_v3.dcvsEnable = kDcvsEnable;\r\n      dcvs_v3.setBusParams = 1;\r\n      dcvs_v3.busVoltageCornerMin = DCVS_VOLTAGE_VCORNER_SVS2;\r\n      dcvs_v3.busVoltageCornerTarget = DCVS_VOLTAGE_VCORNER_SVS2;\r\n      dcvs_v3.busVoltageCornerMax = DCVS_VOLTAGE_VCORNER_SVS2;\r\n      dcvs_v3.setCoreParams = 1;\r\n      dcvs_v3.coreVoltageCornerMin = DCVS_VOLTAGE_VCORNER_SVS2;\r\n      dcvs_v3.coreVoltageCornerTarget = DCVS_VOLTAGE_VCORNER_SVS2;\r\n      dcvs_v3.coreVoltageCornerMax = DCVS_VOLTAGE_VCORNER_SVS2;\r\n      break;\r\n    case HtpPerformanceMode::kHtpExtremePowerSaver:\r\n      dcvs_v3.powerMode = QNN_HTP_PERF_INFRASTRUCTURE_POWERMODE_POWER_SAVER_MODE;\r\n      dcvs_v3.setSleepLatency = 1;  // true\r\n      dcvs_v3.sleepLatency = kSleepMediumLatency;\r\n      dcvs_v3.dcvsEnable = kDcvsEnable;\r\n      dcvs_v3.setBusParams = 1;\r\n      dcvs_v3.busVoltageCornerMin = DCVS_VOLTAGE_CORNER_DISABLE;\r\n      dcvs_v3.busVoltageCornerTarget = DCVS_VOLTAGE_CORNER_DISABLE;\r\n      dcvs_v3.busVoltageCornerMax = DCVS_VOLTAGE_CORNER_DISABLE;\r\n      dcvs_v3.setCoreParams = 1;\r\n      dcvs_v3.coreVoltageCornerMin = DCVS_VOLTAGE_CORNER_DISABLE;\r\n      dcvs_v3.coreVoltageCornerTarget = DCVS_VOLTAGE_CORNER_DISABLE;\r\n      dcvs_v3.coreVoltageCornerMax = DCVS_VOLTAGE_CORNER_DISABLE;\r\n      break;\r\n    default:\r\n      ORT_THROW(\"Invalid performance profile %d\", static_cast<int>(htp_performance_mode));\r\n      break;\r\n  }\r\n  std::vector<const QnnHtpPerfInfrastructure_PowerConfig_t*> perf_power_configs_ptr = ObtainNullTermPtrVector(power_configs);\r\n  status = htp_perf_infra.setPowerConfig(htp_power_config_client_id, perf_power_configs_ptr.data());\r\n  ORT_RETURN_IF(QNN_SUCCESS != status, \"setPowerConfig failed for HTP performance mode.\");\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::SetRpcPowerConfigs(uint32_t htp_power_config_client_id,\r\n                                             uint32_t rpc_control_latency,\r\n                                             uint32_t rpc_polling_time) {\r\n  // This function is called in QNN EP's OnRunStart() even if QNN backend setup failed and the model is assigned\r\n  // to a different EP. Therefore, we have to check that backend setup actually completed before trying to\r\n  // set RPC control latency. Otherwise, this causes a segfault because the QNN backend library is unloaded.\r\n  ORT_RETURN_IF_NOT(backend_setup_completed_, \"Cannot set HTP RPC control latency if backend setup is not complete.\");\r\n\r\n  constexpr int kNumRpcPollingPowerConfigs = 2;\r\n  std::vector<QnnHtpPerfInfrastructure_PowerConfig_t> rpc_power_configs;\r\n  rpc_power_configs.reserve(kNumRpcPollingPowerConfigs);\r\n\r\n  // Set rpc control latency here\r\n  if (rpc_control_latency != 0) {\r\n    auto& rpc_control_latency_cfg = rpc_power_configs.emplace_back();\r\n    rpc_control_latency_cfg.option = QNN_HTP_PERF_INFRASTRUCTURE_POWER_CONFIGOPTION_RPC_CONTROL_LATENCY;\r\n    rpc_control_latency_cfg.rpcControlLatencyConfig = rpc_control_latency;\r\n  }\r\n\r\n  // Note: v68 does not support rpc polling mode\r\n  if (rpc_polling_time != 0) {\r\n    auto& rpc_polling_time_cfg = rpc_power_configs.emplace_back();\r\n    rpc_polling_time_cfg.option = QNN_HTP_PERF_INFRASTRUCTURE_POWER_CONFIGOPTION_RPC_POLLING_TIME;\r\n    rpc_polling_time_cfg.rpcPollingTimeConfig = rpc_polling_time;\r\n  }\r\n\r\n  if (rpc_power_configs.size() > 0) {\r\n    QnnDevice_Infrastructure_t qnn_device_infra = nullptr;\r\n    auto status = qnn_interface_.deviceGetInfrastructure(&qnn_device_infra);\r\n    ORT_RETURN_IF(QNN_SUCCESS != status, \"backendGetPerfInfrastructure failed.\");\r\n\r\n    auto* htp_infra = static_cast<QnnHtpDevice_Infrastructure_t*>(qnn_device_infra);\r\n    ORT_RETURN_IF(QNN_HTP_DEVICE_INFRASTRUCTURE_TYPE_PERF != htp_infra->infraType,\r\n                  \"HTP infra type = \", htp_infra->infraType, \", which is not perf infra type.\");\r\n    QnnHtpDevice_PerfInfrastructure_t& htp_perf_infra = htp_infra->perfInfra;\r\n\r\n    std::vector<const QnnHtpPerfInfrastructure_PowerConfig_t*> perf_power_configs_ptr =\r\n        ObtainNullTermPtrVector(rpc_power_configs);\r\n    status = htp_perf_infra.setPowerConfig(htp_power_config_client_id, perf_power_configs_ptr.data());\r\n    ORT_RETURN_IF(QNN_SUCCESS != status, \"setPowerConfig failed for RPC control latency.\");\r\n  }\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::DestroyHTPPowerConfigID(uint32_t htp_power_config_id) {\r\n  QnnDevice_Infrastructure_t qnn_device_infra = nullptr;\r\n  auto status = qnn_interface_.deviceGetInfrastructure(&qnn_device_infra);\r\n  ORT_RETURN_IF(QNN_SUCCESS != status, \"backendGetPerfInfrastructure failed.\");\r\n\r\n  auto* htp_infra = static_cast<QnnHtpDevice_Infrastructure_t*>(qnn_device_infra);\r\n  ORT_RETURN_IF(QNN_HTP_DEVICE_INFRASTRUCTURE_TYPE_PERF != htp_infra->infraType,\r\n                \"HTP infra type = \", htp_infra->infraType, \", which is not perf infra type.\");\r\n  QnnHtpDevice_PerfInfrastructure_t& htp_perf_infra = htp_infra->perfInfra;\r\n\r\n  Qnn_ErrorHandle_t destroy_ret = htp_perf_infra.destroyPowerConfigId(htp_power_config_id);\r\n  ORT_RETURN_IF(QNN_SUCCESS != destroy_ret, \"destroyPowerConfigId failed.\");\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::TerminateQnnLog() {\r\n  std::lock_guard<std::recursive_mutex> lock(logger_recursive_mutex_);\r\n  if (logger_ == nullptr) {\r\n    return Status::OK();\r\n  }\r\n\r\n  if (nullptr != qnn_interface_.logFree && nullptr != log_handle_) {\r\n    auto ret_val = qnn_interface_.logFree(log_handle_);\r\n\r\n    // Reset QNN log handle to nullptr so other threads that are waiting on logger_recursive_mutex_ know it was freed.\r\n    log_handle_ = nullptr;\r\n    ORT_RETURN_IF(QNN_SUCCESS != ret_val,\r\n                  \"Unable to terminate logging in the backend.\");\r\n  }\r\n\r\n  return Status::OK();\r\n}\r\n\r\nvoid QnnBackendManager::ReleaseResources() {\r\n  auto result = ReleaseContext();\r\n  if (Status::OK() != result) {\r\n    LOGS_DEFAULT(ERROR) << \"Failed to ReleaseContext: \" << result.ErrorMessage();\r\n  }\r\n\r\n  result = ReleaseProfilehandle();\r\n  if (Status::OK() != result) {\r\n    LOGS_DEFAULT(ERROR) << \"Failed to ReleaseProfilehandle: \" << result.ErrorMessage();\r\n  }\r\n\r\n  result = ReleaseDevice();\r\n  if (Status::OK() != result) {\r\n    LOGS_DEFAULT(ERROR) << \"Failed to ReleaseDevice: \" << result.ErrorMessage();\r\n  }\r\n\r\n  result = ShutdownBackend();\r\n  if (Status::OK() != result) {\r\n    LOGS_DEFAULT(ERROR) << \"Failed to ShutdownBackend: \" << result.ErrorMessage();\r\n  }\r\n\r\n  result = TerminateQnnLog();\r\n  if (Status::OK() != result) {\r\n    LOGS_DEFAULT(ERROR) << \"Failed to TerminateQnnLog: \" << result.ErrorMessage();\r\n  }\r\n\r\n  if (backend_lib_handle_) {\r\n    result = UnloadLib(backend_lib_handle_);\r\n    if (Status::OK() != result) {\r\n      LOGS_DEFAULT(ERROR) << \"Failed to unload backend library: \" << result.ErrorMessage();\r\n    }\r\n  }\r\n\r\n  backend_setup_completed_ = false;\r\n\r\n  return;\r\n}\r\n\r\nStatus QnnBackendManager::ExtractBackendProfilingInfo(qnn::profile::ProfilingInfo& profiling_info) {\r\n  if (ProfilingLevel::OFF == profiling_level_merge_ || ProfilingLevel::INVALID == profiling_level_merge_) {\r\n    return Status::OK();\r\n  }\r\n\r\n  bool tracelogging_provider_ep_enabled = false;\r\n#ifdef _WIN32\r\n  auto& provider = QnnTelemetry::Instance();\r\n  if (provider.IsEnabled()) {\r\n    auto level = provider.Level();\r\n    auto keyword = provider.Keyword();\r\n    if ((keyword & static_cast<uint64_t>(onnxruntime::logging::ORTTraceLoggingKeyword::Profiling)) != 0 && level >= 5) {\r\n      tracelogging_provider_ep_enabled = true;\r\n    }\r\n  }\r\n#endif  // defined(_WIN32)\r\n\r\n  // ETW disabled previously, but enabled now\r\n  if (ProfilingLevel::INVALID == profiling_level_etw_ && tracelogging_provider_ep_enabled) {\r\n    LOGS(*logger_, ERROR) << \"ETW disabled previously, but enabled now. Can't do the switch! Won't output any profiling.\";\r\n    return Status::OK();\r\n  }\r\n\r\n  // ETW enabled previously, but disabled now\r\n  if (ProfilingLevel::INVALID != profiling_level_etw_ && !tracelogging_provider_ep_enabled) {\r\n    LOGS(*logger_, ERROR) << \"ETW enabled previously, but disabled now. Can't do the switch! Won't output any profiling.\";\r\n    return Status::OK();\r\n  }\r\n\r\n  ORT_RETURN_IF(!tracelogging_provider_ep_enabled && profiling_file_path_.empty(),\r\n                \"Need to specify a CSV file via provider option profiling_file_path if ETW not enabled.\");\r\n\r\n  ORT_RETURN_IF(nullptr == profile_backend_handle_, \"Backend profile handle not valid.\");\r\n\r\n  LOGS(*logger_, VERBOSE) << \"Extracting profiling events for graph \" << profiling_info.graph_name;\r\n  const QnnProfile_EventId_t* profile_events{nullptr};\r\n  uint32_t num_events{0};\r\n  Qnn_ErrorHandle_t result = qnn_interface_.profileGetEvents(profile_backend_handle_, &profile_events, &num_events);\r\n  if (qnn_serializer_config_) {  // Using QNN Saver or IR backend\r\n    // QNN SDK 2.28.2 returns QNN_SAVER_ERROR_DUMMY_RETVALUE, but previous QNN versions return QNN_PROFILE_NO_ERROR.\r\n    // We accept both values.\r\n    ORT_RETURN_IF(QNN_PROFILE_NO_ERROR != result && QNN_SAVER_ERROR_DUMMY_RETVALUE != result,\r\n                  \"Failed to get profile events. Error: \", QnnErrorHandleToString(result));\r\n  } else {\r\n    ORT_RETURN_IF(QNN_PROFILE_NO_ERROR != result, \"Failed to get profile events. Error: \", QnnErrorHandleToString(result));\r\n  }\r\n\r\n  if (num_events > 0) {\r\n    LOGS(*logger_, VERBOSE) << \"profile_events: \" << profile_events << \" num_events: \" << num_events;\r\n\r\n    bool backendSupportsExtendedEventData = false;\r\n    Qnn_ErrorHandle_t resultPropertyHasCapability =\r\n        qnn_interface_.propertyHasCapability(QNN_PROPERTY_PROFILE_SUPPORTS_EXTENDED_EVENT);\r\n    uint16_t errorCodePropertyHasCapability = static_cast<uint16_t>(resultPropertyHasCapability & 0xFFFF);\r\n    if (errorCodePropertyHasCapability == QNN_PROPERTY_SUPPORTED) {\r\n      LOGS(*logger_, VERBOSE) << \"The QNN backend supports extended event data.\";\r\n      backendSupportsExtendedEventData = true;\r\n    } else {\r\n      LOGS(*logger_, VERBOSE) << \"The QNN backend does not support extended event data.\";\r\n    }\r\n\r\n    profiling_info.csv_output_filepath = profiling_file_path_;\r\n#ifdef QNN_SYSTEM_PROFILE_API_ENABLED\r\n    profiling_info.num_events = num_events;\r\n#endif\r\n\r\n    profile::Serializer profile_writer(profiling_info,\r\n                                       qnn_sys_interface_,\r\n                                       tracelogging_provider_ep_enabled);\r\n    if (!profiling_file_path_.empty()) {\r\n      ORT_RETURN_IF_ERROR(profile_writer.InitCsvFile());\r\n    }\r\n\r\n    for (size_t event_idx = 0; event_idx < num_events; event_idx++) {\r\n      ORT_RETURN_IF_ERROR(\r\n          ExtractProfilingEvent(*(profile_events + event_idx), \"ROOT\", profile_writer,\r\n                                backendSupportsExtendedEventData));\r\n      ORT_RETURN_IF_ERROR(\r\n          ExtractProfilingSubEvents(*(profile_events + event_idx), profile_writer,\r\n                                    backendSupportsExtendedEventData));\r\n    }\r\n#ifdef QNN_SYSTEM_PROFILE_API_ENABLED\r\n    ORT_RETURN_IF_ERROR(profile_writer.SerializeEventsToQnnLog());\r\n#endif\r\n\r\n    if (!profiling_file_path_.empty()) {\r\n      LOGS(*logger_, VERBOSE) << \"Wrote QNN profiling events (\" << num_events << \") to file (\"\r\n                              << profiling_file_path_ << \")\";\r\n    }\r\n\r\n    if (tracelogging_provider_ep_enabled) {\r\n      LOGS(*logger_, VERBOSE) << \"Wrote QNN profiling events (\" << num_events << \") to ETW\";\r\n    }\r\n  }\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::ExtractProfilingSubEvents(\r\n    QnnProfile_EventId_t profile_event_id,\r\n    profile::Serializer& profile_writer,\r\n    bool useExtendedEventData) {\r\n  const QnnProfile_EventId_t* profile_sub_events{nullptr};\r\n  uint32_t num_sub_events{0};\r\n  Qnn_ErrorHandle_t result = qnn_interface_.profileGetSubEvents(profile_event_id, &profile_sub_events, &num_sub_events);\r\n  ORT_RETURN_IF(QNN_PROFILE_NO_ERROR != result, \"Failed to get profile sub events. Error: \", QnnErrorHandleToString(result));\r\n\r\n  if (num_sub_events > 0) {\r\n    LOGS(*logger_, VERBOSE) << \"profile_sub_events: \" << profile_sub_events << \" num_sub_events: \" << num_sub_events;\r\n\r\n#ifdef QNN_SYSTEM_PROFILE_API_ENABLED\r\n    QnnSystemProfile_ProfileEventV1_t* parent_system_event = nullptr;\r\n    parent_system_event = profile_writer.GetParentSystemEvent(profile_event_id);\r\n    if (parent_system_event == nullptr) {\r\n      parent_system_event = profile_writer.GetSystemEventPointer(profile_event_id);\r\n      profile_writer.AddSubEventList(num_sub_events, parent_system_event);\r\n    }\r\n#endif\r\n\r\n    for (size_t sub_event_idx = 0; sub_event_idx < num_sub_events; sub_event_idx++) {\r\n      QnnProfile_EventId_t subevent_id = *(profile_sub_events + sub_event_idx);\r\n\r\n#ifdef QNN_SYSTEM_PROFILE_API_ENABLED\r\n\r\n      ORT_RETURN_IF_ERROR(profile_writer.SetParentSystemEvent(subevent_id, parent_system_event));\r\n\r\n#endif\r\n\r\n      ORT_RETURN_IF_ERROR(\r\n          ExtractProfilingEvent(subevent_id, \"SUB-EVENT\", profile_writer, useExtendedEventData));\r\n      ORT_RETURN_IF_ERROR(\r\n          ExtractProfilingSubEvents(subevent_id, profile_writer, useExtendedEventData));\r\n    }\r\n\r\n    LOGS(*logger_, VERBOSE) << \"Wrote QNN profiling sub events (\" << num_sub_events << \")\";\r\n  }\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::ExtractProfilingEvent(\r\n    QnnProfile_EventId_t profile_event_id,\r\n    const std::string& event_level,\r\n    profile::Serializer& profile_writer,\r\n    bool useExtendedEventData) {\r\n  if (useExtendedEventData) {\r\n    return ExtractProfilingEventExtended(profile_event_id, event_level, profile_writer);\r\n  } else {\r\n    return ExtractProfilingEventBasic(profile_event_id, event_level, profile_writer);\r\n  }\r\n}\r\n\r\nStatus QnnBackendManager::ExtractProfilingEventBasic(\r\n    QnnProfile_EventId_t profile_event_id,\r\n    const std::string& event_level,\r\n    profile::Serializer& profile_writer) {\r\n  QnnProfile_EventData_t event_data;\r\n  Qnn_ErrorHandle_t result = qnn_interface_.profileGetEventData(profile_event_id, &event_data);\r\n  QnnProfile_Error_t errorCode = static_cast<QnnProfile_Error_t>(result & 0xFFFF);\r\n  ORT_RETURN_IF(QNN_PROFILE_NO_ERROR != result, \"Failed to get profile event data: \" + std::string(QnnProfileErrorToString(errorCode)));\r\n\r\n  ORT_RETURN_IF_ERROR(profile_writer.ProcessEvent(profile_event_id, event_level, event_data));\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::ExtractProfilingEventExtended(\r\n    QnnProfile_EventId_t profile_event_id,\r\n    const std::string& event_level,\r\n    profile::Serializer& profile_writer) {\r\n  QnnProfile_ExtendedEventData_t event_data_extended;\r\n  auto resultGetExtendedEventData = qnn_interface_.profileGetExtendedEventData(profile_event_id, &event_data_extended);\r\n  QnnProfile_Error_t errorCode = static_cast<QnnProfile_Error_t>(resultGetExtendedEventData & 0xFFFF);\r\n  ORT_RETURN_IF(QNN_PROFILE_NO_ERROR != errorCode, \"Failed to get profile event data: \" + std::string(QnnProfileErrorToString(errorCode)));\r\n\r\n  ORT_RETURN_IF_ERROR(profile_writer.ProcessExtendedEvent(profile_event_id, event_level, event_data_extended));\r\n\r\n  return Status::OK();\r\n}\r\n\r\nconst char* QnnBackendManager::QnnProfileErrorToString(QnnProfile_Error_t error) {\r\n  switch (error) {\r\n    case QNN_PROFILE_NO_ERROR:\r\n      return \"QNN_PROFILE_NO_ERROR\";\r\n    case QNN_PROFILE_ERROR_UNSUPPORTED:\r\n      return \"QNN_PROFILE_ERROR_UNSUPPORTED\";\r\n    case QNN_PROFILE_ERROR_INVALID_ARGUMENT:\r\n      return \"QNN_PROFILE_ERROR_INVALID_ARGUMENT\";\r\n    case QNN_PROFILE_ERROR_MEM_ALLOC:\r\n      return \"QNN_PROFILE_ERROR_MEM_ALLOC\";\r\n    case QNN_PROFILE_ERROR_INVALID_HANDLE:\r\n      return \"QNN_PROFILE_ERROR_INVALID_HANDLE\";\r\n    case QNN_PROFILE_ERROR_HANDLE_IN_USE:\r\n      return \"QNN_PROFILE_ERROR_HANDLE_IN_USE\";\r\n    case QNN_PROFILE_ERROR_INCOMPATIBLE_EVENT:\r\n      return \"QNN_PROFILE_ERROR_INCOMPATIBLE_EVENT\";\r\n    default:\r\n      return \"UNKNOWN_ERROR\";\r\n  }\r\n}\r\n\r\nstd::string QnnBackendManager::QnnErrorHandleToString(Qnn_ErrorHandle_t error) {\r\n  return utils::GetQnnErrorMessage(qnn_interface_, error);\r\n}\r\n\r\nQnnBackendManager::~QnnBackendManager() {\r\n  ReleaseResources();\r\n}\r\n\r\nvoid* QnnBackendManager::LoadLib(const char* file_name, int flags, std::string& error_msg) {\r\n#ifdef _WIN32\r\n  DWORD as_is, to_be;\r\n  bool loaded_before = false;\r\n\r\n  if (!file_name || ::strlen(file_name) == 0) {\r\n    error_msg = \"filename is null or empty\";\r\n    return nullptr;\r\n  }\r\n\r\n  // POSIX asks one of symbol resolving approaches:\r\n  // NOW or LAZY must be specified\r\n  if (!(flags & static_cast<int>(DlOpenFlag::DL_NOW))) {\r\n    error_msg = \"flags must include DL_NOW\";\r\n    return nullptr;\r\n  }\r\n\r\n  HANDLE cur_proc = GetCurrentProcess();\r\n\r\n  if (EnumProcessModules(cur_proc, nullptr, 0, &as_is) == 0) {\r\n    error_msg = \"enumerate modules failed before loading module\";\r\n    return nullptr;\r\n  }\r\n\r\n  HMODULE mod;\r\n  auto file_path = std::filesystem::path(file_name);\r\n  if (!file_path.is_absolute()) {\r\n    // construct an absolute path from ORT runtime path + file_name and check whether it exists.\r\n    const Env& env = GetDefaultEnv();\r\n    auto pathstring = env.GetRuntimePath() + ToPathString(file_name);\r\n    auto absolute_path = pathstring.c_str();\r\n    if (std::filesystem::exists(std::filesystem::path(absolute_path))) {\r\n      // load library from absolute path and search for dependencies there.\r\n      mod = LoadLibraryExW(absolute_path, nullptr, LOAD_WITH_ALTERED_SEARCH_PATH);\r\n    } else {\r\n      // use default dll search order for file_name.\r\n      mod = LoadLibraryExA(file_name, nullptr, 0);\r\n    }\r\n  } else {\r\n    // file_name represents an absolute path.\r\n    // load library from absolute path and search for dependencies there.\r\n    mod = LoadLibraryExA(file_name, nullptr, LOAD_WITH_ALTERED_SEARCH_PATH);\r\n  }\r\n  if (!mod) {\r\n    error_msg = \"load library failed\";\r\n    return nullptr;\r\n  }\r\n\r\n  if (EnumProcessModules(cur_proc, nullptr, 0, &to_be) == 0) {\r\n    error_msg = \"enumerate modules failed after loading module\";\r\n    FreeLibrary(mod);\r\n    return nullptr;\r\n  }\r\n\r\n  if (as_is == to_be) {\r\n    loaded_before = true;\r\n  }\r\n\r\n  // (not loaded_before) and DL_LOCAL means this lib was not loaded yet\r\n  // add it into the local set\r\n  //\r\n  // If loaded_before and DL_LOCAL, means this lib was already loaded\r\n  // 2 cases here for how it was loaded before:\r\n  // a. with DL_LOCAL, just ignore since it was already in local set\r\n  // b. with DL_GLOBAL, POSIX asks it in global, ignore it, too\r\n  if ((!loaded_before) && (flags & static_cast<int>(DlOpenFlag::DL_LOCAL))) {\r\n    mod_handles_.insert(mod);\r\n  }\r\n\r\n  // once callers ask for global, needs to be in global thereafter\r\n  // so the lib should be removed from local set\r\n  if (flags & static_cast<int>(DlOpenFlag::DL_GLOBAL)) {\r\n    mod_handles_.erase(mod);\r\n  }\r\n\r\n  return static_cast<void*>(mod);\r\n#else\r\n  ORT_UNUSED_PARAMETER(error_msg);\r\n  int real_flags = 0;\r\n\r\n  if (flags & static_cast<int>(DlOpenFlag::DL_NOW)) {\r\n    real_flags |= RTLD_NOW;\r\n  }\r\n\r\n  if (flags & static_cast<int>(DlOpenFlag::DL_LOCAL)) {\r\n    real_flags |= RTLD_LOCAL;\r\n  }\r\n\r\n  if (flags & static_cast<int>(DlOpenFlag::DL_GLOBAL)) {\r\n    real_flags |= RTLD_GLOBAL;\r\n  }\r\n\r\n  return ::dlopen(file_name, real_flags);\r\n#endif\r\n}\r\n\r\nStatus QnnBackendManager::UnloadLib(void* handle) {\r\n  if (!handle) {\r\n    return Status::OK();\r\n  }\r\n\r\n#ifdef _WIN32\r\n  HMODULE mod = static_cast<HMODULE>(handle);\r\n\r\n  if (FreeLibrary(mod) == 0) {\r\n    return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, \"Failed to free library.\");\r\n  }\r\n  mod_handles_.erase(mod);\r\n#else\r\n  auto rt = ::dlclose(handle);\r\n  if (rt != 0) {\r\n    return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, \"Failed to free library.\");\r\n  }\r\n#endif  // defined(_WIN32)\r\n\r\n  return Status::OK();\r\n}\r\n\r\nvoid* QnnBackendManager::LibFunction(void* handle, const char* symbol, std::string& error_msg) {\r\n#ifdef _WIN32\r\n  FARPROC sym_addr = nullptr;\r\n  DWORD size, size_needed;\r\n  HMODULE mod = 0;\r\n\r\n  if ((!handle) || (!symbol)) {\r\n    return nullptr;\r\n  }\r\n\r\n  HANDLE cur_proc = GetCurrentProcess();\r\n\r\n  if (EnumProcessModules(cur_proc, nullptr, 0, &size) == 0) {\r\n    error_msg = \"enumerate modules failed before memory allocation\";\r\n    return nullptr;\r\n  }\r\n\r\n  HMODULE* mod_list = static_cast<HMODULE*>(malloc(size));\r\n  if (!mod_list) {\r\n    error_msg = \"malloc failed\";\r\n    return nullptr;\r\n  }\r\n\r\n  if (EnumProcessModules(cur_proc, mod_list, size, &size_needed) == 0) {\r\n    error_msg = \"enumerate modules failed after memory allocation\";\r\n    free(mod_list);\r\n    return nullptr;\r\n  }\r\n\r\n  // DL_DEFAULT needs to bypass those modules with DL_LOCAL flag\r\n  if (handle == DL_DEFAULT) {\r\n    for (size_t i = 0; i < (size / sizeof(HMODULE)); i++) {\r\n      auto iter = mod_handles_.find(mod_list[i]);\r\n      if (iter != mod_handles_.end()) {\r\n        continue;\r\n      }\r\n      // once find the first non-local module with symbol\r\n      // return its address here to avoid unnecessary looping\r\n      sym_addr = GetProcAddress(mod_list[i], symbol);\r\n      if (sym_addr) {\r\n        free(mod_list);\r\n        return *(void**)(&sym_addr);\r\n      }\r\n    }\r\n  } else {\r\n    mod = static_cast<HMODULE>(handle);\r\n  }\r\n\r\n  free(mod_list);\r\n  sym_addr = GetProcAddress(mod, symbol);\r\n  if (!sym_addr) {\r\n    error_msg = \"can't resolve symbol\";\r\n    return NULL;\r\n  }\r\n\r\n  return *(void**)(&sym_addr);\r\n#else\r\n  ORT_UNUSED_PARAMETER(error_msg);\r\n  if (handle == DL_DEFAULT) {\r\n    return ::dlsym(RTLD_DEFAULT, symbol);\r\n  }\r\n\r\n  return ::dlsym(handle, symbol);\r\n#endif\r\n}\r\n\r\nStatus QnnBackendManager::AddQnnContextHandle(Qnn_ContextHandle_t raw_context_handle) {\r\n  ORT_RETURN_IF(logger_ == nullptr, \"logger_ should be set.\");\r\n\r\n  auto free_context_handle = [this, &logger = *logger_](Qnn_ContextHandle_t raw_context_handle) {\r\n    const auto free_result = qnn_interface_.contextFree(raw_context_handle, nullptr);\r\n    if (free_result != QNN_CONTEXT_NO_ERROR) {\r\n      LOGS(logger, ERROR) << \"qnn_interface.contextFree() failed: \"\r\n                          << utils::GetVerboseQnnErrorMessage(qnn_interface_, free_result);\r\n    }\r\n  };\r\n\r\n  // take ownership of `raw_context_handle`\r\n  auto context_handle = UniqueQnnContextHandle(raw_context_handle, free_context_handle);\r\n  auto mem_handle_manager = std::make_unique<QnnContextMemHandleManager>(GetQnnInterface(), raw_context_handle,\r\n                                                                         *logger_);\r\n\r\n  auto context_handle_record = std::make_shared<QnnContextHandleRecord>();\r\n  context_handle_record->context_handle = std::move(context_handle);\r\n  context_handle_record->mem_handles = std::move(mem_handle_manager);\r\n\r\n  const bool inserted = context_map_.try_emplace(raw_context_handle, std::move(context_handle_record)).second;\r\n  ORT_RETURN_IF_NOT(inserted, \"QNN context was already added: \", raw_context_handle);\r\n\r\n  contexts_.push_back(raw_context_handle);\r\n\r\n  return Status::OK();\r\n}\r\n\r\nStatus QnnBackendManager::GetOrRegisterContextMemHandle(Qnn_ContextHandle_t context_handle,\r\n                                                        void* shared_memory_address,\r\n                                                        const Qnn_Tensor_t& qnn_tensor,\r\n                                                        Qnn_MemHandle_t& mem_handle) {\r\n  // Multi-threading situations to consider:\r\n  // 1) Shared memory allocation is being freed in another thread while we are processing `shared_memory_address`.\r\n  //    This implies incorrect usage as the memory is being freed while it is still in use. Let's assume this won't\r\n  //    happen.\r\n  // 2) The shared memory allocation clean up function is being run from another thread while the\r\n  //    QnnContextHandleRecord or QnnBackendManager objects are being destroyed.\r\n  //    Usage of weak_ptrs from the clean up function should ensure that those objects are only accessed while they are\r\n  //    in scope.\r\n\r\n  const auto context_handle_record_it = context_map_.find(context_handle);\r\n  ORT_RETURN_IF_NOT(context_handle_record_it != context_map_.end(), \"QNN context not found: \", context_handle);\r\n\r\n  auto& context_handle_record = context_handle_record_it->second;\r\n  auto& context_mem_handle_manager = context_handle_record->mem_handles;\r\n\r\n  bool did_register{};\r\n  ORT_RETURN_IF_ERROR(context_mem_handle_manager->GetOrRegister(shared_memory_address, qnn_tensor,\r\n                                                                mem_handle, did_register));\r\n\r\n  if (did_register) {\r\n    HtpSharedMemoryAllocator::AllocationCleanUpFn unregister_mem_handle =\r\n        [&logger = *logger_,\r\n         shared_memory_address,\r\n         weak_backend_manager = weak_from_this(),\r\n         weak_context_handle_record = std::weak_ptr{context_handle_record}](\r\n            void* /* allocation_base_address */) {\r\n          // Lock QnnBackendManager shared_ptr to ensure that QNN interface is still valid.\r\n          auto backend_manager = weak_backend_manager.lock();\r\n          if (!backend_manager) {\r\n            return;\r\n          }\r\n\r\n          // Lock QnnContextHandleRecord shared_ptr to ensure that QNN context handle is still valid.\r\n          auto context_handle_record = weak_context_handle_record.lock();\r\n          if (!context_handle_record) {\r\n            return;\r\n          }\r\n\r\n          auto& context_mem_handle_manager = context_handle_record->mem_handles;\r\n\r\n          auto unregister_status = context_mem_handle_manager->Unregister(shared_memory_address);\r\n          if (!unregister_status.IsOK()) {\r\n            LOGS(logger, ERROR) << \"Failed to unregister shared memory mem handle for address: \"\r\n                                << shared_memory_address << \", error: \" << unregister_status.ErrorMessage();\r\n          }\r\n        };\r\n\r\n    ORT_RETURN_IF_ERROR(HtpSharedMemoryAllocator::AddAllocationCleanUp(shared_memory_address,\r\n                                                                       std::move(unregister_mem_handle)));\r\n  }\r\n\r\n  return Status::OK();\r\n}\r\n\r\n}  // namespace qnn\r\n}  // namespace onnxruntime\r\n"}
